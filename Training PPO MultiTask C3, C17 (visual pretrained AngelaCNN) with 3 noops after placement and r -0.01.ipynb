{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Conv3d(7, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(visual_features_dim + target_features_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            #nn.ELU(),\n",
    "            #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            visual_features = self.visual_encoder(pov)\n",
    "            \n",
    "        target_features = self.target_encoder(target)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C3', 'C17', 'C20', 'C22', 'C32', 'C40', 'C85', 'C87', 'C93']))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-10-24 08:07:35,318\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-24 08:07:35,332\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id f2834_00000 but id 722d3_00000 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=43204)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43204)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO MultiTask (C3, C17) pretrained (AngelaCNN) (3 noops after placement) r: -0.01</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/722d3_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/722d3_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211024_080735-722d3_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43204)\u001b[0m 2021-10-24 08:07:38,823\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=43204)\u001b[0m 2021-10-24 08:07:38,824\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=43204)\u001b[0m 2021-10-24 08:07:45,004\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-08-42\n",
      "  done: false\n",
      "  episode_len_mean: 411.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.069999999999958\n",
      "  episode_reward_mean: -5.549999999999958\n",
      "  episode_reward_min: -7.0299999999999585\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8744514915678234\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011118928727669659\n",
      "          policy_loss: 0.010810027685430315\n",
      "          total_loss: 0.1933779288911157\n",
      "          vf_explained_var: -0.34938526153564453\n",
      "          vf_loss: 0.2090886280949538\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.267469879518075\n",
      "    ram_util_percent: 35.606024096385546\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040937494207452706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 54.82594807307561\n",
      "    mean_inference_ms: 2.028094185934915\n",
      "    mean_raw_obs_processing_ms: 0.21165448587972088\n",
      "  time_since_restore: 57.94627523422241\n",
      "  time_this_iter_s: 57.94627523422241\n",
      "  time_total_s: 57.94627523422241\n",
      "  timers:\n",
      "    learn_throughput: 1456.903\n",
      "    learn_time_ms: 686.388\n",
      "    load_throughput: 38567.603\n",
      "    load_time_ms: 25.928\n",
      "    sample_throughput: 17.474\n",
      "    sample_time_ms: 57227.965\n",
      "    update_time_ms: 2.309\n",
      "  timestamp: 1635062922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         57.9463</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">   -5.55</td><td style=\"text-align: right;\">               -4.07</td><td style=\"text-align: right;\">               -7.03</td><td style=\"text-align: right;\">               411</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-09-03\n",
      "  done: false\n",
      "  episode_len_mean: 413.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.069999999999958\n",
      "  episode_reward_mean: -4.849999999999957\n",
      "  episode_reward_min: -7.0299999999999585\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 4\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.861661916308933\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010620355901958254\n",
      "          policy_loss: -0.06807942920260959\n",
      "          total_loss: -0.07977767321798536\n",
      "          vf_explained_var: -0.28024330735206604\n",
      "          vf_loss: 0.014794305949989293\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.45666666666668\n",
      "    ram_util_percent: 37.60333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040458018138985086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.6174655717646\n",
      "    mean_inference_ms: 2.006688608665907\n",
      "    mean_raw_obs_processing_ms: 0.20716764977806731\n",
      "  time_since_restore: 78.85448861122131\n",
      "  time_this_iter_s: 20.9082133769989\n",
      "  time_total_s: 78.85448861122131\n",
      "  timers:\n",
      "    learn_throughput: 1457.078\n",
      "    learn_time_ms: 686.305\n",
      "    load_throughput: 37628.719\n",
      "    load_time_ms: 26.575\n",
      "    sample_throughput: 25.834\n",
      "    sample_time_ms: 38708.696\n",
      "    update_time_ms: 2.281\n",
      "  timestamp: 1635062943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         78.8545</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">   -4.85</td><td style=\"text-align: right;\">               -4.07</td><td style=\"text-align: right;\">               -7.03</td><td style=\"text-align: right;\">               413</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-09-24\n",
      "  done: false\n",
      "  episode_len_mean: 410.85714285714283\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.0299999999999585\n",
      "  episode_reward_mean: -4.519999999999958\n",
      "  episode_reward_min: -7.0299999999999585\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 7\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8465657393137613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0083489802030968\n",
      "          policy_loss: -0.08325904938909742\n",
      "          total_loss: -0.10181963907347785\n",
      "          vf_explained_var: -0.20599365234375\n",
      "          vf_loss: 0.00823527249869787\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.27\n",
      "    ram_util_percent: 37.60666666666666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040164609892830834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.008643639803765\n",
      "    mean_inference_ms: 1.9924891518261127\n",
      "    mean_raw_obs_processing_ms: 0.20908831485496313\n",
      "  time_since_restore: 99.64188957214355\n",
      "  time_this_iter_s: 20.78740096092224\n",
      "  time_total_s: 99.64188957214355\n",
      "  timers:\n",
      "    learn_throughput: 1427.97\n",
      "    learn_time_ms: 700.295\n",
      "    load_throughput: 39624.48\n",
      "    load_time_ms: 25.237\n",
      "    sample_throughput: 30.786\n",
      "    sample_time_ms: 32482.787\n",
      "    update_time_ms: 2.371\n",
      "  timestamp: 1635062964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         99.6419</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">   -4.52</td><td style=\"text-align: right;\">               -4.03</td><td style=\"text-align: right;\">               -7.03</td><td style=\"text-align: right;\">           410.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-09-44\n",
      "  done: false\n",
      "  episode_len_mean: 411.1111111111111\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -5.076666666666624\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 9\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.859794529279073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013833740618012768\n",
      "          policy_loss: 0.0337354924943712\n",
      "          total_loss: 0.11869491048985058\n",
      "          vf_explained_var: 0.4513717293739319\n",
      "          vf_loss: 0.11079061517698897\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.88571428571429\n",
      "    ram_util_percent: 37.957142857142856\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04010123555678483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.29475308260765\n",
      "    mean_inference_ms: 1.9888150071354933\n",
      "    mean_raw_obs_processing_ms: 0.20915519042415864\n",
      "  time_since_restore: 119.21390509605408\n",
      "  time_this_iter_s: 19.572015523910522\n",
      "  time_total_s: 119.21390509605408\n",
      "  timers:\n",
      "    learn_throughput: 1436.351\n",
      "    learn_time_ms: 696.209\n",
      "    load_throughput: 40211.146\n",
      "    load_time_ms: 24.869\n",
      "    sample_throughput: 34.392\n",
      "    sample_time_ms: 29076.8\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1635062984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         119.214</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-5.07667</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           411.111</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-10-05\n",
      "  done: false\n",
      "  episode_len_mean: 412.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.846666666666624\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 12\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.837435743543837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008711702867448039\n",
      "          policy_loss: 0.0065767794847488405\n",
      "          total_loss: -0.0132725457350413\n",
      "          vf_explained_var: -0.11718424409627914\n",
      "          vf_loss: 0.006782690314058628\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.57666666666667\n",
      "    ram_util_percent: 37.99666666666668\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04001107104359014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.47721455961774\n",
      "    mean_inference_ms: 1.9842296758851952\n",
      "    mean_raw_obs_processing_ms: 0.20992594463456238\n",
      "  time_since_restore: 140.14229559898376\n",
      "  time_this_iter_s: 20.928390502929688\n",
      "  time_total_s: 140.14229559898376\n",
      "  timers:\n",
      "    learn_throughput: 1432.076\n",
      "    learn_time_ms: 698.287\n",
      "    load_throughput: 40138.724\n",
      "    load_time_ms: 24.914\n",
      "    sample_throughput: 36.63\n",
      "    sample_time_ms: 27299.67\n",
      "    update_time_ms: 2.341\n",
      "  timestamp: 1635063005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         140.142</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">-4.84667</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">            412.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-10-24\n",
      "  done: false\n",
      "  episode_len_mean: 416.07142857142856\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.781428571428528\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 14\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.823223026593526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009976829696551092\n",
      "          policy_loss: 0.049318941434224446\n",
      "          total_loss: 0.028417245464192498\n",
      "          vf_explained_var: 0.26819100975990295\n",
      "          vf_loss: 0.00533516977868405\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.61071428571428\n",
      "    ram_util_percent: 38.160714285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03999225392503171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.0679623054158\n",
      "    mean_inference_ms: 1.9821148056292428\n",
      "    mean_raw_obs_processing_ms: 0.20975090151912737\n",
      "  time_since_restore: 159.69931960105896\n",
      "  time_this_iter_s: 19.557024002075195\n",
      "  time_total_s: 159.69931960105896\n",
      "  timers:\n",
      "    learn_throughput: 1418.123\n",
      "    learn_time_ms: 705.158\n",
      "    load_throughput: 41498.385\n",
      "    load_time_ms: 24.097\n",
      "    sample_throughput: 38.637\n",
      "    sample_time_ms: 25881.738\n",
      "    update_time_ms: 2.344\n",
      "  timestamp: 1635063024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         159.699</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-4.78143</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           416.071</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 419.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.735624999999956\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 16\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8353933016459147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010141403166554635\n",
      "          policy_loss: -0.0946184415784147\n",
      "          total_loss: -0.11425459103451835\n",
      "          vf_explained_var: 0.24659539759159088\n",
      "          vf_loss: 0.006689502454052369\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.662962962962965\n",
      "    ram_util_percent: 38.144444444444446\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03997653998363787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.878944183879312\n",
      "    mean_inference_ms: 1.980240967066453\n",
      "    mean_raw_obs_processing_ms: 0.2093198594082541\n",
      "  time_since_restore: 178.92274141311646\n",
      "  time_this_iter_s: 19.223421812057495\n",
      "  time_total_s: 178.92274141311646\n",
      "  timers:\n",
      "    learn_throughput: 1405.441\n",
      "    learn_time_ms: 711.521\n",
      "    load_throughput: 40657.282\n",
      "    load_time_ms: 24.596\n",
      "    sample_throughput: 40.292\n",
      "    sample_time_ms: 24818.73\n",
      "    update_time_ms: 2.34\n",
      "  timestamp: 1635063044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         178.923</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-4.73562</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">            419.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-11-03\n",
      "  done: false\n",
      "  episode_len_mean: 420.05263157894734\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.657894736842061\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 19\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.839690974023607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00967025522509629\n",
      "          policy_loss: -0.021223402188883887\n",
      "          total_loss: -0.03841776235236062\n",
      "          vf_explained_var: -0.052891794592142105\n",
      "          vf_loss: 0.009268496644734923\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.69642857142858\n",
      "    ram_util_percent: 37.99285714285714\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03995698693010841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.45008214855243\n",
      "    mean_inference_ms: 1.9779189555521424\n",
      "    mean_raw_obs_processing_ms: 0.20929182705251576\n",
      "  time_since_restore: 198.66353034973145\n",
      "  time_this_iter_s: 19.74078893661499\n",
      "  time_total_s: 198.66353034973145\n",
      "  timers:\n",
      "    learn_throughput: 1409.017\n",
      "    learn_time_ms: 709.715\n",
      "    load_throughput: 40680.273\n",
      "    load_time_ms: 24.582\n",
      "    sample_throughput: 41.506\n",
      "    sample_time_ms: 24093.104\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1635063063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         198.664</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-4.65789</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           420.053</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-11-22\n",
      "  done: false\n",
      "  episode_len_mean: 422.04761904761904\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.634285714285669\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 21\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8076964537302653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011854977983411865\n",
      "          policy_loss: 0.07734611423479186\n",
      "          total_loss: 0.0551722440454695\n",
      "          vf_explained_var: 0.17189611494541168\n",
      "          vf_loss: 0.003532092437510275\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.781481481481485\n",
      "    ram_util_percent: 38.025925925925925\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03993756909276126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.659194623803995\n",
      "    mean_inference_ms: 1.9764918989315976\n",
      "    mean_raw_obs_processing_ms: 0.2090714802086521\n",
      "  time_since_restore: 217.30942273139954\n",
      "  time_this_iter_s: 18.64589238166809\n",
      "  time_total_s: 217.30942273139954\n",
      "  timers:\n",
      "    learn_throughput: 1405.264\n",
      "    learn_time_ms: 711.61\n",
      "    load_throughput: 40423.26\n",
      "    load_time_ms: 24.738\n",
      "    sample_throughput: 42.728\n",
      "    sample_time_ms: 23403.599\n",
      "    update_time_ms: 2.339\n",
      "  timestamp: 1635063082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         217.309</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-4.63429</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           422.048</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-11-39\n",
      "  done: false\n",
      "  episode_len_mean: 424.2608695652174\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.62043478260865\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 23\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7861166662640042\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011098304905285733\n",
      "          policy_loss: -0.039184086190329655\n",
      "          total_loss: -0.058166556888156466\n",
      "          vf_explained_var: 0.5752285718917847\n",
      "          vf_loss: 0.006659034571688001\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.1625\n",
      "    ram_util_percent: 37.96666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03991543037192503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.946027054746388\n",
      "    mean_inference_ms: 1.9750731977719418\n",
      "    mean_raw_obs_processing_ms: 0.20872660932636233\n",
      "  time_since_restore: 234.49252462387085\n",
      "  time_this_iter_s: 17.183101892471313\n",
      "  time_total_s: 234.49252462387085\n",
      "  timers:\n",
      "    learn_throughput: 1409.297\n",
      "    learn_time_ms: 709.573\n",
      "    load_throughput: 41618.871\n",
      "    load_time_ms: 24.028\n",
      "    sample_throughput: 44.033\n",
      "    sample_time_ms: 22710.097\n",
      "    update_time_ms: 2.343\n",
      "  timestamp: 1635063099\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         234.493</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">-4.62043</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           424.261</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-11-56\n",
      "  done: false\n",
      "  episode_len_mean: 426.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.614399999999954\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 25\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.755362383524577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009791346183630978\n",
      "          policy_loss: -0.03095960517724355\n",
      "          total_loss: -0.04901136772500144\n",
      "          vf_explained_var: 0.634289026260376\n",
      "          vf_loss: 0.007543595983750291\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.632\n",
      "    ram_util_percent: 38.048\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03989880784952248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.29998521481251\n",
      "    mean_inference_ms: 1.9738308939658105\n",
      "    mean_raw_obs_processing_ms: 0.208312222033556\n",
      "  time_since_restore: 251.4379460811615\n",
      "  time_this_iter_s: 16.94542145729065\n",
      "  time_total_s: 251.4379460811615\n",
      "  timers:\n",
      "    learn_throughput: 1405.271\n",
      "    learn_time_ms: 711.606\n",
      "    load_throughput: 43704.871\n",
      "    load_time_ms: 22.881\n",
      "    sample_throughput: 53.737\n",
      "    sample_time_ms: 18609.162\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1635063116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         251.438</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\"> -4.6144</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">            426.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-12-13\n",
      "  done: false\n",
      "  episode_len_mean: 427.7857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.588214285714239\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 28\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.736412432458666\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01204689264868616\n",
      "          policy_loss: -0.008875059253639645\n",
      "          total_loss: -0.02515063981215159\n",
      "          vf_explained_var: 0.27871230244636536\n",
      "          vf_loss: 0.00867916217733485\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.520833333333336\n",
      "    ram_util_percent: 38.15\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039872792377094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.451931567753807\n",
      "    mean_inference_ms: 1.9720174064251472\n",
      "    mean_raw_obs_processing_ms: 0.20803991715293257\n",
      "  time_since_restore: 268.4497833251953\n",
      "  time_this_iter_s: 17.011837244033813\n",
      "  time_total_s: 268.4497833251953\n",
      "  timers:\n",
      "    learn_throughput: 1400.923\n",
      "    learn_time_ms: 713.815\n",
      "    load_throughput: 45895.114\n",
      "    load_time_ms: 21.789\n",
      "    sample_throughput: 54.89\n",
      "    sample_time_ms: 18218.397\n",
      "    update_time_ms: 2.364\n",
      "  timestamp: 1635063133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">          268.45</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-4.58821</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           427.786</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-12-47\n",
      "  done: false\n",
      "  episode_len_mean: 428.96666666666664\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.579333333333286\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 30\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.703014678425259\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010390502215191728\n",
      "          policy_loss: 0.11042388992177116\n",
      "          total_loss: 0.090174510412746\n",
      "          vf_explained_var: 0.6712636947631836\n",
      "          vf_loss: 0.004702663324941467\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.7125\n",
      "    ram_util_percent: 37.81041666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0398584428809376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.950515775803236\n",
      "    mean_inference_ms: 1.9708979795289452\n",
      "    mean_raw_obs_processing_ms: 0.2955607800550582\n",
      "  time_since_restore: 301.9728899002075\n",
      "  time_this_iter_s: 33.52310657501221\n",
      "  time_total_s: 301.9728899002075\n",
      "  timers:\n",
      "    learn_throughput: 1408.391\n",
      "    learn_time_ms: 710.03\n",
      "    load_throughput: 45312.965\n",
      "    load_time_ms: 22.069\n",
      "    sample_throughput: 51.294\n",
      "    sample_time_ms: 19495.459\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1635063167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         301.973</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-4.57933</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           428.967</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-13-06\n",
      "  done: false\n",
      "  episode_len_mean: 429.09375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.562499999999954\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 32\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.732137902577718\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011573955213368277\n",
      "          policy_loss: -0.08442385378811094\n",
      "          total_loss: -0.1006220183438725\n",
      "          vf_explained_var: 0.38129377365112305\n",
      "          vf_loss: 0.008808421273069042\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.22962962962962\n",
      "    ram_util_percent: 37.24074074074074\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03984631830154875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.50088216402366\n",
      "    mean_inference_ms: 1.9698389035734123\n",
      "    mean_raw_obs_processing_ms: 0.36619387167648454\n",
      "  time_since_restore: 321.3956916332245\n",
      "  time_this_iter_s: 19.422801733016968\n",
      "  time_total_s: 321.3956916332245\n",
      "  timers:\n",
      "    learn_throughput: 1405.91\n",
      "    learn_time_ms: 711.283\n",
      "    load_throughput: 45279.31\n",
      "    load_time_ms: 22.085\n",
      "    sample_throughput: 51.337\n",
      "    sample_time_ms: 19479.227\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1635063186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         321.396</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\"> -4.5625</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           429.094</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 427.51428571428573\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.523428571428526\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 35\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6919498576058283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008920100553558524\n",
      "          policy_loss: 0.024468512501981524\n",
      "          total_loss: 0.008813465055492188\n",
      "          vf_explained_var: 0.3119675815105438\n",
      "          vf_loss: 0.009480430388551515\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.3962962962963\n",
      "    ram_util_percent: 37.648148148148145\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03982635825993246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.902856829920395\n",
      "    mean_inference_ms: 1.9683913687951096\n",
      "    mean_raw_obs_processing_ms: 0.45013465863755064\n",
      "  time_since_restore: 339.63996744155884\n",
      "  time_this_iter_s: 18.24427580833435\n",
      "  time_total_s: 339.63996744155884\n",
      "  timers:\n",
      "    learn_throughput: 1408.253\n",
      "    learn_time_ms: 710.1\n",
      "    load_throughput: 45586.247\n",
      "    load_time_ms: 21.936\n",
      "    sample_throughput: 52.05\n",
      "    sample_time_ms: 19212.132\n",
      "    update_time_ms: 2.381\n",
      "  timestamp: 1635063204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">          339.64</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-4.52343</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           427.514</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-13-41\n",
      "  done: false\n",
      "  episode_len_mean: 426.6216216216216\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.501081081081035\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 37\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.683370831277635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010233115300331995\n",
      "          policy_loss: 0.05383368730545044\n",
      "          total_loss: 0.03492405100001229\n",
      "          vf_explained_var: 0.2828112840652466\n",
      "          vf_loss: 0.005877447223691787\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.48333333333333\n",
      "    ram_util_percent: 37.666666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03981246423533384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.542751793723134\n",
      "    mean_inference_ms: 1.967455101411334\n",
      "    mean_raw_obs_processing_ms: 0.49462388066612156\n",
      "  time_since_restore: 356.5380792617798\n",
      "  time_this_iter_s: 16.898111820220947\n",
      "  time_total_s: 356.5380792617798\n",
      "  timers:\n",
      "    learn_throughput: 1419.633\n",
      "    learn_time_ms: 704.407\n",
      "    load_throughput: 45600.224\n",
      "    load_time_ms: 21.93\n",
      "    sample_throughput: 52.765\n",
      "    sample_time_ms: 18951.94\n",
      "    update_time_ms: 2.376\n",
      "  timestamp: 1635063221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         356.538</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-4.50108</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           426.622</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-13-58\n",
      "  done: false\n",
      "  episode_len_mean: 426.7435897435897\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9599999999999596\n",
      "  episode_reward_mean: -4.4902564102563645\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 39\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6746112532085844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013353589200841636\n",
      "          policy_loss: -0.09082194599840376\n",
      "          total_loss: -0.10448178781403436\n",
      "          vf_explained_var: 0.2782447040081024\n",
      "          vf_loss: 0.010415550476884366\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.708333333333336\n",
      "    ram_util_percent: 37.645833333333336\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03979712017910651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.207385613789533\n",
      "    mean_inference_ms: 1.9665157588885522\n",
      "    mean_raw_obs_processing_ms: 0.5312673539063366\n",
      "  time_since_restore: 373.6470913887024\n",
      "  time_this_iter_s: 17.109012126922607\n",
      "  time_total_s: 373.6470913887024\n",
      "  timers:\n",
      "    learn_throughput: 1429.269\n",
      "    learn_time_ms: 699.658\n",
      "    load_throughput: 47265.089\n",
      "    load_time_ms: 21.157\n",
      "    sample_throughput: 53.345\n",
      "    sample_time_ms: 18745.998\n",
      "    update_time_ms: 2.376\n",
      "  timestamp: 1635063238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         373.647</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-4.49026</td><td style=\"text-align: right;\">               -3.96</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           426.744</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 425.2857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.459761904761859\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 42\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.679297873708937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012186090876523458\n",
      "          policy_loss: 0.04440627329879337\n",
      "          total_loss: 0.031174172378248637\n",
      "          vf_explained_var: 0.10144650936126709\n",
      "          vf_loss: 0.011123659937745995\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.63076923076923\n",
      "    ram_util_percent: 37.67307692307693\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03977451340498496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.75311210641402\n",
      "    mean_inference_ms: 1.9652052744629653\n",
      "    mean_raw_obs_processing_ms: 0.5757581917809809\n",
      "  time_since_restore: 391.80127143859863\n",
      "  time_this_iter_s: 18.15418004989624\n",
      "  time_total_s: 391.80127143859863\n",
      "  timers:\n",
      "    learn_throughput: 1429.78\n",
      "    learn_time_ms: 699.408\n",
      "    load_throughput: 48771.714\n",
      "    load_time_ms: 20.504\n",
      "    sample_throughput: 53.798\n",
      "    sample_time_ms: 18588.088\n",
      "    update_time_ms: 2.394\n",
      "  timestamp: 1635063257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         391.801</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-4.45976</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           425.286</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-14-33\n",
      "  done: false\n",
      "  episode_len_mean: 425.5681818181818\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.453181818181772\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 44\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6550605376561482\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012156281070587452\n",
      "          policy_loss: -0.11297136495510737\n",
      "          total_loss: -0.12583498888545566\n",
      "          vf_explained_var: 0.37492379546165466\n",
      "          vf_loss: 0.011255726176831458\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.800000000000004\n",
      "    ram_util_percent: 37.695652173913054\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03975829355947181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.474112611552822\n",
      "    mean_inference_ms: 1.9643298936027642\n",
      "    mean_raw_obs_processing_ms: 0.5997345963853018\n",
      "  time_since_restore: 408.1164801120758\n",
      "  time_this_iter_s: 16.315208673477173\n",
      "  time_total_s: 408.1164801120758\n",
      "  timers:\n",
      "    learn_throughput: 1436.915\n",
      "    learn_time_ms: 695.935\n",
      "    load_throughput: 51747.48\n",
      "    load_time_ms: 19.325\n",
      "    sample_throughput: 54.468\n",
      "    sample_time_ms: 18359.568\n",
      "    update_time_ms: 2.489\n",
      "  timestamp: 1635063273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         408.116</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-4.45318</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           425.568</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-14-50\n",
      "  done: false\n",
      "  episode_len_mean: 426.39130434782606\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.452826086956475\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 46\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.671281335088942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009857625127480714\n",
      "          policy_loss: -0.08990887453158697\n",
      "          total_loss: -0.10416888263490465\n",
      "          vf_explained_var: 0.28837883472442627\n",
      "          vf_loss: 0.010481280243160048\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.208\n",
      "    ram_util_percent: 37.732\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03974298259015575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.211533114377847\n",
      "    mean_inference_ms: 1.9634891973007036\n",
      "    mean_raw_obs_processing_ms: 0.6196367859176002\n",
      "  time_since_restore: 425.0802273750305\n",
      "  time_this_iter_s: 16.963747262954712\n",
      "  time_total_s: 425.0802273750305\n",
      "  timers:\n",
      "    learn_throughput: 1429.339\n",
      "    learn_time_ms: 699.624\n",
      "    load_throughput: 52552.364\n",
      "    load_time_ms: 19.029\n",
      "    sample_throughput: 54.547\n",
      "    sample_time_ms: 18332.955\n",
      "    update_time_ms: 2.496\n",
      "  timestamp: 1635063290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">          425.08</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-4.45283</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           426.391</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 426.3061224489796\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.44040816326526\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 49\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.623565181096395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011883695335593922\n",
      "          policy_loss: 0.05453990739252832\n",
      "          total_loss: 0.04195162389013502\n",
      "          vf_explained_var: 0.2568868398666382\n",
      "          vf_loss: 0.011270633825592489\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.737500000000004\n",
      "    ram_util_percent: 37.68333333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03972255264641234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.84745232308577\n",
      "    mean_inference_ms: 1.9623360535553636\n",
      "    mean_raw_obs_processing_ms: 0.6440011905399058\n",
      "  time_since_restore: 441.90078353881836\n",
      "  time_this_iter_s: 16.820556163787842\n",
      "  time_total_s: 441.90078353881836\n",
      "  timers:\n",
      "    learn_throughput: 1431.891\n",
      "    learn_time_ms: 698.377\n",
      "    load_throughput: 52565.997\n",
      "    load_time_ms: 19.024\n",
      "    sample_throughput: 54.58\n",
      "    sample_time_ms: 18321.67\n",
      "    update_time_ms: 2.521\n",
      "  timestamp: 1635063307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         441.901</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-4.44041</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           426.306</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-15-22\n",
      "  done: false\n",
      "  episode_len_mean: 427.72549019607845\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.447647058823483\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 51\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.563354664378696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011002444768424062\n",
      "          policy_loss: 0.06023230022854275\n",
      "          total_loss: 0.043240092529190914\n",
      "          vf_explained_var: 0.2143336981534958\n",
      "          vf_loss: 0.006440854060201673\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.77727272727273\n",
      "    ram_util_percent: 37.690909090909095\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039708738273192046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.62024785523466\n",
      "    mean_inference_ms: 1.9615825317915747\n",
      "    mean_raw_obs_processing_ms: 0.6571706884370381\n",
      "  time_since_restore: 457.3976707458496\n",
      "  time_this_iter_s: 15.49688720703125\n",
      "  time_total_s: 457.3976707458496\n",
      "  timers:\n",
      "    learn_throughput: 1437.0\n",
      "    learn_time_ms: 695.894\n",
      "    load_throughput: 54064.383\n",
      "    load_time_ms: 18.496\n",
      "    sample_throughput: 55.026\n",
      "    sample_time_ms: 18173.196\n",
      "    update_time_ms: 2.506\n",
      "  timestamp: 1635063322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         457.398</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-4.44765</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           427.725</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-15-38\n",
      "  done: false\n",
      "  episode_len_mean: 429.0943396226415\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.454905660377312\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 53\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5675582355923123\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011907603282679228\n",
      "          policy_loss: -0.07467967006895278\n",
      "          total_loss: -0.08574499355422126\n",
      "          vf_explained_var: -0.13716085255146027\n",
      "          vf_loss: 0.012228738061579255\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.78260869565217\n",
      "    ram_util_percent: 37.626086956521746\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039694795791097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.403574930674345\n",
      "    mean_inference_ms: 1.9608499954523906\n",
      "    mean_raw_obs_processing_ms: 0.6680487337109693\n",
      "  time_since_restore: 473.32545256614685\n",
      "  time_this_iter_s: 15.927781820297241\n",
      "  time_total_s: 473.32545256614685\n",
      "  timers:\n",
      "    learn_throughput: 1437.076\n",
      "    learn_time_ms: 695.857\n",
      "    load_throughput: 58506.695\n",
      "    load_time_ms: 17.092\n",
      "    sample_throughput: 60.919\n",
      "    sample_time_ms: 16415.111\n",
      "    update_time_ms: 2.519\n",
      "  timestamp: 1635063338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         473.325</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-4.45491</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           429.094</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 430.6363636363636\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.46436363636359\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 55\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.520619861284892\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009983726878339599\n",
      "          policy_loss: -0.0759664461016655\n",
      "          total_loss: -0.08699069635735618\n",
      "          vf_explained_var: 0.343707412481308\n",
      "          vf_loss: 0.012185199071730797\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03333333333334\n",
      "    ram_util_percent: 37.58571428571429\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03968118537731857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.195682762698862\n",
      "    mean_inference_ms: 1.9601354143375729\n",
      "    mean_raw_obs_processing_ms: 0.6769819564342029\n",
      "  time_since_restore: 488.4812693595886\n",
      "  time_this_iter_s: 15.155816793441772\n",
      "  time_total_s: 488.4812693595886\n",
      "  timers:\n",
      "    learn_throughput: 1440.17\n",
      "    learn_time_ms: 694.362\n",
      "    load_throughput: 62741.456\n",
      "    load_time_ms: 15.938\n",
      "    sample_throughput: 62.535\n",
      "    sample_time_ms: 15991.066\n",
      "    update_time_ms: 2.515\n",
      "  timestamp: 1635063353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         488.481</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-4.46436</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           430.636</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-16-09\n",
      "  done: false\n",
      "  episode_len_mean: 432.2105263157895\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.474561403508725\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 57\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5381327205234103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01114251347515969\n",
      "          policy_loss: -0.10203038983874851\n",
      "          total_loss: -0.11309113684627745\n",
      "          vf_explained_var: 0.42134422063827515\n",
      "          vf_loss: 0.012092076353858122\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.77826086956521\n",
      "    ram_util_percent: 37.56956521739131\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03966849020495046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.997307233476\n",
      "    mean_inference_ms: 1.9594590726021544\n",
      "    mean_raw_obs_processing_ms: 0.6842841283930924\n",
      "  time_since_restore: 504.47986030578613\n",
      "  time_this_iter_s: 15.99859094619751\n",
      "  time_total_s: 504.47986030578613\n",
      "  timers:\n",
      "    learn_throughput: 1440.44\n",
      "    learn_time_ms: 694.233\n",
      "    load_throughput: 66079.4\n",
      "    load_time_ms: 15.133\n",
      "    sample_throughput: 63.422\n",
      "    sample_time_ms: 15767.455\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1635063369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">          504.48</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">-4.47456</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           432.211</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-16-42\n",
      "  done: false\n",
      "  episode_len_mean: 433.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.475833333333285\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 60\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.530299435721503\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009969468588732709\n",
      "          policy_loss: 0.017101978758970897\n",
      "          total_loss: 0.005712219741609362\n",
      "          vf_explained_var: 0.5163366198539734\n",
      "          vf_loss: 0.011919343473790731\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.86382978723404\n",
      "    ram_util_percent: 37.44893617021276\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03964994022563175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.717199291518366\n",
      "    mean_inference_ms: 1.9584847849293818\n",
      "    mean_raw_obs_processing_ms: 0.7257713506728837\n",
      "  time_since_restore: 537.1850490570068\n",
      "  time_this_iter_s: 32.7051887512207\n",
      "  time_total_s: 537.1850490570068\n",
      "  timers:\n",
      "    learn_throughput: 1432.968\n",
      "    learn_time_ms: 697.852\n",
      "    load_throughput: 66557.026\n",
      "    load_time_ms: 15.025\n",
      "    sample_throughput: 57.655\n",
      "    sample_time_ms: 17344.613\n",
      "    update_time_ms: 2.524\n",
      "  timestamp: 1635063402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         537.185</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-4.47583</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">             433.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-17-01\n",
      "  done: false\n",
      "  episode_len_mean: 434.43548387096774\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.484516129032209\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 62\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5289483308792113\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010785509053018514\n",
      "          policy_loss: 0.06586531839436954\n",
      "          total_loss: 0.048698706014288794\n",
      "          vf_explained_var: 0.6415259838104248\n",
      "          vf_loss: 0.005965768612804822\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.21153846153845\n",
      "    ram_util_percent: 37.330769230769235\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03963968527897119\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.54441289721851\n",
      "    mean_inference_ms: 1.95790902088497\n",
      "    mean_raw_obs_processing_ms: 0.7496200249244396\n",
      "  time_since_restore: 555.611494064331\n",
      "  time_this_iter_s: 18.42644500732422\n",
      "  time_total_s: 555.611494064331\n",
      "  timers:\n",
      "    learn_throughput: 1434.503\n",
      "    learn_time_ms: 697.105\n",
      "    load_throughput: 64650.191\n",
      "    load_time_ms: 15.468\n",
      "    sample_throughput: 57.219\n",
      "    sample_time_ms: 17476.566\n",
      "    update_time_ms: 2.619\n",
      "  timestamp: 1635063421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         555.611</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-4.48452</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           434.435</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-17-16\n",
      "  done: false\n",
      "  episode_len_mean: 435.734375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.493124999999951\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 64\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4877820518281726\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009382942761941375\n",
      "          policy_loss: 0.13314620819356707\n",
      "          total_loss: 0.11571755127774344\n",
      "          vf_explained_var: 0.3060520887374878\n",
      "          vf_loss: 0.005572573777640678\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24090909090909\n",
      "    ram_util_percent: 37.45454545454545\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039629389690522224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.37820583267098\n",
      "    mean_inference_ms: 1.957371698101515\n",
      "    mean_raw_obs_processing_ms: 0.7705491923089492\n",
      "  time_since_restore: 571.0990314483643\n",
      "  time_this_iter_s: 15.487537384033203\n",
      "  time_total_s: 571.0990314483643\n",
      "  timers:\n",
      "    learn_throughput: 1436.756\n",
      "    learn_time_ms: 696.013\n",
      "    load_throughput: 67830.581\n",
      "    load_time_ms: 14.743\n",
      "    sample_throughput: 58.099\n",
      "    sample_time_ms: 17211.856\n",
      "    update_time_ms: 2.594\n",
      "  timestamp: 1635063436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         571.099</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-4.49312</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           435.734</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-17-33\n",
      "  done: false\n",
      "  episode_len_mean: 435.7121212121212\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.839999999999962\n",
      "  episode_reward_mean: -4.4887878787878295\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 66\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5270359410179988\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012996312800641065\n",
      "          policy_loss: -0.06688814494344923\n",
      "          total_loss: -0.07676625400781631\n",
      "          vf_explained_var: 0.22202962636947632\n",
      "          vf_loss: 0.012792980916856322\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11249999999999\n",
      "    ram_util_percent: 37.449999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039619332923569485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.219790472043513\n",
      "    mean_inference_ms: 1.956841758173972\n",
      "    mean_raw_obs_processing_ms: 0.7889194891187852\n",
      "  time_since_restore: 588.0444314479828\n",
      "  time_this_iter_s: 16.94539999961853\n",
      "  time_total_s: 588.0444314479828\n",
      "  timers:\n",
      "    learn_throughput: 1435.106\n",
      "    learn_time_ms: 696.812\n",
      "    load_throughput: 66441.367\n",
      "    load_time_ms: 15.051\n",
      "    sample_throughput: 57.891\n",
      "    sample_time_ms: 17273.857\n",
      "    update_time_ms: 2.511\n",
      "  timestamp: 1635063453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         588.044</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-4.48879</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           435.712</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 433.8985507246377\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.799999999999963\n",
      "  episode_reward_mean: -4.464927536231835\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 69\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5135796944300335\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011944662645418753\n",
      "          policy_loss: 0.02727724379963345\n",
      "          total_loss: 0.016026020215617286\n",
      "          vf_explained_var: 0.21930065751075745\n",
      "          vf_loss: 0.011495639601101478\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.70769230769231\n",
      "    ram_util_percent: 37.4\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039604325502139694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.997664463007204\n",
      "    mean_inference_ms: 1.9560710453596373\n",
      "    mean_raw_obs_processing_ms: 0.812810122722525\n",
      "  time_since_restore: 605.9465827941895\n",
      "  time_this_iter_s: 17.902151346206665\n",
      "  time_total_s: 605.9465827941895\n",
      "  timers:\n",
      "    learn_throughput: 1442.373\n",
      "    learn_time_ms: 693.302\n",
      "    load_throughput: 64732.907\n",
      "    load_time_ms: 15.448\n",
      "    sample_throughput: 57.564\n",
      "    sample_time_ms: 17372.089\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1635063471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         605.947</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-4.46493</td><td style=\"text-align: right;\">                -3.8</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           433.899</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-18-09\n",
      "  done: false\n",
      "  episode_len_mean: 433.0422535211268\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.799999999999963\n",
      "  episode_reward_mean: -4.452816901408402\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 71\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.437154931492276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016295060089372378\n",
      "          policy_loss: -0.09422760274675157\n",
      "          total_loss: -0.10228910313712226\n",
      "          vf_explained_var: 0.17173489928245544\n",
      "          vf_loss: 0.01305103307758044\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.661538461538456\n",
      "    ram_util_percent: 37.449999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03959593960882826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.859048070448562\n",
      "    mean_inference_ms: 1.955600448205868\n",
      "    mean_raw_obs_processing_ms: 0.8265739222663187\n",
      "  time_since_restore: 623.987476348877\n",
      "  time_this_iter_s: 18.0408935546875\n",
      "  time_total_s: 623.987476348877\n",
      "  timers:\n",
      "    learn_throughput: 1433.353\n",
      "    learn_time_ms: 697.665\n",
      "    load_throughput: 63008.474\n",
      "    load_time_ms: 15.871\n",
      "    sample_throughput: 57.178\n",
      "    sample_time_ms: 17489.338\n",
      "    update_time_ms: 2.503\n",
      "  timestamp: 1635063489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         623.987</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-4.45282</td><td style=\"text-align: right;\">                -3.8</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           433.042</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-18-29\n",
      "  done: false\n",
      "  episode_len_mean: 431.43243243243245\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.799999999999963\n",
      "  episode_reward_mean: -4.431756756756708\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 74\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.46384694841173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010209836227356502\n",
      "          policy_loss: 0.04388834916883045\n",
      "          total_loss: 0.03522456793321504\n",
      "          vf_explained_var: 0.31172850728034973\n",
      "          vf_loss: 0.013932719443659557\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.84285714285715\n",
      "    ram_util_percent: 37.56785714285714\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03958613652286047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.665715369128343\n",
      "    mean_inference_ms: 1.9549905470057891\n",
      "    mean_raw_obs_processing_ms: 0.8444731923735973\n",
      "  time_since_restore: 643.5229997634888\n",
      "  time_this_iter_s: 19.535523414611816\n",
      "  time_total_s: 643.5229997634888\n",
      "  timers:\n",
      "    learn_throughput: 1429.554\n",
      "    learn_time_ms: 699.519\n",
      "    load_throughput: 58135.529\n",
      "    load_time_ms: 17.201\n",
      "    sample_throughput: 55.897\n",
      "    sample_time_ms: 17889.921\n",
      "    update_time_ms: 2.554\n",
      "  timestamp: 1635063509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         643.523</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-4.43176</td><td style=\"text-align: right;\">                -3.8</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           431.432</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-18-49\n",
      "  done: false\n",
      "  episode_len_mean: 429.69736842105266\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.5299999999999687\n",
      "  episode_reward_mean: -4.411315789473636\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 76\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4499763912624783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01043101857484924\n",
      "          policy_loss: -0.10733234435319901\n",
      "          total_loss: -0.11643342855903838\n",
      "          vf_explained_var: 0.1555488258600235\n",
      "          vf_loss: 0.013312480237800628\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.925\n",
      "    ram_util_percent: 37.66428571428572\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03958091720942441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.54595029208726\n",
      "    mean_inference_ms: 1.9546298931805786\n",
      "    mean_raw_obs_processing_ms: 0.8547669336993626\n",
      "  time_since_restore: 663.445689201355\n",
      "  time_this_iter_s: 19.92268943786621\n",
      "  time_total_s: 663.445689201355\n",
      "  timers:\n",
      "    learn_throughput: 1418.82\n",
      "    learn_time_ms: 704.811\n",
      "    load_throughput: 53298.634\n",
      "    load_time_ms: 18.762\n",
      "    sample_throughput: 54.697\n",
      "    sample_time_ms: 18282.499\n",
      "    update_time_ms: 2.601\n",
      "  timestamp: 1635063529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         663.446</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-4.41132</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           429.697</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-19-08\n",
      "  done: false\n",
      "  episode_len_mean: 427.3544303797468\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.5299999999999687\n",
      "  episode_reward_mean: -4.383544303797421\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 79\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.417565001381768\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013471375473407063\n",
      "          policy_loss: -0.0849065302974648\n",
      "          total_loss: -0.09017350872357686\n",
      "          vf_explained_var: 0.1353253573179245\n",
      "          vf_loss: 0.01621439487983783\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.89642857142856\n",
      "    ram_util_percent: 37.53928571428571\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039573374334374106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.378230518516276\n",
      "    mean_inference_ms: 1.9541072750684194\n",
      "    mean_raw_obs_processing_ms: 0.8680898834937969\n",
      "  time_since_restore: 683.0038065910339\n",
      "  time_this_iter_s: 19.558117389678955\n",
      "  time_total_s: 683.0038065910339\n",
      "  timers:\n",
      "    learn_throughput: 1416.955\n",
      "    learn_time_ms: 705.739\n",
      "    load_throughput: 49719.285\n",
      "    load_time_ms: 20.113\n",
      "    sample_throughput: 53.417\n",
      "    sample_time_ms: 18720.466\n",
      "    update_time_ms: 2.599\n",
      "  timestamp: 1635063548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         683.004</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-4.38354</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           427.354</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-19-26\n",
      "  done: false\n",
      "  episode_len_mean: 425.5487804878049\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.5299999999999687\n",
      "  episode_reward_mean: -4.3614634146341\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 82\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4415612432691787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012099474625682453\n",
      "          policy_loss: 0.04749069834748904\n",
      "          total_loss: 0.038251183513138026\n",
      "          vf_explained_var: 0.30535924434661865\n",
      "          vf_loss: 0.012756204310183724\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18846153846154\n",
      "    ram_util_percent: 37.45384615384616\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03956622605295981\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.22184645156168\n",
      "    mean_inference_ms: 1.953603369686003\n",
      "    mean_raw_obs_processing_ms: 0.8794014497883266\n",
      "  time_since_restore: 701.1861414909363\n",
      "  time_this_iter_s: 18.182334899902344\n",
      "  time_total_s: 701.1861414909363\n",
      "  timers:\n",
      "    learn_throughput: 1415.15\n",
      "    learn_time_ms: 706.639\n",
      "    load_throughput: 47845.282\n",
      "    load_time_ms: 20.901\n",
      "    sample_throughput: 52.806\n",
      "    sample_time_ms: 18937.087\n",
      "    update_time_ms: 2.623\n",
      "  timestamp: 1635063566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         701.186</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-4.36146</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           425.549</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-19-45\n",
      "  done: false\n",
      "  episode_len_mean: 424.0833333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.5299999999999687\n",
      "  episode_reward_mean: -4.344285714285668\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 84\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.402297102080451\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0122483763742256\n",
      "          policy_loss: -0.10054667327139112\n",
      "          total_loss: -0.11010134634044436\n",
      "          vf_explained_var: 0.2945574223995209\n",
      "          vf_loss: 0.012018624146650028\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24444444444444\n",
      "    ram_util_percent: 37.422222222222224\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03956136477641081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.12366009590309\n",
      "    mean_inference_ms: 1.953279174614512\n",
      "    mean_raw_obs_processing_ms: 0.8858409483553499\n",
      "  time_since_restore: 720.020889043808\n",
      "  time_this_iter_s: 18.834747552871704\n",
      "  time_total_s: 720.020889043808\n",
      "  timers:\n",
      "    learn_throughput: 1417.434\n",
      "    learn_time_ms: 705.5\n",
      "    load_throughput: 46848.343\n",
      "    load_time_ms: 21.345\n",
      "    sample_throughput: 56.978\n",
      "    sample_time_ms: 17550.573\n",
      "    update_time_ms: 2.805\n",
      "  timestamp: 1635063585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         720.021</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-4.34429</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           424.083</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-20-05\n",
      "  done: false\n",
      "  episode_len_mean: 421.9655172413793\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.509999999999969\n",
      "  episode_reward_mean: -4.319540229885012\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 87\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3138816462622747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014836582666710384\n",
      "          policy_loss: -0.10346178172363175\n",
      "          total_loss: -0.10832933361331622\n",
      "          vf_explained_var: 0.20402416586875916\n",
      "          vf_loss: 0.015303946472704411\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.282142857142865\n",
      "    ram_util_percent: 37.39999999999999\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039554045239022985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.98546288841958\n",
      "    mean_inference_ms: 1.9528056785837196\n",
      "    mean_raw_obs_processing_ms: 0.8940836439718101\n",
      "  time_since_restore: 739.7476570606232\n",
      "  time_this_iter_s: 19.726768016815186\n",
      "  time_total_s: 739.7476570606232\n",
      "  timers:\n",
      "    learn_throughput: 1415.613\n",
      "    learn_time_ms: 706.408\n",
      "    load_throughput: 46564.419\n",
      "    load_time_ms: 21.476\n",
      "    sample_throughput: 56.562\n",
      "    sample_time_ms: 17679.656\n",
      "    update_time_ms: 2.721\n",
      "  timestamp: 1635063605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         739.748</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-4.31954</td><td style=\"text-align: right;\">               -3.51</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           421.966</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-20-41\n",
      "  done: false\n",
      "  episode_len_mean: 419.3666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2599999999999745\n",
      "  episode_reward_mean: -4.2902222222221775\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 90\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.255222135119968\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009693279864070566\n",
      "          policy_loss: -0.10384100824594497\n",
      "          total_loss: -0.10881879130999247\n",
      "          vf_explained_var: 0.19107858836650848\n",
      "          vf_loss: 0.015635784746458135\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.21568627450981\n",
      "    ram_util_percent: 37.34117647058823\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03954673829340043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.856260253973407\n",
      "    mean_inference_ms: 1.9523560356785046\n",
      "    mean_raw_obs_processing_ms: 0.9158514651905421\n",
      "  time_since_restore: 775.5330731868744\n",
      "  time_this_iter_s: 35.78541612625122\n",
      "  time_total_s: 775.5330731868744\n",
      "  timers:\n",
      "    learn_throughput: 1411.198\n",
      "    learn_time_ms: 708.618\n",
      "    load_throughput: 43580.399\n",
      "    load_time_ms: 22.946\n",
      "    sample_throughput: 50.747\n",
      "    sample_time_ms: 19705.737\n",
      "    update_time_ms: 2.729\n",
      "  timestamp: 1635063641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         775.533</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-4.29022</td><td style=\"text-align: right;\">               -3.26</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           419.367</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-21-03\n",
      "  done: false\n",
      "  episode_len_mean: 417.1505376344086\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2599999999999745\n",
      "  episode_reward_mean: -4.264946236559096\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 93\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.306727584203084\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01072664265548787\n",
      "          policy_loss: 0.013165178563859728\n",
      "          total_loss: 0.0034286708467536503\n",
      "          vf_explained_var: 0.288860559463501\n",
      "          vf_loss: 0.01118543958873488\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.981249999999996\n",
      "    ram_util_percent: 37.165625\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03953998928159071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.73811879320811\n",
      "    mean_inference_ms: 1.9519407623299319\n",
      "    mean_raw_obs_processing_ms: 0.9351154339191611\n",
      "  time_since_restore: 797.9496712684631\n",
      "  time_this_iter_s: 22.416598081588745\n",
      "  time_total_s: 797.9496712684631\n",
      "  timers:\n",
      "    learn_throughput: 1403.141\n",
      "    learn_time_ms: 712.687\n",
      "    load_throughput: 42986.45\n",
      "    load_time_ms: 23.263\n",
      "    sample_throughput: 49.387\n",
      "    sample_time_ms: 20248.443\n",
      "    update_time_ms: 2.736\n",
      "  timestamp: 1635063663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">          797.95</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-4.26495</td><td style=\"text-align: right;\">               -3.26</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           417.151</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-21-25\n",
      "  done: false\n",
      "  episode_len_mean: 414.2604166666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.229999999999975\n",
      "  episode_reward_mean: -4.233124999999956\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 96\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.282320949766371\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006877908231229904\n",
      "          policy_loss: -0.08852307010028097\n",
      "          total_loss: -0.09493836934367815\n",
      "          vf_explained_var: 0.1551164835691452\n",
      "          vf_loss: 0.015032326978527837\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.9774193548387\n",
      "    ram_util_percent: 37.4225806451613\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039534524743134465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.629164099590383\n",
      "    mean_inference_ms: 1.9515779652061276\n",
      "    mean_raw_obs_processing_ms: 0.9521719587389733\n",
      "  time_since_restore: 819.5126202106476\n",
      "  time_this_iter_s: 21.56294894218445\n",
      "  time_total_s: 819.5126202106476\n",
      "  timers:\n",
      "    learn_throughput: 1393.605\n",
      "    learn_time_ms: 717.563\n",
      "    load_throughput: 41233.535\n",
      "    load_time_ms: 24.252\n",
      "    sample_throughput: 48.523\n",
      "    sample_time_ms: 20608.603\n",
      "    update_time_ms: 2.739\n",
      "  timestamp: 1635063685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         819.513</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-4.23312</td><td style=\"text-align: right;\">               -3.23</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">            414.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-21-46\n",
      "  done: false\n",
      "  episode_len_mean: 411.3939393939394\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.201717171717128\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 99\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.225383069780138\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008831454083722868\n",
      "          policy_loss: -0.10611400827765465\n",
      "          total_loss: -0.11243879480494393\n",
      "          vf_explained_var: 0.2681999206542969\n",
      "          vf_loss: 0.014162752259936597\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.406451612903226\n",
      "    ram_util_percent: 37.49677419354838\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0395296533614124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.528613798039224\n",
      "    mean_inference_ms: 1.9512410004980745\n",
      "    mean_raw_obs_processing_ms: 0.9672617627438411\n",
      "  time_since_restore: 841.204746723175\n",
      "  time_this_iter_s: 21.692126512527466\n",
      "  time_total_s: 841.204746723175\n",
      "  timers:\n",
      "    learn_throughput: 1394.285\n",
      "    learn_time_ms: 717.213\n",
      "    load_throughput: 41280.407\n",
      "    load_time_ms: 24.225\n",
      "    sample_throughput: 47.678\n",
      "    sample_time_ms: 20974.098\n",
      "    update_time_ms: 2.742\n",
      "  timestamp: 1635063706\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         841.205</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-4.20172</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">           411.394</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 408.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.146499999999956\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 102\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1703269481658936\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00704059778421888\n",
      "          policy_loss: -0.12398050791687436\n",
      "          total_loss: -0.12973860369788276\n",
      "          vf_explained_var: 0.25312182307243347\n",
      "          vf_loss: 0.014537052220354478\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.013333333333335\n",
      "    ram_util_percent: 37.680000000000014\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039497511490824165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.727542627349944\n",
      "    mean_inference_ms: 1.9494006261408185\n",
      "    mean_raw_obs_processing_ms: 0.9960027971898087\n",
      "  time_since_restore: 862.5184795856476\n",
      "  time_this_iter_s: 21.313732862472534\n",
      "  time_total_s: 862.5184795856476\n",
      "  timers:\n",
      "    learn_throughput: 1393.499\n",
      "    learn_time_ms: 717.618\n",
      "    load_throughput: 41185.515\n",
      "    load_time_ms: 24.28\n",
      "    sample_throughput: 47.278\n",
      "    sample_time_ms: 21151.477\n",
      "    update_time_ms: 2.719\n",
      "  timestamp: 1635063728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         862.518</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\"> -4.1465</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">            408.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-22-29\n",
      "  done: false\n",
      "  episode_len_mean: 406.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.120199999999957\n",
      "  episode_reward_min: -10.089999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 105\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.139013658629523\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011516283211298155\n",
      "          policy_loss: -0.12916147278414833\n",
      "          total_loss: -0.13436100482940674\n",
      "          vf_explained_var: 0.2880769371986389\n",
      "          vf_loss: 0.01388734994042251\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.943333333333335\n",
      "    ram_util_percent: 37.623333333333335\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03948199772871644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.18916641528978\n",
      "    mean_inference_ms: 1.9481890802415245\n",
      "    mean_raw_obs_processing_ms: 1.0316395662206126\n",
      "  time_since_restore: 883.3570756912231\n",
      "  time_this_iter_s: 20.83859610557556\n",
      "  time_total_s: 883.3570756912231\n",
      "  timers:\n",
      "    learn_throughput: 1395.835\n",
      "    learn_time_ms: 716.417\n",
      "    load_throughput: 42084.064\n",
      "    load_time_ms: 23.762\n",
      "    sample_throughput: 47.07\n",
      "    sample_time_ms: 21244.846\n",
      "    update_time_ms: 2.641\n",
      "  timestamp: 1635063749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         883.357</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\"> -4.1202</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">              -10.09</td><td style=\"text-align: right;\">            406.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-22-50\n",
      "  done: false\n",
      "  episode_len_mean: 403.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.038399999999958\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 108\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.167924279636807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012825093253100937\n",
      "          policy_loss: -0.13624530169698928\n",
      "          total_loss: -0.14168901104066106\n",
      "          vf_explained_var: 0.32442471385002136\n",
      "          vf_loss: 0.013670514399806658\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.02\n",
      "    ram_util_percent: 37.66666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039470032700091466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.810048390001793\n",
      "    mean_inference_ms: 1.9471971399774695\n",
      "    mean_raw_obs_processing_ms: 1.0663182994330993\n",
      "  time_since_restore: 904.2647857666016\n",
      "  time_this_iter_s: 20.907710075378418\n",
      "  time_total_s: 904.2647857666016\n",
      "  timers:\n",
      "    learn_throughput: 1384.213\n",
      "    learn_time_ms: 722.432\n",
      "    load_throughput: 42921.654\n",
      "    load_time_ms: 23.298\n",
      "    sample_throughput: 46.786\n",
      "    sample_time_ms: 21374.031\n",
      "    update_time_ms: 2.847\n",
      "  timestamp: 1635063770\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         904.265</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -4.0384</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            403.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-23-11\n",
      "  done: false\n",
      "  episode_len_mean: 400.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.003999999999959\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 112\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1177324453989663\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011636431947971617\n",
      "          policy_loss: -0.019991370870007408\n",
      "          total_loss: -0.02630259816845258\n",
      "          vf_explained_var: 0.38280394673347473\n",
      "          vf_loss: 0.012538810219201777\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.79666666666667\n",
      "    ram_util_percent: 37.769999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03945570388433453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.449964145662545\n",
      "    mean_inference_ms: 1.9459829152775165\n",
      "    mean_raw_obs_processing_ms: 1.111539796892466\n",
      "  time_since_restore: 925.4845173358917\n",
      "  time_this_iter_s: 21.21973156929016\n",
      "  time_total_s: 925.4845173358917\n",
      "  timers:\n",
      "    learn_throughput: 1382.752\n",
      "    learn_time_ms: 723.196\n",
      "    load_throughput: 42705.2\n",
      "    load_time_ms: 23.416\n",
      "    sample_throughput: 46.132\n",
      "    sample_time_ms: 21676.927\n",
      "    update_time_ms: 2.827\n",
      "  timestamp: 1635063791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         925.485</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">  -4.004</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">             400.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 397.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -3.9709999999999592\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 115\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0983935409122045\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010292460598659956\n",
      "          policy_loss: -0.008295932743284438\n",
      "          total_loss: -0.01687710169288847\n",
      "          vf_explained_var: 0.5496423244476318\n",
      "          vf_loss: 0.010344273077660344\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.553333333333335\n",
      "    ram_util_percent: 37.84\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039442013237826756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.247176847905195\n",
      "    mean_inference_ms: 1.9451738591739502\n",
      "    mean_raw_obs_processing_ms: 1.1448332657522018\n",
      "  time_since_restore: 946.5913846492767\n",
      "  time_this_iter_s: 21.10686731338501\n",
      "  time_total_s: 946.5913846492767\n",
      "  timers:\n",
      "    learn_throughput: 1377.32\n",
      "    learn_time_ms: 726.048\n",
      "    load_throughput: 42526.116\n",
      "    load_time_ms: 23.515\n",
      "    sample_throughput: 45.659\n",
      "    sample_time_ms: 21901.344\n",
      "    update_time_ms: 2.631\n",
      "  timestamp: 1635063812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         946.591</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">  -3.971</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">             397.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-23-52\n",
      "  done: false\n",
      "  episode_len_mean: 395.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -3.9549999999999597\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 117\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.089154595798916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00922194843908607\n",
      "          policy_loss: -0.10494527121384939\n",
      "          total_loss: -0.11272411031855477\n",
      "          vf_explained_var: 0.43306809663772583\n",
      "          vf_loss: 0.011268320754364443\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.053333333333335\n",
      "    ram_util_percent: 37.87666666666666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039433660253146076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.133733860476138\n",
      "    mean_inference_ms: 1.944690189711626\n",
      "    mean_raw_obs_processing_ms: 1.1665539702925405\n",
      "  time_since_restore: 967.000750541687\n",
      "  time_this_iter_s: 20.40936589241028\n",
      "  time_total_s: 967.000750541687\n",
      "  timers:\n",
      "    learn_throughput: 1376.979\n",
      "    learn_time_ms: 726.227\n",
      "    load_throughput: 42413.448\n",
      "    load_time_ms: 23.577\n",
      "    sample_throughput: 45.518\n",
      "    sample_time_ms: 21969.364\n",
      "    update_time_ms: 2.628\n",
      "  timestamp: 1635063832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         967.001</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">  -3.955</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">             395.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-24-31\n",
      "  done: false\n",
      "  episode_len_mean: 392.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.9239999999999604\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 120\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.022998176680671\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012240771058387483\n",
      "          policy_loss: -0.10362455927663379\n",
      "          total_loss: -0.1085675737924046\n",
      "          vf_explained_var: 0.2935118079185486\n",
      "          vf_loss: 0.012838814324802822\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.53148148148148\n",
      "    ram_util_percent: 37.840740740740735\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0394228306493593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.982465892140386\n",
      "    mean_inference_ms: 1.94402525461784\n",
      "    mean_raw_obs_processing_ms: 1.2090254583838425\n",
      "  time_since_restore: 1005.3417670726776\n",
      "  time_this_iter_s: 38.3410165309906\n",
      "  time_total_s: 1005.3417670726776\n",
      "  timers:\n",
      "    learn_throughput: 1374.422\n",
      "    learn_time_ms: 727.579\n",
      "    load_throughput: 41657.892\n",
      "    load_time_ms: 24.005\n",
      "    sample_throughput: 45.0\n",
      "    sample_time_ms: 22222.34\n",
      "    update_time_ms: 2.648\n",
      "  timestamp: 1635063871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1005.34</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  -3.924</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">             392.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-24-49\n",
      "  done: false\n",
      "  episode_len_mean: 390.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.901999999999962\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 123\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9688320305612352\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006836758869380017\n",
      "          policy_loss: -0.013539054741462072\n",
      "          total_loss: -0.02007720892628034\n",
      "          vf_explained_var: 0.0753578469157219\n",
      "          vf_loss: 0.011782817495986819\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.885185185185186\n",
      "    ram_util_percent: 37.77407407407407\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03941544556846349\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.858070171340874\n",
      "    mean_inference_ms: 1.9434655495300797\n",
      "    mean_raw_obs_processing_ms: 1.2507209458336896\n",
      "  time_since_restore: 1023.9927294254303\n",
      "  time_this_iter_s: 18.650962352752686\n",
      "  time_total_s: 1023.9927294254303\n",
      "  timers:\n",
      "    learn_throughput: 1377.943\n",
      "    learn_time_ms: 725.719\n",
      "    load_throughput: 40382.81\n",
      "    load_time_ms: 24.763\n",
      "    sample_throughput: 45.773\n",
      "    sample_time_ms: 21846.905\n",
      "    update_time_ms: 2.641\n",
      "  timestamp: 1635063889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1023.99</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">  -3.902</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">             390.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 387.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.8758999999999624\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 126\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9329132053587172\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00702432304609412\n",
      "          policy_loss: 0.045699185132980345\n",
      "          total_loss: 0.03793884184625414\n",
      "          vf_explained_var: 0.3555440306663513\n",
      "          vf_loss: 0.010163927084714587\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.62222222222221\n",
      "    ram_util_percent: 37.86296296296296\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940840469591709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.762749649254342\n",
      "    mean_inference_ms: 1.9429779560990497\n",
      "    mean_raw_obs_processing_ms: 1.2916333225528276\n",
      "  time_since_restore: 1043.0475387573242\n",
      "  time_this_iter_s: 19.05480933189392\n",
      "  time_total_s: 1043.0475387573242\n",
      "  timers:\n",
      "    learn_throughput: 1384.783\n",
      "    learn_time_ms: 722.135\n",
      "    load_throughput: 41047.667\n",
      "    load_time_ms: 24.362\n",
      "    sample_throughput: 46.296\n",
      "    sample_time_ms: 21600.057\n",
      "    update_time_ms: 2.679\n",
      "  timestamp: 1635063909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1043.05</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\"> -3.8759</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            387.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 385.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.851099999999962\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 129\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.866403913497925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008687259510645143\n",
      "          policy_loss: 0.07155091646644804\n",
      "          total_loss: 0.06368989513980018\n",
      "          vf_explained_var: 0.37934768199920654\n",
      "          vf_loss: 0.009065564325687269\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.03214285714285\n",
      "    ram_util_percent: 38.00714285714286\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039402646624661905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.681429446235637\n",
      "    mean_inference_ms: 1.9425654394470686\n",
      "    mean_raw_obs_processing_ms: 1.3185689859762042\n",
      "  time_since_restore: 1062.2041115760803\n",
      "  time_this_iter_s: 19.156572818756104\n",
      "  time_total_s: 1062.2041115760803\n",
      "  timers:\n",
      "    learn_throughput: 1381.524\n",
      "    learn_time_ms: 723.838\n",
      "    load_throughput: 39724.656\n",
      "    load_time_ms: 25.173\n",
      "    sample_throughput: 46.852\n",
      "    sample_time_ms: 21344.024\n",
      "    update_time_ms: 2.657\n",
      "  timestamp: 1635063928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">          1062.2</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\"> -3.8511</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            385.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-25-48\n",
      "  done: false\n",
      "  episode_len_mean: 383.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.8381999999999623\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 131\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.865984825293223\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005055859477642461\n",
      "          policy_loss: -0.09855056239498985\n",
      "          total_loss: -0.1037836684121026\n",
      "          vf_explained_var: 0.461625874042511\n",
      "          vf_loss: 0.01241556713745619\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.97857142857142\n",
      "    ram_util_percent: 38.13928571428573\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039398994266253304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.63528640281776\n",
      "    mean_inference_ms: 1.9423386214708431\n",
      "    mean_raw_obs_processing_ms: 1.3194231411242046\n",
      "  time_since_restore: 1082.4247126579285\n",
      "  time_this_iter_s: 20.220601081848145\n",
      "  time_total_s: 1082.4247126579285\n",
      "  timers:\n",
      "    learn_throughput: 1380.646\n",
      "    learn_time_ms: 724.299\n",
      "    load_throughput: 39799.971\n",
      "    load_time_ms: 25.126\n",
      "    sample_throughput: 47.094\n",
      "    sample_time_ms: 21234.179\n",
      "    update_time_ms: 2.737\n",
      "  timestamp: 1635063948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         1082.42</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -3.8382</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            383.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-26-08\n",
      "  done: false\n",
      "  episode_len_mean: 381.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.8183999999999627\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 134\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7607927097214593\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008125986304557959\n",
      "          policy_loss: -0.09755575218134456\n",
      "          total_loss: -0.0989173481033908\n",
      "          vf_explained_var: 0.28694474697113037\n",
      "          vf_loss: 0.01462113375051154\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.88965517241381\n",
      "    ram_util_percent: 38.14827586206897\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03939462959137968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.57358201310577\n",
      "    mean_inference_ms: 1.9420448892177071\n",
      "    mean_raw_obs_processing_ms: 1.3230037708518219\n",
      "  time_since_restore: 1102.4571695327759\n",
      "  time_this_iter_s: 20.032456874847412\n",
      "  time_total_s: 1102.4571695327759\n",
      "  timers:\n",
      "    learn_throughput: 1373.664\n",
      "    learn_time_ms: 727.98\n",
      "    load_throughput: 39717.547\n",
      "    load_time_ms: 25.178\n",
      "    sample_throughput: 47.282\n",
      "    sample_time_ms: 21149.71\n",
      "    update_time_ms: 2.848\n",
      "  timestamp: 1635063968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1102.46</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\"> -3.8184</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            381.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-26-28\n",
      "  done: false\n",
      "  episode_len_mean: 380.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.8020999999999634\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 137\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7439117458131579\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003591101851237887\n",
      "          policy_loss: -0.02893312391307619\n",
      "          total_loss: -0.03543046365181605\n",
      "          vf_explained_var: 0.43719762563705444\n",
      "          vf_loss: 0.010223552592813373\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.95714285714286\n",
      "    ram_util_percent: 38.135714285714286\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039391760604936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.519838866464813\n",
      "    mean_inference_ms: 1.941806474334874\n",
      "    mean_raw_obs_processing_ms: 1.3281304642297003\n",
      "  time_since_restore: 1122.002815246582\n",
      "  time_this_iter_s: 19.545645713806152\n",
      "  time_total_s: 1122.002815246582\n",
      "  timers:\n",
      "    learn_throughput: 1375.002\n",
      "    learn_time_ms: 727.271\n",
      "    load_throughput: 38967.664\n",
      "    load_time_ms: 25.662\n",
      "    sample_throughput: 47.588\n",
      "    sample_time_ms: 21013.91\n",
      "    update_time_ms: 2.658\n",
      "  timestamp: 1635063988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">            1122</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\"> -3.8021</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            380.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-26-47\n",
      "  done: false\n",
      "  episode_len_mean: 378.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.782399999999963\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 140\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7415945026609632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010558143968108051\n",
      "          policy_loss: 0.05305341995424694\n",
      "          total_loss: 0.0492022847963704\n",
      "          vf_explained_var: 0.22362232208251953\n",
      "          vf_loss: 0.012508996498460571\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.27407407407408\n",
      "    ram_util_percent: 38.1037037037037\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03939117464915254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.477433939258496\n",
      "    mean_inference_ms: 1.9416554536378094\n",
      "    mean_raw_obs_processing_ms: 1.3357678352974975\n",
      "  time_since_restore: 1141.1824967861176\n",
      "  time_this_iter_s: 19.179681539535522\n",
      "  time_total_s: 1141.1824967861176\n",
      "  timers:\n",
      "    learn_throughput: 1377.673\n",
      "    learn_time_ms: 725.862\n",
      "    load_throughput: 38878.735\n",
      "    load_time_ms: 25.721\n",
      "    sample_throughput: 48.051\n",
      "    sample_time_ms: 20811.161\n",
      "    update_time_ms: 2.736\n",
      "  timestamp: 1635064007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1141.18</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\"> -3.7824</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            378.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-27-07\n",
      "  done: false\n",
      "  episode_len_mean: 376.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.760199999999963\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 143\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7415026757452223\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01237097055622469\n",
      "          policy_loss: 0.03785173743963242\n",
      "          total_loss: 0.03593963210781415\n",
      "          vf_explained_var: -0.08670564740896225\n",
      "          vf_loss: 0.014265824760271547\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.19655172413793\n",
      "    ram_util_percent: 38.02758620689655\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039391277242868575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.440780916969075\n",
      "    mean_inference_ms: 1.9415320405824361\n",
      "    mean_raw_obs_processing_ms: 1.3443630908342628\n",
      "  time_since_restore: 1161.2640011310577\n",
      "  time_this_iter_s: 20.081504344940186\n",
      "  time_total_s: 1161.2640011310577\n",
      "  timers:\n",
      "    learn_throughput: 1385.343\n",
      "    learn_time_ms: 721.843\n",
      "    load_throughput: 38588.503\n",
      "    load_time_ms: 25.914\n",
      "    sample_throughput: 48.28\n",
      "    sample_time_ms: 20712.467\n",
      "    update_time_ms: 2.743\n",
      "  timestamp: 1635064027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         1161.26</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -3.7602</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            376.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-27-26\n",
      "  done: false\n",
      "  episode_len_mean: 373.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.7341999999999644\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 146\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7756770332654317\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010361319000686306\n",
      "          policy_loss: 0.06176967720190684\n",
      "          total_loss: 0.05597426725758447\n",
      "          vf_explained_var: 0.42495986819267273\n",
      "          vf_loss: 0.010925228949539208\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.2037037037037\n",
      "    ram_util_percent: 37.98148148148148\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03939236344214368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.4120535328831\n",
      "    mean_inference_ms: 1.941456735094822\n",
      "    mean_raw_obs_processing_ms: 1.354254553486137\n",
      "  time_since_restore: 1180.1256816387177\n",
      "  time_this_iter_s: 18.861680507659912\n",
      "  time_total_s: 1180.1256816387177\n",
      "  timers:\n",
      "    learn_throughput: 1385.777\n",
      "    learn_time_ms: 721.617\n",
      "    load_throughput: 38836.184\n",
      "    load_time_ms: 25.749\n",
      "    sample_throughput: 48.643\n",
      "    sample_time_ms: 20558.112\n",
      "    update_time_ms: 2.728\n",
      "  timestamp: 1635064046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1180.13</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\"> -3.7342</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            373.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-27-45\n",
      "  done: false\n",
      "  episode_len_mean: 372.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0599999999999787\n",
      "  episode_reward_mean: -3.723099999999964\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 148\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.811038006676568\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013406364485792781\n",
      "          policy_loss: -0.09335294134087033\n",
      "          total_loss: -0.09907068444622888\n",
      "          vf_explained_var: 0.5388706922531128\n",
      "          vf_loss: 0.01105199902748508\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.25925925925927\n",
      "    ram_util_percent: 38.0\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039392713695698675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.397311382178984\n",
      "    mean_inference_ms: 1.941408792988893\n",
      "    mean_raw_obs_processing_ms: 1.3615326024945877\n",
      "  time_since_restore: 1199.0052437782288\n",
      "  time_this_iter_s: 18.87956213951111\n",
      "  time_total_s: 1199.0052437782288\n",
      "  timers:\n",
      "    learn_throughput: 1391.945\n",
      "    learn_time_ms: 718.419\n",
      "    load_throughput: 39536.568\n",
      "    load_time_ms: 25.293\n",
      "    sample_throughput: 53.716\n",
      "    sample_time_ms: 18616.424\n",
      "    update_time_ms: 2.712\n",
      "  timestamp: 1635064065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1199.01</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\"> -3.7231</td><td style=\"text-align: right;\">               -3.06</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            372.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 368.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.6841999999999655\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 151\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8214438411924574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014956307331141576\n",
      "          policy_loss: -0.135616528077258\n",
      "          total_loss: -0.13986210889286466\n",
      "          vf_explained_var: 0.49320492148399353\n",
      "          vf_loss: 0.012473226017836068\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.33207547169812\n",
      "    ram_util_percent: 37.898113207547176\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0393935843784325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.379708636380872\n",
      "    mean_inference_ms: 1.9413564315041614\n",
      "    mean_raw_obs_processing_ms: 1.381410906390204\n",
      "  time_since_restore: 1236.3348290920258\n",
      "  time_this_iter_s: 37.329585313797\n",
      "  time_total_s: 1236.3348290920258\n",
      "  timers:\n",
      "    learn_throughput: 1398.682\n",
      "    learn_time_ms: 714.959\n",
      "    load_throughput: 40217.662\n",
      "    load_time_ms: 24.865\n",
      "    sample_throughput: 48.809\n",
      "    sample_time_ms: 20488.166\n",
      "    update_time_ms: 2.703\n",
      "  timestamp: 1635064102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1236.33</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\"> -3.6842</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            368.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-28-43\n",
      "  done: false\n",
      "  episode_len_mean: 362.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.6244999999999665\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 155\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.894412222173479\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01018708583713198\n",
      "          policy_loss: 0.024392758806546528\n",
      "          total_loss: 0.016251179037822618\n",
      "          vf_explained_var: 0.5709729790687561\n",
      "          vf_loss: 0.009783833794709708\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.351612903225806\n",
      "    ram_util_percent: 37.97096774193549\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03939644124655949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.371217195340797\n",
      "    mean_inference_ms: 1.9413513561503464\n",
      "    mean_raw_obs_processing_ms: 1.4094899604984672\n",
      "  time_since_restore: 1257.8171656131744\n",
      "  time_this_iter_s: 21.48233652114868\n",
      "  time_total_s: 1257.8171656131744\n",
      "  timers:\n",
      "    learn_throughput: 1401.085\n",
      "    learn_time_ms: 713.732\n",
      "    load_throughput: 40035.317\n",
      "    load_time_ms: 24.978\n",
      "    sample_throughput: 48.234\n",
      "    sample_time_ms: 20732.092\n",
      "    update_time_ms: 2.668\n",
      "  timestamp: 1635064123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1257.82</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -3.6245</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            362.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-29-04\n",
      "  done: false\n",
      "  episode_len_mean: 358.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.582899999999967\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 158\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8662583867708842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013804730969481425\n",
      "          policy_loss: 0.05307852774858475\n",
      "          total_loss: 0.0436277509563499\n",
      "          vf_explained_var: 0.6569839715957642\n",
      "          vf_loss: 0.007831333135254682\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.13103448275862\n",
      "    ram_util_percent: 37.996551724137944\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039398924753124204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.373922552583565\n",
      "    mean_inference_ms: 1.9413713419959238\n",
      "    mean_raw_obs_processing_ms: 1.4248998684473237\n",
      "  time_since_restore: 1278.1334733963013\n",
      "  time_this_iter_s: 20.31630778312683\n",
      "  time_total_s: 1278.1334733963013\n",
      "  timers:\n",
      "    learn_throughput: 1411.234\n",
      "    learn_time_ms: 708.6\n",
      "    load_throughput: 40405.334\n",
      "    load_time_ms: 24.749\n",
      "    sample_throughput: 47.954\n",
      "    sample_time_ms: 20853.414\n",
      "    update_time_ms: 2.676\n",
      "  timestamp: 1635064144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1278.13</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\"> -3.5829</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            358.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-29-26\n",
      "  done: false\n",
      "  episode_len_mean: 353.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.539199999999968\n",
      "  episode_reward_min: -4.9699999999999385\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 161\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8311823023690117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009739059895931589\n",
      "          policy_loss: -0.015966878996955022\n",
      "          total_loss: -0.026600599454508888\n",
      "          vf_explained_var: 0.7732773423194885\n",
      "          vf_loss: 0.006704194644569523\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.448387096774205\n",
      "    ram_util_percent: 38.00322580645162\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940096723765267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.381043275049187\n",
      "    mean_inference_ms: 1.9413938952685923\n",
      "    mean_raw_obs_processing_ms: 1.427550178139896\n",
      "  time_since_restore: 1300.0118401050568\n",
      "  time_this_iter_s: 21.878366708755493\n",
      "  time_total_s: 1300.0118401050568\n",
      "  timers:\n",
      "    learn_throughput: 1414.889\n",
      "    learn_time_ms: 706.769\n",
      "    load_throughput: 40418.261\n",
      "    load_time_ms: 24.741\n",
      "    sample_throughput: 47.571\n",
      "    sample_time_ms: 21021.205\n",
      "    update_time_ms: 2.568\n",
      "  timestamp: 1635064166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1300.01</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\"> -3.5392</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -4.97</td><td style=\"text-align: right;\">            353.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-29-47\n",
      "  done: false\n",
      "  episode_len_mean: 348.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.489699999999969\n",
      "  episode_reward_min: -4.379999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 164\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8307203226619297\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013578465374648848\n",
      "          policy_loss: -0.014523006478945414\n",
      "          total_loss: -0.02267890969912211\n",
      "          vf_explained_var: 0.7049886584281921\n",
      "          vf_loss: 0.008793453757082008\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.436666666666675\n",
      "    ram_util_percent: 38.07333333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039402475097381644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.392188951154203\n",
      "    mean_inference_ms: 1.9413946001510967\n",
      "    mean_raw_obs_processing_ms: 1.4314426618206946\n",
      "  time_since_restore: 1320.9117205142975\n",
      "  time_this_iter_s: 20.899880409240723\n",
      "  time_total_s: 1320.9117205142975\n",
      "  timers:\n",
      "    learn_throughput: 1431.503\n",
      "    learn_time_ms: 698.566\n",
      "    load_throughput: 40089.502\n",
      "    load_time_ms: 24.944\n",
      "    sample_throughput: 47.357\n",
      "    sample_time_ms: 21116.065\n",
      "    update_time_ms: 2.473\n",
      "  timestamp: 1635064187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1320.91</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\"> -3.4897</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -4.38</td><td style=\"text-align: right;\">            348.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-30-09\n",
      "  done: false\n",
      "  episode_len_mean: 345.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.45779999999997\n",
      "  episode_reward_min: -4.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 167\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.770756799644894\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012562000068824533\n",
      "          policy_loss: -0.056044003201855555\n",
      "          total_loss: -0.06341979884439045\n",
      "          vf_explained_var: 0.6888992190361023\n",
      "          vf_loss: 0.00907557372831636\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.383870967741935\n",
      "    ram_util_percent: 38.08387096774193\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940435218649456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.408449738148697\n",
      "    mean_inference_ms: 1.9414176082385186\n",
      "    mean_raw_obs_processing_ms: 1.4368215964754338\n",
      "  time_since_restore: 1342.8693869113922\n",
      "  time_this_iter_s: 21.957666397094727\n",
      "  time_total_s: 1342.8693869113922\n",
      "  timers:\n",
      "    learn_throughput: 1440.656\n",
      "    learn_time_ms: 694.128\n",
      "    load_throughput: 40123.058\n",
      "    load_time_ms: 24.923\n",
      "    sample_throughput: 46.813\n",
      "    sample_time_ms: 21361.774\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1635064209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1342.87</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -3.4578</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">                -4.1</td><td style=\"text-align: right;\">            345.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-30-30\n",
      "  done: false\n",
      "  episode_len_mean: 341.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.4189999999999707\n",
      "  episode_reward_min: -4.039999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 171\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.807886016368866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012238267718497219\n",
      "          policy_loss: 0.03253235924575064\n",
      "          total_loss: 0.023507712615860835\n",
      "          vf_explained_var: 0.7521864175796509\n",
      "          vf_loss: 0.00783038576030069\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.90967741935484\n",
      "    ram_util_percent: 37.929032258064524\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940630872147282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.432816364280935\n",
      "    mean_inference_ms: 1.9414518514934584\n",
      "    mean_raw_obs_processing_ms: 1.4449556508446006\n",
      "  time_since_restore: 1364.0483622550964\n",
      "  time_this_iter_s: 21.178975343704224\n",
      "  time_total_s: 1364.0483622550964\n",
      "  timers:\n",
      "    learn_throughput: 1442.786\n",
      "    learn_time_ms: 693.103\n",
      "    load_throughput: 40099.16\n",
      "    load_time_ms: 24.938\n",
      "    sample_throughput: 46.376\n",
      "    sample_time_ms: 21562.773\n",
      "    update_time_ms: 2.383\n",
      "  timestamp: 1635064230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1364.05</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">  -3.419</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -4.04</td><td style=\"text-align: right;\">             341.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-30-51\n",
      "  done: false\n",
      "  episode_len_mean: 339.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.3961999999999715\n",
      "  episode_reward_min: -3.92999999999996\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 174\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.796355508433448\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013805927456565965\n",
      "          policy_loss: -0.004179738296402826\n",
      "          total_loss: -0.008866153905789058\n",
      "          vf_explained_var: 0.3493923246860504\n",
      "          vf_loss: 0.011896547437128093\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.33666666666667\n",
      "    ram_util_percent: 37.88999999999999\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039405448985413295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.45185250665527\n",
      "    mean_inference_ms: 1.9414281698283216\n",
      "    mean_raw_obs_processing_ms: 1.4520152105705915\n",
      "  time_since_restore: 1385.1120579242706\n",
      "  time_this_iter_s: 21.063695669174194\n",
      "  time_total_s: 1385.1120579242706\n",
      "  timers:\n",
      "    learn_throughput: 1444.581\n",
      "    learn_time_ms: 692.242\n",
      "    load_throughput: 40348.934\n",
      "    load_time_ms: 24.784\n",
      "    sample_throughput: 46.164\n",
      "    sample_time_ms: 21662.003\n",
      "    update_time_ms: 2.384\n",
      "  timestamp: 1635064251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1385.11</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\"> -3.3962</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -3.93</td><td style=\"text-align: right;\">            339.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-31-13\n",
      "  done: false\n",
      "  episode_len_mean: 337.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.3748999999999723\n",
      "  episode_reward_min: -3.92999999999996\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 177\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.811749964290195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012457437173675699\n",
      "          policy_loss: -0.013705792360835606\n",
      "          total_loss: -0.017697597377830082\n",
      "          vf_explained_var: 0.4034450948238373\n",
      "          vf_loss: 0.012879953772709188\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.365625\n",
      "    ram_util_percent: 37.878125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940338591415275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.47127512893019\n",
      "    mean_inference_ms: 1.9413807989099003\n",
      "    mean_raw_obs_processing_ms: 1.4597717872205833\n",
      "  time_since_restore: 1407.6221442222595\n",
      "  time_this_iter_s: 22.51008629798889\n",
      "  time_total_s: 1407.6221442222595\n",
      "  timers:\n",
      "    learn_throughput: 1444.465\n",
      "    learn_time_ms: 692.298\n",
      "    load_throughput: 40398.368\n",
      "    load_time_ms: 24.753\n",
      "    sample_throughput: 45.399\n",
      "    sample_time_ms: 22026.762\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1635064273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1407.62</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\"> -3.3749</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -3.93</td><td style=\"text-align: right;\">            337.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-31-53\n",
      "  done: false\n",
      "  episode_len_mean: 334.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.3426999999999727\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 181\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7822019947899712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01320787361204694\n",
      "          policy_loss: -0.0024102050397131177\n",
      "          total_loss: -0.0027154053250948587\n",
      "          vf_explained_var: 0.2899261713027954\n",
      "          vf_loss: 0.016196028795093298\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.15892857142857\n",
      "    ram_util_percent: 37.7875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940049582205995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.499331750076543\n",
      "    mean_inference_ms: 1.9413301603260709\n",
      "    mean_raw_obs_processing_ms: 1.4803960827943488\n",
      "  time_since_restore: 1447.2238779067993\n",
      "  time_this_iter_s: 39.601733684539795\n",
      "  time_total_s: 1447.2238779067993\n",
      "  timers:\n",
      "    learn_throughput: 1443.5\n",
      "    learn_time_ms: 692.761\n",
      "    load_throughput: 40329.148\n",
      "    load_time_ms: 24.796\n",
      "    sample_throughput: 41.496\n",
      "    sample_time_ms: 24098.498\n",
      "    update_time_ms: 2.409\n",
      "  timestamp: 1635064313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1447.22</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -3.3427</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            334.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-32-16\n",
      "  done: false\n",
      "  episode_len_mean: 331.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.3184999999999722\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 184\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7795431719885932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006074596159440541\n",
      "          policy_loss: -0.002547616180446413\n",
      "          total_loss: -0.008371209767129685\n",
      "          vf_explained_var: 0.28584325313568115\n",
      "          vf_loss: 0.011364378526599871\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.50294117647059\n",
      "    ram_util_percent: 37.66764705882353\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03939856466275289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.52235268401255\n",
      "    mean_inference_ms: 1.941304140427433\n",
      "    mean_raw_obs_processing_ms: 1.4961969769288985\n",
      "  time_since_restore: 1470.42045545578\n",
      "  time_this_iter_s: 23.196577548980713\n",
      "  time_total_s: 1470.42045545578\n",
      "  timers:\n",
      "    learn_throughput: 1441.745\n",
      "    learn_time_ms: 693.604\n",
      "    load_throughput: 40154.556\n",
      "    load_time_ms: 24.904\n",
      "    sample_throughput: 44.083\n",
      "    sample_time_ms: 22684.267\n",
      "    update_time_ms: 2.412\n",
      "  timestamp: 1635064336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1470.42</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\"> -3.3185</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            331.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-32-39\n",
      "  done: false\n",
      "  episode_len_mean: 329.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.2899999999999734\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 188\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.784897842672136\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009695851989925468\n",
      "          policy_loss: -0.001725244190957811\n",
      "          total_loss: -0.003128023776743147\n",
      "          vf_explained_var: 0.2168770432472229\n",
      "          vf_loss: 0.015476612891587947\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.36875\n",
      "    ram_util_percent: 37.625\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039396365679556955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.554258142783624\n",
      "    mean_inference_ms: 1.9412882733138843\n",
      "    mean_raw_obs_processing_ms: 1.5135913937352468\n",
      "  time_since_restore: 1493.4111421108246\n",
      "  time_this_iter_s: 22.990686655044556\n",
      "  time_total_s: 1493.4111421108246\n",
      "  timers:\n",
      "    learn_throughput: 1440.258\n",
      "    learn_time_ms: 694.32\n",
      "    load_throughput: 40504.6\n",
      "    load_time_ms: 24.689\n",
      "    sample_throughput: 43.793\n",
      "    sample_time_ms: 22834.548\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1635064359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1493.41</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">   -3.29</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">               329</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-33-02\n",
      "  done: false\n",
      "  episode_len_mean: 327.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.275099999999974\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 191\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8240939180056255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007776384080073247\n",
      "          policy_loss: 0.020491718583636815\n",
      "          total_loss: 0.017300126287672253\n",
      "          vf_explained_var: -0.052982673048973083\n",
      "          vf_loss: 0.014271708324344622\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.051515151515154\n",
      "    ram_util_percent: 37.63636363636363\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0393947458054563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.578599286011418\n",
      "    mean_inference_ms: 1.9412753395968383\n",
      "    mean_raw_obs_processing_ms: 1.5168690119470838\n",
      "  time_since_restore: 1515.9115688800812\n",
      "  time_this_iter_s: 22.500426769256592\n",
      "  time_total_s: 1515.9115688800812\n",
      "  timers:\n",
      "    learn_throughput: 1440.467\n",
      "    learn_time_ms: 694.219\n",
      "    load_throughput: 40298.46\n",
      "    load_time_ms: 24.815\n",
      "    sample_throughput: 43.378\n",
      "    sample_time_ms: 23052.915\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1635064382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1515.91</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\"> -3.2751</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            327.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-33-25\n",
      "  done: false\n",
      "  episode_len_mean: 326.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.2599999999999745\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 194\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7852803932295904\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0069100546582751225\n",
      "          policy_loss: -0.1025126670797666\n",
      "          total_loss: -0.10444908936818441\n",
      "          vf_explained_var: 0.22104428708553314\n",
      "          vf_loss: 0.015225374719334973\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.42121212121212\n",
      "    ram_util_percent: 37.70606060606061\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039392403992360006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.60206912637503\n",
      "    mean_inference_ms: 1.9412446727587211\n",
      "    mean_raw_obs_processing_ms: 1.5205582789291965\n",
      "  time_since_restore: 1539.1853313446045\n",
      "  time_this_iter_s: 23.273762464523315\n",
      "  time_total_s: 1539.1853313446045\n",
      "  timers:\n",
      "    learn_throughput: 1435.892\n",
      "    learn_time_ms: 696.431\n",
      "    load_throughput: 40186.296\n",
      "    load_time_ms: 24.884\n",
      "    sample_throughput: 43.122\n",
      "    sample_time_ms: 23190.172\n",
      "    update_time_ms: 2.467\n",
      "  timestamp: 1635064405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1539.19</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">   -3.26</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">               326</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 324.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.2455999999999743\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 198\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8315850310855442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010483736574511265\n",
      "          policy_loss: 0.05397894299692578\n",
      "          total_loss: 0.047612876445055005\n",
      "          vf_explained_var: 0.4256829023361206\n",
      "          vf_loss: 0.010901412246231404\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.240625\n",
      "    ram_util_percent: 37.787499999999994\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039387811352626534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.63214072057238\n",
      "    mean_inference_ms: 1.9411726800126992\n",
      "    mean_raw_obs_processing_ms: 1.5262002120870932\n",
      "  time_since_restore: 1561.9113721847534\n",
      "  time_this_iter_s: 22.726040840148926\n",
      "  time_total_s: 1561.9113721847534\n",
      "  timers:\n",
      "    learn_throughput: 1435.711\n",
      "    learn_time_ms: 696.519\n",
      "    load_throughput: 40207.754\n",
      "    load_time_ms: 24.871\n",
      "    sample_throughput: 42.785\n",
      "    sample_time_ms: 23372.642\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1635064428\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1561.91</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\"> -3.2456</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            324.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-34-10\n",
      "  done: false\n",
      "  episode_len_mean: 323.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.236399999999975\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 201\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8601400441593594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00875263669141396\n",
      "          policy_loss: -0.09201036476426655\n",
      "          total_loss: -0.09389106780290604\n",
      "          vf_explained_var: 0.2356061339378357\n",
      "          vf_loss: 0.015845431573688985\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.02424242424242\n",
      "    ram_util_percent: 37.7969696969697\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0393835256025453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.6539269993288\n",
      "    mean_inference_ms: 1.9410975637182646\n",
      "    mean_raw_obs_processing_ms: 1.5309061538359763\n",
      "  time_since_restore: 1584.5172262191772\n",
      "  time_this_iter_s: 22.605854034423828\n",
      "  time_total_s: 1584.5172262191772\n",
      "  timers:\n",
      "    learn_throughput: 1436.677\n",
      "    learn_time_ms: 696.051\n",
      "    load_throughput: 40280.0\n",
      "    load_time_ms: 24.826\n",
      "    sample_throughput: 42.666\n",
      "    sample_time_ms: 23437.956\n",
      "    update_time_ms: 2.52\n",
      "  timestamp: 1635064450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1584.52</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\"> -3.2364</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            323.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-34-33\n",
      "  done: false\n",
      "  episode_len_mean: 322.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.2264999999999744\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 205\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.862502442465888\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011891248014964183\n",
      "          policy_loss: 0.044365895125601025\n",
      "          total_loss: 0.03781205258435673\n",
      "          vf_explained_var: 0.5631396174430847\n",
      "          vf_loss: 0.010882055417944987\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.554838709677405\n",
      "    ram_util_percent: 37.767741935483876\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039377314058601884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.682481290807104\n",
      "    mean_inference_ms: 1.9409874187185656\n",
      "    mean_raw_obs_processing_ms: 1.5376934218607443\n",
      "  time_since_restore: 1606.766834974289\n",
      "  time_this_iter_s: 22.249608755111694\n",
      "  time_total_s: 1606.766834974289\n",
      "  timers:\n",
      "    learn_throughput: 1435.647\n",
      "    learn_time_ms: 696.55\n",
      "    load_throughput: 40548.692\n",
      "    load_time_ms: 24.662\n",
      "    sample_throughput: 42.472\n",
      "    sample_time_ms: 23544.713\n",
      "    update_time_ms: 2.495\n",
      "  timestamp: 1635064473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1606.77</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\"> -3.2265</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            322.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 322.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.226399999999975\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 208\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8615435653262669\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01254416172580706\n",
      "          policy_loss: 0.0928399013976256\n",
      "          total_loss: 0.08414632769094574\n",
      "          vf_explained_var: 0.5193389058113098\n",
      "          vf_loss: 0.008667448300143911\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.96206896551725\n",
      "    ram_util_percent: 37.727586206896554\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03937194254480697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.702685115969544\n",
      "    mean_inference_ms: 1.9408904040067705\n",
      "    mean_raw_obs_processing_ms: 1.5432632312746517\n",
      "  time_since_restore: 1626.9585959911346\n",
      "  time_this_iter_s: 20.191761016845703\n",
      "  time_total_s: 1626.9585959911346\n",
      "  timers:\n",
      "    learn_throughput: 1433.687\n",
      "    learn_time_ms: 697.502\n",
      "    load_throughput: 40487.67\n",
      "    load_time_ms: 24.699\n",
      "    sample_throughput: 42.632\n",
      "    sample_time_ms: 23456.518\n",
      "    update_time_ms: 2.496\n",
      "  timestamp: 1635064493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1626.96</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -3.2264</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            322.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-35-32\n",
      "  done: false\n",
      "  episode_len_mean: 321.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.2189999999999754\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 211\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8231226828363207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013673045374752175\n",
      "          policy_loss: 0.007151890463299221\n",
      "          total_loss: -0.0013140320777893066\n",
      "          vf_explained_var: 0.7122997045516968\n",
      "          vf_loss: 0.008398000723941045\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.19464285714286\n",
      "    ram_util_percent: 37.675000000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039366030293863194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.72227856026165\n",
      "    mean_inference_ms: 1.9407828452462288\n",
      "    mean_raw_obs_processing_ms: 1.5558016448298795\n",
      "  time_since_restore: 1665.7808368206024\n",
      "  time_this_iter_s: 38.82224082946777\n",
      "  time_total_s: 1665.7808368206024\n",
      "  timers:\n",
      "    learn_throughput: 1436.41\n",
      "    learn_time_ms: 696.18\n",
      "    load_throughput: 40053.362\n",
      "    load_time_ms: 24.967\n",
      "    sample_throughput: 39.858\n",
      "    sample_time_ms: 25088.818\n",
      "    update_time_ms: 2.467\n",
      "  timestamp: 1635064532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1665.78</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">  -3.219</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">             321.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-35-55\n",
      "  done: false\n",
      "  episode_len_mean: 320.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.208899999999976\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 214\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8486288468043008\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011301520309505339\n",
      "          policy_loss: -0.1323500297135777\n",
      "          total_loss: -0.1298369537625048\n",
      "          vf_explained_var: 0.27436986565589905\n",
      "          vf_loss: 0.019869211533417305\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.46875\n",
      "    ram_util_percent: 37.628125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039359761967404205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.742220523646512\n",
      "    mean_inference_ms: 1.9406664692884232\n",
      "    mean_raw_obs_processing_ms: 1.5682331613081002\n",
      "  time_since_restore: 1688.5802927017212\n",
      "  time_this_iter_s: 22.799455881118774\n",
      "  time_total_s: 1688.5802927017212\n",
      "  timers:\n",
      "    learn_throughput: 1436.744\n",
      "    learn_time_ms: 696.018\n",
      "    load_throughput: 40111.7\n",
      "    load_time_ms: 24.93\n",
      "    sample_throughput: 42.719\n",
      "    sample_time_ms: 23408.788\n",
      "    update_time_ms: 2.463\n",
      "  timestamp: 1635064555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1688.58</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\"> -3.2089</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            320.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 319.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1929999999999756\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 218\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8469188716676501\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013920881935998688\n",
      "          policy_loss: 0.023328382376995353\n",
      "          total_loss: 0.021239706542756823\n",
      "          vf_explained_var: 0.5263829827308655\n",
      "          vf_loss: 0.014988422507627143\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.071875000000006\n",
      "    ram_util_percent: 37.712500000000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03935030167867147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.768284002221762\n",
      "    mean_inference_ms: 1.940485150040952\n",
      "    mean_raw_obs_processing_ms: 1.5817907797993878\n",
      "  time_since_restore: 1710.5483305454254\n",
      "  time_this_iter_s: 21.968037843704224\n",
      "  time_total_s: 1710.5483305454254\n",
      "  timers:\n",
      "    learn_throughput: 1436.057\n",
      "    learn_time_ms: 696.351\n",
      "    load_throughput: 40220.632\n",
      "    load_time_ms: 24.863\n",
      "    sample_throughput: 42.945\n",
      "    sample_time_ms: 23285.654\n",
      "    update_time_ms: 2.463\n",
      "  timestamp: 1635064577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1710.55</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">  -3.193</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">             319.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-36-39\n",
      "  done: false\n",
      "  episode_len_mean: 317.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1797999999999758\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 221\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.908127702607049\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012487058531984967\n",
      "          policy_loss: -0.04062764619787534\n",
      "          total_loss: -0.04897925191455417\n",
      "          vf_explained_var: 0.6751773357391357\n",
      "          vf_loss: 0.009480965067632496\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.1375\n",
      "    ram_util_percent: 37.775\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039342389489148324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.787875435426628\n",
      "    mean_inference_ms: 1.9403340956360646\n",
      "    mean_raw_obs_processing_ms: 1.584426084470252\n",
      "  time_since_restore: 1733.0154128074646\n",
      "  time_this_iter_s: 22.467082262039185\n",
      "  time_total_s: 1733.0154128074646\n",
      "  timers:\n",
      "    learn_throughput: 1436.881\n",
      "    learn_time_ms: 695.952\n",
      "    load_throughput: 40316.123\n",
      "    load_time_ms: 24.804\n",
      "    sample_throughput: 43.041\n",
      "    sample_time_ms: 23233.839\n",
      "    update_time_ms: 2.396\n",
      "  timestamp: 1635064599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1733.02</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -3.1798</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            317.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 316.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.168599999999976\n",
      "  episode_reward_min: -3.759999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 224\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.874155212773217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014300264560587313\n",
      "          policy_loss: 0.07565100838740667\n",
      "          total_loss: 0.07520383753710323\n",
      "          vf_explained_var: 0.3609612286090851\n",
      "          vf_loss: 0.01686435449971921\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.58214285714285\n",
      "    ram_util_percent: 37.83214285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03933417735991189\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.80785325284302\n",
      "    mean_inference_ms: 1.9401789649416017\n",
      "    mean_raw_obs_processing_ms: 1.587358468677599\n",
      "  time_since_restore: 1752.9414055347443\n",
      "  time_this_iter_s: 19.925992727279663\n",
      "  time_total_s: 1752.9414055347443\n",
      "  timers:\n",
      "    learn_throughput: 1439.007\n",
      "    learn_time_ms: 694.924\n",
      "    load_throughput: 40492.361\n",
      "    load_time_ms: 24.696\n",
      "    sample_throughput: 43.521\n",
      "    sample_time_ms: 22977.564\n",
      "    update_time_ms: 2.4\n",
      "  timestamp: 1635064619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1752.94</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\"> -3.1686</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.76</td><td style=\"text-align: right;\">            316.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-37-20\n",
      "  done: false\n",
      "  episode_len_mean: 316.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.160299999999977\n",
      "  episode_reward_min: -3.699999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 227\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.917638705836402\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011440639305780959\n",
      "          policy_loss: 0.044486584266026814\n",
      "          total_loss: 0.03605242719252904\n",
      "          vf_explained_var: 0.5517355799674988\n",
      "          vf_loss: 0.009598164904552202\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.848275862068974\n",
      "    ram_util_percent: 37.83448275862068\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03932560948354958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.828234081152885\n",
      "    mean_inference_ms: 1.940019351628163\n",
      "    mean_raw_obs_processing_ms: 1.5905654099240303\n",
      "  time_since_restore: 1773.4543471336365\n",
      "  time_this_iter_s: 20.512941598892212\n",
      "  time_total_s: 1773.4543471336365\n",
      "  timers:\n",
      "    learn_throughput: 1443.082\n",
      "    learn_time_ms: 692.961\n",
      "    load_throughput: 40594.962\n",
      "    load_time_ms: 24.634\n",
      "    sample_throughput: 44.046\n",
      "    sample_time_ms: 22703.485\n",
      "    update_time_ms: 2.406\n",
      "  timestamp: 1635064640\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1773.45</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\"> -3.1603</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">                -3.7</td><td style=\"text-align: right;\">            316.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-37-40\n",
      "  done: false\n",
      "  episode_len_mean: 315.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.155599999999976\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 230\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9611210054821437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009974431320803234\n",
      "          policy_loss: 0.04384655331571897\n",
      "          total_loss: 0.03620958353082339\n",
      "          vf_explained_var: 0.4519950747489929\n",
      "          vf_loss: 0.01097679804596636\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.59999999999999\n",
      "    ram_util_percent: 37.83448275862068\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03931659746794789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.848664100438302\n",
      "    mean_inference_ms: 1.9398467679872837\n",
      "    mean_raw_obs_processing_ms: 1.5940298841847493\n",
      "  time_since_restore: 1793.6519820690155\n",
      "  time_this_iter_s: 20.19763493537903\n",
      "  time_total_s: 1793.6519820690155\n",
      "  timers:\n",
      "    learn_throughput: 1442.453\n",
      "    learn_time_ms: 693.264\n",
      "    load_throughput: 40726.657\n",
      "    load_time_ms: 24.554\n",
      "    sample_throughput: 44.543\n",
      "    sample_time_ms: 22450.437\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1635064660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1793.65</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\"> -3.1556</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            315.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-38-00\n",
      "  done: false\n",
      "  episode_len_mean: 315.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1565999999999756\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 233\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8386574109395346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01370065814261723\n",
      "          policy_loss: 0.05642582161558999\n",
      "          total_loss: 0.04948973672257529\n",
      "          vf_explained_var: 0.533844530582428\n",
      "          vf_loss: 0.01008042455650866\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.66206896551724\n",
      "    ram_util_percent: 37.84137931034482\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03930684182246162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.868539063453895\n",
      "    mean_inference_ms: 1.9396497615970174\n",
      "    mean_raw_obs_processing_ms: 1.5979930145942691\n",
      "  time_since_restore: 1813.4743094444275\n",
      "  time_this_iter_s: 19.822327375411987\n",
      "  time_total_s: 1813.4743094444275\n",
      "  timers:\n",
      "    learn_throughput: 1442.385\n",
      "    learn_time_ms: 693.296\n",
      "    load_throughput: 40919.281\n",
      "    load_time_ms: 24.438\n",
      "    sample_throughput: 45.102\n",
      "    sample_time_ms: 22172.112\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1635064680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1813.47</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -3.1566</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            315.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-38-21\n",
      "  done: false\n",
      "  episode_len_mean: 314.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1478999999999773\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 236\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8184290634261238\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00997481995697653\n",
      "          policy_loss: 0.041675014048814775\n",
      "          total_loss: 0.03897348551286591\n",
      "          vf_explained_var: 0.25501781702041626\n",
      "          vf_loss: 0.01448527914730625\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.106896551724134\n",
      "    ram_util_percent: 37.78965517241379\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03929671768131485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.888618806397034\n",
      "    mean_inference_ms: 1.939439500910447\n",
      "    mean_raw_obs_processing_ms: 1.6021683560537046\n",
      "  time_since_restore: 1834.3851747512817\n",
      "  time_this_iter_s: 20.910865306854248\n",
      "  time_total_s: 1834.3851747512817\n",
      "  timers:\n",
      "    learn_throughput: 1442.273\n",
      "    learn_time_ms: 693.35\n",
      "    load_throughput: 40615.443\n",
      "    load_time_ms: 24.621\n",
      "    sample_throughput: 45.376\n",
      "    sample_time_ms: 22038.014\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1635064701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1834.39</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\"> -3.1479</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            314.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-38-40\n",
      "  done: false\n",
      "  episode_len_mean: 314.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1481999999999766\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 239\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8408338136143154\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007693522874255147\n",
      "          policy_loss: 0.01983871923552619\n",
      "          total_loss: 0.013706247011820475\n",
      "          vf_explained_var: 0.3878909945487976\n",
      "          vf_loss: 0.011506510820860663\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.91428571428572\n",
      "    ram_util_percent: 37.79285714285714\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039286361627293626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.90863292586938\n",
      "    mean_inference_ms: 1.9392199176136637\n",
      "    mean_raw_obs_processing_ms: 1.606544327205564\n",
      "  time_since_restore: 1853.980771780014\n",
      "  time_this_iter_s: 19.5955970287323\n",
      "  time_total_s: 1853.980771780014\n",
      "  timers:\n",
      "    learn_throughput: 1442.38\n",
      "    learn_time_ms: 693.298\n",
      "    load_throughput: 40832.558\n",
      "    load_time_ms: 24.49\n",
      "    sample_throughput: 45.499\n",
      "    sample_time_ms: 21978.566\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1635064720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1853.98</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\"> -3.1482</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            314.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-39-18\n",
      "  done: false\n",
      "  episode_len_mean: 313.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.138099999999976\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 242\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8364426255226136\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011351833709150336\n",
      "          policy_loss: 0.01928789516290029\n",
      "          total_loss: 0.015416435731781853\n",
      "          vf_explained_var: 0.17821598052978516\n",
      "          vf_loss: 0.013357785010399918\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.69818181818181\n",
      "    ram_util_percent: 37.74363636363636\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03927635408310378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.928722244512606\n",
      "    mean_inference_ms: 1.9390076896546387\n",
      "    mean_raw_obs_processing_ms: 1.6171327269925815\n",
      "  time_since_restore: 1892.2754046916962\n",
      "  time_this_iter_s: 38.29463291168213\n",
      "  time_total_s: 1892.2754046916962\n",
      "  timers:\n",
      "    learn_throughput: 1441.587\n",
      "    learn_time_ms: 693.68\n",
      "    load_throughput: 41003.206\n",
      "    load_time_ms: 24.388\n",
      "    sample_throughput: 45.609\n",
      "    sample_time_ms: 21925.516\n",
      "    update_time_ms: 2.466\n",
      "  timestamp: 1635064758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1892.28</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\"> -3.1381</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            313.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-39-40\n",
      "  done: false\n",
      "  episode_len_mean: 312.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1265999999999767\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 245\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7876235485076903\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008719502842163607\n",
      "          policy_loss: 0.051985925436019896\n",
      "          total_loss: 0.046137358910507624\n",
      "          vf_explained_var: 0.3317962884902954\n",
      "          vf_loss: 0.01115572091528318\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.85806451612903\n",
      "    ram_util_percent: 37.7\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03926666470669911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.94943669707767\n",
      "    mean_inference_ms: 1.9388058755791213\n",
      "    mean_raw_obs_processing_ms: 1.6278304525257914\n",
      "  time_since_restore: 1913.932849407196\n",
      "  time_this_iter_s: 21.657444715499878\n",
      "  time_total_s: 1913.932849407196\n",
      "  timers:\n",
      "    learn_throughput: 1441.49\n",
      "    learn_time_ms: 693.727\n",
      "    load_throughput: 41309.68\n",
      "    load_time_ms: 24.207\n",
      "    sample_throughput: 45.848\n",
      "    sample_time_ms: 21811.419\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1635064780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1913.93</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -3.1266</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            312.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-40-00\n",
      "  done: false\n",
      "  episode_len_mean: 312.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1200999999999777\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 248\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8017919487423366\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008296675016718937\n",
      "          policy_loss: 0.08001318987872866\n",
      "          total_loss: 0.07469533731540044\n",
      "          vf_explained_var: 0.30294641852378845\n",
      "          vf_loss: 0.011870399317962842\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.8448275862069\n",
      "    ram_util_percent: 37.706896551724135\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039257235033973104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.97047664988385\n",
      "    mean_inference_ms: 1.938615058281178\n",
      "    mean_raw_obs_processing_ms: 1.6386361685007602\n",
      "  time_since_restore: 1934.1787321567535\n",
      "  time_this_iter_s: 20.245882749557495\n",
      "  time_total_s: 1934.1787321567535\n",
      "  timers:\n",
      "    learn_throughput: 1441.928\n",
      "    learn_time_ms: 693.516\n",
      "    load_throughput: 41119.821\n",
      "    load_time_ms: 24.319\n",
      "    sample_throughput: 46.212\n",
      "    sample_time_ms: 21639.322\n",
      "    update_time_ms: 2.48\n",
      "  timestamp: 1635064800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1934.18</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\"> -3.1201</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            312.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-40-21\n",
      "  done: false\n",
      "  episode_len_mean: 312.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.124199999999978\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 251\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7654356956481934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007985027576535625\n",
      "          policy_loss: 0.0678437708152665\n",
      "          total_loss: 0.06337318122386933\n",
      "          vf_explained_var: 0.25988438725471497\n",
      "          vf_loss: 0.012385268609634497\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.88965517241379\n",
      "    ram_util_percent: 37.813793103448276\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03924823175950521\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.991384268719745\n",
      "    mean_inference_ms: 1.9384365095526388\n",
      "    mean_raw_obs_processing_ms: 1.6409379341538948\n",
      "  time_since_restore: 1954.5451481342316\n",
      "  time_this_iter_s: 20.366415977478027\n",
      "  time_total_s: 1954.5451481342316\n",
      "  timers:\n",
      "    learn_throughput: 1442.326\n",
      "    learn_time_ms: 693.325\n",
      "    load_throughput: 40878.562\n",
      "    load_time_ms: 24.463\n",
      "    sample_throughput: 46.665\n",
      "    sample_time_ms: 21429.276\n",
      "    update_time_ms: 2.486\n",
      "  timestamp: 1635064821\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1954.55</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\"> -3.1242</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            312.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-40-41\n",
      "  done: false\n",
      "  episode_len_mean: 313.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.131399999999976\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 254\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.772431610690223\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008771190760365815\n",
      "          policy_loss: 0.04812655862834719\n",
      "          total_loss: 0.044475344651275214\n",
      "          vf_explained_var: 0.31879922747612\n",
      "          vf_loss: 0.013195982688598128\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.685714285714276\n",
      "    ram_util_percent: 37.87142857142857\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039239306493139114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.011208845379883\n",
      "    mean_inference_ms: 1.9382630037138349\n",
      "    mean_raw_obs_processing_ms: 1.6434439321576915\n",
      "  time_since_restore: 1974.4276413917542\n",
      "  time_this_iter_s: 19.882493257522583\n",
      "  time_total_s: 1974.4276413917542\n",
      "  timers:\n",
      "    learn_throughput: 1439.624\n",
      "    learn_time_ms: 694.626\n",
      "    load_throughput: 41136.638\n",
      "    load_time_ms: 24.309\n",
      "    sample_throughput: 46.677\n",
      "    sample_time_ms: 21423.777\n",
      "    update_time_ms: 2.476\n",
      "  timestamp: 1635064841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1974.43</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\"> -3.1314</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            313.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-41-02\n",
      "  done: false\n",
      "  episode_len_mean: 313.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1312999999999764\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 257\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7936824599901835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013032018755557059\n",
      "          policy_loss: 0.03703169888920254\n",
      "          total_loss: 0.032268248664008244\n",
      "          vf_explained_var: 0.37421104311943054\n",
      "          vf_loss: 0.011870173352912792\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.51612903225807\n",
      "    ram_util_percent: 37.89354838709677\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039230281157735976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.031257776900627\n",
      "    mean_inference_ms: 1.9380925868993149\n",
      "    mean_raw_obs_processing_ms: 1.6459237736388785\n",
      "  time_since_restore: 1996.0955274105072\n",
      "  time_this_iter_s: 21.66788601875305\n",
      "  time_total_s: 1996.0955274105072\n",
      "  timers:\n",
      "    learn_throughput: 1439.039\n",
      "    learn_time_ms: 694.908\n",
      "    load_throughput: 40915.249\n",
      "    load_time_ms: 24.441\n",
      "    sample_throughput: 46.428\n",
      "    sample_time_ms: 21538.823\n",
      "    update_time_ms: 2.493\n",
      "  timestamp: 1635064862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">          1996.1</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -3.1313</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            313.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-41-24\n",
      "  done: false\n",
      "  episode_len_mean: 313.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.135599999999977\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 260\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7974641429053413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012303739011352772\n",
      "          policy_loss: 0.035120577861865364\n",
      "          total_loss: 0.02901088289088673\n",
      "          vf_explained_var: 0.4027714729309082\n",
      "          vf_loss: 0.010634573142225337\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.780645161290316\n",
      "    ram_util_percent: 37.85806451612902\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0392215824400233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.05075696399443\n",
      "    mean_inference_ms: 1.9379291986276581\n",
      "    mean_raw_obs_processing_ms: 1.6486076319998824\n",
      "  time_since_restore: 2017.3547341823578\n",
      "  time_this_iter_s: 21.259206771850586\n",
      "  time_total_s: 2017.3547341823578\n",
      "  timers:\n",
      "    learn_throughput: 1437.243\n",
      "    learn_time_ms: 695.776\n",
      "    load_throughput: 40814.757\n",
      "    load_time_ms: 24.501\n",
      "    sample_throughput: 46.202\n",
      "    sample_time_ms: 21644.078\n",
      "    update_time_ms: 2.465\n",
      "  timestamp: 1635064884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         2017.35</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\"> -3.1356</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            313.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 313.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1364999999999763\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 263\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7205819765726724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010975900694352544\n",
      "          policy_loss: 0.06479984828167491\n",
      "          total_loss: 0.05510880707038773\n",
      "          vf_explained_var: 0.3376244008541107\n",
      "          vf_loss: 0.006417188030253682\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.40333333333333\n",
      "    ram_util_percent: 37.83333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921308384822116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.06986484780729\n",
      "    mean_inference_ms: 1.937769552577113\n",
      "    mean_raw_obs_processing_ms: 1.6514810381521605\n",
      "  time_since_restore: 2038.8468499183655\n",
      "  time_this_iter_s: 21.49211573600769\n",
      "  time_total_s: 2038.8468499183655\n",
      "  timers:\n",
      "    learn_throughput: 1437.426\n",
      "    learn_time_ms: 695.688\n",
      "    load_throughput: 41172.86\n",
      "    load_time_ms: 24.288\n",
      "    sample_throughput: 45.848\n",
      "    sample_time_ms: 21811.382\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1635064905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         2038.85</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\"> -3.1365</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            313.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-42-06\n",
      "  done: false\n",
      "  episode_len_mean: 313.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.137899999999977\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 266\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6749282505777148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00760371479244976\n",
      "          policy_loss: -0.0358060612446732\n",
      "          total_loss: -0.03806615066197183\n",
      "          vf_explained_var: 0.1269427239894867\n",
      "          vf_loss: 0.01372882093095945\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.66774193548388\n",
      "    ram_util_percent: 37.82580645161289\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039204844112857525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.088342933599147\n",
      "    mean_inference_ms: 1.9376152759342429\n",
      "    mean_raw_obs_processing_ms: 1.6545358587456063\n",
      "  time_since_restore: 2060.0910222530365\n",
      "  time_this_iter_s: 21.24417233467102\n",
      "  time_total_s: 2060.0910222530365\n",
      "  timers:\n",
      "    learn_throughput: 1436.615\n",
      "    learn_time_ms: 696.081\n",
      "    load_throughput: 41400.977\n",
      "    load_time_ms: 24.154\n",
      "    sample_throughput: 45.778\n",
      "    sample_time_ms: 21844.461\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1635064926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         2060.09</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\"> -3.1379</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            313.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-42-28\n",
      "  done: false\n",
      "  episode_len_mean: 314.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1417999999999764\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 269\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7101621945699057\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006984580369274498\n",
      "          policy_loss: -0.112366640733348\n",
      "          total_loss: -0.11453491035434935\n",
      "          vf_explained_var: 0.2773580849170685\n",
      "          vf_loss: 0.014234894338167376\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.29\n",
      "    ram_util_percent: 37.80666666666665\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919693611359369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.106264453703627\n",
      "    mean_inference_ms: 1.9374653812077072\n",
      "    mean_raw_obs_processing_ms: 1.6577542425553378\n",
      "  time_since_restore: 2081.2318341732025\n",
      "  time_this_iter_s: 21.140811920166016\n",
      "  time_total_s: 2081.2318341732025\n",
      "  timers:\n",
      "    learn_throughput: 1438.39\n",
      "    learn_time_ms: 695.222\n",
      "    load_throughput: 41084.94\n",
      "    load_time_ms: 24.34\n",
      "    sample_throughput: 45.455\n",
      "    sample_time_ms: 21999.675\n",
      "    update_time_ms: 2.397\n",
      "  timestamp: 1635064948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         2081.23</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\"> -3.1418</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            314.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-43-07\n",
      "  done: false\n",
      "  episode_len_mean: 313.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1354999999999764\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 273\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6462408728069728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009830532614694536\n",
      "          policy_loss: 0.008317708472410838\n",
      "          total_loss: 0.00789637714624405\n",
      "          vf_explained_var: 0.21232418715953827\n",
      "          vf_loss: 0.015058023958570428\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.291071428571435\n",
      "    ram_util_percent: 37.71964285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03918684270325176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.130488806616466\n",
      "    mean_inference_ms: 1.9372720549728664\n",
      "    mean_raw_obs_processing_ms: 1.6689817031537717\n",
      "  time_since_restore: 2120.7164855003357\n",
      "  time_this_iter_s: 39.48465132713318\n",
      "  time_total_s: 2120.7164855003357\n",
      "  timers:\n",
      "    learn_throughput: 1439.794\n",
      "    learn_time_ms: 694.544\n",
      "    load_throughput: 40678.429\n",
      "    load_time_ms: 24.583\n",
      "    sample_throughput: 45.21\n",
      "    sample_time_ms: 22119.125\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1635064987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         2120.72</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\"> -3.1355</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            313.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 313.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1352999999999764\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 276\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5951176603635153\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008278628382715465\n",
      "          policy_loss: 0.04997204376591576\n",
      "          total_loss: 0.046647879315747154\n",
      "          vf_explained_var: 0.22184894979000092\n",
      "          vf_loss: 0.011799148177184785\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.2375\n",
      "    ram_util_percent: 37.821875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0391796212902156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.14809484240101\n",
      "    mean_inference_ms: 1.9371342827240685\n",
      "    mean_raw_obs_processing_ms: 1.6776011761741685\n",
      "  time_since_restore: 2142.9720888137817\n",
      "  time_this_iter_s: 22.255603313446045\n",
      "  time_total_s: 2142.9720888137817\n",
      "  timers:\n",
      "    learn_throughput: 1439.833\n",
      "    learn_time_ms: 694.525\n",
      "    load_throughput: 39948.454\n",
      "    load_time_ms: 25.032\n",
      "    sample_throughput: 45.089\n",
      "    sample_time_ms: 22178.538\n",
      "    update_time_ms: 2.369\n",
      "  timestamp: 1635065009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         2142.97</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\"> -3.1353</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            313.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-43-50\n",
      "  done: false\n",
      "  episode_len_mean: 314.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1417999999999755\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 279\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6274190770255195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010129371840926953\n",
      "          policy_loss: -0.04981508155663808\n",
      "          total_loss: -0.05344880289501614\n",
      "          vf_explained_var: 0.26917564868927\n",
      "          vf_loss: 0.011627535482855618\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.936666666666675\n",
      "    ram_util_percent: 37.81333333333332\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917245359855645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.164402755247753\n",
      "    mean_inference_ms: 1.936998777838928\n",
      "    mean_raw_obs_processing_ms: 1.6814817446894463\n",
      "  time_since_restore: 2163.7321531772614\n",
      "  time_this_iter_s: 20.760064363479614\n",
      "  time_total_s: 2163.7321531772614\n",
      "  timers:\n",
      "    learn_throughput: 1439.112\n",
      "    learn_time_ms: 694.873\n",
      "    load_throughput: 40283.83\n",
      "    load_time_ms: 24.824\n",
      "    sample_throughput: 44.985\n",
      "    sample_time_ms: 22229.802\n",
      "    update_time_ms: 2.367\n",
      "  timestamp: 1635065030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         2163.73</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\"> -3.1418</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            314.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-44-12\n",
      "  done: false\n",
      "  episode_len_mean: 314.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.149599999999976\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 282\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.606866811381446\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010270151999380575\n",
      "          policy_loss: -0.10105655954943762\n",
      "          total_loss: -0.10047691820396318\n",
      "          vf_explained_var: 0.03916563093662262\n",
      "          vf_loss: 0.01562129601629244\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.125806451612895\n",
      "    ram_util_percent: 37.848387096774196\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039165230020004764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.180145102135665\n",
      "    mean_inference_ms: 1.936864040546415\n",
      "    mean_raw_obs_processing_ms: 1.6828840614191767\n",
      "  time_since_restore: 2185.482572078705\n",
      "  time_this_iter_s: 21.75041890144348\n",
      "  time_total_s: 2185.482572078705\n",
      "  timers:\n",
      "    learn_throughput: 1439.824\n",
      "    learn_time_ms: 694.53\n",
      "    load_throughput: 40148.944\n",
      "    load_time_ms: 24.907\n",
      "    sample_throughput: 44.706\n",
      "    sample_time_ms: 22368.47\n",
      "    update_time_ms: 2.364\n",
      "  timestamp: 1635065052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2185.48</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -3.1496</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            314.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-44-33\n",
      "  done: false\n",
      "  episode_len_mean: 315.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.1576999999999766\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 286\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.589032452636295\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007813415986917881\n",
      "          policy_loss: 0.023422385710808964\n",
      "          total_loss: 0.023287218891912036\n",
      "          vf_explained_var: 0.07909048348665237\n",
      "          vf_loss: 0.014973815851327446\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18620689655172\n",
      "    ram_util_percent: 37.89310344827586\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03915583400143718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.199069461089618\n",
      "    mean_inference_ms: 1.9366850144228431\n",
      "    mean_raw_obs_processing_ms: 1.685136004270119\n",
      "  time_since_restore: 2206.1718740463257\n",
      "  time_this_iter_s: 20.68930196762085\n",
      "  time_total_s: 2206.1718740463257\n",
      "  timers:\n",
      "    learn_throughput: 1441.938\n",
      "    learn_time_ms: 693.511\n",
      "    load_throughput: 40009.692\n",
      "    load_time_ms: 24.994\n",
      "    sample_throughput: 44.543\n",
      "    sample_time_ms: 22450.078\n",
      "    update_time_ms: 2.368\n",
      "  timestamp: 1635065073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         2206.17</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\"> -3.1577</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            315.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-44-53\n",
      "  done: false\n",
      "  episode_len_mean: 316.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.168899999999976\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 289\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5389278279410468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013799118568206679\n",
      "          policy_loss: 0.062084410753515035\n",
      "          total_loss: 0.05863683389292823\n",
      "          vf_explained_var: 0.008233300410211086\n",
      "          vf_loss: 0.010561791146109398\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.068965517241374\n",
      "    ram_util_percent: 37.9\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03914890308642801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.212231056708255\n",
      "    mean_inference_ms: 1.9365506255372664\n",
      "    mean_raw_obs_processing_ms: 1.6868877533756717\n",
      "  time_since_restore: 2226.5089087486267\n",
      "  time_this_iter_s: 20.337034702301025\n",
      "  time_total_s: 2226.5089087486267\n",
      "  timers:\n",
      "    learn_throughput: 1441.471\n",
      "    learn_time_ms: 693.736\n",
      "    load_throughput: 40045.982\n",
      "    load_time_ms: 24.971\n",
      "    sample_throughput: 44.809\n",
      "    sample_time_ms: 22316.841\n",
      "    update_time_ms: 2.346\n",
      "  timestamp: 1635065093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         2226.51</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\"> -3.1689</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            316.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 317.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.170699999999976\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 292\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5248416211869982\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005710764211398909\n",
      "          policy_loss: 0.05736920568678114\n",
      "          total_loss: 0.05433366596698761\n",
      "          vf_explained_var: 0.050971921533346176\n",
      "          vf_loss: 0.01164180071791634\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.015625\n",
      "    ram_util_percent: 37.881249999999994\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03914227610671032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.224776389487715\n",
      "    mean_inference_ms: 1.9364195950645424\n",
      "    mean_raw_obs_processing_ms: 1.688804807742744\n",
      "  time_since_restore: 2248.833399772644\n",
      "  time_this_iter_s: 22.324491024017334\n",
      "  time_total_s: 2248.833399772644\n",
      "  timers:\n",
      "    learn_throughput: 1444.146\n",
      "    learn_time_ms: 692.451\n",
      "    load_throughput: 39731.693\n",
      "    load_time_ms: 25.169\n",
      "    sample_throughput: 44.594\n",
      "    sample_time_ms: 22424.397\n",
      "    update_time_ms: 2.428\n",
      "  timestamp: 1635065115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         2248.83</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\"> -3.1707</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            317.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-45-36\n",
      "  done: false\n",
      "  episode_len_mean: 317.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.1771999999999765\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 295\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5029194010628595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010517524258031812\n",
      "          policy_loss: -0.05314660651816262\n",
      "          total_loss: -0.05219640897379981\n",
      "          vf_explained_var: 0.0041991122998297215\n",
      "          vf_loss: 0.01492763968805472\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.21\n",
      "    ram_util_percent: 37.886666666666656\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913610609526783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.236240691959427\n",
      "    mean_inference_ms: 1.9362911630629358\n",
      "    mean_raw_obs_processing_ms: 1.6908774842960403\n",
      "  time_since_restore: 2269.9574308395386\n",
      "  time_this_iter_s: 21.12403106689453\n",
      "  time_total_s: 2269.9574308395386\n",
      "  timers:\n",
      "    learn_throughput: 1443.421\n",
      "    learn_time_ms: 692.798\n",
      "    load_throughput: 39362.365\n",
      "    load_time_ms: 25.405\n",
      "    sample_throughput: 44.669\n",
      "    sample_time_ms: 22387.0\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1635065136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         2269.96</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -3.1772</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            317.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-45-59\n",
      "  done: false\n",
      "  episode_len_mean: 317.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.1784999999999752\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 299\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.512626404232449\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032040618836819313\n",
      "          policy_loss: 0.013867036832703485\n",
      "          total_loss: 0.016131694118181866\n",
      "          vf_explained_var: 0.06377854943275452\n",
      "          vf_loss: 0.01707051406717963\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.987878787878785\n",
      "    ram_util_percent: 37.84242424242424\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03912816319241561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.25103969608479\n",
      "    mean_inference_ms: 1.936122547555231\n",
      "    mean_raw_obs_processing_ms: 1.6937767895838847\n",
      "  time_since_restore: 2292.6063425540924\n",
      "  time_this_iter_s: 22.648911714553833\n",
      "  time_total_s: 2292.6063425540924\n",
      "  timers:\n",
      "    learn_throughput: 1442.394\n",
      "    learn_time_ms: 693.292\n",
      "    load_throughput: 40368.74\n",
      "    load_time_ms: 24.772\n",
      "    sample_throughput: 44.39\n",
      "    sample_time_ms: 22527.557\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1635065159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         2292.61</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\"> -3.1785</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            317.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-46-41\n",
      "  done: false\n",
      "  episode_len_mean: 317.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.173799999999976\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 302\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5499297314220004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008071487220084035\n",
      "          policy_loss: 0.002596844070487552\n",
      "          total_loss: -0.0012052971455785964\n",
      "          vf_explained_var: 0.2166038453578949\n",
      "          vf_loss: 0.01129358231685021\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.53898305084746\n",
      "    ram_util_percent: 37.69491525423729\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039122368249543585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.2623081740969\n",
      "    mean_inference_ms: 1.9359994067268815\n",
      "    mean_raw_obs_processing_ms: 1.700921332189206\n",
      "  time_since_restore: 2334.3252403736115\n",
      "  time_this_iter_s: 41.71889781951904\n",
      "  time_total_s: 2334.3252403736115\n",
      "  timers:\n",
      "    learn_throughput: 1439.313\n",
      "    learn_time_ms: 694.776\n",
      "    load_throughput: 40506.947\n",
      "    load_time_ms: 24.687\n",
      "    sample_throughput: 40.677\n",
      "    sample_time_ms: 24583.982\n",
      "    update_time_ms: 2.477\n",
      "  timestamp: 1635065201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         2334.33</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\"> -3.1738</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            317.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-47-04\n",
      "  done: false\n",
      "  episode_len_mean: 316.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.1642999999999764\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 306\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5979947527249654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009725948693696139\n",
      "          policy_loss: 0.016769379874070487\n",
      "          total_loss: 0.01494353852338261\n",
      "          vf_explained_var: 0.3012915551662445\n",
      "          vf_loss: 0.013667807386567195\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.330303030303035\n",
      "    ram_util_percent: 37.654545454545456\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911476263864879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.27749104616056\n",
      "    mean_inference_ms: 1.9358368279568763\n",
      "    mean_raw_obs_processing_ms: 1.7105111910608466\n",
      "  time_since_restore: 2357.3142881393433\n",
      "  time_this_iter_s: 22.98904776573181\n",
      "  time_total_s: 2357.3142881393433\n",
      "  timers:\n",
      "    learn_throughput: 1438.729\n",
      "    learn_time_ms: 695.058\n",
      "    load_throughput: 41216.881\n",
      "    load_time_ms: 24.262\n",
      "    sample_throughput: 43.602\n",
      "    sample_time_ms: 22934.533\n",
      "    update_time_ms: 2.499\n",
      "  timestamp: 1635065224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         2357.31</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\"> -3.1643</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            316.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 315.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.1520999999999764\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 309\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5726515094439188\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009904759759418229\n",
      "          policy_loss: 0.040094629757934146\n",
      "          total_loss: 0.030706160763899485\n",
      "          vf_explained_var: 0.5486465692520142\n",
      "          vf_loss: 0.005842808480115814\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03636363636364\n",
      "    ram_util_percent: 37.830303030303035\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910917242114021\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.28924977473808\n",
      "    mean_inference_ms: 1.9357171870140786\n",
      "    mean_raw_obs_processing_ms: 1.71558518368704\n",
      "  time_since_restore: 2379.9030973911285\n",
      "  time_this_iter_s: 22.58880925178528\n",
      "  time_total_s: 2379.9030973911285\n",
      "  timers:\n",
      "    learn_throughput: 1437.775\n",
      "    learn_time_ms: 695.519\n",
      "    load_throughput: 41848.588\n",
      "    load_time_ms: 23.896\n",
      "    sample_throughput: 43.539\n",
      "    sample_time_ms: 22967.756\n",
      "    update_time_ms: 2.492\n",
      "  timestamp: 1635065246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">          2379.9</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -3.1521</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            315.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-47-49\n",
      "  done: false\n",
      "  episode_len_mean: 315.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.1510999999999765\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 313\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5890247013833787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008981062194893339\n",
      "          policy_loss: 0.030209820303652022\n",
      "          total_loss: 0.030097718867990706\n",
      "          vf_explained_var: 0.4187091290950775\n",
      "          vf_loss: 0.01532909314458569\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.554838709677405\n",
      "    ram_util_percent: 37.896774193548396\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0391020728698519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.304416186047185\n",
      "    mean_inference_ms: 1.935560343004309\n",
      "    mean_raw_obs_processing_ms: 1.716645051321533\n",
      "  time_since_restore: 2402.2737398147583\n",
      "  time_this_iter_s: 22.37064242362976\n",
      "  time_total_s: 2402.2737398147583\n",
      "  timers:\n",
      "    learn_throughput: 1438.279\n",
      "    learn_time_ms: 695.276\n",
      "    load_throughput: 41618.499\n",
      "    load_time_ms: 24.028\n",
      "    sample_throughput: 43.236\n",
      "    sample_time_ms: 23128.881\n",
      "    update_time_ms: 2.535\n",
      "  timestamp: 1635065269\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         2402.27</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\"> -3.1511</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            315.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-48-11\n",
      "  done: false\n",
      "  episode_len_mean: 315.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.1508999999999765\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 316\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.679560375213623\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00954493927131534\n",
      "          policy_loss: 0.035571161243650645\n",
      "          total_loss: 0.02958140356673135\n",
      "          vf_explained_var: 0.5522876977920532\n",
      "          vf_loss: 0.010328598268097266\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.840625\n",
      "    ram_util_percent: 37.934375\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039096891758755976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.315184319776417\n",
      "    mean_inference_ms: 1.9354449059712227\n",
      "    mean_raw_obs_processing_ms: 1.7176897852456645\n",
      "  time_since_restore: 2424.149382829666\n",
      "  time_this_iter_s: 21.875643014907837\n",
      "  time_total_s: 2424.149382829666\n",
      "  timers:\n",
      "    learn_throughput: 1436.128\n",
      "    learn_time_ms: 696.317\n",
      "    load_throughput: 41647.84\n",
      "    load_time_ms: 24.011\n",
      "    sample_throughput: 43.214\n",
      "    sample_time_ms: 23140.388\n",
      "    update_time_ms: 2.535\n",
      "  timestamp: 1635065291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         2424.15</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\"> -3.1509</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            315.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-48-32\n",
      "  done: false\n",
      "  episode_len_mean: 315.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.1563999999999766\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 319\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.696572204430898\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015785457157608027\n",
      "          policy_loss: 0.005114524728722042\n",
      "          total_loss: 0.0017353615827030607\n",
      "          vf_explained_var: 0.4749094247817993\n",
      "          vf_loss: 0.012797282918149399\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.970000000000006\n",
      "    ram_util_percent: 37.92666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039091823037148726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.325594019411394\n",
      "    mean_inference_ms: 1.9353297910307492\n",
      "    mean_raw_obs_processing_ms: 1.7186937302578005\n",
      "  time_since_restore: 2445.2415854930878\n",
      "  time_this_iter_s: 21.09220266342163\n",
      "  time_total_s: 2445.2415854930878\n",
      "  timers:\n",
      "    learn_throughput: 1434.507\n",
      "    learn_time_ms: 697.103\n",
      "    load_throughput: 41401.181\n",
      "    load_time_ms: 24.154\n",
      "    sample_throughput: 43.141\n",
      "    sample_time_ms: 23179.754\n",
      "    update_time_ms: 2.521\n",
      "  timestamp: 1635065312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         2445.24</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\"> -3.1564</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            315.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-48-53\n",
      "  done: false\n",
      "  episode_len_mean: 316.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.165199999999976\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 322\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6839678353733487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013764360560417464\n",
      "          policy_loss: -0.014123290156324705\n",
      "          total_loss: -0.016311274303330316\n",
      "          vf_explained_var: 0.3240639269351959\n",
      "          vf_loss: 0.01396347563713789\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.02000000000001\n",
      "    ram_util_percent: 37.92666666666666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0390868659494056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.335630853307492\n",
      "    mean_inference_ms: 1.93521564357824\n",
      "    mean_raw_obs_processing_ms: 1.719845901832141\n",
      "  time_since_restore: 2466.4034938812256\n",
      "  time_this_iter_s: 21.161908388137817\n",
      "  time_total_s: 2466.4034938812256\n",
      "  timers:\n",
      "    learn_throughput: 1436.036\n",
      "    learn_time_ms: 696.361\n",
      "    load_throughput: 41651.397\n",
      "    load_time_ms: 24.009\n",
      "    sample_throughput: 42.987\n",
      "    sample_time_ms: 23263.072\n",
      "    update_time_ms: 2.55\n",
      "  timestamp: 1635065333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">          2466.4</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -3.1652</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            316.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-49-14\n",
      "  done: false\n",
      "  episode_len_mean: 315.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.156499999999976\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 325\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6224944922659132\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012492815251706466\n",
      "          policy_loss: -0.09686867656807105\n",
      "          total_loss: -0.09479828737676144\n",
      "          vf_explained_var: 0.21790704131126404\n",
      "          vf_loss: 0.0176706966633598\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.189655172413794\n",
      "    ram_util_percent: 37.88965517241378\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039082118934506214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.345709278369924\n",
      "    mean_inference_ms: 1.9351026382269105\n",
      "    mean_raw_obs_processing_ms: 1.7211396091211089\n",
      "  time_since_restore: 2486.991464138031\n",
      "  time_this_iter_s: 20.58797025680542\n",
      "  time_total_s: 2486.991464138031\n",
      "  timers:\n",
      "    learn_throughput: 1435.773\n",
      "    learn_time_ms: 696.489\n",
      "    load_throughput: 41814.711\n",
      "    load_time_ms: 23.915\n",
      "    sample_throughput: 43.31\n",
      "    sample_time_ms: 23089.464\n",
      "    update_time_ms: 2.471\n",
      "  timestamp: 1635065354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         2486.99</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\"> -3.1565</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            315.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-49-36\n",
      "  done: false\n",
      "  episode_len_mean: 314.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.1418999999999766\n",
      "  episode_reward_min: -3.7199999999999647\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 329\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6032930705282422\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008145970624320419\n",
      "          policy_loss: -0.015637757173842855\n",
      "          total_loss: -0.0142878041913112\n",
      "          vf_explained_var: 0.04749152809381485\n",
      "          vf_loss: 0.016975581459701062\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.065625\n",
      "    ram_util_percent: 37.887499999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03907615552725227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.359522092802525\n",
      "    mean_inference_ms: 1.9349533963238261\n",
      "    mean_raw_obs_processing_ms: 1.7231726374758722\n",
      "  time_since_restore: 2508.9408638477325\n",
      "  time_this_iter_s: 21.949399709701538\n",
      "  time_total_s: 2508.9408638477325\n",
      "  timers:\n",
      "    learn_throughput: 1435.882\n",
      "    learn_time_ms: 696.436\n",
      "    load_throughput: 41734.285\n",
      "    load_time_ms: 23.961\n",
      "    sample_throughput: 43.155\n",
      "    sample_time_ms: 23172.02\n",
      "    update_time_ms: 2.474\n",
      "  timestamp: 1635065376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         2508.94</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\"> -3.1419</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.72</td><td style=\"text-align: right;\">            314.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-50-14\n",
      "  done: false\n",
      "  episode_len_mean: 313.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.1320999999999772\n",
      "  episode_reward_min: -3.7199999999999647\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 332\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.600946275393168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011185916897121393\n",
      "          policy_loss: 0.04411390937036938\n",
      "          total_loss: 0.04018847288356887\n",
      "          vf_explained_var: 0.1886018067598343\n",
      "          vf_loss: 0.011524732452946612\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.50363636363637\n",
      "    ram_util_percent: 37.81636363636363\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03907180088417315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.370373983448108\n",
      "    mean_inference_ms: 1.9348434676933828\n",
      "    mean_raw_obs_processing_ms: 1.7292687597242378\n",
      "  time_since_restore: 2547.7622764110565\n",
      "  time_this_iter_s: 38.821412563323975\n",
      "  time_total_s: 2547.7622764110565\n",
      "  timers:\n",
      "    learn_throughput: 1436.601\n",
      "    learn_time_ms: 696.087\n",
      "    load_throughput: 40245.833\n",
      "    load_time_ms: 24.847\n",
      "    sample_throughput: 40.341\n",
      "    sample_time_ms: 24788.79\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1635065414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         2547.76</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\"> -3.1321</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.72</td><td style=\"text-align: right;\">            313.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-50-36\n",
      "  done: false\n",
      "  episode_len_mean: 311.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.119299999999977\n",
      "  episode_reward_min: -3.709999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 335\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5212813072734408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010834865573946039\n",
      "          policy_loss: 0.01840950118170844\n",
      "          total_loss: 0.01515498459339142\n",
      "          vf_explained_var: 0.05161081254482269\n",
      "          vf_loss: 0.011416552677918744\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.193548387096776\n",
      "    ram_util_percent: 37.8290322580645\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906768465601332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.38132648616302\n",
      "    mean_inference_ms: 1.9347367521930232\n",
      "    mean_raw_obs_processing_ms: 1.7354472289218805\n",
      "  time_since_restore: 2569.2016971111298\n",
      "  time_this_iter_s: 21.439420700073242\n",
      "  time_total_s: 2569.2016971111298\n",
      "  timers:\n",
      "    learn_throughput: 1439.318\n",
      "    learn_time_ms: 694.774\n",
      "    load_throughput: 40176.134\n",
      "    load_time_ms: 24.89\n",
      "    sample_throughput: 43.933\n",
      "    sample_time_ms: 22762.107\n",
      "    update_time_ms: 2.443\n",
      "  timestamp: 1635065436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">          2569.2</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -3.1193</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.71</td><td style=\"text-align: right;\">            311.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-50-57\n",
      "  done: false\n",
      "  episode_len_mean: 311.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.114399999999977\n",
      "  episode_reward_min: -3.6899999999999653\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 338\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.544397058751848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006581315336038429\n",
      "          policy_loss: 0.007891623179117839\n",
      "          total_loss: 0.0050626706745889455\n",
      "          vf_explained_var: 0.01625249907374382\n",
      "          vf_loss: 0.012285952527438186\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.09666666666668\n",
      "    ram_util_percent: 37.83333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039063756186463995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.392413218922112\n",
      "    mean_inference_ms: 1.9346323535709504\n",
      "    mean_raw_obs_processing_ms: 1.7417049142925538\n",
      "  time_since_restore: 2590.0841891765594\n",
      "  time_this_iter_s: 20.882492065429688\n",
      "  time_total_s: 2590.0841891765594\n",
      "  timers:\n",
      "    learn_throughput: 1439.393\n",
      "    learn_time_ms: 694.737\n",
      "    load_throughput: 39671.678\n",
      "    load_time_ms: 25.207\n",
      "    sample_throughput: 44.344\n",
      "    sample_time_ms: 22551.198\n",
      "    update_time_ms: 2.432\n",
      "  timestamp: 1635065457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         2590.08</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\"> -3.1144</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.69</td><td style=\"text-align: right;\">            311.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-51-17\n",
      "  done: false\n",
      "  episode_len_mean: 311.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.1169999999999773\n",
      "  episode_reward_min: -3.6899999999999653\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 341\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5839109235339695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008720769846776477\n",
      "          policy_loss: -0.09397589597437117\n",
      "          total_loss: -0.09354680197106467\n",
      "          vf_explained_var: 0.06265605241060257\n",
      "          vf_loss: 0.015832163848810726\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.09655172413794\n",
      "    ram_util_percent: 37.87586206896552\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905985057333027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.403348938336528\n",
      "    mean_inference_ms: 1.9345266113596709\n",
      "    mean_raw_obs_processing_ms: 1.744018626016849\n",
      "  time_since_restore: 2610.354379415512\n",
      "  time_this_iter_s: 20.270190238952637\n",
      "  time_total_s: 2610.354379415512\n",
      "  timers:\n",
      "    learn_throughput: 1438.033\n",
      "    learn_time_ms: 695.394\n",
      "    load_throughput: 39663.499\n",
      "    load_time_ms: 25.212\n",
      "    sample_throughput: 44.806\n",
      "    sample_time_ms: 22318.614\n",
      "    update_time_ms: 2.477\n",
      "  timestamp: 1635065477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         2610.35</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">  -3.117</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.69</td><td style=\"text-align: right;\">             311.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-51-37\n",
      "  done: false\n",
      "  episode_len_mean: 311.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.1178999999999775\n",
      "  episode_reward_min: -3.6899999999999653\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 344\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5088275816705492\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017226692240642195\n",
      "          policy_loss: -0.11249543925126394\n",
      "          total_loss: -0.1123393072022332\n",
      "          vf_explained_var: 0.17377817630767822\n",
      "          vf_loss: 0.014383074672271809\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.17142857142857\n",
      "    ram_util_percent: 37.92857142857143\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039056021694964194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.413734856972834\n",
      "    mean_inference_ms: 1.9344194652092555\n",
      "    mean_raw_obs_processing_ms: 1.744437787158973\n",
      "  time_since_restore: 2630.326904773712\n",
      "  time_this_iter_s: 19.972525358200073\n",
      "  time_total_s: 2630.326904773712\n",
      "  timers:\n",
      "    learn_throughput: 1439.48\n",
      "    learn_time_ms: 694.695\n",
      "    load_throughput: 39629.471\n",
      "    load_time_ms: 25.234\n",
      "    sample_throughput: 45.291\n",
      "    sample_time_ms: 22079.532\n",
      "    update_time_ms: 2.443\n",
      "  timestamp: 1635065497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         2630.33</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\"> -3.1179</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.69</td><td style=\"text-align: right;\">            311.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 311.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.114999999999978\n",
      "  episode_reward_min: -3.6899999999999653\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 347\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.468955041302575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013009816499564881\n",
      "          policy_loss: -0.13142916940980487\n",
      "          total_loss: -0.13084577959444788\n",
      "          vf_explained_var: 0.19104112684726715\n",
      "          vf_loss: 0.014622450371583303\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06129032258065\n",
      "    ram_util_percent: 37.93870967741936\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039052331438959614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.424174506204025\n",
      "    mean_inference_ms: 1.9343136711699669\n",
      "    mean_raw_obs_processing_ms: 1.7449919237709528\n",
      "  time_since_restore: 2651.7973923683167\n",
      "  time_this_iter_s: 21.470487594604492\n",
      "  time_total_s: 2651.7973923683167\n",
      "  timers:\n",
      "    learn_throughput: 1442.564\n",
      "    learn_time_ms: 693.21\n",
      "    load_throughput: 39507.408\n",
      "    load_time_ms: 25.312\n",
      "    sample_throughput: 45.371\n",
      "    sample_time_ms: 22040.426\n",
      "    update_time_ms: 2.43\n",
      "  timestamp: 1635065519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">          2651.8</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  -3.115</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.69</td><td style=\"text-align: right;\">             311.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-52-20\n",
      "  done: false\n",
      "  episode_len_mean: 310.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.101299999999978\n",
      "  episode_reward_min: -3.6899999999999653\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 351\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3730272120899625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011765386971199129\n",
      "          policy_loss: 0.03987538640697797\n",
      "          total_loss: 0.038018659502267835\n",
      "          vf_explained_var: 0.15319298207759857\n",
      "          vf_loss: 0.011285273084003065\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.938709677419354\n",
      "    ram_util_percent: 37.8774193548387\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039047635766469184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.43840081789945\n",
      "    mean_inference_ms: 1.9341757027105615\n",
      "    mean_raw_obs_processing_ms: 1.7459666392342768\n",
      "  time_since_restore: 2673.4836740493774\n",
      "  time_this_iter_s: 21.68628168106079\n",
      "  time_total_s: 2673.4836740493774\n",
      "  timers:\n",
      "    learn_throughput: 1446.365\n",
      "    learn_time_ms: 691.389\n",
      "    load_throughput: 39351.323\n",
      "    load_time_ms: 25.412\n",
      "    sample_throughput: 45.246\n",
      "    sample_time_ms: 22101.474\n",
      "    update_time_ms: 2.449\n",
      "  timestamp: 1635065540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         2673.48</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\"> -3.1013</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.69</td><td style=\"text-align: right;\">            310.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-52-43\n",
      "  done: false\n",
      "  episode_len_mean: 308.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.085799999999978\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 354\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3024946636623806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007516454506898442\n",
      "          policy_loss: 0.045140905098782644\n",
      "          total_loss: 0.040480034136109885\n",
      "          vf_explained_var: 0.2759151756763458\n",
      "          vf_loss: 0.007988252574836627\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.228125\n",
      "    ram_util_percent: 37.89375\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039044315259456196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.44965682378115\n",
      "    mean_inference_ms: 1.9340751677905628\n",
      "    mean_raw_obs_processing_ms: 1.746944681367387\n",
      "  time_since_restore: 2696.063006401062\n",
      "  time_this_iter_s: 22.57933235168457\n",
      "  time_total_s: 2696.063006401062\n",
      "  timers:\n",
      "    learn_throughput: 1445.528\n",
      "    learn_time_ms: 691.789\n",
      "    load_throughput: 38979.615\n",
      "    load_time_ms: 25.654\n",
      "    sample_throughput: 44.959\n",
      "    sample_time_ms: 22242.614\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1635065563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         2696.06</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\"> -3.0858</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            308.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 306.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.0662999999999783\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 358\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1853251973787944\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009324330988134713\n",
      "          policy_loss: 0.010402016590038936\n",
      "          total_loss: 0.014479472984870274\n",
      "          vf_explained_var: 0.12596666812896729\n",
      "          vf_loss: 0.015464491604102983\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.01470588235294\n",
      "    ram_util_percent: 37.88235294117647\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039040024747035364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.46490307191034\n",
      "    mean_inference_ms: 1.933944077120316\n",
      "    mean_raw_obs_processing_ms: 1.7485698725703152\n",
      "  time_since_restore: 2719.7143847942352\n",
      "  time_this_iter_s: 23.651378393173218\n",
      "  time_total_s: 2719.7143847942352\n",
      "  timers:\n",
      "    learn_throughput: 1442.385\n",
      "    learn_time_ms: 693.296\n",
      "    load_throughput: 39305.411\n",
      "    load_time_ms: 25.442\n",
      "    sample_throughput: 44.35\n",
      "    sample_time_ms: 22547.684\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1635065587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         2719.71</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\"> -3.0663</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            306.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-53-49\n",
      "  done: false\n",
      "  episode_len_mean: 304.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.0445999999999787\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 362\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2571058048142327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005870147967576905\n",
      "          policy_loss: 0.01294346335861418\n",
      "          total_loss: 0.013845172441667981\n",
      "          vf_explained_var: 0.33719679713249207\n",
      "          vf_loss: 0.013179261620259947\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.59344262295083\n",
      "    ram_util_percent: 37.772131147540996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903584987557538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.48115040143482\n",
      "    mean_inference_ms: 1.933816531322273\n",
      "    mean_raw_obs_processing_ms: 1.7559792271944736\n",
      "  time_since_restore: 2762.306251525879\n",
      "  time_this_iter_s: 42.59186673164368\n",
      "  time_total_s: 2762.306251525879\n",
      "  timers:\n",
      "    learn_throughput: 1443.84\n",
      "    learn_time_ms: 692.597\n",
      "    load_throughput: 39261.407\n",
      "    load_time_ms: 25.47\n",
      "    sample_throughput: 40.63\n",
      "    sample_time_ms: 24612.629\n",
      "    update_time_ms: 2.392\n",
      "  timestamp: 1635065629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         2762.31</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -3.0446</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            304.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-54-14\n",
      "  done: false\n",
      "  episode_len_mean: 303.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.0319999999999796\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 365\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1044425262345208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009241080531303207\n",
      "          policy_loss: -0.056386112421751025\n",
      "          total_loss: -0.055919013255172303\n",
      "          vf_explained_var: 0.30345505475997925\n",
      "          vf_loss: 0.011049470584839582\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.33714285714286\n",
      "    ram_util_percent: 37.871428571428574\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039032853818949505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.493881238593623\n",
      "    mean_inference_ms: 1.9337244437342755\n",
      "    mean_raw_obs_processing_ms: 1.7616797224980076\n",
      "  time_since_restore: 2786.952456474304\n",
      "  time_this_iter_s: 24.646204948425293\n",
      "  time_total_s: 2786.952456474304\n",
      "  timers:\n",
      "    learn_throughput: 1445.363\n",
      "    learn_time_ms: 691.868\n",
      "    load_throughput: 39411.779\n",
      "    load_time_ms: 25.373\n",
      "    sample_throughput: 43.112\n",
      "    sample_time_ms: 23195.572\n",
      "    update_time_ms: 2.736\n",
      "  timestamp: 1635065654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         2786.95</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">  -3.032</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">             303.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-54-38\n",
      "  done: false\n",
      "  episode_len_mean: 301.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.01279999999998\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 369\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.123243839210934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007881958498559606\n",
      "          policy_loss: 0.009612794468800227\n",
      "          total_loss: 0.009844052212105856\n",
      "          vf_explained_var: 0.20023944973945618\n",
      "          vf_loss: 0.011069597327150404\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.12647058823529\n",
      "    ram_util_percent: 37.8735294117647\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03902896246795159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.511497052217535\n",
      "    mean_inference_ms: 1.933606766330966\n",
      "    mean_raw_obs_processing_ms: 1.7694224999012707\n",
      "  time_since_restore: 2811.002677679062\n",
      "  time_this_iter_s: 24.05022120475769\n",
      "  time_total_s: 2811.002677679062\n",
      "  timers:\n",
      "    learn_throughput: 1446.632\n",
      "    learn_time_ms: 691.261\n",
      "    load_throughput: 39402.93\n",
      "    load_time_ms: 25.379\n",
      "    sample_throughput: 42.631\n",
      "    sample_time_ms: 23457.128\n",
      "    update_time_ms: 2.738\n",
      "  timestamp: 1635065678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">            2811</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\"> -3.0128</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            301.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-55-03\n",
      "  done: false\n",
      "  episode_len_mean: 299.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.9959999999999805\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 373\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0491707583268484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005694415545187641\n",
      "          policy_loss: 0.02298137843608856\n",
      "          total_loss: 0.026886275907357534\n",
      "          vf_explained_var: 0.2495683878660202\n",
      "          vf_loss: 0.014111885024855534\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.7638888888889\n",
      "    ram_util_percent: 38.011111111111106\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03902517018615931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.529359255106638\n",
      "    mean_inference_ms: 1.9335001464746022\n",
      "    mean_raw_obs_processing_ms: 1.7705441582157755\n",
      "  time_since_restore: 2836.279701948166\n",
      "  time_this_iter_s: 25.277024269104004\n",
      "  time_total_s: 2836.279701948166\n",
      "  timers:\n",
      "    learn_throughput: 1441.08\n",
      "    learn_time_ms: 693.924\n",
      "    load_throughput: 39471.456\n",
      "    load_time_ms: 25.335\n",
      "    sample_throughput: 41.852\n",
      "    sample_time_ms: 23893.925\n",
      "    update_time_ms: 2.735\n",
      "  timestamp: 1635065703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         2836.28</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">  -2.996</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">             299.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-55-30\n",
      "  done: false\n",
      "  episode_len_mean: 297.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.978499999999981\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 377\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1118462867206997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007768919009734917\n",
      "          policy_loss: 0.021045209136274126\n",
      "          total_loss: 0.026151558922396765\n",
      "          vf_explained_var: 0.10495080053806305\n",
      "          vf_loss: 0.01583636553130216\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.32051282051282\n",
      "    ram_util_percent: 38.02051282051283\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039021919189486394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.548250535649814\n",
      "    mean_inference_ms: 1.9334079984076293\n",
      "    mean_raw_obs_processing_ms: 1.7719794132463778\n",
      "  time_since_restore: 2863.1783697605133\n",
      "  time_this_iter_s: 26.898667812347412\n",
      "  time_total_s: 2863.1783697605133\n",
      "  timers:\n",
      "    learn_throughput: 1431.032\n",
      "    learn_time_ms: 698.796\n",
      "    load_throughput: 39526.806\n",
      "    load_time_ms: 25.299\n",
      "    sample_throughput: 40.73\n",
      "    sample_time_ms: 24551.979\n",
      "    update_time_ms: 2.709\n",
      "  timestamp: 1635065730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         2863.18</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -2.9785</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            297.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-55-56\n",
      "  done: false\n",
      "  episode_len_mean: 295.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.95589999999998\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 381\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2326341536309984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008230695940908727\n",
      "          policy_loss: -0.010741967583696048\n",
      "          total_loss: -0.007777122408151626\n",
      "          vf_explained_var: 0.2009439915418625\n",
      "          vf_loss: 0.014879653064741028\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.17837837837837\n",
      "    ram_util_percent: 37.9972972972973\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039019302454400974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.568337706562644\n",
      "    mean_inference_ms: 1.9333306259807006\n",
      "    mean_raw_obs_processing_ms: 1.7737140900631463\n",
      "  time_since_restore: 2889.290231704712\n",
      "  time_this_iter_s: 26.11186194419861\n",
      "  time_total_s: 2889.290231704712\n",
      "  timers:\n",
      "    learn_throughput: 1427.069\n",
      "    learn_time_ms: 700.737\n",
      "    load_throughput: 39462.172\n",
      "    load_time_ms: 25.341\n",
      "    sample_throughput: 39.739\n",
      "    sample_time_ms: 25163.896\n",
      "    update_time_ms: 2.738\n",
      "  timestamp: 1635065756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         2889.29</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\"> -2.9559</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            295.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-56-21\n",
      "  done: false\n",
      "  episode_len_mean: 294.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.9478999999999815\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 384\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2679143243365818\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007032834181557254\n",
      "          policy_loss: -0.0036859283016787633\n",
      "          total_loss: -0.007553822878334258\n",
      "          vf_explained_var: 0.44428351521492004\n",
      "          vf_loss: 0.008459605566329426\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.12285714285715\n",
      "    ram_util_percent: 38.08857142857143\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901774551127041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.583950441928405\n",
      "    mean_inference_ms: 1.933280604278798\n",
      "    mean_raw_obs_processing_ms: 1.7751809250649415\n",
      "  time_since_restore: 2913.762984275818\n",
      "  time_this_iter_s: 24.472752571105957\n",
      "  time_total_s: 2913.762984275818\n",
      "  timers:\n",
      "    learn_throughput: 1424.705\n",
      "    learn_time_ms: 701.9\n",
      "    load_throughput: 39350.436\n",
      "    load_time_ms: 25.413\n",
      "    sample_throughput: 39.273\n",
      "    sample_time_ms: 25462.873\n",
      "    update_time_ms: 2.753\n",
      "  timestamp: 1635065781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         2913.76</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\"> -2.9479</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            294.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-56-45\n",
      "  done: false\n",
      "  episode_len_mean: 292.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.926899999999981\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 388\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1959870828522576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006773082613422766\n",
      "          policy_loss: 0.020623510165346994\n",
      "          total_loss: 0.020021780497497983\n",
      "          vf_explained_var: 0.4884040057659149\n",
      "          vf_loss: 0.011019484791904687\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.35142857142857\n",
      "    ram_util_percent: 38.082857142857144\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901589018938774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.60585772390668\n",
      "    mean_inference_ms: 1.9332211703562683\n",
      "    mean_raw_obs_processing_ms: 1.777162403683285\n",
      "  time_since_restore: 2938.3976209163666\n",
      "  time_this_iter_s: 24.634636640548706\n",
      "  time_total_s: 2938.3976209163666\n",
      "  timers:\n",
      "    learn_throughput: 1419.701\n",
      "    learn_time_ms: 704.374\n",
      "    load_throughput: 39561.59\n",
      "    load_time_ms: 25.277\n",
      "    sample_throughput: 38.827\n",
      "    sample_time_ms: 25755.442\n",
      "    update_time_ms: 2.755\n",
      "  timestamp: 1635065805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">          2938.4</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\"> -2.9269</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            292.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-57-28\n",
      "  done: false\n",
      "  episode_len_mean: 290.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.9097999999999815\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 392\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.17236524687873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014280831464861551\n",
      "          policy_loss: 0.02078663408756256\n",
      "          total_loss: 0.019469421522484886\n",
      "          vf_explained_var: 0.6619657874107361\n",
      "          vf_loss: 0.009692398277628753\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.70666666666668\n",
      "    ram_util_percent: 38.105000000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039014336826674365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.62834676989114\n",
      "    mean_inference_ms: 1.933170465484573\n",
      "    mean_raw_obs_processing_ms: 1.784682839800039\n",
      "  time_since_restore: 2980.55996966362\n",
      "  time_this_iter_s: 42.16234874725342\n",
      "  time_total_s: 2980.55996966362\n",
      "  timers:\n",
      "    learn_throughput: 1421.852\n",
      "    learn_time_ms: 703.308\n",
      "    load_throughput: 39907.783\n",
      "    load_time_ms: 25.058\n",
      "    sample_throughput: 36.082\n",
      "    sample_time_ms: 27715.023\n",
      "    update_time_ms: 2.748\n",
      "  timestamp: 1635065848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         2980.56</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -2.9098</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            290.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-57-53\n",
      "  done: false\n",
      "  episode_len_mean: 289.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.896299999999982\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 395\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.106015051735772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008772367645833196\n",
      "          policy_loss: -0.09768587127327918\n",
      "          total_loss: -0.099592900607321\n",
      "          vf_explained_var: 0.6507740020751953\n",
      "          vf_loss: 0.008714503349943293\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.54166666666666\n",
      "    ram_util_percent: 38.044444444444444\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039012977466193255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.64583858275688\n",
      "    mean_inference_ms: 1.9331342568064012\n",
      "    mean_raw_obs_processing_ms: 1.7904800748868224\n",
      "  time_since_restore: 3005.8824315071106\n",
      "  time_this_iter_s: 25.3224618434906\n",
      "  time_total_s: 3005.8824315071106\n",
      "  timers:\n",
      "    learn_throughput: 1421.551\n",
      "    learn_time_ms: 703.457\n",
      "    load_throughput: 39679.296\n",
      "    load_time_ms: 25.202\n",
      "    sample_throughput: 35.866\n",
      "    sample_time_ms: 27881.812\n",
      "    update_time_ms: 2.759\n",
      "  timestamp: 1635065873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         3005.88</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\"> -2.8963</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            289.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 288.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.8854999999999817\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 399\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1084800958633423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008688004926471276\n",
      "          policy_loss: -0.01708822896083196\n",
      "          total_loss: -0.020925074484613206\n",
      "          vf_explained_var: 0.761309027671814\n",
      "          vf_loss: 0.0068135530222207304\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.04571428571428\n",
      "    ram_util_percent: 38.08857142857143\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901120617836623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.66931503140264\n",
      "    mean_inference_ms: 1.9330872878462162\n",
      "    mean_raw_obs_processing_ms: 1.7982708030643928\n",
      "  time_since_restore: 3030.258907556534\n",
      "  time_this_iter_s: 24.376476049423218\n",
      "  time_total_s: 3030.258907556534\n",
      "  timers:\n",
      "    learn_throughput: 1421.675\n",
      "    learn_time_ms: 703.396\n",
      "    load_throughput: 39824.194\n",
      "    load_time_ms: 25.11\n",
      "    sample_throughput: 38.372\n",
      "    sample_time_ms: 26060.361\n",
      "    update_time_ms: 2.787\n",
      "  timestamp: 1635065897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         3030.26</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\"> -2.8855</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            288.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 287.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.8783999999999828\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 403\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1261211680041419\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010880533286088104\n",
      "          policy_loss: 0.031288726462258235\n",
      "          total_loss: 0.027126348349783155\n",
      "          vf_explained_var: 0.8040520548820496\n",
      "          vf_loss: 0.0065548059013154775\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.178378378378376\n",
      "    ram_util_percent: 38.235135135135145\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039009694956201636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.692319387009416\n",
      "    mean_inference_ms: 1.93305212209338\n",
      "    mean_raw_obs_processing_ms: 1.7999012946374717\n",
      "  time_since_restore: 3056.1796736717224\n",
      "  time_this_iter_s: 25.9207661151886\n",
      "  time_total_s: 3056.1796736717224\n",
      "  timers:\n",
      "    learn_throughput: 1417.893\n",
      "    learn_time_ms: 705.272\n",
      "    load_throughput: 40220.477\n",
      "    load_time_ms: 24.863\n",
      "    sample_throughput: 38.188\n",
      "    sample_time_ms: 26186.536\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1635065923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         3056.18</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\"> -2.8784</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            287.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-59-10\n",
      "  done: false\n",
      "  episode_len_mean: 286.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.8683999999999834\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 407\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9526654150750902\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005081121499039851\n",
      "          policy_loss: 0.014915872696373197\n",
      "          total_loss: 0.014313240266508527\n",
      "          vf_explained_var: 0.718739926815033\n",
      "          vf_loss: 0.008669964094749755\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.32051282051283\n",
      "    ram_util_percent: 38.73589743589743\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039008551988190865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.716098928910778\n",
      "    mean_inference_ms: 1.9330344356798241\n",
      "    mean_raw_obs_processing_ms: 1.8016452409211912\n",
      "  time_since_restore: 3083.088485956192\n",
      "  time_this_iter_s: 26.908812284469604\n",
      "  time_total_s: 3083.088485956192\n",
      "  timers:\n",
      "    learn_throughput: 1401.125\n",
      "    learn_time_ms: 713.712\n",
      "    load_throughput: 40787.807\n",
      "    load_time_ms: 24.517\n",
      "    sample_throughput: 37.787\n",
      "    sample_time_ms: 26464.14\n",
      "    update_time_ms: 2.65\n",
      "  timestamp: 1635065950\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         3083.09</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -2.8684</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            286.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_08-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 285.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.8543999999999836\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 411\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9786177575588226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005069556054616698\n",
      "          policy_loss: 0.0008985062336756124\n",
      "          total_loss: -0.0009457790189319186\n",
      "          vf_explained_var: 0.7529829144477844\n",
      "          vf_loss: 0.007688414145054089\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.3111111111111\n",
      "    ram_util_percent: 38.761111111111106\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03900793778413516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.74019656468691\n",
      "    mean_inference_ms: 1.9330324894804547\n",
      "    mean_raw_obs_processing_ms: 1.8036533438419071\n",
      "  time_since_restore: 3108.618480205536\n",
      "  time_this_iter_s: 25.529994249343872\n",
      "  time_total_s: 3108.618480205536\n",
      "  timers:\n",
      "    learn_throughput: 1392.405\n",
      "    learn_time_ms: 718.182\n",
      "    load_throughput: 41217.165\n",
      "    load_time_ms: 24.262\n",
      "    sample_throughput: 37.757\n",
      "    sample_time_ms: 26485.239\n",
      "    update_time_ms: 2.666\n",
      "  timestamp: 1635065976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         3108.62</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\"> -2.8544</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            285.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 283.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.8395999999999835\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 415\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0288697216245863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009927026415945958\n",
      "          policy_loss: -0.029295032719771068\n",
      "          total_loss: -0.03150775987241003\n",
      "          vf_explained_var: 0.7295660972595215\n",
      "          vf_loss: 0.0075796175437668955\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.942105263157885\n",
      "    ram_util_percent: 38.76842105263158\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03900780883015105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.765204737696298\n",
      "    mean_inference_ms: 1.9330465271993966\n",
      "    mean_raw_obs_processing_ms: 1.8057738774135321\n",
      "  time_since_restore: 3135.0759835243225\n",
      "  time_this_iter_s: 26.45750331878662\n",
      "  time_total_s: 3135.0759835243225\n",
      "  timers:\n",
      "    learn_throughput: 1389.902\n",
      "    learn_time_ms: 719.475\n",
      "    load_throughput: 40249.309\n",
      "    load_time_ms: 24.845\n",
      "    sample_throughput: 37.823\n",
      "    sample_time_ms: 26439.006\n",
      "    update_time_ms: 2.884\n",
      "  timestamp: 1635066002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         3135.08</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\"> -2.8396</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            283.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-00-29\n",
      "  done: false\n",
      "  episode_len_mean: 282.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.8249999999999833\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 418\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9602206753359901\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007784465293064989\n",
      "          policy_loss: -0.11132673728797171\n",
      "          total_loss: -0.10843348585897022\n",
      "          vf_explained_var: 0.46720030903816223\n",
      "          vf_loss: 0.012106234973503483\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.54736842105264\n",
      "    ram_util_percent: 38.9263157894737\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03900810975337492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.78484566649827\n",
      "    mean_inference_ms: 1.9330705327573268\n",
      "    mean_raw_obs_processing_ms: 1.8075114241447858\n",
      "  time_since_restore: 3161.7490513324738\n",
      "  time_this_iter_s: 26.673067808151245\n",
      "  time_total_s: 3161.7490513324738\n",
      "  timers:\n",
      "    learn_throughput: 1390.994\n",
      "    learn_time_ms: 718.911\n",
      "    load_throughput: 40575.68\n",
      "    load_time_ms: 24.645\n",
      "    sample_throughput: 37.742\n",
      "    sample_time_ms: 26495.919\n",
      "    update_time_ms: 2.849\n",
      "  timestamp: 1635066029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         3161.75</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">  -2.825</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">             282.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-01-13\n",
      "  done: false\n",
      "  episode_len_mean: 279.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.7986999999999838\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 422\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.900942196448644\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015260974401793071\n",
      "          policy_loss: -0.11194546181294653\n",
      "          total_loss: -0.1055663089785311\n",
      "          vf_explained_var: 0.24134230613708496\n",
      "          vf_loss: 0.014625527513109976\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.7953125\n",
      "    ram_util_percent: 38.9421875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0390093895459483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.81256967956637\n",
      "    mean_inference_ms: 1.9331258337951749\n",
      "    mean_raw_obs_processing_ms: 1.8148249442318485\n",
      "  time_since_restore: 3206.271120071411\n",
      "  time_this_iter_s: 44.52206873893738\n",
      "  time_total_s: 3206.271120071411\n",
      "  timers:\n",
      "    learn_throughput: 1385.265\n",
      "    learn_time_ms: 721.883\n",
      "    load_throughput: 41115.428\n",
      "    load_time_ms: 24.322\n",
      "    sample_throughput: 35.09\n",
      "    sample_time_ms: 28498.183\n",
      "    update_time_ms: 2.862\n",
      "  timestamp: 1635066073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         3206.27</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -2.7987</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            279.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-01-40\n",
      "  done: false\n",
      "  episode_len_mean: 277.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.7741999999999845\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 426\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7526913073327807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009188398613931762\n",
      "          policy_loss: 0.00014875630537668864\n",
      "          total_loss: 0.004975490934318967\n",
      "          vf_explained_var: 0.13116449117660522\n",
      "          vf_loss: 0.011894228485309416\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.027027027027025\n",
      "    ram_util_percent: 38.962162162162166\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901119936146578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.841519103314145\n",
      "    mean_inference_ms: 1.9331937714641076\n",
      "    mean_raw_obs_processing_ms: 1.8224712345600946\n",
      "  time_since_restore: 3232.6405181884766\n",
      "  time_this_iter_s: 26.36939811706543\n",
      "  time_total_s: 3232.6405181884766\n",
      "  timers:\n",
      "    learn_throughput: 1388.623\n",
      "    learn_time_ms: 720.138\n",
      "    load_throughput: 41193.686\n",
      "    load_time_ms: 24.276\n",
      "    sample_throughput: 34.875\n",
      "    sample_time_ms: 28673.454\n",
      "    update_time_ms: 2.859\n",
      "  timestamp: 1635066100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         3232.64</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\"> -2.7742</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            277.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-02-06\n",
      "  done: false\n",
      "  episode_len_mean: 275.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.758999999999985\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 430\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8290146423710717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025204010000892098\n",
      "          policy_loss: 0.007591059721178479\n",
      "          total_loss: 0.011527974986367755\n",
      "          vf_explained_var: 0.23396944999694824\n",
      "          vf_loss: 0.010966860440870125\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.91891891891892\n",
      "    ram_util_percent: 38.9054054054054\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901327677028381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.871172104344236\n",
      "    mean_inference_ms: 1.93326991139646\n",
      "    mean_raw_obs_processing_ms: 1.828714802119648\n",
      "  time_since_restore: 3258.267796278\n",
      "  time_this_iter_s: 25.627278089523315\n",
      "  time_total_s: 3258.267796278\n",
      "  timers:\n",
      "    learn_throughput: 1384.346\n",
      "    learn_time_ms: 722.363\n",
      "    load_throughput: 41231.792\n",
      "    load_time_ms: 24.253\n",
      "    sample_throughput: 37.013\n",
      "    sample_time_ms: 27017.736\n",
      "    update_time_ms: 2.884\n",
      "  timestamp: 1635066126\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         3258.27</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">  -2.759</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">             275.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 273.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.7332999999999856\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 434\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7511215455002255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004559217487291751\n",
      "          policy_loss: 0.018376136736737356\n",
      "          total_loss: 0.023949508782890108\n",
      "          vf_explained_var: 0.10433515161275864\n",
      "          vf_loss: 0.012742646514541572\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.097297297297295\n",
      "    ram_util_percent: 38.94594594594595\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039015743035935004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.901609471707292\n",
      "    mean_inference_ms: 1.9333592540356797\n",
      "    mean_raw_obs_processing_ms: 1.830808241476099\n",
      "  time_since_restore: 3284.549063205719\n",
      "  time_this_iter_s: 26.281266927719116\n",
      "  time_total_s: 3284.549063205719\n",
      "  timers:\n",
      "    learn_throughput: 1372.731\n",
      "    learn_time_ms: 728.475\n",
      "    load_throughput: 42224.806\n",
      "    load_time_ms: 23.683\n",
      "    sample_throughput: 36.891\n",
      "    sample_time_ms: 27106.83\n",
      "    update_time_ms: 4.105\n",
      "  timestamp: 1635066152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         3284.55</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\"> -2.7333</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            273.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-02-57\n",
      "  done: false\n",
      "  episode_len_mean: 271.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.7100999999999855\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 438\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8860604650444455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018466493924831046\n",
      "          policy_loss: 0.030947652873065737\n",
      "          total_loss: 0.03583263680338859\n",
      "          vf_explained_var: 0.04825029894709587\n",
      "          vf_loss: 0.013053096416923735\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.69459459459459\n",
      "    ram_util_percent: 39.01081081081082\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901899596442733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.932871891152942\n",
      "    mean_inference_ms: 1.9334691356082276\n",
      "    mean_raw_obs_processing_ms: 1.8331419801026663\n",
      "  time_since_restore: 3309.8977525234222\n",
      "  time_this_iter_s: 25.348689317703247\n",
      "  time_total_s: 3309.8977525234222\n",
      "  timers:\n",
      "    learn_throughput: 1359.058\n",
      "    learn_time_ms: 735.804\n",
      "    load_throughput: 41806.208\n",
      "    load_time_ms: 23.92\n",
      "    sample_throughput: 36.769\n",
      "    sample_time_ms: 27196.501\n",
      "    update_time_ms: 4.131\n",
      "  timestamp: 1635066177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">          3309.9</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -2.7101</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            271.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-03-21\n",
      "  done: false\n",
      "  episode_len_mean: 270.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.7002999999999866\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 441\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.011917761961619\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006556323067319722\n",
      "          policy_loss: -0.08800672325823042\n",
      "          total_loss: -0.08446644345919291\n",
      "          vf_explained_var: 0.05774659663438797\n",
      "          vf_loss: 0.01341359620499942\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.82121212121212\n",
      "    ram_util_percent: 39.06969696969697\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03902176715262702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.956834524020312\n",
      "    mean_inference_ms: 1.9335644591377397\n",
      "    mean_raw_obs_processing_ms: 1.8350601630254046\n",
      "  time_since_restore: 3333.5594358444214\n",
      "  time_this_iter_s: 23.661683320999146\n",
      "  time_total_s: 3333.5594358444214\n",
      "  timers:\n",
      "    learn_throughput: 1359.499\n",
      "    learn_time_ms: 735.565\n",
      "    load_throughput: 41815.336\n",
      "    load_time_ms: 23.915\n",
      "    sample_throughput: 37.077\n",
      "    sample_time_ms: 26970.835\n",
      "    update_time_ms: 4.124\n",
      "  timestamp: 1635066201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         3333.56</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\"> -2.7003</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            270.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 268.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.688399999999987\n",
      "  episode_reward_min: -3.3099999999999734\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 445\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1325374046961467\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009138094658653036\n",
      "          policy_loss: 0.01607730620437198\n",
      "          total_loss: 0.019230373203754425\n",
      "          vf_explained_var: 0.014814731664955616\n",
      "          vf_loss: 0.014135762624856498\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.190625000000004\n",
      "    ram_util_percent: 39.1\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03902582071794988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.989093168934822\n",
      "    mean_inference_ms: 1.9337057999484648\n",
      "    mean_raw_obs_processing_ms: 1.8378387785208465\n",
      "  time_since_restore: 3355.751838207245\n",
      "  time_this_iter_s: 22.192402362823486\n",
      "  time_total_s: 3355.751838207245\n",
      "  timers:\n",
      "    learn_throughput: 1368.837\n",
      "    learn_time_ms: 730.547\n",
      "    load_throughput: 41341.358\n",
      "    load_time_ms: 24.189\n",
      "    sample_throughput: 37.73\n",
      "    sample_time_ms: 26504.209\n",
      "    update_time_ms: 3.919\n",
      "  timestamp: 1635066223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         3355.75</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\"> -2.6884</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -3.31</td><td style=\"text-align: right;\">            268.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 269.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.6946999999999868\n",
      "  episode_reward_min: -3.3099999999999734\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 448\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2823309752676222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011731490006358552\n",
      "          policy_loss: 0.05584626777304543\n",
      "          total_loss: 0.05366210291783015\n",
      "          vf_explained_var: -0.09554627537727356\n",
      "          vf_loss: 0.010199212207387771\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.57333333333333\n",
      "    ram_util_percent: 39.166666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03902898846807864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.012895847929304\n",
      "    mean_inference_ms: 1.9338145446718291\n",
      "    mean_raw_obs_processing_ms: 1.840014488369796\n",
      "  time_since_restore: 3376.5426280498505\n",
      "  time_this_iter_s: 20.79078984260559\n",
      "  time_total_s: 3376.5426280498505\n",
      "  timers:\n",
      "    learn_throughput: 1382.302\n",
      "    learn_time_ms: 723.431\n",
      "    load_throughput: 41463.187\n",
      "    load_time_ms: 24.118\n",
      "    sample_throughput: 38.406\n",
      "    sample_time_ms: 26037.486\n",
      "    update_time_ms: 3.913\n",
      "  timestamp: 1635066244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         3376.54</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\"> -2.6947</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -3.31</td><td style=\"text-align: right;\">            269.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 270.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.7023999999999866\n",
      "  episode_reward_min: -3.6799999999999655\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 450\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2221248745918274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011946985915296754\n",
      "          policy_loss: -0.08011542740795348\n",
      "          total_loss: -0.08120681328905953\n",
      "          vf_explained_var: -0.11666162312030792\n",
      "          vf_loss: 0.010681849233353406\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.036734693877555\n",
      "    ram_util_percent: 39.0\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903100211404211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.028131964059867\n",
      "    mean_inference_ms: 1.9338856797076402\n",
      "    mean_raw_obs_processing_ms: 1.843682834156603\n",
      "  time_since_restore: 3410.8859226703644\n",
      "  time_this_iter_s: 34.343294620513916\n",
      "  time_total_s: 3410.8859226703644\n",
      "  timers:\n",
      "    learn_throughput: 1398.415\n",
      "    learn_time_ms: 715.096\n",
      "    load_throughput: 42456.208\n",
      "    load_time_ms: 23.554\n",
      "    sample_throughput: 37.264\n",
      "    sample_time_ms: 26835.228\n",
      "    update_time_ms: 3.668\n",
      "  timestamp: 1635066278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         3410.89</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -2.7024</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -3.68</td><td style=\"text-align: right;\">            270.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-04-55\n",
      "  done: false\n",
      "  episode_len_mean: 273.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.7363999999999855\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 452\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2600433667500814\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01634961154366847\n",
      "          policy_loss: -0.089965168469482\n",
      "          total_loss: -0.09032602839999729\n",
      "          vf_explained_var: -0.11453621834516525\n",
      "          vf_loss: 0.011626463195231434\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.15833333333334\n",
      "    ram_util_percent: 38.9125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0390329979052348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.042626348085598\n",
      "    mean_inference_ms: 1.933954135704917\n",
      "    mean_raw_obs_processing_ms: 1.8472417931419078\n",
      "  time_since_restore: 3427.8476502895355\n",
      "  time_this_iter_s: 16.961727619171143\n",
      "  time_total_s: 3427.8476502895355\n",
      "  timers:\n",
      "    learn_throughput: 1400.681\n",
      "    learn_time_ms: 713.938\n",
      "    load_throughput: 44034.919\n",
      "    load_time_ms: 22.709\n",
      "    sample_throughput: 38.661\n",
      "    sample_time_ms: 25866.073\n",
      "    update_time_ms: 3.687\n",
      "  timestamp: 1635066295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         3427.85</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\"> -2.7364</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            273.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-05-11\n",
      "  done: false\n",
      "  episode_len_mean: 278.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.7876999999999845\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 455\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2808010511928134\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011227994159608635\n",
      "          policy_loss: 0.04595045215553707\n",
      "          total_loss: 0.045727734764417015\n",
      "          vf_explained_var: -0.3287859261035919\n",
      "          vf_loss: 0.012164244614218155\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.8909090909091\n",
      "    ram_util_percent: 38.94545454545454\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903596428543617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.062701912906476\n",
      "    mean_inference_ms: 1.9340526276626913\n",
      "    mean_raw_obs_processing_ms: 1.8525527199954614\n",
      "  time_since_restore: 3443.268030166626\n",
      "  time_this_iter_s: 15.420379877090454\n",
      "  time_total_s: 3443.268030166626\n",
      "  timers:\n",
      "    learn_throughput: 1408.309\n",
      "    learn_time_ms: 710.072\n",
      "    load_throughput: 46691.989\n",
      "    load_time_ms: 21.417\n",
      "    sample_throughput: 43.552\n",
      "    sample_time_ms: 22961.066\n",
      "    update_time_ms: 3.696\n",
      "  timestamp: 1635066311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         3443.27</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\"> -2.7877</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            278.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-05-28\n",
      "  done: false\n",
      "  episode_len_mean: 281.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.810299999999984\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 457\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.241976147227817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0056784622792363845\n",
      "          policy_loss: -0.08178992966810862\n",
      "          total_loss: -0.08212032914161682\n",
      "          vf_explained_var: -0.11438853293657303\n",
      "          vf_loss: 0.011876414831688938\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.211999999999996\n",
      "    ram_util_percent: 38.94\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903787972214344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.075286163066227\n",
      "    mean_inference_ms: 1.9341155666917422\n",
      "    mean_raw_obs_processing_ms: 1.8560271668385073\n",
      "  time_since_restore: 3460.927827358246\n",
      "  time_this_iter_s: 17.659797191619873\n",
      "  time_total_s: 3460.927827358246\n",
      "  timers:\n",
      "    learn_throughput: 1407.061\n",
      "    learn_time_ms: 710.701\n",
      "    load_throughput: 46690.014\n",
      "    load_time_ms: 21.418\n",
      "    sample_throughput: 45.27\n",
      "    sample_time_ms: 22089.491\n",
      "    update_time_ms: 3.69\n",
      "  timestamp: 1635066328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         3460.93</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\"> -2.8103</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            281.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-05-45\n",
      "  done: false\n",
      "  episode_len_mean: 285.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.8528999999999836\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 460\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2491137305895488\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00986277427222871\n",
      "          policy_loss: 0.04279180781708823\n",
      "          total_loss: 0.043525944898525876\n",
      "          vf_explained_var: -0.23795059323310852\n",
      "          vf_loss: 0.012855418850409073\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.99583333333333\n",
      "    ram_util_percent: 38.94166666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03904061091800461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.092506693133036\n",
      "    mean_inference_ms: 1.9342068239646435\n",
      "    mean_raw_obs_processing_ms: 1.8584054207342833\n",
      "  time_since_restore: 3477.809489250183\n",
      "  time_this_iter_s: 16.881661891937256\n",
      "  time_total_s: 3477.809489250183\n",
      "  timers:\n",
      "    learn_throughput: 1410.206\n",
      "    learn_time_ms: 709.116\n",
      "    load_throughput: 48782.038\n",
      "    load_time_ms: 20.499\n",
      "    sample_throughput: 47.131\n",
      "    sample_time_ms: 21217.432\n",
      "    update_time_ms: 3.672\n",
      "  timestamp: 1635066345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         3477.81</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -2.8529</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            285.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-06-02\n",
      "  done: false\n",
      "  episode_len_mean: 288.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.885899999999982\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 462\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.197713757885827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010020651300555036\n",
      "          policy_loss: 0.07192905180984073\n",
      "          total_loss: 0.06729415473010805\n",
      "          vf_explained_var: -0.2729989290237427\n",
      "          vf_loss: 0.0069664658722306035\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.46666666666667\n",
      "    ram_util_percent: 38.95416666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03904229493565052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.10311470972801\n",
      "    mean_inference_ms: 1.934264111577295\n",
      "    mean_raw_obs_processing_ms: 1.8589339595192678\n",
      "  time_since_restore: 3494.298675060272\n",
      "  time_this_iter_s: 16.48918581008911\n",
      "  time_total_s: 3494.298675060272\n",
      "  timers:\n",
      "    learn_throughput: 1426.975\n",
      "    learn_time_ms: 700.783\n",
      "    load_throughput: 49831.815\n",
      "    load_time_ms: 20.068\n",
      "    sample_throughput: 49.387\n",
      "    sample_time_ms: 20248.234\n",
      "    update_time_ms: 2.449\n",
      "  timestamp: 1635066362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">          3494.3</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\"> -2.8859</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            288.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-06-19\n",
      "  done: false\n",
      "  episode_len_mean: 292.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.921799999999982\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 465\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.181081438064575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008067247039282598\n",
      "          policy_loss: 0.10493196133110258\n",
      "          total_loss: 0.09974751671155294\n",
      "          vf_explained_var: -0.32083261013031006\n",
      "          vf_loss: 0.006323847813392704\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.74000000000001\n",
      "    ram_util_percent: 38.976000000000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03904470597012535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.117376031983905\n",
      "    mean_inference_ms: 1.93434599775446\n",
      "    mean_raw_obs_processing_ms: 1.8597975017799306\n",
      "  time_since_restore: 3511.9163897037506\n",
      "  time_this_iter_s: 17.617714643478394\n",
      "  time_total_s: 3511.9163897037506\n",
      "  timers:\n",
      "    learn_throughput: 1442.798\n",
      "    learn_time_ms: 693.098\n",
      "    load_throughput: 50560.769\n",
      "    load_time_ms: 19.778\n",
      "    sample_throughput: 51.326\n",
      "    sample_time_ms: 19483.172\n",
      "    update_time_ms: 2.388\n",
      "  timestamp: 1635066379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         3511.92</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\"> -2.9218</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            292.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-06-38\n",
      "  done: false\n",
      "  episode_len_mean: 293.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.938799999999981\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 467\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1281354639265273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012504591835510049\n",
      "          policy_loss: -0.09210888031456206\n",
      "          total_loss: -0.08969123860200247\n",
      "          vf_explained_var: -0.07564524561166763\n",
      "          vf_loss: 0.013230076648889937\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.79629629629629\n",
      "    ram_util_percent: 39.007407407407406\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03904620048868781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.126055205330946\n",
      "    mean_inference_ms: 1.934398210088703\n",
      "    mean_raw_obs_processing_ms: 1.8604093191501085\n",
      "  time_since_restore: 3530.72824883461\n",
      "  time_this_iter_s: 18.811859130859375\n",
      "  time_total_s: 3530.72824883461\n",
      "  timers:\n",
      "    learn_throughput: 1445.809\n",
      "    learn_time_ms: 691.654\n",
      "    load_throughput: 49944.974\n",
      "    load_time_ms: 20.022\n",
      "    sample_throughput: 52.633\n",
      "    sample_time_ms: 18999.407\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1635066398\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         3530.73</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\"> -2.9388</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            293.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 296.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.9607999999999812\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 470\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0775548974672953\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004991068509878034\n",
      "          policy_loss: -0.10388831131988102\n",
      "          total_loss: -0.09597747863994704\n",
      "          vf_explained_var: 0.012473915703594685\n",
      "          vf_loss: 0.01849921587854624\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.192857142857136\n",
      "    ram_util_percent: 38.91071428571428\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03904825556080837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.138485575882065\n",
      "    mean_inference_ms: 1.9344716691997754\n",
      "    mean_raw_obs_processing_ms: 1.8611339891815035\n",
      "  time_since_restore: 3550.612382411957\n",
      "  time_this_iter_s: 19.8841335773468\n",
      "  time_total_s: 3550.612382411957\n",
      "  timers:\n",
      "    learn_throughput: 1451.015\n",
      "    learn_time_ms: 689.173\n",
      "    load_throughput: 50094.162\n",
      "    load_time_ms: 19.962\n",
      "    sample_throughput: 53.273\n",
      "    sample_time_ms: 18771.112\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1635066418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         3550.61</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -2.9608</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            296.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-07-20\n",
      "  done: false\n",
      "  episode_len_mean: 297.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -2.9783999999999797\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 473\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0146641665034823\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007462867987630596\n",
      "          policy_loss: -0.10316513925790786\n",
      "          total_loss: -0.09656894720262951\n",
      "          vf_explained_var: 0.10033362358808517\n",
      "          vf_loss: 0.016602904980795252\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.05161290322581\n",
      "    ram_util_percent: 38.78709677419355\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905011177859398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.150271678898036\n",
      "    mean_inference_ms: 1.934538314815227\n",
      "    mean_raw_obs_processing_ms: 1.8617945556428988\n",
      "  time_since_restore: 3572.050572156906\n",
      "  time_this_iter_s: 21.43818974494934\n",
      "  time_total_s: 3572.050572156906\n",
      "  timers:\n",
      "    learn_throughput: 1449.843\n",
      "    learn_time_ms: 689.73\n",
      "    load_throughput: 49983.304\n",
      "    load_time_ms: 20.007\n",
      "    sample_throughput: 53.092\n",
      "    sample_time_ms: 18835.231\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1635066440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         3572.05</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\"> -2.9784</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            297.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 300.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.0003999999999804\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 476\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.025025184949239\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007344110013530683\n",
      "          policy_loss: -0.06842400183280309\n",
      "          total_loss: -0.06517508890893724\n",
      "          vf_explained_var: -0.06521070003509521\n",
      "          vf_loss: 0.013361458423443967\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.1344827586207\n",
      "    ram_util_percent: 38.793103448275865\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905170040544987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.16047931626657\n",
      "    mean_inference_ms: 1.9345970623575055\n",
      "    mean_raw_obs_processing_ms: 1.8625151411299459\n",
      "  time_since_restore: 3592.319820165634\n",
      "  time_this_iter_s: 20.269248008728027\n",
      "  time_total_s: 3592.319820165634\n",
      "  timers:\n",
      "    learn_throughput: 1447.744\n",
      "    learn_time_ms: 690.73\n",
      "    load_throughput: 49950.803\n",
      "    load_time_ms: 20.02\n",
      "    sample_throughput: 57.383\n",
      "    sample_time_ms: 17426.819\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1635066460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         3592.32</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\"> -3.0004</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            300.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 302.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.02619999999998\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 479\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.060323581430647\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006311707507109471\n",
      "          policy_loss: -0.10020561367273331\n",
      "          total_loss: -0.09388911359839969\n",
      "          vf_explained_var: 0.04983319342136383\n",
      "          vf_loss: 0.01680138897564676\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.11034482758621\n",
      "    ram_util_percent: 38.77241379310345\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905328998831187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.169744641090297\n",
      "    mean_inference_ms: 1.934654143163243\n",
      "    mean_raw_obs_processing_ms: 1.8631739226628188\n",
      "  time_since_restore: 3612.7887094020844\n",
      "  time_this_iter_s: 20.468889236450195\n",
      "  time_total_s: 3612.7887094020844\n",
      "  timers:\n",
      "    learn_throughput: 1447.092\n",
      "    learn_time_ms: 691.041\n",
      "    load_throughput: 47048.14\n",
      "    load_time_ms: 21.255\n",
      "    sample_throughput: 56.256\n",
      "    sample_time_ms: 17776.002\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1635066480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         3612.79</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\"> -3.0262</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            302.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-08-36\n",
      "  done: false\n",
      "  episode_len_mean: 305.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.059899999999979\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 482\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1615265594588386\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005622322799203328\n",
      "          policy_loss: 0.07957392276989089\n",
      "          total_loss: 0.07720018633537823\n",
      "          vf_explained_var: -0.28716039657592773\n",
      "          vf_loss: 0.00913611187457314\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.103846153846156\n",
      "    ram_util_percent: 38.625\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905499690972657\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.17791446045751\n",
      "    mean_inference_ms: 1.9347121040289919\n",
      "    mean_raw_obs_processing_ms: 1.8669596446527335\n",
      "  time_since_restore: 3648.8107929229736\n",
      "  time_this_iter_s: 36.02208352088928\n",
      "  time_total_s: 3648.8107929229736\n",
      "  timers:\n",
      "    learn_throughput: 1430.756\n",
      "    learn_time_ms: 698.931\n",
      "    load_throughput: 44720.353\n",
      "    load_time_ms: 22.361\n",
      "    sample_throughput: 50.436\n",
      "    sample_time_ms: 19827.103\n",
      "    update_time_ms: 2.456\n",
      "  timestamp: 1635066516\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         3648.81</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -3.0599</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            305.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-08-53\n",
      "  done: false\n",
      "  episode_len_mean: 309.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.090799999999979\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 484\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1781876378589207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003393209346453416\n",
      "          policy_loss: -0.0741748109459877\n",
      "          total_loss: -0.07329014903969235\n",
      "          vf_explained_var: -0.09304490685462952\n",
      "          vf_loss: 0.012602916312688547\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.508333333333326\n",
      "    ram_util_percent: 38.791666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039056115885432875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.182460777593015\n",
      "    mean_inference_ms: 1.9347502892712543\n",
      "    mean_raw_obs_processing_ms: 1.8694336957391788\n",
      "  time_since_restore: 3665.7128055095673\n",
      "  time_this_iter_s: 16.902012586593628\n",
      "  time_total_s: 3665.7128055095673\n",
      "  timers:\n",
      "    learn_throughput: 1420.869\n",
      "    learn_time_ms: 703.795\n",
      "    load_throughput: 46275.745\n",
      "    load_time_ms: 21.61\n",
      "    sample_throughput: 50.64\n",
      "    sample_time_ms: 19747.17\n",
      "    update_time_ms: 2.484\n",
      "  timestamp: 1635066533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         3665.71</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\"> -3.0908</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            309.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-09-12\n",
      "  done: false\n",
      "  episode_len_mean: 312.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.1278999999999773\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 487\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2120215243763395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012682394740858131\n",
      "          policy_loss: 0.03932982443107499\n",
      "          total_loss: 0.0397860007153617\n",
      "          vf_explained_var: -0.180496484041214\n",
      "          vf_loss: 0.012457495905497733\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.670370370370364\n",
      "    ram_util_percent: 38.86296296296296\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039057934829531654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.187986339536494\n",
      "    mean_inference_ms: 1.934810615304654\n",
      "    mean_raw_obs_processing_ms: 1.8731854200178262\n",
      "  time_since_restore: 3684.488372564316\n",
      "  time_this_iter_s: 18.775567054748535\n",
      "  time_total_s: 3684.488372564316\n",
      "  timers:\n",
      "    learn_throughput: 1415.711\n",
      "    learn_time_ms: 706.359\n",
      "    load_throughput: 44474.2\n",
      "    load_time_ms: 22.485\n",
      "    sample_throughput: 50.168\n",
      "    sample_time_ms: 19933.137\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1635066552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         3684.49</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\"> -3.1279</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            312.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-09-32\n",
      "  done: false\n",
      "  episode_len_mean: 315.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.1526999999999763\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 490\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1910756985346476\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012994168278121521\n",
      "          policy_loss: 0.05151348014672597\n",
      "          total_loss: 0.05133034620020124\n",
      "          vf_explained_var: -0.1762692928314209\n",
      "          vf_loss: 0.011605805008568698\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.18214285714284\n",
      "    ram_util_percent: 38.86071428571428\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905969579489219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.192728642537325\n",
      "    mean_inference_ms: 1.9348687300102312\n",
      "    mean_raw_obs_processing_ms: 1.8742276658599721\n",
      "  time_since_restore: 3704.4927701950073\n",
      "  time_this_iter_s: 20.00439763069153\n",
      "  time_total_s: 3704.4927701950073\n",
      "  timers:\n",
      "    learn_throughput: 1413.007\n",
      "    learn_time_ms: 707.711\n",
      "    load_throughput: 42745.109\n",
      "    load_time_ms: 23.394\n",
      "    sample_throughput: 49.304\n",
      "    sample_time_ms: 20282.227\n",
      "    update_time_ms: 2.48\n",
      "  timestamp: 1635066572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         3704.49</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\"> -3.1527</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            315.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-09-52\n",
      "  done: false\n",
      "  episode_len_mean: 317.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.174599999999976\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 493\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.239028059111701\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009107887088689322\n",
      "          policy_loss: 0.05464886873960495\n",
      "          total_loss: 0.05444408257802327\n",
      "          vf_explained_var: -0.019777506589889526\n",
      "          vf_loss: 0.012100107341797815\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.57142857142857\n",
      "    ram_util_percent: 38.9\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906143162176147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.196582713247484\n",
      "    mean_inference_ms: 1.9349260344884007\n",
      "    mean_raw_obs_processing_ms: 1.8738895423884723\n",
      "  time_since_restore: 3724.1053714752197\n",
      "  time_this_iter_s: 19.612601280212402\n",
      "  time_total_s: 3724.1053714752197\n",
      "  timers:\n",
      "    learn_throughput: 1411.701\n",
      "    learn_time_ms: 708.365\n",
      "    load_throughput: 42763.195\n",
      "    load_time_ms: 23.385\n",
      "    sample_throughput: 48.826\n",
      "    sample_time_ms: 20481.045\n",
      "    update_time_ms: 2.504\n",
      "  timestamp: 1635066592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         3724.11</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -3.1746</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            317.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 320.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.2003999999999757\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 495\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2071121626430088\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014521143672725619\n",
      "          policy_loss: -0.0897514369752672\n",
      "          total_loss: -0.08956130676799351\n",
      "          vf_explained_var: -0.12619450688362122\n",
      "          vf_loss: 0.012125117582682934\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.992\n",
      "    ram_util_percent: 38.924\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906259744281336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.198275238481866\n",
      "    mean_inference_ms: 1.9349619426131306\n",
      "    mean_raw_obs_processing_ms: 1.8736271272147653\n",
      "  time_since_restore: 3741.5000081062317\n",
      "  time_this_iter_s: 17.394636631011963\n",
      "  time_total_s: 3741.5000081062317\n",
      "  timers:\n",
      "    learn_throughput: 1411.102\n",
      "    learn_time_ms: 708.666\n",
      "    load_throughput: 44956.569\n",
      "    load_time_ms: 22.244\n",
      "    sample_throughput: 49.164\n",
      "    sample_time_ms: 20340.164\n",
      "    update_time_ms: 2.505\n",
      "  timestamp: 1635066609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">          3741.5</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\"> -3.2004</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            320.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-10-25\n",
      "  done: false\n",
      "  episode_len_mean: 323.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.236299999999975\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 497\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1084121253755357\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017323485988917517\n",
      "          policy_loss: -0.07681016756428613\n",
      "          total_loss: -0.07507695787482792\n",
      "          vf_explained_var: -0.30217310786247253\n",
      "          vf_loss: 0.012654921286027982\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.69130434782608\n",
      "    ram_util_percent: 38.908695652173904\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039063771701431674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.198867063767068\n",
      "    mean_inference_ms: 1.934995927130723\n",
      "    mean_raw_obs_processing_ms: 1.8734067131518655\n",
      "  time_since_restore: 3757.576468229294\n",
      "  time_this_iter_s: 16.076460123062134\n",
      "  time_total_s: 3757.576468229294\n",
      "  timers:\n",
      "    learn_throughput: 1410.205\n",
      "    learn_time_ms: 709.117\n",
      "    load_throughput: 47002.323\n",
      "    load_time_ms: 21.276\n",
      "    sample_throughput: 50.1\n",
      "    sample_time_ms: 19959.945\n",
      "    update_time_ms: 2.482\n",
      "  timestamp: 1635066625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         3757.58</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\"> -3.2363</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            323.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 328.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.2850999999999737\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 500\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1247998634974161\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014004837987170523\n",
      "          policy_loss: 0.045767036908202705\n",
      "          total_loss: 0.04724530014726851\n",
      "          vf_explained_var: -0.2255365550518036\n",
      "          vf_loss: 0.012594967773960282\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.67391304347826\n",
      "    ram_util_percent: 38.89565217391304\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039065321839168686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.198509596213214\n",
      "    mean_inference_ms: 1.9350401953141363\n",
      "    mean_raw_obs_processing_ms: 1.8728998323689825\n",
      "  time_since_restore: 3773.601382255554\n",
      "  time_this_iter_s: 16.024914026260376\n",
      "  time_total_s: 3773.601382255554\n",
      "  timers:\n",
      "    learn_throughput: 1410.484\n",
      "    learn_time_ms: 708.976\n",
      "    load_throughput: 48249.49\n",
      "    load_time_ms: 20.726\n",
      "    sample_throughput: 51.495\n",
      "    sample_time_ms: 19419.338\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1635066641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">          3773.6</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\"> -3.2851</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            328.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-10-59\n",
      "  done: false\n",
      "  episode_len_mean: 331.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3131999999999726\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 502\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1943215595351324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015957402252407445\n",
      "          policy_loss: -0.09844604664378696\n",
      "          total_loss: -0.09764003091388279\n",
      "          vf_explained_var: 0.10566046088933945\n",
      "          vf_loss: 0.012599627149433622\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.792\n",
      "    ram_util_percent: 38.872\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906621895998935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.1973802279741\n",
      "    mean_inference_ms: 1.9350643289225076\n",
      "    mean_raw_obs_processing_ms: 1.8725224788433514\n",
      "  time_since_restore: 3791.0465619564056\n",
      "  time_this_iter_s: 17.44517970085144\n",
      "  time_total_s: 3791.0465619564056\n",
      "  timers:\n",
      "    learn_throughput: 1409.594\n",
      "    learn_time_ms: 709.424\n",
      "    load_throughput: 50258.391\n",
      "    load_time_ms: 19.897\n",
      "    sample_throughput: 52.254\n",
      "    sample_time_ms: 19137.321\n",
      "    update_time_ms: 2.438\n",
      "  timestamp: 1635066659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         3791.05</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -3.3132</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            331.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-11-19\n",
      "  done: false\n",
      "  episode_len_mean: 334.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3422999999999727\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 505\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1450693017906612\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014647245908256945\n",
      "          policy_loss: -0.002040776610374451\n",
      "          total_loss: -0.0006210390892293718\n",
      "          vf_explained_var: 0.029400914907455444\n",
      "          vf_loss: 0.012733115642590241\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.925000000000004\n",
      "    ram_util_percent: 38.84642857142857\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906741585726222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.194668337967776\n",
      "    mean_inference_ms: 1.9350906909109842\n",
      "    mean_raw_obs_processing_ms: 1.8719057078134056\n",
      "  time_since_restore: 3810.9947588443756\n",
      "  time_this_iter_s: 19.94819688796997\n",
      "  time_total_s: 3810.9947588443756\n",
      "  timers:\n",
      "    learn_throughput: 1411.522\n",
      "    learn_time_ms: 708.455\n",
      "    load_throughput: 51110.718\n",
      "    load_time_ms: 19.565\n",
      "    sample_throughput: 52.393\n",
      "    sample_time_ms: 19086.549\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1635066679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         3810.99</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\"> -3.3423</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            334.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-11-40\n",
      "  done: false\n",
      "  episode_len_mean: 336.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.361699999999973\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 508\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1068587236934238\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008783706086767514\n",
      "          policy_loss: -0.09769050396151013\n",
      "          total_loss: -0.09160402284728156\n",
      "          vf_explained_var: 0.058070771396160126\n",
      "          vf_loss: 0.01707271672785282\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.01935483870967\n",
      "    ram_util_percent: 38.81935483870967\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906843473416137\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.191322355202566\n",
      "    mean_inference_ms: 1.9351082700989368\n",
      "    mean_raw_obs_processing_ms: 1.8712423759876222\n",
      "  time_since_restore: 3832.535449743271\n",
      "  time_this_iter_s: 21.540690898895264\n",
      "  time_total_s: 3832.535449743271\n",
      "  timers:\n",
      "    learn_throughput: 1425.719\n",
      "    learn_time_ms: 701.401\n",
      "    load_throughput: 50837.645\n",
      "    load_time_ms: 19.67\n",
      "    sample_throughput: 56.672\n",
      "    sample_time_ms: 17645.471\n",
      "    update_time_ms: 2.389\n",
      "  timestamp: 1635066700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         3832.54</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\"> -3.3617</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            336.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-12-18\n",
      "  done: false\n",
      "  episode_len_mean: 337.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.378799999999972\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 511\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.078105754322476\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009597148016299822\n",
      "          policy_loss: -0.11026834497849146\n",
      "          total_loss: -0.10507154969705476\n",
      "          vf_explained_var: 0.0965883657336235\n",
      "          vf_loss: 0.01588788278814819\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.08363636363637\n",
      "    ram_util_percent: 38.665454545454544\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906926194501549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.18742070142041\n",
      "    mean_inference_ms: 1.9351192301943365\n",
      "    mean_raw_obs_processing_ms: 1.8734624520546734\n",
      "  time_since_restore: 3870.6359922885895\n",
      "  time_this_iter_s: 38.1005425453186\n",
      "  time_total_s: 3870.6359922885895\n",
      "  timers:\n",
      "    learn_throughput: 1434.471\n",
      "    learn_time_ms: 697.121\n",
      "    load_throughput: 49117.426\n",
      "    load_time_ms: 20.359\n",
      "    sample_throughput: 50.584\n",
      "    sample_time_ms: 19768.951\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1635066738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         3870.64</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\"> -3.3788</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            337.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-12-40\n",
      "  done: false\n",
      "  episode_len_mean: 339.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3989999999999725\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 514\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.129743398560418\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008523363354757072\n",
      "          policy_loss: -0.12987081706523895\n",
      "          total_loss: -0.1255031171772215\n",
      "          vf_explained_var: 0.16782252490520477\n",
      "          vf_loss: 0.015585227414137787\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.63666666666667\n",
      "    ram_util_percent: 38.59666666666668\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906976959771046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.18239520012533\n",
      "    mean_inference_ms: 1.9351185248419283\n",
      "    mean_raw_obs_processing_ms: 1.8757293788703302\n",
      "  time_since_restore: 3891.6526114940643\n",
      "  time_this_iter_s: 21.016619205474854\n",
      "  time_total_s: 3891.6526114940643\n",
      "  timers:\n",
      "    learn_throughput: 1438.049\n",
      "    learn_time_ms: 695.386\n",
      "    load_throughput: 49017.776\n",
      "    load_time_ms: 20.401\n",
      "    sample_throughput: 50.013\n",
      "    sample_time_ms: 19994.742\n",
      "    update_time_ms: 2.346\n",
      "  timestamp: 1635066760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         3891.65</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  -3.399</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">             339.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 342.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.4253999999999714\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 518\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0535721196068657\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006894068865573407\n",
      "          policy_loss: -0.0021386474370956423\n",
      "          total_loss: 0.0018663708534505633\n",
      "          vf_explained_var: 0.11321963369846344\n",
      "          vf_loss: 0.014476104732602834\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.8448275862069\n",
      "    ram_util_percent: 38.741379310344826\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03907001480420806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.174407337371132\n",
      "    mean_inference_ms: 1.9351031871814761\n",
      "    mean_raw_obs_processing_ms: 1.8787151375776399\n",
      "  time_since_restore: 3912.1784353256226\n",
      "  time_this_iter_s: 20.525823831558228\n",
      "  time_total_s: 3912.1784353256226\n",
      "  timers:\n",
      "    learn_throughput: 1436.977\n",
      "    learn_time_ms: 695.905\n",
      "    load_throughput: 48443.42\n",
      "    load_time_ms: 20.643\n",
      "    sample_throughput: 49.885\n",
      "    sample_time_ms: 20046.292\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1635066780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         3912.18</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\"> -3.4254</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            342.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 344.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.44159999999997\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 521\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9341247803635068\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009274818244752936\n",
      "          policy_loss: 0.06277186671892802\n",
      "          total_loss: 0.06395288474029964\n",
      "          vf_explained_var: 0.2069471925497055\n",
      "          vf_loss: 0.010435315303685558\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.877419354838715\n",
      "    ram_util_percent: 38.861290322580636\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039069555229258375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.16687203180822\n",
      "    mean_inference_ms: 1.9350730470399964\n",
      "    mean_raw_obs_processing_ms: 1.8774530374947653\n",
      "  time_since_restore: 3933.6749930381775\n",
      "  time_this_iter_s: 21.49655771255493\n",
      "  time_total_s: 3933.6749930381775\n",
      "  timers:\n",
      "    learn_throughput: 1436.873\n",
      "    learn_time_ms: 695.955\n",
      "    load_throughput: 48369.119\n",
      "    load_time_ms: 20.674\n",
      "    sample_throughput: 49.42\n",
      "    sample_time_ms: 20234.623\n",
      "    update_time_ms: 2.334\n",
      "  timestamp: 1635066802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         3933.67</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\"> -3.4416</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            344.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-13-45\n",
      "  done: false\n",
      "  episode_len_mean: 344.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.4486999999999703\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 524\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8011409064133962\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020109023735754662\n",
      "          policy_loss: -0.028720146748754713\n",
      "          total_loss: -0.025941636496120028\n",
      "          vf_explained_var: 0.012422783300280571\n",
      "          vf_loss: 0.010601393557671044\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.21818181818182\n",
      "    ram_util_percent: 38.92121212121212\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0390688382514496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.15899560753637\n",
      "    mean_inference_ms: 1.9350376894777717\n",
      "    mean_raw_obs_processing_ms: 1.876143299962954\n",
      "  time_since_restore: 3956.857427597046\n",
      "  time_this_iter_s: 23.182434558868408\n",
      "  time_total_s: 3956.857427597046\n",
      "  timers:\n",
      "    learn_throughput: 1436.364\n",
      "    learn_time_ms: 696.203\n",
      "    load_throughput: 46059.467\n",
      "    load_time_ms: 21.711\n",
      "    sample_throughput: 48.049\n",
      "    sample_time_ms: 20812.114\n",
      "    update_time_ms: 2.332\n",
      "  timestamp: 1635066825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         3956.86</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\"> -3.4487</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            344.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-14-08\n",
      "  done: false\n",
      "  episode_len_mean: 345.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.4587999999999703\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 528\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.933175202873018\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015485539513695952\n",
      "          policy_loss: 0.024231924447748396\n",
      "          total_loss: 0.030496347033315236\n",
      "          vf_explained_var: 0.05285933241248131\n",
      "          vf_loss: 0.015378407647626267\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.25757575757575\n",
      "    ram_util_percent: 38.93030303030303\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039067502356850005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.148011724266613\n",
      "    mean_inference_ms: 1.9349833193307036\n",
      "    mean_raw_obs_processing_ms: 1.8744192131691793\n",
      "  time_since_restore: 3980.5188143253326\n",
      "  time_this_iter_s: 23.661386728286743\n",
      "  time_total_s: 3980.5188143253326\n",
      "  timers:\n",
      "    learn_throughput: 1437.174\n",
      "    learn_time_ms: 695.81\n",
      "    load_throughput: 43794.45\n",
      "    load_time_ms: 22.834\n",
      "    sample_throughput: 46.361\n",
      "    sample_time_ms: 21569.805\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1635066848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         3980.52</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -3.4588</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            345.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-14-32\n",
      "  done: false\n",
      "  episode_len_mean: 346.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.4658999999999702\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 532\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9814588824907938\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014335532831298537\n",
      "          policy_loss: -0.009994698440035183\n",
      "          total_loss: -0.004840350730551613\n",
      "          vf_explained_var: 0.17044450342655182\n",
      "          vf_loss: 0.014767341688275338\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.90285714285714\n",
      "    ram_util_percent: 38.83714285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906585535635352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.136385957315444\n",
      "    mean_inference_ms: 1.9349210890062336\n",
      "    mean_raw_obs_processing_ms: 1.8727949206762107\n",
      "  time_since_restore: 4004.400797843933\n",
      "  time_this_iter_s: 23.881983518600464\n",
      "  time_total_s: 4004.400797843933\n",
      "  timers:\n",
      "    learn_throughput: 1435.693\n",
      "    learn_time_ms: 696.528\n",
      "    load_throughput: 42166.862\n",
      "    load_time_ms: 23.715\n",
      "    sample_throughput: 44.735\n",
      "    sample_time_ms: 22353.915\n",
      "    update_time_ms: 2.372\n",
      "  timestamp: 1635066872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">          4004.4</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\"> -3.4659</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            346.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-14-56\n",
      "  done: false\n",
      "  episode_len_mean: 347.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.4717999999999694\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 535\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9301949653360578\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009769652400571142\n",
      "          policy_loss: 0.008513582994540532\n",
      "          total_loss: 0.009805248015456729\n",
      "          vf_explained_var: 0.11278408765792847\n",
      "          vf_loss: 0.010456228101005157\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.15588235294118\n",
      "    ram_util_percent: 38.747058823529414\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039064290217533944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.127375638448513\n",
      "    mean_inference_ms: 1.9348660248767602\n",
      "    mean_raw_obs_processing_ms: 1.8715871825284194\n",
      "  time_since_restore: 4028.2859978675842\n",
      "  time_this_iter_s: 23.885200023651123\n",
      "  time_total_s: 4028.2859978675842\n",
      "  timers:\n",
      "    learn_throughput: 1436.193\n",
      "    learn_time_ms: 696.285\n",
      "    load_throughput: 40545.596\n",
      "    load_time_ms: 24.664\n",
      "    sample_throughput: 43.484\n",
      "    sample_time_ms: 22997.227\n",
      "    update_time_ms: 2.363\n",
      "  timestamp: 1635066896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         4028.29</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\"> -3.4718</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            347.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-15-21\n",
      "  done: false\n",
      "  episode_len_mean: 347.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.4700999999999698\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 539\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.021517910559972\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005324517866786651\n",
      "          policy_loss: 0.024644906487729816\n",
      "          total_loss: 0.027964500586191814\n",
      "          vf_explained_var: 0.19863519072532654\n",
      "          vf_loss: 0.013459894019696448\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.300000000000004\n",
      "    ram_util_percent: 38.699999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039061631153099595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.115292773121777\n",
      "    mean_inference_ms: 1.9347797372102695\n",
      "    mean_raw_obs_processing_ms: 1.870036745295204\n",
      "  time_since_restore: 4052.940100669861\n",
      "  time_this_iter_s: 24.65410280227661\n",
      "  time_total_s: 4052.940100669861\n",
      "  timers:\n",
      "    learn_throughput: 1433.402\n",
      "    learn_time_ms: 697.641\n",
      "    load_throughput: 40556.769\n",
      "    load_time_ms: 24.657\n",
      "    sample_throughput: 42.614\n",
      "    sample_time_ms: 23466.446\n",
      "    update_time_ms: 2.367\n",
      "  timestamp: 1635066921\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         4052.94</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\"> -3.4701</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            347.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-16-03\n",
      "  done: false\n",
      "  episode_len_mean: 346.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.465999999999969\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 542\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0585189329253302\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008223995073694389\n",
      "          policy_loss: -0.10426954668429163\n",
      "          total_loss: -0.09996529759632217\n",
      "          vf_explained_var: 0.16978207230567932\n",
      "          vf_loss: 0.014773788075480196\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.31000000000001\n",
      "    ram_util_percent: 38.53\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039059445421594854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.10648664775256\n",
      "    mean_inference_ms: 1.9347066317046338\n",
      "    mean_raw_obs_processing_ms: 1.8717923732761532\n",
      "  time_since_restore: 4094.8631715774536\n",
      "  time_this_iter_s: 41.92307090759277\n",
      "  time_total_s: 4094.8631715774536\n",
      "  timers:\n",
      "    learn_throughput: 1432.787\n",
      "    learn_time_ms: 697.941\n",
      "    load_throughput: 40302.41\n",
      "    load_time_ms: 24.812\n",
      "    sample_throughput: 39.209\n",
      "    sample_time_ms: 25504.211\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1635066963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         4094.86</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  -3.466</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">             346.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-16-27\n",
      "  done: false\n",
      "  episode_len_mean: 345.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.4518999999999704\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 546\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9859861592451732\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012650307213830184\n",
      "          policy_loss: -0.011929910878340404\n",
      "          total_loss: -0.01022275338570277\n",
      "          vf_explained_var: 0.1417582482099533\n",
      "          vf_loss: 0.011389124745296107\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.37941176470588\n",
      "    ram_util_percent: 38.62352941176471\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905639069223515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.095302343983057\n",
      "    mean_inference_ms: 1.9346053820057256\n",
      "    mean_raw_obs_processing_ms: 1.874168237495691\n",
      "  time_since_restore: 4118.909897327423\n",
      "  time_this_iter_s: 24.046725749969482\n",
      "  time_total_s: 4118.909897327423\n",
      "  timers:\n",
      "    learn_throughput: 1434.873\n",
      "    learn_time_ms: 696.926\n",
      "    load_throughput: 40064.879\n",
      "    load_time_ms: 24.96\n",
      "    sample_throughput: 41.494\n",
      "    sample_time_ms: 24099.721\n",
      "    update_time_ms: 2.358\n",
      "  timestamp: 1635066987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         4118.91</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\"> -3.4519</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            345.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-16-52\n",
      "  done: false\n",
      "  episode_len_mean: 342.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.424799999999971\n",
      "  episode_reward_min: -4.729999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 550\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8422287431028154\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009954505694026889\n",
      "          policy_loss: 0.018329594284296036\n",
      "          total_loss: 0.02554566827085283\n",
      "          vf_explained_var: 0.08637325465679169\n",
      "          vf_loss: 0.015498372788230578\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.97777777777778\n",
      "    ram_util_percent: 38.67222222222224\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039053392191187176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.08556552575326\n",
      "    mean_inference_ms: 1.934505667423461\n",
      "    mean_raw_obs_processing_ms: 1.8744598395352623\n",
      "  time_since_restore: 4143.750017881393\n",
      "  time_this_iter_s: 24.840120553970337\n",
      "  time_total_s: 4143.750017881393\n",
      "  timers:\n",
      "    learn_throughput: 1436.098\n",
      "    learn_time_ms: 696.331\n",
      "    load_throughput: 40112.813\n",
      "    load_time_ms: 24.93\n",
      "    sample_throughput: 40.845\n",
      "    sample_time_ms: 24482.697\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1635067012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         4143.75</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\"> -3.4248</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.73</td><td style=\"text-align: right;\">            342.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-17-18\n",
      "  done: false\n",
      "  episode_len_mean: 333.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.338299999999972\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 554\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9223314642906189\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008146634233600973\n",
      "          policy_loss: 0.01355574598742856\n",
      "          total_loss: 0.018333371521698102\n",
      "          vf_explained_var: 0.1962776780128479\n",
      "          vf_loss: 0.01388637341765894\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.21891891891892\n",
      "    ram_util_percent: 38.705405405405415\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905061250087576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.07933171315567\n",
      "    mean_inference_ms: 1.9344181864777727\n",
      "    mean_raw_obs_processing_ms: 1.873039152223238\n",
      "  time_since_restore: 4169.757718324661\n",
      "  time_this_iter_s: 26.007700443267822\n",
      "  time_total_s: 4169.757718324661\n",
      "  timers:\n",
      "    learn_throughput: 1436.251\n",
      "    learn_time_ms: 696.257\n",
      "    load_throughput: 40124.478\n",
      "    load_time_ms: 24.922\n",
      "    sample_throughput: 39.951\n",
      "    sample_time_ms: 25030.968\n",
      "    update_time_ms: 2.358\n",
      "  timestamp: 1635067038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         4169.76</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\"> -3.3383</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            333.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-17-43\n",
      "  done: false\n",
      "  episode_len_mean: 327.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.2763999999999744\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 558\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8829015281465319\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005618343382453933\n",
      "          policy_loss: 0.03461388159129355\n",
      "          total_loss: 0.037912382723556626\n",
      "          vf_explained_var: 0.22612646222114563\n",
      "          vf_loss: 0.012048506105525627\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.175\n",
      "    ram_util_percent: 38.725\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03904805201224726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.075774923555826\n",
      "    mean_inference_ms: 1.9343392721405541\n",
      "    mean_raw_obs_processing_ms: 1.8719422912910142\n",
      "  time_since_restore: 4195.0987639427185\n",
      "  time_this_iter_s: 25.34104561805725\n",
      "  time_total_s: 4195.0987639427185\n",
      "  timers:\n",
      "    learn_throughput: 1435.303\n",
      "    learn_time_ms: 696.717\n",
      "    load_throughput: 39937.765\n",
      "    load_time_ms: 25.039\n",
      "    sample_throughput: 39.347\n",
      "    sample_time_ms: 25414.809\n",
      "    update_time_ms: 2.386\n",
      "  timestamp: 1635067063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">          4195.1</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -3.2764</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            327.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-18-07\n",
      "  done: false\n",
      "  episode_len_mean: 323.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.2330999999999754\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 561\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0047313696808284\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007922497480997019\n",
      "          policy_loss: -0.11120826734436882\n",
      "          total_loss: -0.10649751648306846\n",
      "          vf_explained_var: 0.16241882741451263\n",
      "          vf_loss: 0.014646653986225526\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.25\n",
      "    ram_util_percent: 38.72058823529412\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03904631797125351\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.074711154724334\n",
      "    mean_inference_ms: 1.934284254296434\n",
      "    mean_raw_obs_processing_ms: 1.8712984972058408\n",
      "  time_since_restore: 4219.237491369247\n",
      "  time_this_iter_s: 24.13872742652893\n",
      "  time_total_s: 4219.237491369247\n",
      "  timers:\n",
      "    learn_throughput: 1435.213\n",
      "    learn_time_ms: 696.761\n",
      "    load_throughput: 40061.32\n",
      "    load_time_ms: 24.962\n",
      "    sample_throughput: 39.2\n",
      "    sample_time_ms: 25510.466\n",
      "    update_time_ms: 2.389\n",
      "  timestamp: 1635067087\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         4219.24</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\"> -3.2331</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            323.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-18-30\n",
      "  done: false\n",
      "  episode_len_mean: 318.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.188299999999976\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 565\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0941379189491272\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01162352418585877\n",
      "          policy_loss: 0.03931584602428807\n",
      "          total_loss: 0.038880849877993263\n",
      "          vf_explained_var: 0.2342676818370819\n",
      "          vf_loss: 0.010342929996032682\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.00606060606061\n",
      "    ram_util_percent: 38.70303030303031\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039044167380541506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.075095511454194\n",
      "    mean_inference_ms: 1.9342171005192987\n",
      "    mean_raw_obs_processing_ms: 1.8707131754713786\n",
      "  time_since_restore: 4241.843332290649\n",
      "  time_this_iter_s: 22.605840921401978\n",
      "  time_total_s: 4241.843332290649\n",
      "  timers:\n",
      "    learn_throughput: 1436.912\n",
      "    learn_time_ms: 695.937\n",
      "    load_throughput: 40051.029\n",
      "    load_time_ms: 24.968\n",
      "    sample_throughput: 39.361\n",
      "    sample_time_ms: 25405.789\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1635067110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         4241.84</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\"> -3.1883</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            318.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-18-52\n",
      "  done: false\n",
      "  episode_len_mean: 316.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.169199999999977\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 568\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014062500000000006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0148552437623342\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004208825081632502\n",
      "          policy_loss: -0.08179274996121724\n",
      "          total_loss: -0.07806623743640052\n",
      "          vf_explained_var: 0.046128612011671066\n",
      "          vf_loss: 0.013815875713609987\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.25\n",
      "    ram_util_percent: 38.743750000000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03904282466095074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.07628238833508\n",
      "    mean_inference_ms: 1.9341718487323245\n",
      "    mean_raw_obs_processing_ms: 1.870523833868812\n",
      "  time_since_restore: 4264.293516397476\n",
      "  time_this_iter_s: 22.450184106826782\n",
      "  time_total_s: 4264.293516397476\n",
      "  timers:\n",
      "    learn_throughput: 1439.256\n",
      "    learn_time_ms: 694.803\n",
      "    load_throughput: 40519.978\n",
      "    load_time_ms: 24.679\n",
      "    sample_throughput: 39.582\n",
      "    sample_time_ms: 25264.016\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1635067132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         4264.29</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\"> -3.1692</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            316.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-19-34\n",
      "  done: false\n",
      "  episode_len_mean: 315.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.1504999999999774\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 572\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9162542283535003\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014439785887371236\n",
      "          policy_loss: 0.011914116309748755\n",
      "          total_loss: 0.01753151011135843\n",
      "          vf_explained_var: 0.0393756702542305\n",
      "          vf_loss: 0.014678407605323527\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.970689655172414\n",
      "    ram_util_percent: 38.63793103448276\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039041146210890935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.07873630015071\n",
      "    mean_inference_ms: 1.93411456679778\n",
      "    mean_raw_obs_processing_ms: 1.8739562441090771\n",
      "  time_since_restore: 4305.401714324951\n",
      "  time_this_iter_s: 41.108197927474976\n",
      "  time_total_s: 4305.401714324951\n",
      "  timers:\n",
      "    learn_throughput: 1441.499\n",
      "    learn_time_ms: 693.723\n",
      "    load_throughput: 40793.361\n",
      "    load_time_ms: 24.514\n",
      "    sample_throughput: 37.054\n",
      "    sample_time_ms: 26987.524\n",
      "    update_time_ms: 2.367\n",
      "  timestamp: 1635067174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">          4305.4</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -3.1505</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            315.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 313.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.1389999999999763\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 575\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8464949793285794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024863771073677692\n",
      "          policy_loss: -0.11401751645737224\n",
      "          total_loss: -0.10783302378323344\n",
      "          vf_explained_var: 0.0664740800857544\n",
      "          vf_loss: 0.014474618300381634\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.21142857142857\n",
      "    ram_util_percent: 38.70857142857143\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903980549642831\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.081126699227564\n",
      "    mean_inference_ms: 1.9340718553333358\n",
      "    mean_raw_obs_processing_ms: 1.8766219599524658\n",
      "  time_since_restore: 4329.576494932175\n",
      "  time_this_iter_s: 24.17478060722351\n",
      "  time_total_s: 4329.576494932175\n",
      "  timers:\n",
      "    learn_throughput: 1442.553\n",
      "    learn_time_ms: 693.215\n",
      "    load_throughput: 40505.109\n",
      "    load_time_ms: 24.688\n",
      "    sample_throughput: 37.12\n",
      "    sample_time_ms: 26939.973\n",
      "    update_time_ms: 2.351\n",
      "  timestamp: 1635067198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         4329.58</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">  -3.139</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">             313.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 310.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.1080999999999777\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 579\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010546874999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7504314992162916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010479130493711688\n",
      "          policy_loss: 0.005313595218790902\n",
      "          total_loss: 0.007898759014076656\n",
      "          vf_explained_var: 0.23124147951602936\n",
      "          vf_loss: 0.009978955761632985\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.28571428571429\n",
      "    ram_util_percent: 38.67714285714286\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903767711907782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.085282797355276\n",
      "    mean_inference_ms: 1.934010522856598\n",
      "    mean_raw_obs_processing_ms: 1.8802668814122803\n",
      "  time_since_restore: 4354.45800280571\n",
      "  time_this_iter_s: 24.881507873535156\n",
      "  time_total_s: 4354.45800280571\n",
      "  timers:\n",
      "    learn_throughput: 1446.159\n",
      "    learn_time_ms: 691.487\n",
      "    load_throughput: 40326.822\n",
      "    load_time_ms: 24.797\n",
      "    sample_throughput: 39.624\n",
      "    sample_time_ms: 25237.419\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1635067223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         4354.46</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\"> -3.1081</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            310.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 304.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.048899999999979\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 583\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010546874999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6377560648653242\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0056084278864659805\n",
      "          policy_loss: -0.039224838299883734\n",
      "          total_loss: -0.02959747165441513\n",
      "          vf_explained_var: 0.04889344051480293\n",
      "          vf_loss: 0.015945776758922472\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.20789473684211\n",
      "    ram_util_percent: 38.786842105263155\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039034951369940875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.091381039075618\n",
      "    mean_inference_ms: 1.9339407185376893\n",
      "    mean_raw_obs_processing_ms: 1.8799232440326643\n",
      "  time_since_restore: 4380.914654970169\n",
      "  time_this_iter_s: 26.45665216445923\n",
      "  time_total_s: 4380.914654970169\n",
      "  timers:\n",
      "    learn_throughput: 1444.787\n",
      "    learn_time_ms: 692.143\n",
      "    load_throughput: 40414.99\n",
      "    load_time_ms: 24.743\n",
      "    sample_throughput: 39.25\n",
      "    sample_time_ms: 25477.788\n",
      "    update_time_ms: 2.372\n",
      "  timestamp: 1635067249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         4380.91</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\"> -3.0489</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            304.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-21-16\n",
      "  done: false\n",
      "  episode_len_mean: 298.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.9861999999999806\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 587\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010546874999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6852959811687469\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003629261640041174\n",
      "          policy_loss: -0.1137866158452299\n",
      "          total_loss: -0.10266065945227941\n",
      "          vf_explained_var: 0.12166339159011841\n",
      "          vf_loss: 0.01794063966307375\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.323684210526324\n",
      "    ram_util_percent: 38.797368421052624\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903173820377405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.099848816192445\n",
      "    mean_inference_ms: 1.9338624674857352\n",
      "    mean_raw_obs_processing_ms: 1.8798648318845568\n",
      "  time_since_restore: 4407.661442041397\n",
      "  time_this_iter_s: 26.746787071228027\n",
      "  time_total_s: 4407.661442041397\n",
      "  timers:\n",
      "    learn_throughput: 1446.193\n",
      "    learn_time_ms: 691.471\n",
      "    load_throughput: 40263.527\n",
      "    load_time_ms: 24.836\n",
      "    sample_throughput: 38.957\n",
      "    sample_time_ms: 25669.047\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1635067276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         4407.66</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> -2.9862</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            298.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-21-42\n",
      "  done: false\n",
      "  episode_len_mean: 293.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.9397999999999813\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 592\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7669538974761962\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011827676565330354\n",
      "          policy_loss: -0.005766919172472424\n",
      "          total_loss: 0.0006660285095373789\n",
      "          vf_explained_var: 0.3229653537273407\n",
      "          vf_loss: 0.014040116417325206\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06578947368421\n",
      "    ram_util_percent: 38.77105263157895\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03902745814883276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.11244749966644\n",
      "    mean_inference_ms: 1.9337617539089427\n",
      "    mean_raw_obs_processing_ms: 1.8802214115456284\n",
      "  time_since_restore: 4434.130346536636\n",
      "  time_this_iter_s: 26.468904495239258\n",
      "  time_total_s: 4434.130346536636\n",
      "  timers:\n",
      "    learn_throughput: 1449.138\n",
      "    learn_time_ms: 690.065\n",
      "    load_throughput: 40595.866\n",
      "    load_time_ms: 24.633\n",
      "    sample_throughput: 38.885\n",
      "    sample_time_ms: 25716.741\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1635067302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         4434.13</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\"> -2.9398</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            293.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 290.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.902399999999982\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 595\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8445833451218075\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013589703063238584\n",
      "          policy_loss: -0.11853563611706099\n",
      "          total_loss: -0.11529850843879912\n",
      "          vf_explained_var: 0.46765056252479553\n",
      "          vf_loss: 0.011611295719113615\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.88108108108108\n",
      "    ram_util_percent: 38.74594594594595\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0390249010519541\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.12131924030371\n",
      "    mean_inference_ms: 1.9337045190892876\n",
      "    mean_raw_obs_processing_ms: 1.8805666059633197\n",
      "  time_since_restore: 4460.106959104538\n",
      "  time_this_iter_s: 25.97661256790161\n",
      "  time_total_s: 4460.106959104538\n",
      "  timers:\n",
      "    learn_throughput: 1449.188\n",
      "    learn_time_ms: 690.042\n",
      "    load_throughput: 40802.845\n",
      "    load_time_ms: 24.508\n",
      "    sample_throughput: 38.789\n",
      "    sample_time_ms: 25780.467\n",
      "    update_time_ms: 2.379\n",
      "  timestamp: 1635067328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         4460.11</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\"> -2.9024</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            290.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-22-51\n",
      "  done: false\n",
      "  episode_len_mean: 280.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.8072999999999837\n",
      "  episode_reward_min: -4.069999999999958\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 600\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8989180624485016\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016881465805045022\n",
      "          policy_loss: -0.004981250067551931\n",
      "          total_loss: -0.004279740610056453\n",
      "          vf_explained_var: 0.6842136383056641\n",
      "          vf_loss: 0.009601668014915453\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.40483870967742\n",
      "    ram_util_percent: 38.71774193548386\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039020914432045445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.140071107071893\n",
      "    mean_inference_ms: 1.9336215512714232\n",
      "    mean_raw_obs_processing_ms: 1.8860219275917987\n",
      "  time_since_restore: 4503.181332826614\n",
      "  time_this_iter_s: 43.074373722076416\n",
      "  time_total_s: 4503.181332826614\n",
      "  timers:\n",
      "    learn_throughput: 1451.727\n",
      "    learn_time_ms: 688.835\n",
      "    load_throughput: 40739.513\n",
      "    load_time_ms: 24.546\n",
      "    sample_throughput: 36.133\n",
      "    sample_time_ms: 27675.182\n",
      "    update_time_ms: 2.381\n",
      "  timestamp: 1635067371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         4503.18</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\"> -2.8073</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -4.07</td><td style=\"text-align: right;\">            280.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-23-16\n",
      "  done: false\n",
      "  episode_len_mean: 277.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.775199999999984\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 603\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.10825903084543\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009756927087541101\n",
      "          policy_loss: 0.04261741240819295\n",
      "          total_loss: 0.04436029709047741\n",
      "          vf_explained_var: 0.414367139339447\n",
      "          vf_loss: 0.012774021918368008\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.354285714285716\n",
      "    ram_util_percent: 38.50285714285713\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901866656412095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.15287101030674\n",
      "    mean_inference_ms: 1.933576970806409\n",
      "    mean_raw_obs_processing_ms: 1.8895383199901452\n",
      "  time_since_restore: 4527.915162563324\n",
      "  time_this_iter_s: 24.733829736709595\n",
      "  time_total_s: 4527.915162563324\n",
      "  timers:\n",
      "    learn_throughput: 1450.691\n",
      "    learn_time_ms: 689.327\n",
      "    load_throughput: 40907.108\n",
      "    load_time_ms: 24.446\n",
      "    sample_throughput: 35.858\n",
      "    sample_time_ms: 27887.581\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1635067396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         4527.92</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> -2.7752</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            277.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-23-39\n",
      "  done: false\n",
      "  episode_len_mean: 276.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7600999999999853\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 606\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9501050843132867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015163061417526534\n",
      "          policy_loss: -0.0771956322921647\n",
      "          total_loss: -0.0722233562833733\n",
      "          vf_explained_var: 0.3127531409263611\n",
      "          vf_loss: 0.014393361409505208\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.125\n",
      "    ram_util_percent: 38.63125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901640566923734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.1659509454291\n",
      "    mean_inference_ms: 1.9335341586570198\n",
      "    mean_raw_obs_processing_ms: 1.8930753919967105\n",
      "  time_since_restore: 4550.583853721619\n",
      "  time_this_iter_s: 22.668691158294678\n",
      "  time_total_s: 4550.583853721619\n",
      "  timers:\n",
      "    learn_throughput: 1450.734\n",
      "    learn_time_ms: 689.306\n",
      "    load_throughput: 40501.51\n",
      "    load_time_ms: 24.69\n",
      "    sample_throughput: 35.83\n",
      "    sample_time_ms: 27909.222\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1635067419\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         4550.58</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\"> -2.7601</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            276.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-24-02\n",
      "  done: false\n",
      "  episode_len_mean: 275.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7511999999999848\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 610\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9246089140574137\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007357286029849784\n",
      "          policy_loss: 0.027601581977473366\n",
      "          total_loss: 0.03191153564386898\n",
      "          vf_explained_var: 0.25249236822128296\n",
      "          vf_loss: 0.013517245246718327\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.85882352941176\n",
      "    ram_util_percent: 38.71764705882352\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039013311343376815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.18359655253659\n",
      "    mean_inference_ms: 1.9334788108756584\n",
      "    mean_raw_obs_processing_ms: 1.8959293463444715\n",
      "  time_since_restore: 4573.713850736618\n",
      "  time_this_iter_s: 23.12999701499939\n",
      "  time_total_s: 4573.713850736618\n",
      "  timers:\n",
      "    learn_throughput: 1448.114\n",
      "    learn_time_ms: 690.553\n",
      "    load_throughput: 40382.499\n",
      "    load_time_ms: 24.763\n",
      "    sample_throughput: 38.299\n",
      "    sample_time_ms: 26110.062\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1635067442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         4573.71</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\"> -2.7512</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            275.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-24-26\n",
      "  done: false\n",
      "  episode_len_mean: 273.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7343999999999857\n",
      "  episode_reward_min: -3.3599999999999723\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 613\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8579705191983117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011643961659285138\n",
      "          policy_loss: -0.1258517359693845\n",
      "          total_loss: -0.12189288031723765\n",
      "          vf_explained_var: 0.28446245193481445\n",
      "          vf_loss: 0.012477155557523172\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.21818181818182\n",
      "    ram_util_percent: 38.79393939393939\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039010876757835614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.197115916047462\n",
      "    mean_inference_ms: 1.9334387519149192\n",
      "    mean_raw_obs_processing_ms: 1.8966840205351858\n",
      "  time_since_restore: 4597.301379919052\n",
      "  time_this_iter_s: 23.587529182434082\n",
      "  time_total_s: 4597.301379919052\n",
      "  timers:\n",
      "    learn_throughput: 1450.727\n",
      "    learn_time_ms: 689.31\n",
      "    load_throughput: 40441.722\n",
      "    load_time_ms: 24.727\n",
      "    sample_throughput: 38.384\n",
      "    sample_time_ms: 26052.588\n",
      "    update_time_ms: 2.438\n",
      "  timestamp: 1635067466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">          4597.3</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\"> -2.7344</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.36</td><td style=\"text-align: right;\">            273.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-24-48\n",
      "  done: false\n",
      "  episode_len_mean: 272.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7214999999999856\n",
      "  episode_reward_min: -3.189999999999976\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 617\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8498096982638041\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005471120873725965\n",
      "          policy_loss: 0.0659836181335979\n",
      "          total_loss: 0.0694425602753957\n",
      "          vf_explained_var: 0.1635911911725998\n",
      "          vf_loss: 0.011928186694987946\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.034375\n",
      "    ram_util_percent: 38.8\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039007615356258026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.215394553863582\n",
      "    mean_inference_ms: 1.9333871758949313\n",
      "    mean_raw_obs_processing_ms: 1.897770455888436\n",
      "  time_since_restore: 4619.620555639267\n",
      "  time_this_iter_s: 22.319175720214844\n",
      "  time_total_s: 4619.620555639267\n",
      "  timers:\n",
      "    learn_throughput: 1447.641\n",
      "    learn_time_ms: 690.779\n",
      "    load_throughput: 40619.77\n",
      "    load_time_ms: 24.619\n",
      "    sample_throughput: 38.767\n",
      "    sample_time_ms: 25795.007\n",
      "    update_time_ms: 2.456\n",
      "  timestamp: 1635067488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         4619.62</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> -2.7215</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.19</td><td style=\"text-align: right;\">            272.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 270.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7079999999999864\n",
      "  episode_reward_min: -3.089999999999978\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 621\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.829505189259847\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009065927012292087\n",
      "          policy_loss: -0.013289888492888875\n",
      "          total_loss: -0.007592696075638135\n",
      "          vf_explained_var: 0.12677377462387085\n",
      "          vf_loss: 0.013944433732993073\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.129411764705885\n",
      "    ram_util_percent: 38.79705882352941\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039004321310283814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.234118066254123\n",
      "    mean_inference_ms: 1.933337420997464\n",
      "    mean_raw_obs_processing_ms: 1.898906510626039\n",
      "  time_since_restore: 4643.629764556885\n",
      "  time_this_iter_s: 24.009208917617798\n",
      "  time_total_s: 4643.629764556885\n",
      "  timers:\n",
      "    learn_throughput: 1450.15\n",
      "    learn_time_ms: 689.584\n",
      "    load_throughput: 40306.786\n",
      "    load_time_ms: 24.81\n",
      "    sample_throughput: 39.137\n",
      "    sample_time_ms: 25551.266\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1635067512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         4643.63</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\">  -2.708</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.09</td><td style=\"text-align: right;\">             270.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-25-34\n",
      "  done: false\n",
      "  episode_len_mean: 270.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.709299999999986\n",
      "  episode_reward_min: -3.089999999999978\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 624\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7613679548104604\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00709662629457368\n",
      "          policy_loss: 0.05254074219200346\n",
      "          total_loss: 0.055639647609657714\n",
      "          vf_explained_var: 0.03896815702319145\n",
      "          vf_loss: 0.010675159146517722\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.8\n",
      "    ram_util_percent: 38.796875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03900185959660637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.247824227586555\n",
      "    mean_inference_ms: 1.9333014308008365\n",
      "    mean_raw_obs_processing_ms: 1.899862141675374\n",
      "  time_since_restore: 4665.523989439011\n",
      "  time_this_iter_s: 21.894224882125854\n",
      "  time_total_s: 4665.523989439011\n",
      "  timers:\n",
      "    learn_throughput: 1447.768\n",
      "    learn_time_ms: 690.718\n",
      "    load_throughput: 40229.041\n",
      "    load_time_ms: 24.858\n",
      "    sample_throughput: 39.897\n",
      "    sample_time_ms: 25064.797\n",
      "    update_time_ms: 2.463\n",
      "  timestamp: 1635067534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         4665.52</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\"> -2.7093</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.09</td><td style=\"text-align: right;\">            270.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-25-58\n",
      "  done: false\n",
      "  episode_len_mean: 271.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7130999999999865\n",
      "  episode_reward_min: -3.1199999999999775\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 627\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6776842289500766\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010076159427301345\n",
      "          policy_loss: -0.11231108986669117\n",
      "          total_loss: -0.1046509182287587\n",
      "          vf_explained_var: 0.1250460147857666\n",
      "          vf_loss: 0.014383881818503141\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.088235294117645\n",
      "    ram_util_percent: 38.758823529411764\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038999372722786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.261435733699383\n",
      "    mean_inference_ms: 1.9332658671363072\n",
      "    mean_raw_obs_processing_ms: 1.9008457148705296\n",
      "  time_since_restore: 4689.382532119751\n",
      "  time_this_iter_s: 23.858542680740356\n",
      "  time_total_s: 4689.382532119751\n",
      "  timers:\n",
      "    learn_throughput: 1446.126\n",
      "    learn_time_ms: 691.503\n",
      "    load_throughput: 40263.798\n",
      "    load_time_ms: 24.836\n",
      "    sample_throughput: 40.318\n",
      "    sample_time_ms: 24803.052\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1635067558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         4689.38</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\"> -2.7131</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.12</td><td style=\"text-align: right;\">            271.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-26-40\n",
      "  done: false\n",
      "  episode_len_mean: 270.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.708499999999986\n",
      "  episode_reward_min: -3.1199999999999775\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 631\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6987946344746484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005109417044155314\n",
      "          policy_loss: -0.053615400112337536\n",
      "          total_loss: -0.04686129308409161\n",
      "          vf_explained_var: 0.02957269363105297\n",
      "          vf_loss: 0.013715107840188366\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.43666666666668\n",
      "    ram_util_percent: 38.635\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038996068382576354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.279917836714013\n",
      "    mean_inference_ms: 1.93321897570285\n",
      "    mean_raw_obs_processing_ms: 1.9054039177663156\n",
      "  time_since_restore: 4731.82518863678\n",
      "  time_this_iter_s: 42.44265651702881\n",
      "  time_total_s: 4731.82518863678\n",
      "  timers:\n",
      "    learn_throughput: 1447.937\n",
      "    learn_time_ms: 690.638\n",
      "    load_throughput: 40015.837\n",
      "    load_time_ms: 24.99\n",
      "    sample_throughput: 37.807\n",
      "    sample_time_ms: 26450.365\n",
      "    update_time_ms: 2.421\n",
      "  timestamp: 1635067600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         4731.83</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> -2.7085</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.12</td><td style=\"text-align: right;\">            270.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-27-04\n",
      "  done: false\n",
      "  episode_len_mean: 270.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.706899999999986\n",
      "  episode_reward_min: -3.1199999999999775\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 635\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005273437499999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.682581326034334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004504878386422975\n",
      "          policy_loss: 0.02998970929119322\n",
      "          total_loss: 0.036786277012692555\n",
      "          vf_explained_var: 0.16441980004310608\n",
      "          vf_loss: 0.01359862731769681\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.297142857142866\n",
      "    ram_util_percent: 38.69714285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03899287790937138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.298247560021697\n",
      "    mean_inference_ms: 1.9331746447168667\n",
      "    mean_raw_obs_processing_ms: 1.9099953530326632\n",
      "  time_since_restore: 4755.873109579086\n",
      "  time_this_iter_s: 24.04792094230652\n",
      "  time_total_s: 4755.873109579086\n",
      "  timers:\n",
      "    learn_throughput: 1447.718\n",
      "    learn_time_ms: 690.742\n",
      "    load_throughput: 39844.169\n",
      "    load_time_ms: 25.098\n",
      "    sample_throughput: 40.737\n",
      "    sample_time_ms: 24547.526\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1635067624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         4755.87</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\"> -2.7069</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.12</td><td style=\"text-align: right;\">            270.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-27-27\n",
      "  done: false\n",
      "  episode_len_mean: 271.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7153999999999865\n",
      "  episode_reward_min: -3.1199999999999775\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 638\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0026367187499999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7162509825494554\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011571867532039743\n",
      "          policy_loss: -0.07549912598397997\n",
      "          total_loss: -0.06895272955298423\n",
      "          vf_explained_var: 0.03502770885825157\n",
      "          vf_loss: 0.013678392654077874\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.415625000000006\n",
      "    ram_util_percent: 38.7125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03899053830064492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.311489237407905\n",
      "    mean_inference_ms: 1.9331414078747275\n",
      "    mean_raw_obs_processing_ms: 1.9135233198555976\n",
      "  time_since_restore: 4778.872062921524\n",
      "  time_this_iter_s: 22.998953342437744\n",
      "  time_total_s: 4778.872062921524\n",
      "  timers:\n",
      "    learn_throughput: 1447.693\n",
      "    learn_time_ms: 690.754\n",
      "    load_throughput: 39944.345\n",
      "    load_time_ms: 25.035\n",
      "    sample_throughput: 41.027\n",
      "    sample_time_ms: 24374.094\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1635067647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         4778.87</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\"> -2.7154</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.12</td><td style=\"text-align: right;\">            271.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-27-49\n",
      "  done: false\n",
      "  episode_len_mean: 272.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.722299999999986\n",
      "  episode_reward_min: -3.1999999999999758\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 642\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0026367187499999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6910538600550757\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006478074073574507\n",
      "          policy_loss: 0.0429423976275656\n",
      "          total_loss: 0.050013127426306404\n",
      "          vf_explained_var: 0.06768281012773514\n",
      "          vf_loss: 0.0139641883265641\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03125\n",
      "    ram_util_percent: 38.703125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038987399837139766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.328592332439023\n",
      "    mean_inference_ms: 1.9330958492299803\n",
      "    mean_raw_obs_processing_ms: 1.9153484021248133\n",
      "  time_since_restore: 4800.928125619888\n",
      "  time_this_iter_s: 22.056062698364258\n",
      "  time_total_s: 4800.928125619888\n",
      "  timers:\n",
      "    learn_throughput: 1447.943\n",
      "    learn_time_ms: 690.635\n",
      "    load_throughput: 40259.663\n",
      "    load_time_ms: 24.839\n",
      "    sample_throughput: 41.13\n",
      "    sample_time_ms: 24313.143\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1635067669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         4800.93</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\"> -2.7223</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">                -3.2</td><td style=\"text-align: right;\">            272.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-28-11\n",
      "  done: false\n",
      "  episode_len_mean: 273.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.732499999999986\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 645\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0026367187499999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7176627006795672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006646126543123524\n",
      "          policy_loss: 0.048467638840277986\n",
      "          total_loss: 0.0530634885860814\n",
      "          vf_explained_var: -0.020498132333159447\n",
      "          vf_loss: 0.011754947605853279\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.78387096774194\n",
      "    ram_util_percent: 38.78064516129032\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03898499799372555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.34078919105787\n",
      "    mean_inference_ms: 1.933059905418655\n",
      "    mean_raw_obs_processing_ms: 1.9161085216730211\n",
      "  time_since_restore: 4822.581734418869\n",
      "  time_this_iter_s: 21.653608798980713\n",
      "  time_total_s: 4822.581734418869\n",
      "  timers:\n",
      "    learn_throughput: 1448.498\n",
      "    learn_time_ms: 690.37\n",
      "    load_throughput: 40351.652\n",
      "    load_time_ms: 24.782\n",
      "    sample_throughput: 41.381\n",
      "    sample_time_ms: 24165.789\n",
      "    update_time_ms: 2.444\n",
      "  timestamp: 1635067691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         4822.58</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> -2.7325</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            273.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-28-33\n",
      "  done: false\n",
      "  episode_len_mean: 274.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.7458999999999856\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 648\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0026367187499999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7317285378774007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007541014228045414\n",
      "          policy_loss: -0.03608293450540966\n",
      "          total_loss: -0.030687562459044988\n",
      "          vf_explained_var: -0.21096216142177582\n",
      "          vf_loss: 0.012692767858000782\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.012903225806454\n",
      "    ram_util_percent: 38.79677419354839\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0389828554755181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.352564902471933\n",
      "    mean_inference_ms: 1.9330237351037902\n",
      "    mean_raw_obs_processing_ms: 1.916804901547672\n",
      "  time_since_restore: 4844.009397506714\n",
      "  time_this_iter_s: 21.42766308784485\n",
      "  time_total_s: 4844.009397506714\n",
      "  timers:\n",
      "    learn_throughput: 1446.171\n",
      "    learn_time_ms: 691.481\n",
      "    load_throughput: 40484.348\n",
      "    load_time_ms: 24.701\n",
      "    sample_throughput: 41.756\n",
      "    sample_time_ms: 23948.803\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1635067713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         4844.01</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\"> -2.7459</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            274.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-28-56\n",
      "  done: false\n",
      "  episode_len_mean: 275.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.755899999999985\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 652\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0026367187499999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6845016512605879\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010847869576323117\n",
      "          policy_loss: 0.016808301624324586\n",
      "          total_loss: 0.026965695205661985\n",
      "          vf_explained_var: 0.06163075193762779\n",
      "          vf_loss: 0.016973805696600012\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03333333333334\n",
      "    ram_util_percent: 38.78787878787879\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03897999228598874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.367789515757778\n",
      "    mean_inference_ms: 1.932975638690284\n",
      "    mean_raw_obs_processing_ms: 1.9177177142069592\n",
      "  time_since_restore: 4867.25955581665\n",
      "  time_this_iter_s: 23.250158309936523\n",
      "  time_total_s: 4867.25955581665\n",
      "  timers:\n",
      "    learn_throughput: 1447.577\n",
      "    learn_time_ms: 690.81\n",
      "    load_throughput: 40315.077\n",
      "    load_time_ms: 24.805\n",
      "    sample_throughput: 41.593\n",
      "    sample_time_ms: 24042.474\n",
      "    update_time_ms: 2.394\n",
      "  timestamp: 1635067736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         4867.26</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\"> -2.7559</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            275.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-29-18\n",
      "  done: false\n",
      "  episode_len_mean: 277.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.770199999999985\n",
      "  episode_reward_min: -3.369999999999972\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 655\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0026367187499999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6990063144101037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014131637589043362\n",
      "          policy_loss: 0.04210004260142644\n",
      "          total_loss: 0.04843143762813674\n",
      "          vf_explained_var: 0.26493602991104126\n",
      "          vf_loss: 0.013284197908392848\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.934375\n",
      "    ram_util_percent: 38.759375\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03897785531357025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.37869073192669\n",
      "    mean_inference_ms: 1.9329398535684843\n",
      "    mean_raw_obs_processing_ms: 1.9183857061720884\n",
      "  time_since_restore: 4889.52666592598\n",
      "  time_this_iter_s: 22.267110109329224\n",
      "  time_total_s: 4889.52666592598\n",
      "  timers:\n",
      "    learn_throughput: 1445.56\n",
      "    learn_time_ms: 691.773\n",
      "    load_throughput: 40535.368\n",
      "    load_time_ms: 24.67\n",
      "    sample_throughput: 41.898\n",
      "    sample_time_ms: 23867.418\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1635067758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         4889.53</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\"> -2.7702</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.37</td><td style=\"text-align: right;\">            277.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-29-41\n",
      "  done: false\n",
      "  episode_len_mean: 278.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.7828999999999837\n",
      "  episode_reward_min: -3.369999999999972\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 658\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0026367187499999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7907501313421461\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021872112006661205\n",
      "          policy_loss: -0.10596111855573125\n",
      "          total_loss: -0.09594203498628405\n",
      "          vf_explained_var: 0.09261095523834229\n",
      "          vf_loss: 0.017868916918006208\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.0625\n",
      "    ram_util_percent: 38.725\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03897573095803828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.38935832273816\n",
      "    mean_inference_ms: 1.9329039639837533\n",
      "    mean_raw_obs_processing_ms: 1.9189961822883286\n",
      "  time_since_restore: 4912.599596500397\n",
      "  time_this_iter_s: 23.072930574417114\n",
      "  time_total_s: 4912.599596500397\n",
      "  timers:\n",
      "    learn_throughput: 1446.527\n",
      "    learn_time_ms: 691.311\n",
      "    load_throughput: 40756.655\n",
      "    load_time_ms: 24.536\n",
      "    sample_throughput: 41.691\n",
      "    sample_time_ms: 23985.904\n",
      "    update_time_ms: 2.397\n",
      "  timestamp: 1635067781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">          4912.6</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\"> -2.7829</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.37</td><td style=\"text-align: right;\">            278.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-30-19\n",
      "  done: false\n",
      "  episode_len_mean: 279.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.7950999999999833\n",
      "  episode_reward_min: -3.369999999999972\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 662\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.003955078125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8467236002286275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006966149776747501\n",
      "          policy_loss: 0.027483062280548944\n",
      "          total_loss: 0.03572603455848164\n",
      "          vf_explained_var: 0.10971072316169739\n",
      "          vf_loss: 0.016682655674715836\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.70727272727274\n",
      "    ram_util_percent: 38.68545454545454\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038972951135495836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.402728988765926\n",
      "    mean_inference_ms: 1.9328570967192116\n",
      "    mean_raw_obs_processing_ms: 1.923192863969382\n",
      "  time_since_restore: 4950.73296380043\n",
      "  time_this_iter_s: 38.13336730003357\n",
      "  time_total_s: 4950.73296380043\n",
      "  timers:\n",
      "    learn_throughput: 1447.799\n",
      "    learn_time_ms: 690.704\n",
      "    load_throughput: 40476.339\n",
      "    load_time_ms: 24.706\n",
      "    sample_throughput: 39.349\n",
      "    sample_time_ms: 25413.718\n",
      "    update_time_ms: 2.513\n",
      "  timestamp: 1635067819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         4950.73</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\"> -2.7951</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.37</td><td style=\"text-align: right;\">            279.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-30-39\n",
      "  done: false\n",
      "  episode_len_mean: 280.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.8040999999999836\n",
      "  episode_reward_min: -3.45999999999997\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 664\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.003955078125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8373099956247542\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010538531816393614\n",
      "          policy_loss: -0.08154165397087733\n",
      "          total_loss: -0.0781905750433604\n",
      "          vf_explained_var: 0.049423884600400925\n",
      "          vf_loss: 0.011682500873899295\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.846428571428575\n",
      "    ram_util_percent: 38.74642857142857\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038971593293242984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.40912416665516\n",
      "    mean_inference_ms: 1.9328328895738227\n",
      "    mean_raw_obs_processing_ms: 1.925256118772335\n",
      "  time_since_restore: 4970.324816226959\n",
      "  time_this_iter_s: 19.59185242652893\n",
      "  time_total_s: 4970.324816226959\n",
      "  timers:\n",
      "    learn_throughput: 1446.166\n",
      "    learn_time_ms: 691.483\n",
      "    load_throughput: 40727.804\n",
      "    load_time_ms: 24.553\n",
      "    sample_throughput: 43.238\n",
      "    sample_time_ms: 23127.919\n",
      "    update_time_ms: 2.604\n",
      "  timestamp: 1635067839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         4970.32</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\"> -2.8041</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.46</td><td style=\"text-align: right;\">            280.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-31-00\n",
      "  done: false\n",
      "  episode_len_mean: 282.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.822399999999984\n",
      "  episode_reward_min: -3.7699999999999636\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 667\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.003955078125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8711178229914771\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012322433145316225\n",
      "          policy_loss: -0.10331842228770256\n",
      "          total_loss: -0.09743738853269153\n",
      "          vf_explained_var: 0.10763037949800491\n",
      "          vf_loss: 0.014543475976420774\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.06333333333335\n",
      "    ram_util_percent: 38.74333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03896948150018838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.418483123856202\n",
      "    mean_inference_ms: 1.9327955891808595\n",
      "    mean_raw_obs_processing_ms: 1.928283206180521\n",
      "  time_since_restore: 4991.112284183502\n",
      "  time_this_iter_s: 20.78746795654297\n",
      "  time_total_s: 4991.112284183502\n",
      "  timers:\n",
      "    learn_throughput: 1444.692\n",
      "    learn_time_ms: 692.189\n",
      "    load_throughput: 40552.691\n",
      "    load_time_ms: 24.659\n",
      "    sample_throughput: 43.858\n",
      "    sample_time_ms: 22801.084\n",
      "    update_time_ms: 2.59\n",
      "  timestamp: 1635067860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         4991.11</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\"> -2.8224</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.77</td><td style=\"text-align: right;\">            282.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-31-19\n",
      "  done: false\n",
      "  episode_len_mean: 284.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.847399999999983\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 670\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.003955078125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8560397969351874\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00438354300100245\n",
      "          policy_loss: -0.02011920909086863\n",
      "          total_loss: -0.016134812848435508\n",
      "          vf_explained_var: 0.11540555953979492\n",
      "          vf_loss: 0.01252745538432565\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.84444444444445\n",
      "    ram_util_percent: 38.78518518518518\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03896738903791876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.427104028392282\n",
      "    mean_inference_ms: 1.9327580528325032\n",
      "    mean_raw_obs_processing_ms: 1.9295586450106421\n",
      "  time_since_restore: 5010.292512893677\n",
      "  time_this_iter_s: 19.18022871017456\n",
      "  time_total_s: 5010.292512893677\n",
      "  timers:\n",
      "    learn_throughput: 1446.566\n",
      "    learn_time_ms: 691.293\n",
      "    load_throughput: 39184.235\n",
      "    load_time_ms: 25.52\n",
      "    sample_throughput: 44.604\n",
      "    sample_time_ms: 22419.268\n",
      "    update_time_ms: 2.571\n",
      "  timestamp: 1635067879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         5010.29</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\"> -2.8474</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            284.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 286.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.865999999999983\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 673\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019775390625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9173774116569096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01186969895987065\n",
      "          policy_loss: 0.03649543250600497\n",
      "          total_loss: 0.03948986033598582\n",
      "          vf_explained_var: 0.19331346452236176\n",
      "          vf_loss: 0.012144725153403771\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.68518518518519\n",
      "    ram_util_percent: 38.8111111111111\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03896520841390781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.435045086102164\n",
      "    mean_inference_ms: 1.9327185838585441\n",
      "    mean_raw_obs_processing_ms: 1.929885151683206\n",
      "  time_since_restore: 5029.224067211151\n",
      "  time_this_iter_s: 18.931554317474365\n",
      "  time_total_s: 5029.224067211151\n",
      "  timers:\n",
      "    learn_throughput: 1445.732\n",
      "    learn_time_ms: 691.691\n",
      "    load_throughput: 39398.785\n",
      "    load_time_ms: 25.381\n",
      "    sample_throughput: 45.235\n",
      "    sample_time_ms: 22106.555\n",
      "    update_time_ms: 2.56\n",
      "  timestamp: 1635067898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         5029.22</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">  -2.866</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">             286.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-31-56\n",
      "  done: false\n",
      "  episode_len_mean: 289.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.8951999999999822\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 676\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019775390625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8454334563679166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005252527090567859\n",
      "          policy_loss: 0.056204361385769314\n",
      "          total_loss: 0.05965979082716836\n",
      "          vf_explained_var: -0.11869246512651443\n",
      "          vf_loss: 0.011899376235346103\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.51923076923076\n",
      "    ram_util_percent: 38.85384615384615\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03896305631495486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.44199715504543\n",
      "    mean_inference_ms: 1.9326771490033627\n",
      "    mean_raw_obs_processing_ms: 1.9302406206311784\n",
      "  time_since_restore: 5047.1498210430145\n",
      "  time_this_iter_s: 17.925753831863403\n",
      "  time_total_s: 5047.1498210430145\n",
      "  timers:\n",
      "    learn_throughput: 1444.591\n",
      "    learn_time_ms: 692.237\n",
      "    load_throughput: 39604.064\n",
      "    load_time_ms: 25.25\n",
      "    sample_throughput: 46.012\n",
      "    sample_time_ms: 21733.416\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1635067916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         5047.15</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\"> -2.8952</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            289.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-32-16\n",
      "  done: false\n",
      "  episode_len_mean: 292.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.9202999999999815\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 679\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019775390625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.794838007291158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01126811290230856\n",
      "          policy_loss: 0.03195317544870906\n",
      "          total_loss: 0.03597019770079189\n",
      "          vf_explained_var: -0.00602182699367404\n",
      "          vf_loss: 0.011943119010538794\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.003571428571426\n",
      "    ram_util_percent: 38.82857142857142\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03896096472226474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.448329168139022\n",
      "    mean_inference_ms: 1.9326339356010944\n",
      "    mean_raw_obs_processing_ms: 1.9305358604849148\n",
      "  time_since_restore: 5066.844998121262\n",
      "  time_this_iter_s: 19.69517707824707\n",
      "  time_total_s: 5066.844998121262\n",
      "  timers:\n",
      "    learn_throughput: 1445.451\n",
      "    learn_time_ms: 691.825\n",
      "    load_throughput: 39246.382\n",
      "    load_time_ms: 25.48\n",
      "    sample_throughput: 46.381\n",
      "    sample_time_ms: 21560.326\n",
      "    update_time_ms: 2.541\n",
      "  timestamp: 1635067936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         5066.84</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\"> -2.9203</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            292.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-32-38\n",
      "  done: false\n",
      "  episode_len_mean: 293.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.932899999999981\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 682\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019775390625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7598089834054311\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006962395494817386\n",
      "          policy_loss: 0.04727683671646648\n",
      "          total_loss: 0.05124665722250939\n",
      "          vf_explained_var: 0.0705261155962944\n",
      "          vf_loss: 0.011554141420250137\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.36875\n",
      "    ram_util_percent: 38.834374999999994\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0389589439610923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.45400155811006\n",
      "    mean_inference_ms: 1.932590960190278\n",
      "    mean_raw_obs_processing_ms: 1.930859318994016\n",
      "  time_since_restore: 5089.436396360397\n",
      "  time_this_iter_s: 22.591398239135742\n",
      "  time_total_s: 5089.436396360397\n",
      "  timers:\n",
      "    learn_throughput: 1445.22\n",
      "    learn_time_ms: 691.936\n",
      "    load_throughput: 39285.715\n",
      "    load_time_ms: 25.455\n",
      "    sample_throughput: 46.524\n",
      "    sample_time_ms: 21494.356\n",
      "    update_time_ms: 2.55\n",
      "  timestamp: 1635067958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         5089.44</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -2.9329</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            293.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-33-02\n",
      "  done: false\n",
      "  episode_len_mean: 294.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.9431999999999814\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 686\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019775390625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6714432272646162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008032189131959161\n",
      "          policy_loss: 0.022115484873453776\n",
      "          total_loss: 0.03069432477156321\n",
      "          vf_explained_var: 0.08153916150331497\n",
      "          vf_loss: 0.015277387325962384\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.14117647058823\n",
      "    ram_util_percent: 38.79117647058823\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03895629670101574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.461164971460256\n",
      "    mean_inference_ms: 1.932532864989709\n",
      "    mean_raw_obs_processing_ms: 1.931244703743951\n",
      "  time_since_restore: 5113.391861200333\n",
      "  time_this_iter_s: 23.955464839935303\n",
      "  time_total_s: 5113.391861200333\n",
      "  timers:\n",
      "    learn_throughput: 1446.503\n",
      "    learn_time_ms: 691.322\n",
      "    load_throughput: 39495.801\n",
      "    load_time_ms: 25.319\n",
      "    sample_throughput: 46.16\n",
      "    sample_time_ms: 21663.931\n",
      "    update_time_ms: 2.566\n",
      "  timestamp: 1635067982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         5113.39</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\"> -2.9432</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            294.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-33-26\n",
      "  done: false\n",
      "  episode_len_mean: 295.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.9548999999999803\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 689\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019775390625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.865153306722641\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025922558279099988\n",
      "          policy_loss: 0.02618957700000869\n",
      "          total_loss: 0.028132074657413693\n",
      "          vf_explained_var: 0.23936070501804352\n",
      "          vf_loss: 0.010542765187306537\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.982352941176465\n",
      "    ram_util_percent: 38.77647058823529\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038954365624797184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.466052344789176\n",
      "    mean_inference_ms: 1.9324888261271271\n",
      "    mean_raw_obs_processing_ms: 1.9315380390896661\n",
      "  time_since_restore: 5136.769028425217\n",
      "  time_this_iter_s: 23.377167224884033\n",
      "  time_total_s: 5136.769028425217\n",
      "  timers:\n",
      "    learn_throughput: 1447.543\n",
      "    learn_time_ms: 690.826\n",
      "    load_throughput: 39369.717\n",
      "    load_time_ms: 25.4\n",
      "    sample_throughput: 46.094\n",
      "    sample_time_ms: 21694.771\n",
      "    update_time_ms: 2.562\n",
      "  timestamp: 1635068006\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         5136.77</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\"> -2.9549</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            295.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-34-07\n",
      "  done: false\n",
      "  episode_len_mean: 296.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.965999999999981\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 693\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0029663085937500014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0465031663576763\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013567809773372849\n",
      "          policy_loss: 0.013674607459041808\n",
      "          total_loss: 0.02038939976029926\n",
      "          vf_explained_var: 0.09890877455472946\n",
      "          vf_loss: 0.017139574461099174\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.11724137931034\n",
      "    ram_util_percent: 38.672413793103445\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03895168194646152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.47249348824771\n",
      "    mean_inference_ms: 1.9324277138921584\n",
      "    mean_raw_obs_processing_ms: 1.9348295028851583\n",
      "  time_since_restore: 5177.853828668594\n",
      "  time_this_iter_s: 41.084800243377686\n",
      "  time_total_s: 5177.853828668594\n",
      "  timers:\n",
      "    learn_throughput: 1447.316\n",
      "    learn_time_ms: 690.934\n",
      "    load_throughput: 39509.083\n",
      "    load_time_ms: 25.311\n",
      "    sample_throughput: 45.475\n",
      "    sample_time_ms: 21989.996\n",
      "    update_time_ms: 2.461\n",
      "  timestamp: 1635068047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         5177.85</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\">  -2.966</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             296.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-34-28\n",
      "  done: false\n",
      "  episode_len_mean: 298.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.9845999999999804\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 696\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0029663085937500014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1139583792951373\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012125235852736003\n",
      "          policy_loss: 0.051551187617911234\n",
      "          total_loss: 0.05120601322915819\n",
      "          vf_explained_var: 0.23145675659179688\n",
      "          vf_loss: 0.010758443555742916\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.86774193548386\n",
      "    ram_util_percent: 38.78709677419354\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03894967644052053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.476576227650508\n",
      "    mean_inference_ms: 1.932380265359491\n",
      "    mean_raw_obs_processing_ms: 1.9364739777037294\n",
      "  time_since_restore: 5199.620126724243\n",
      "  time_this_iter_s: 21.766298055648804\n",
      "  time_total_s: 5199.620126724243\n",
      "  timers:\n",
      "    learn_throughput: 1447.635\n",
      "    learn_time_ms: 690.782\n",
      "    load_throughput: 39302.759\n",
      "    load_time_ms: 25.444\n",
      "    sample_throughput: 45.03\n",
      "    sample_time_ms: 22207.57\n",
      "    update_time_ms: 2.363\n",
      "  timestamp: 1635068068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         5199.62</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\"> -2.9846</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            298.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-34-51\n",
      "  done: false\n",
      "  episode_len_mean: 299.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.9975999999999807\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 699\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0029663085937500014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.051825280321969\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00711656880285188\n",
      "          policy_loss: -0.11383669715788629\n",
      "          total_loss: -0.10968500384026103\n",
      "          vf_explained_var: 0.3259303867816925\n",
      "          vf_loss: 0.014648836799379852\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.054545454545455\n",
      "    ram_util_percent: 38.81212121212121\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03894764707957132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.480335558197\n",
      "    mean_inference_ms: 1.9323327603166858\n",
      "    mean_raw_obs_processing_ms: 1.93632262010716\n",
      "  time_since_restore: 5222.386095046997\n",
      "  time_this_iter_s: 22.765968322753906\n",
      "  time_total_s: 5222.386095046997\n",
      "  timers:\n",
      "    learn_throughput: 1448.78\n",
      "    learn_time_ms: 690.236\n",
      "    load_throughput: 39801.972\n",
      "    load_time_ms: 25.124\n",
      "    sample_throughput: 44.63\n",
      "    sample_time_ms: 22406.242\n",
      "    update_time_ms: 2.379\n",
      "  timestamp: 1635068091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         5222.39</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\"> -2.9976</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            299.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-35-13\n",
      "  done: false\n",
      "  episode_len_mean: 301.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.011099999999979\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 703\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0029663085937500014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1668240931298999\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010260898440246614\n",
      "          policy_loss: -0.0073449584345022835\n",
      "          total_loss: -0.0018942617707782322\n",
      "          vf_explained_var: 0.13060230016708374\n",
      "          vf_loss: 0.01708850052414669\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.97666666666667\n",
      "    ram_util_percent: 38.81666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03894494113615507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.484796538723327\n",
      "    mean_inference_ms: 1.9322686796770046\n",
      "    mean_raw_obs_processing_ms: 1.9360857516719119\n",
      "  time_since_restore: 5243.697469711304\n",
      "  time_this_iter_s: 21.31137466430664\n",
      "  time_total_s: 5243.697469711304\n",
      "  timers:\n",
      "    learn_throughput: 1446.754\n",
      "    learn_time_ms: 691.203\n",
      "    load_throughput: 40914.65\n",
      "    load_time_ms: 24.441\n",
      "    sample_throughput: 44.21\n",
      "    sample_time_ms: 22619.07\n",
      "    update_time_ms: 2.388\n",
      "  timestamp: 1635068113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">          5243.7</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\"> -3.0111</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            301.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-35-34\n",
      "  done: false\n",
      "  episode_len_mean: 301.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.0141999999999793\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 706\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0029663085937500014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0463072233729893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007145120186509166\n",
      "          policy_loss: 0.05496701614724265\n",
      "          total_loss: 0.05514810383319855\n",
      "          vf_explained_var: 0.1783415973186493\n",
      "          vf_loss: 0.010622963443812397\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.719354838709684\n",
      "    ram_util_percent: 38.841935483870955\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03894288820587498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.48783395421811\n",
      "    mean_inference_ms: 1.932220992343536\n",
      "    mean_raw_obs_processing_ms: 1.9360099387977476\n",
      "  time_since_restore: 5265.313638925552\n",
      "  time_this_iter_s: 21.616169214248657\n",
      "  time_total_s: 5265.313638925552\n",
      "  timers:\n",
      "    learn_throughput: 1449.097\n",
      "    learn_time_ms: 690.085\n",
      "    load_throughput: 40367.03\n",
      "    load_time_ms: 24.773\n",
      "    sample_throughput: 43.69\n",
      "    sample_time_ms: 22888.342\n",
      "    update_time_ms: 2.381\n",
      "  timestamp: 1635068134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         5265.31</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\"> -3.0142</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            301.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-35-58\n",
      "  done: false\n",
      "  episode_len_mean: 301.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.0124999999999806\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 709\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0029663085937500014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8944700645075904\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00478118498709341\n",
      "          policy_loss: -0.1068779844376776\n",
      "          total_loss: -0.10233834832906723\n",
      "          vf_explained_var: 0.34625011682510376\n",
      "          vf_loss: 0.013470151906626092\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.970588235294116\n",
      "    ram_util_percent: 38.8470588235294\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038940882782990995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.490941934449992\n",
      "    mean_inference_ms: 1.9321737892342907\n",
      "    mean_raw_obs_processing_ms: 1.9359645865136392\n",
      "  time_since_restore: 5289.314450263977\n",
      "  time_this_iter_s: 24.000811338424683\n",
      "  time_total_s: 5289.314450263977\n",
      "  timers:\n",
      "    learn_throughput: 1450.409\n",
      "    learn_time_ms: 689.461\n",
      "    load_throughput: 40146.062\n",
      "    load_time_ms: 24.909\n",
      "    sample_throughput: 42.56\n",
      "    sample_time_ms: 23496.277\n",
      "    update_time_ms: 2.407\n",
      "  timestamp: 1635068158\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         5289.31</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -3.0125</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            301.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-36-20\n",
      "  done: false\n",
      "  episode_len_mean: 302.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.0207999999999795\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 713\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0014831542968750007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0449641585350036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009914869077241721\n",
      "          policy_loss: 0.023552653441826502\n",
      "          total_loss: 0.029945331522160105\n",
      "          vf_explained_var: 0.10308325290679932\n",
      "          vf_loss: 0.016827611604498492\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.00322580645161\n",
      "    ram_util_percent: 38.82258064516128\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03893835462752381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.494766894831283\n",
      "    mean_inference_ms: 1.9321107599148433\n",
      "    mean_raw_obs_processing_ms: 1.935865290083008\n",
      "  time_since_restore: 5310.890996456146\n",
      "  time_this_iter_s: 21.57654619216919\n",
      "  time_total_s: 5310.890996456146\n",
      "  timers:\n",
      "    learn_throughput: 1449.384\n",
      "    learn_time_ms: 689.948\n",
      "    load_throughput: 40525.42\n",
      "    load_time_ms: 24.676\n",
      "    sample_throughput: 42.222\n",
      "    sample_time_ms: 23684.167\n",
      "    update_time_ms: 2.393\n",
      "  timestamp: 1635068180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         5310.89</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\"> -3.0208</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            302.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-36-44\n",
      "  done: false\n",
      "  episode_len_mean: 301.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.013599999999979\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 716\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0014831542968750007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9005431287818485\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010466645589409908\n",
      "          policy_loss: -0.08888913740714391\n",
      "          total_loss: -0.08283815922008621\n",
      "          vf_explained_var: 0.193014457821846\n",
      "          vf_loss: 0.015040888337211476\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11428571428572\n",
      "    ram_util_percent: 38.80285714285714\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03893650281715879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.497787128794936\n",
      "    mean_inference_ms: 1.932065069303809\n",
      "    mean_raw_obs_processing_ms: 1.9358844082635627\n",
      "  time_since_restore: 5335.215275287628\n",
      "  time_this_iter_s: 24.324278831481934\n",
      "  time_total_s: 5335.215275287628\n",
      "  timers:\n",
      "    learn_throughput: 1449.325\n",
      "    learn_time_ms: 689.977\n",
      "    load_throughput: 40660.447\n",
      "    load_time_ms: 24.594\n",
      "    sample_throughput: 41.916\n",
      "    sample_time_ms: 23857.453\n",
      "    update_time_ms: 2.449\n",
      "  timestamp: 1635068204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         5335.22</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\"> -3.0136</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            301.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-37-24\n",
      "  done: false\n",
      "  episode_len_mean: 301.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -3.012499999999979\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 720\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0014831542968750007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9496441417270236\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004236021614676686\n",
      "          policy_loss: 0.022161853189269703\n",
      "          total_loss: 0.026823666733172206\n",
      "          vf_explained_var: 0.2622843086719513\n",
      "          vf_loss: 0.014151972045914995\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.87719298245613\n",
      "    ram_util_percent: 38.75263157894737\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03893407290139446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.501536268624474\n",
      "    mean_inference_ms: 1.9320036526229902\n",
      "    mean_raw_obs_processing_ms: 1.938924949182922\n",
      "  time_since_restore: 5375.008066654205\n",
      "  time_this_iter_s: 39.79279136657715\n",
      "  time_total_s: 5375.008066654205\n",
      "  timers:\n",
      "    learn_throughput: 1448.281\n",
      "    learn_time_ms: 690.474\n",
      "    load_throughput: 40610.213\n",
      "    load_time_ms: 24.624\n",
      "    sample_throughput: 39.307\n",
      "    sample_time_ms: 25440.68\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1635068244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         5375.01</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\"> -3.0125</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            301.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-37-43\n",
      "  done: false\n",
      "  episode_len_mean: 303.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -3.037599999999979\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 723\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007415771484375003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.318018811278873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019093074687854886\n",
      "          policy_loss: 0.023116533706585566\n",
      "          total_loss: 0.023925929516553878\n",
      "          vf_explained_var: 0.02782437577843666\n",
      "          vf_loss: 0.01397542329505086\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.74814814814814\n",
      "    ram_util_percent: 38.655555555555566\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03893222873440147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.50390923430649\n",
      "    mean_inference_ms: 1.9319572938727112\n",
      "    mean_raw_obs_processing_ms: 1.9412069011110489\n",
      "  time_since_restore: 5394.220949888229\n",
      "  time_this_iter_s: 19.212883234024048\n",
      "  time_total_s: 5394.220949888229\n",
      "  timers:\n",
      "    learn_throughput: 1449.07\n",
      "    learn_time_ms: 690.098\n",
      "    load_throughput: 40180.868\n",
      "    load_time_ms: 24.887\n",
      "    sample_throughput: 39.961\n",
      "    sample_time_ms: 25024.36\n",
      "    update_time_ms: 2.441\n",
      "  timestamp: 1635068263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         5394.22</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\"> -3.0376</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            303.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-38-10\n",
      "  done: false\n",
      "  episode_len_mean: 302.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -3.0232999999999794\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 727\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007415771484375003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.917121003733741\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009341571668987332\n",
      "          policy_loss: 0.02526189105378257\n",
      "          total_loss: 0.028866763330168195\n",
      "          vf_explained_var: 0.3030169904232025\n",
      "          vf_loss: 0.012769153429609206\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.039473684210535\n",
      "    ram_util_percent: 38.70000000000002\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03892999729690989\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.507490834534742\n",
      "    mean_inference_ms: 1.9318981633544166\n",
      "    mean_raw_obs_processing_ms: 1.9443122588061712\n",
      "  time_since_restore: 5420.468105316162\n",
      "  time_this_iter_s: 26.24715542793274\n",
      "  time_total_s: 5420.468105316162\n",
      "  timers:\n",
      "    learn_throughput: 1450.779\n",
      "    learn_time_ms: 689.285\n",
      "    load_throughput: 40222.83\n",
      "    load_time_ms: 24.862\n",
      "    sample_throughput: 42.478\n",
      "    sample_time_ms: 23541.445\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1635068290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         5420.47</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\"> -3.0233</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            302.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-38-33\n",
      "  done: false\n",
      "  episode_len_mean: 302.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -3.0293999999999794\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 730\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007415771484375003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9714514520433214\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007782934387197571\n",
      "          policy_loss: -0.05849104109737608\n",
      "          total_loss: -0.0566282225979699\n",
      "          vf_explained_var: 0.3061445951461792\n",
      "          vf_loss: 0.011571557596067174\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18529411764706\n",
      "    ram_util_percent: 38.7735294117647\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03892841417363458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.509777666042133\n",
      "    mean_inference_ms: 1.9318542373311052\n",
      "    mean_raw_obs_processing_ms: 1.9442479483071862\n",
      "  time_since_restore: 5444.368782758713\n",
      "  time_this_iter_s: 23.90067744255066\n",
      "  time_total_s: 5444.368782758713\n",
      "  timers:\n",
      "    learn_throughput: 1451.567\n",
      "    learn_time_ms: 688.911\n",
      "    load_throughput: 40050.723\n",
      "    load_time_ms: 24.968\n",
      "    sample_throughput: 42.096\n",
      "    sample_time_ms: 23755.125\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1635068313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         5444.37</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\"> -3.0294</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            302.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-38-58\n",
      "  done: false\n",
      "  episode_len_mean: 302.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -3.026599999999979\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 734\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007415771484375003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9595751040511661\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007814270756326517\n",
      "          policy_loss: 0.021541195611159008\n",
      "          total_loss: 0.028739330834812588\n",
      "          vf_explained_var: 0.09950647503137589\n",
      "          vf_loss: 0.016788093207610977\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.037142857142854\n",
      "    ram_util_percent: 38.83714285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03892620324590949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.512985350147744\n",
      "    mean_inference_ms: 1.9317937093907471\n",
      "    mean_raw_obs_processing_ms: 1.9441186761995422\n",
      "  time_since_restore: 5469.036031246185\n",
      "  time_this_iter_s: 24.667248487472534\n",
      "  time_total_s: 5469.036031246185\n",
      "  timers:\n",
      "    learn_throughput: 1450.545\n",
      "    learn_time_ms: 689.396\n",
      "    load_throughput: 40020.266\n",
      "    load_time_ms: 24.987\n",
      "    sample_throughput: 41.763\n",
      "    sample_time_ms: 23944.726\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1635068338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         5469.04</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\"> -3.0266</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            302.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-39-21\n",
      "  done: false\n",
      "  episode_len_mean: 302.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -3.0234999999999785\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 737\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007415771484375003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0945857054657406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010297430134883238\n",
      "          policy_loss: -0.10875963850153816\n",
      "          total_loss: -0.1029748490287198\n",
      "          vf_explained_var: 0.17495915293693542\n",
      "          vf_loss: 0.016723012261920505\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.00606060606061\n",
      "    ram_util_percent: 38.89393939393939\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03892447844918177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.515353236561563\n",
      "    mean_inference_ms: 1.9317470078676704\n",
      "    mean_raw_obs_processing_ms: 1.944030560372321\n",
      "  time_since_restore: 5492.170566558838\n",
      "  time_this_iter_s: 23.134535312652588\n",
      "  time_total_s: 5492.170566558838\n",
      "  timers:\n",
      "    learn_throughput: 1448.738\n",
      "    learn_time_ms: 690.256\n",
      "    load_throughput: 40300.977\n",
      "    load_time_ms: 24.813\n",
      "    sample_throughput: 41.448\n",
      "    sample_time_ms: 24126.339\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1635068361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         5492.17</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> -3.0235</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            302.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 303.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -3.0300999999999796\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 741\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007415771484375003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1138958851496379\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010943999704523696\n",
      "          policy_loss: 0.03087330808242162\n",
      "          total_loss: 0.03495210972097185\n",
      "          vf_explained_var: 0.23733194172382355\n",
      "          vf_loss: 0.015209642890840768\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.675\n",
      "    ram_util_percent: 38.83125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03892210057101672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.518502167859566\n",
      "    mean_inference_ms: 1.9316855664836703\n",
      "    mean_raw_obs_processing_ms: 1.9439839388231468\n",
      "  time_since_restore: 5514.775883674622\n",
      "  time_this_iter_s: 22.60531711578369\n",
      "  time_total_s: 5514.775883674622\n",
      "  timers:\n",
      "    learn_throughput: 1444.43\n",
      "    learn_time_ms: 692.315\n",
      "    load_throughput: 40747.429\n",
      "    load_time_ms: 24.541\n",
      "    sample_throughput: 41.282\n",
      "    sample_time_ms: 24223.441\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1635068384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         5514.78</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\"> -3.0301</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            303.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-40-09\n",
      "  done: false\n",
      "  episode_len_mean: 301.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -3.0103999999999793\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 745\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007415771484375003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9651622454325358\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010711594292686736\n",
      "          policy_loss: -0.007993876644306713\n",
      "          total_loss: -0.0030177159855763118\n",
      "          vf_explained_var: 0.26331302523612976\n",
      "          vf_loss: 0.014619839325961139\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.91351351351352\n",
      "    ram_util_percent: 38.82162162162162\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038919755400704886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.52224438427881\n",
      "    mean_inference_ms: 1.9316266449328463\n",
      "    mean_raw_obs_processing_ms: 1.9439817061436577\n",
      "  time_since_restore: 5540.208904504776\n",
      "  time_this_iter_s: 25.43302083015442\n",
      "  time_total_s: 5540.208904504776\n",
      "  timers:\n",
      "    learn_throughput: 1443.847\n",
      "    learn_time_ms: 692.594\n",
      "    load_throughput: 40852.762\n",
      "    load_time_ms: 24.478\n",
      "    sample_throughput: 41.04\n",
      "    sample_time_ms: 24366.523\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1635068409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         5540.21</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\"> -3.0104</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            301.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-40-35\n",
      "  done: false\n",
      "  episode_len_mean: 298.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.9893999999999803\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 749\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007415771484375003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8688527180088891\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010135313193396951\n",
      "          policy_loss: -0.008780756427182092\n",
      "          total_loss: -0.0038496378395292494\n",
      "          vf_explained_var: 0.23525837063789368\n",
      "          vf_loss: 0.013612126745283604\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.27297297297298\n",
      "    ram_util_percent: 38.81351351351351\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038917112720403886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.52660776676699\n",
      "    mean_inference_ms: 1.9315696478776174\n",
      "    mean_raw_obs_processing_ms: 1.9441847272507973\n",
      "  time_since_restore: 5565.978699922562\n",
      "  time_this_iter_s: 25.769795417785645\n",
      "  time_total_s: 5565.978699922562\n",
      "  timers:\n",
      "    learn_throughput: 1444.514\n",
      "    learn_time_ms: 692.274\n",
      "    load_throughput: 41035.659\n",
      "    load_time_ms: 24.369\n",
      "    sample_throughput: 40.345\n",
      "    sample_time_ms: 24786.273\n",
      "    update_time_ms: 2.45\n",
      "  timestamp: 1635068435\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         5565.98</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\"> -2.9894</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            298.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 297.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.9722999999999815\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 753\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007415771484375003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8688815765910678\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004364293361486836\n",
      "          policy_loss: 0.01431306724747022\n",
      "          total_loss: 0.018395683417717617\n",
      "          vf_explained_var: 0.2938463091850281\n",
      "          vf_loss: 0.012768191554480128\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.9031746031746\n",
      "    ram_util_percent: 38.70634920634921\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03891447078556697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.531671145202452\n",
      "    mean_inference_ms: 1.9315134826466853\n",
      "    mean_raw_obs_processing_ms: 1.9472297982399533\n",
      "  time_since_restore: 5610.351066112518\n",
      "  time_this_iter_s: 44.372366189956665\n",
      "  time_total_s: 5610.351066112518\n",
      "  timers:\n",
      "    learn_throughput: 1440.806\n",
      "    learn_time_ms: 694.056\n",
      "    load_throughput: 40731.047\n",
      "    load_time_ms: 24.551\n",
      "    sample_throughput: 37.329\n",
      "    sample_time_ms: 26789.165\n",
      "    update_time_ms: 2.404\n",
      "  timestamp: 1635068480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         5610.35</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> -2.9723</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            297.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-41-46\n",
      "  done: false\n",
      "  episode_len_mean: 294.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.949099999999981\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 757\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8778884960545434\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006433959107182395\n",
      "          policy_loss: 0.0202686725391282\n",
      "          total_loss: 0.022322445528374778\n",
      "          vf_explained_var: 0.41677623987197876\n",
      "          vf_loss: 0.010830272930777735\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18648648648649\n",
      "    ram_util_percent: 38.77297297297297\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038911885345642654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.5371932254556\n",
      "    mean_inference_ms: 1.9314591788445536\n",
      "    mean_raw_obs_processing_ms: 1.9503871486002868\n",
      "  time_since_restore: 5636.33361697197\n",
      "  time_this_iter_s: 25.982550859451294\n",
      "  time_total_s: 5636.33361697197\n",
      "  timers:\n",
      "    learn_throughput: 1442.701\n",
      "    learn_time_ms: 693.144\n",
      "    load_throughput: 40994.349\n",
      "    load_time_ms: 24.394\n",
      "    sample_throughput: 39.356\n",
      "    sample_time_ms: 25409.194\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1635068506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         5636.33</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\"> -2.9491</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            294.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-42-11\n",
      "  done: false\n",
      "  episode_len_mean: 294.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.9434999999999807\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 760\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9111018697420756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005953247325678982\n",
      "          policy_loss: -0.11483190688822005\n",
      "          total_loss: -0.11114660220013725\n",
      "          vf_explained_var: 0.3644959032535553\n",
      "          vf_loss: 0.012794114721732006\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.98888888888889\n",
      "    ram_util_percent: 38.78333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038909944197889006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.54172054225748\n",
      "    mean_inference_ms: 1.931419733372465\n",
      "    mean_raw_obs_processing_ms: 1.9511870279642716\n",
      "  time_since_restore: 5661.4658143520355\n",
      "  time_this_iter_s: 25.132197380065918\n",
      "  time_total_s: 5661.4658143520355\n",
      "  timers:\n",
      "    learn_throughput: 1443.438\n",
      "    learn_time_ms: 692.79\n",
      "    load_throughput: 41210.806\n",
      "    load_time_ms: 24.265\n",
      "    sample_throughput: 38.459\n",
      "    sample_time_ms: 26001.599\n",
      "    update_time_ms: 2.403\n",
      "  timestamp: 1635068531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         5661.47</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\"> -2.9435</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            294.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 292.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.9201999999999813\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 764\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9890987416108449\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013159130620975448\n",
      "          policy_loss: 0.007187066309981876\n",
      "          total_loss: 0.011968860940800772\n",
      "          vf_explained_var: 0.2298000603914261\n",
      "          vf_loss: 0.014667902203897635\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.12352941176471\n",
      "    ram_util_percent: 38.817647058823525\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038907376517660575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.548319275218624\n",
      "    mean_inference_ms: 1.9313693775734577\n",
      "    mean_raw_obs_processing_ms: 1.9511575209337582\n",
      "  time_since_restore: 5685.247324705124\n",
      "  time_this_iter_s: 23.78151035308838\n",
      "  time_total_s: 5685.247324705124\n",
      "  timers:\n",
      "    learn_throughput: 1441.269\n",
      "    learn_time_ms: 693.833\n",
      "    load_throughput: 40996.513\n",
      "    load_time_ms: 24.392\n",
      "    sample_throughput: 38.829\n",
      "    sample_time_ms: 25753.834\n",
      "    update_time_ms: 2.407\n",
      "  timestamp: 1635068554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         5685.25</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\"> -2.9202</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            292.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-42-59\n",
      "  done: false\n",
      "  episode_len_mean: 288.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.8866999999999825\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 768\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.022154227230284\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005311144679796974\n",
      "          policy_loss: 0.025549709051847457\n",
      "          total_loss: 0.02799701831407017\n",
      "          vf_explained_var: 0.36400240659713745\n",
      "          vf_loss: 0.012666882088200913\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.96857142857143\n",
      "    ram_util_percent: 38.88285714285714\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03890494428953001\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.55576172141045\n",
      "    mean_inference_ms: 1.9313250903638512\n",
      "    mean_raw_obs_processing_ms: 1.9514202069278246\n",
      "  time_since_restore: 5709.647312402725\n",
      "  time_this_iter_s: 24.39998769760132\n",
      "  time_total_s: 5709.647312402725\n",
      "  timers:\n",
      "    learn_throughput: 1439.216\n",
      "    learn_time_ms: 694.823\n",
      "    load_throughput: 41330.399\n",
      "    load_time_ms: 24.195\n",
      "    sample_throughput: 38.757\n",
      "    sample_time_ms: 25801.871\n",
      "    update_time_ms: 2.396\n",
      "  timestamp: 1635068579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         5709.65</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\"> -2.8867</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            288.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-43-23\n",
      "  done: false\n",
      "  episode_len_mean: 286.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.864599999999983\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 771\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0523032936784955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005623462271266138\n",
      "          policy_loss: -0.10335296193758646\n",
      "          total_loss: -0.10280194133520126\n",
      "          vf_explained_var: 0.4670696556568146\n",
      "          vf_loss: 0.011071967457731565\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.27647058823529\n",
      "    ram_util_percent: 38.861764705882344\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03890322831587179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.56199893899977\n",
      "    mean_inference_ms: 1.931295474047672\n",
      "    mean_raw_obs_processing_ms: 1.9516655784129202\n",
      "  time_since_restore: 5733.798743724823\n",
      "  time_this_iter_s: 24.15143132209778\n",
      "  time_total_s: 5733.798743724823\n",
      "  timers:\n",
      "    learn_throughput: 1438.523\n",
      "    learn_time_ms: 695.157\n",
      "    load_throughput: 41227.901\n",
      "    load_time_ms: 24.255\n",
      "    sample_throughput: 38.835\n",
      "    sample_time_ms: 25749.949\n",
      "    update_time_ms: 2.376\n",
      "  timestamp: 1635068603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">          5733.8</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\"> -2.8646</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            286.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 246000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-43-46\n",
      "  done: false\n",
      "  episode_len_mean: 283.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.8325999999999834\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 775\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.151618892616696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008920118859561418\n",
      "          policy_loss: 0.023039223957392906\n",
      "          total_loss: 0.027515455956260364\n",
      "          vf_explained_var: 0.13099405169487\n",
      "          vf_loss: 0.015989114613168768\n",
      "    num_agent_steps_sampled: 246000\n",
      "    num_agent_steps_trained: 246000\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.83939393939394\n",
      "    ram_util_percent: 38.836363636363636\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03890113973039731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.571108179462488\n",
      "    mean_inference_ms: 1.9312613320873473\n",
      "    mean_raw_obs_processing_ms: 1.9520872195435481\n",
      "  time_since_restore: 5756.505420923233\n",
      "  time_this_iter_s: 22.706677198410034\n",
      "  time_total_s: 5756.505420923233\n",
      "  timers:\n",
      "    learn_throughput: 1440.978\n",
      "    learn_time_ms: 693.973\n",
      "    load_throughput: 41272.648\n",
      "    load_time_ms: 24.229\n",
      "    sample_throughput: 38.898\n",
      "    sample_time_ms: 25708.392\n",
      "    update_time_ms: 2.364\n",
      "  timestamp: 1635068626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 246\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         5756.51</td><td style=\"text-align: right;\">246000</td><td style=\"text-align: right;\"> -2.8326</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            283.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 247000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 281.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.813899999999984\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 778\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2422691371705796\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019502585076028863\n",
      "          policy_loss: -0.01106519897778829\n",
      "          total_loss: -0.012944994866847992\n",
      "          vf_explained_var: 0.06014329940080643\n",
      "          vf_loss: 0.010535661157983768\n",
      "    num_agent_steps_sampled: 247000\n",
      "    num_agent_steps_trained: 247000\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.8625\n",
      "    ram_util_percent: 38.8125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038899572573098735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.578420109704663\n",
      "    mean_inference_ms: 1.931238308607736\n",
      "    mean_raw_obs_processing_ms: 1.9524706794559574\n",
      "  time_since_restore: 5778.859034061432\n",
      "  time_this_iter_s: 22.353613138198853\n",
      "  time_total_s: 5778.859034061432\n",
      "  timers:\n",
      "    learn_throughput: 1443.493\n",
      "    learn_time_ms: 692.764\n",
      "    load_throughput: 41310.86\n",
      "    load_time_ms: 24.207\n",
      "    sample_throughput: 38.934\n",
      "    sample_time_ms: 25684.488\n",
      "    update_time_ms: 2.362\n",
      "  timestamp: 1635068648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         5778.86</td><td style=\"text-align: right;\">247000</td><td style=\"text-align: right;\"> -2.8139</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            281.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-44-47\n",
      "  done: false\n",
      "  episode_len_mean: 281.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.815599999999984\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 781\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1247439278496636\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012931507333363177\n",
      "          policy_loss: -0.1214686657819483\n",
      "          total_loss: -0.11802820050054126\n",
      "          vf_explained_var: 0.13676996529102325\n",
      "          vf_loss: 0.014683111425903108\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.667857142857144\n",
      "    ram_util_percent: 38.69464285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038897981008013435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.585809957660086\n",
      "    mean_inference_ms: 1.9312155017634236\n",
      "    mean_raw_obs_processing_ms: 1.9549013109613582\n",
      "  time_since_restore: 5818.106623649597\n",
      "  time_this_iter_s: 39.24758958816528\n",
      "  time_total_s: 5818.106623649597\n",
      "  timers:\n",
      "    learn_throughput: 1443.34\n",
      "    learn_time_ms: 692.837\n",
      "    load_throughput: 40956.12\n",
      "    load_time_ms: 24.416\n",
      "    sample_throughput: 36.947\n",
      "    sample_time_ms: 27065.634\n",
      "    update_time_ms: 2.377\n",
      "  timestamp: 1635068687\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 248\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         5818.11</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> -2.8156</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            281.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 249000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-45-10\n",
      "  done: false\n",
      "  episode_len_mean: 282.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.828199999999983\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 785\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1387056959999933\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008360248631903665\n",
      "          policy_loss: 0.008479065696398417\n",
      "          total_loss: 0.012173971864912245\n",
      "          vf_explained_var: 0.086151123046875\n",
      "          vf_loss: 0.015078864557047685\n",
      "    num_agent_steps_sampled: 249000\n",
      "    num_agent_steps_trained: 249000\n",
      "    num_steps_sampled: 249000\n",
      "    num_steps_trained: 249000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.27419354838709\n",
      "    ram_util_percent: 38.78709677419354\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038895856897184396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.59531129137156\n",
      "    mean_inference_ms: 1.9311857352115485\n",
      "    mean_raw_obs_processing_ms: 1.958193243342797\n",
      "  time_since_restore: 5840.186537265778\n",
      "  time_this_iter_s: 22.07991361618042\n",
      "  time_total_s: 5840.186537265778\n",
      "  timers:\n",
      "    learn_throughput: 1444.002\n",
      "    learn_time_ms: 692.52\n",
      "    load_throughput: 40671.25\n",
      "    load_time_ms: 24.587\n",
      "    sample_throughput: 37.458\n",
      "    sample_time_ms: 26696.791\n",
      "    update_time_ms: 2.369\n",
      "  timestamp: 1635068710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249000\n",
      "  training_iteration: 249\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         5840.19</td><td style=\"text-align: right;\">249000</td><td style=\"text-align: right;\"> -2.8282</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            282.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 250000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-45-30\n",
      "  done: false\n",
      "  episode_len_mean: 283.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.8391999999999835\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 788\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0970475713411967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012403026973361066\n",
      "          policy_loss: 0.04671477542983161\n",
      "          total_loss: 0.04655041429731581\n",
      "          vf_explained_var: 0.17448478937149048\n",
      "          vf_loss: 0.010801515227972737\n",
      "    num_agent_steps_sampled: 250000\n",
      "    num_agent_steps_trained: 250000\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.089999999999996\n",
      "    ram_util_percent: 38.79999999999999\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038894255119341525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.60208048348176\n",
      "    mean_inference_ms: 1.9311629051827413\n",
      "    mean_raw_obs_processing_ms: 1.9606570552760645\n",
      "  time_since_restore: 5861.036341905594\n",
      "  time_this_iter_s: 20.849804639816284\n",
      "  time_total_s: 5861.036341905594\n",
      "  timers:\n",
      "    learn_throughput: 1447.915\n",
      "    learn_time_ms: 690.648\n",
      "    load_throughput: 40940.05\n",
      "    load_time_ms: 24.426\n",
      "    sample_throughput: 41.074\n",
      "    sample_time_ms: 24346.59\n",
      "    update_time_ms: 2.356\n",
      "  timestamp: 1635068730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         5861.04</td><td style=\"text-align: right;\">250000</td><td style=\"text-align: right;\"> -2.8392</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            283.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 251000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-45-50\n",
      "  done: false\n",
      "  episode_len_mean: 286.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.8614999999999826\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 791\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0135544505384233\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009569666806360051\n",
      "          policy_loss: 0.04882367716895209\n",
      "          total_loss: 0.050283071067598134\n",
      "          vf_explained_var: 0.04919978603720665\n",
      "          vf_loss: 0.011591389754580126\n",
      "    num_agent_steps_sampled: 251000\n",
      "    num_agent_steps_trained: 251000\n",
      "    num_steps_sampled: 251000\n",
      "    num_steps_trained: 251000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.767857142857146\n",
      "    ram_util_percent: 38.79642857142857\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03889268978831173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.608229352851954\n",
      "    mean_inference_ms: 1.9311400453426555\n",
      "    mean_raw_obs_processing_ms: 1.961632520471463\n",
      "  time_since_restore: 5880.412487030029\n",
      "  time_this_iter_s: 19.376145124435425\n",
      "  time_total_s: 5880.412487030029\n",
      "  timers:\n",
      "    learn_throughput: 1445.514\n",
      "    learn_time_ms: 691.795\n",
      "    load_throughput: 40799.789\n",
      "    load_time_ms: 24.51\n",
      "    sample_throughput: 42.221\n",
      "    sample_time_ms: 23684.734\n",
      "    update_time_ms: 2.347\n",
      "  timestamp: 1635068750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 251000\n",
      "  training_iteration: 251\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         5880.41</td><td style=\"text-align: right;\">251000</td><td style=\"text-align: right;\"> -2.8615</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            286.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-46-11\n",
      "  done: false\n",
      "  episode_len_mean: 286.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.863699999999983\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 794\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003707885742187502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9759808679421743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02123327687651487\n",
      "          policy_loss: 0.04835246495074696\n",
      "          total_loss: 0.048560577962133616\n",
      "          vf_explained_var: 0.22647033631801605\n",
      "          vf_loss: 0.00996004802097256\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11000000000001\n",
      "    ram_util_percent: 38.839999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03889114002805422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.61422831941126\n",
      "    mean_inference_ms: 1.931117160078843\n",
      "    mean_raw_obs_processing_ms: 1.9618014400145676\n",
      "  time_since_restore: 5902.0177526474\n",
      "  time_this_iter_s: 21.605265617370605\n",
      "  time_total_s: 5902.0177526474\n",
      "  timers:\n",
      "    learn_throughput: 1444.559\n",
      "    learn_time_ms: 692.253\n",
      "    load_throughput: 40919.68\n",
      "    load_time_ms: 24.438\n",
      "    sample_throughput: 42.86\n",
      "    sample_time_ms: 23331.61\n",
      "    update_time_ms: 2.381\n",
      "  timestamp: 1635068771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 252\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         5902.02</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> -2.8637</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            286.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 253000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-46-34\n",
      "  done: false\n",
      "  episode_len_mean: 285.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.8591999999999826\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 797\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.927892064385944\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011878543236801894\n",
      "          policy_loss: -0.0854082149763902\n",
      "          total_loss: -0.07691190549068981\n",
      "          vf_explained_var: 0.03923255205154419\n",
      "          vf_loss: 0.017768619685537286\n",
      "    num_agent_steps_sampled: 253000\n",
      "    num_agent_steps_trained: 253000\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.2030303030303\n",
      "    ram_util_percent: 38.875757575757575\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03888961968468943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.62027672347251\n",
      "    mean_inference_ms: 1.9310952806547597\n",
      "    mean_raw_obs_processing_ms: 1.961996983416026\n",
      "  time_since_restore: 5924.827485561371\n",
      "  time_this_iter_s: 22.809732913970947\n",
      "  time_total_s: 5924.827485561371\n",
      "  timers:\n",
      "    learn_throughput: 1444.074\n",
      "    learn_time_ms: 692.485\n",
      "    load_throughput: 40895.183\n",
      "    load_time_ms: 24.453\n",
      "    sample_throughput: 43.04\n",
      "    sample_time_ms: 23234.201\n",
      "    update_time_ms: 2.377\n",
      "  timestamp: 1635068794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         5924.83</td><td style=\"text-align: right;\">253000</td><td style=\"text-align: right;\"> -2.8592</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            285.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 254000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 285.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.8590999999999824\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 801\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0703293681144714\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013473541458033762\n",
      "          policy_loss: -0.01524277784758144\n",
      "          total_loss: -0.00914615285065439\n",
      "          vf_explained_var: 0.12239428609609604\n",
      "          vf_loss: 0.016792421436144248\n",
      "    num_agent_steps_sampled: 254000\n",
      "    num_agent_steps_trained: 254000\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.92903225806452\n",
      "    ram_util_percent: 38.838709677419345\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03888765560842202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.628218985878693\n",
      "    mean_inference_ms: 1.9310672715173138\n",
      "    mean_raw_obs_processing_ms: 1.9623447570442627\n",
      "  time_since_restore: 5946.636370182037\n",
      "  time_this_iter_s: 21.808884620666504\n",
      "  time_total_s: 5946.636370182037\n",
      "  timers:\n",
      "    learn_throughput: 1445.76\n",
      "    learn_time_ms: 691.677\n",
      "    load_throughput: 40991.304\n",
      "    load_time_ms: 24.395\n",
      "    sample_throughput: 43.522\n",
      "    sample_time_ms: 22977.026\n",
      "    update_time_ms: 2.38\n",
      "  timestamp: 1635068816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 254\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         5946.64</td><td style=\"text-align: right;\">254000</td><td style=\"text-align: right;\"> -2.8591</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            285.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-47-17\n",
      "  done: false\n",
      "  episode_len_mean: 286.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.8655999999999824\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 804\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0757480336560143\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018382704987050445\n",
      "          policy_loss: 0.0778891576661004\n",
      "          total_loss: 0.075915218061871\n",
      "          vf_explained_var: -0.017648398876190186\n",
      "          vf_loss: 0.008773318711124982\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.95\n",
      "    ram_util_percent: 38.80666666666666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03888620909594441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.634096400612282\n",
      "    mean_inference_ms: 1.9310465316629533\n",
      "    mean_raw_obs_processing_ms: 1.9625905668839534\n",
      "  time_since_restore: 5967.591655015945\n",
      "  time_this_iter_s: 20.95528483390808\n",
      "  time_total_s: 5967.591655015945\n",
      "  timers:\n",
      "    learn_throughput: 1446.592\n",
      "    learn_time_ms: 691.28\n",
      "    load_throughput: 40364.777\n",
      "    load_time_ms: 24.774\n",
      "    sample_throughput: 44.136\n",
      "    sample_time_ms: 22657.412\n",
      "    update_time_ms: 2.388\n",
      "  timestamp: 1635068837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 255\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         5967.59</td><td style=\"text-align: right;\">255000</td><td style=\"text-align: right;\"> -2.8656</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            286.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-47-38\n",
      "  done: false\n",
      "  episode_len_mean: 287.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.874799999999983\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 807\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1161467141575283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00675511649123173\n",
      "          policy_loss: 0.04559264837039841\n",
      "          total_loss: 0.04702669348981645\n",
      "          vf_explained_var: -0.026793548837304115\n",
      "          vf_loss: 0.012591752911814386\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.093333333333334\n",
      "    ram_util_percent: 38.79333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03888477207445546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.6397521704155\n",
      "    mean_inference_ms: 1.9310256013060383\n",
      "    mean_raw_obs_processing_ms: 1.9628613656862839\n",
      "  time_since_restore: 5988.431557893753\n",
      "  time_this_iter_s: 20.839902877807617\n",
      "  time_total_s: 5988.431557893753\n",
      "  timers:\n",
      "    learn_throughput: 1446.573\n",
      "    learn_time_ms: 691.289\n",
      "    load_throughput: 40116.726\n",
      "    load_time_ms: 24.927\n",
      "    sample_throughput: 44.503\n",
      "    sample_time_ms: 22470.536\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1635068858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         5988.43</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\"> -2.8748</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            287.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 257000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-48-16\n",
      "  done: false\n",
      "  episode_len_mean: 288.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.881499999999982\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 810\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2101754585901896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010217802873360743\n",
      "          policy_loss: 0.028012228508790333\n",
      "          total_loss: 0.027157134314378103\n",
      "          vf_explained_var: 0.21418631076812744\n",
      "          vf_loss: 0.011240978110840337\n",
      "    num_agent_steps_sampled: 257000\n",
      "    num_agent_steps_trained: 257000\n",
      "    num_steps_sampled: 257000\n",
      "    num_steps_trained: 257000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.487272727272725\n",
      "    ram_util_percent: 38.78909090909091\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03888336365979289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.64509831381344\n",
      "    mean_inference_ms: 1.931004685704455\n",
      "    mean_raw_obs_processing_ms: 1.9651759837403038\n",
      "  time_since_restore: 6026.685304880142\n",
      "  time_this_iter_s: 38.25374698638916\n",
      "  time_total_s: 6026.685304880142\n",
      "  timers:\n",
      "    learn_throughput: 1445.261\n",
      "    learn_time_ms: 691.917\n",
      "    load_throughput: 40201.279\n",
      "    load_time_ms: 24.875\n",
      "    sample_throughput: 41.563\n",
      "    sample_time_ms: 24059.938\n",
      "    update_time_ms: 2.439\n",
      "  timestamp: 1635068896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257000\n",
      "  training_iteration: 257\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         6026.69</td><td style=\"text-align: right;\">257000</td><td style=\"text-align: right;\"> -2.8815</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            288.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 258000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-48-37\n",
      "  done: false\n",
      "  episode_len_mean: 288.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.887099999999983\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 813\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.215063864654965\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008813564852678842\n",
      "          policy_loss: -0.042052259047826133\n",
      "          total_loss: -0.04255525817473729\n",
      "          vf_explained_var: 0.28571778535842896\n",
      "          vf_loss: 0.011642735999905401\n",
      "    num_agent_steps_sampled: 258000\n",
      "    num_agent_steps_trained: 258000\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.96206896551724\n",
      "    ram_util_percent: 38.775862068965516\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038881965259549164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.650339545961398\n",
      "    mean_inference_ms: 1.9309833482997971\n",
      "    mean_raw_obs_processing_ms: 1.9674304311569104\n",
      "  time_since_restore: 6047.495797872543\n",
      "  time_this_iter_s: 20.810492992401123\n",
      "  time_total_s: 6047.495797872543\n",
      "  timers:\n",
      "    learn_throughput: 1444.204\n",
      "    learn_time_ms: 692.423\n",
      "    load_throughput: 40413.198\n",
      "    load_time_ms: 24.744\n",
      "    sample_throughput: 45.013\n",
      "    sample_time_ms: 22215.879\n",
      "    update_time_ms: 2.421\n",
      "  timestamp: 1635068917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 258\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">          6047.5</td><td style=\"text-align: right;\">258000</td><td style=\"text-align: right;\"> -2.8871</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            288.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 259000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-48-56\n",
      "  done: false\n",
      "  episode_len_mean: 291.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.9119999999999817\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 816\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3660906341340806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012330511643732302\n",
      "          policy_loss: 0.039598216861486436\n",
      "          total_loss: 0.03896393221285608\n",
      "          vf_explained_var: 0.044322460889816284\n",
      "          vf_loss: 0.013019763305783272\n",
      "    num_agent_steps_sampled: 259000\n",
      "    num_agent_steps_trained: 259000\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.49642857142857\n",
      "    ram_util_percent: 38.800000000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03888063832972028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.654926533465662\n",
      "    mean_inference_ms: 1.9309614257779626\n",
      "    mean_raw_obs_processing_ms: 1.9697011834943121\n",
      "  time_since_restore: 6066.701676607132\n",
      "  time_this_iter_s: 19.205878734588623\n",
      "  time_total_s: 6066.701676607132\n",
      "  timers:\n",
      "    learn_throughput: 1443.396\n",
      "    learn_time_ms: 692.811\n",
      "    load_throughput: 40440.435\n",
      "    load_time_ms: 24.728\n",
      "    sample_throughput: 45.604\n",
      "    sample_time_ms: 21928.106\n",
      "    update_time_ms: 2.412\n",
      "  timestamp: 1635068936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">          6066.7</td><td style=\"text-align: right;\">259000</td><td style=\"text-align: right;\">  -2.912</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">             291.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-49-18\n",
      "  done: false\n",
      "  episode_len_mean: 292.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.922199999999981\n",
      "  episode_reward_min: -4.319999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 819\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.119860772954093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00857860451110876\n",
      "          policy_loss: 0.045173606193727914\n",
      "          total_loss: 0.046066748268074456\n",
      "          vf_explained_var: -0.21173135936260223\n",
      "          vf_loss: 0.012086976580192439\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.94516129032259\n",
      "    ram_util_percent: 38.79677419354838\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887928033747837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.65943997701163\n",
      "    mean_inference_ms: 1.9309389550258322\n",
      "    mean_raw_obs_processing_ms: 1.9696928391170676\n",
      "  time_since_restore: 6088.409357786179\n",
      "  time_this_iter_s: 21.70768117904663\n",
      "  time_total_s: 6088.409357786179\n",
      "  timers:\n",
      "    learn_throughput: 1443.506\n",
      "    learn_time_ms: 692.758\n",
      "    load_throughput: 40343.501\n",
      "    load_time_ms: 24.787\n",
      "    sample_throughput: 45.426\n",
      "    sample_time_ms: 22013.873\n",
      "    update_time_ms: 2.414\n",
      "  timestamp: 1635068958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 260\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         6088.41</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -2.9222</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.32</td><td style=\"text-align: right;\">            292.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 261000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-49-40\n",
      "  done: false\n",
      "  episode_len_mean: 291.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.912699999999981\n",
      "  episode_reward_min: -3.6699999999999657\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 822\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2765749904844497\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00841866994010223\n",
      "          policy_loss: -0.015924158526791465\n",
      "          total_loss: -0.01610975277920564\n",
      "          vf_explained_var: 0.22520260512828827\n",
      "          vf_loss: 0.012575476119915644\n",
      "    num_agent_steps_sampled: 261000\n",
      "    num_agent_steps_trained: 261000\n",
      "    num_steps_sampled: 261000\n",
      "    num_steps_trained: 261000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.723333333333336\n",
      "    ram_util_percent: 38.86666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887796436780767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.664137615646506\n",
      "    mean_inference_ms: 1.9309165064118108\n",
      "    mean_raw_obs_processing_ms: 1.969633289487153\n",
      "  time_since_restore: 6109.922677516937\n",
      "  time_this_iter_s: 21.513319730758667\n",
      "  time_total_s: 6109.922677516937\n",
      "  timers:\n",
      "    learn_throughput: 1447.212\n",
      "    learn_time_ms: 690.984\n",
      "    load_throughput: 41304.229\n",
      "    load_time_ms: 24.211\n",
      "    sample_throughput: 44.984\n",
      "    sample_time_ms: 22229.913\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1635068980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 261000\n",
      "  training_iteration: 261\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         6109.92</td><td style=\"text-align: right;\">261000</td><td style=\"text-align: right;\"> -2.9127</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">            291.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 262000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-50-01\n",
      "  done: false\n",
      "  episode_len_mean: 292.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.9265999999999814\n",
      "  episode_reward_min: -3.6699999999999657\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 825\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005561828613281248\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4066333916452196\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03550715946047427\n",
      "          policy_loss: -0.09448992104993927\n",
      "          total_loss: -0.09176690528790156\n",
      "          vf_explained_var: 0.24079614877700806\n",
      "          vf_loss: 0.01676960141501493\n",
      "    num_agent_steps_sampled: 262000\n",
      "    num_agent_steps_trained: 262000\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.703333333333326\n",
      "    ram_util_percent: 38.913333333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038876636969646015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.668450470149097\n",
      "    mean_inference_ms: 1.930893447873546\n",
      "    mean_raw_obs_processing_ms: 1.9695973161232718\n",
      "  time_since_restore: 6130.906731843948\n",
      "  time_this_iter_s: 20.98405432701111\n",
      "  time_total_s: 6130.906731843948\n",
      "  timers:\n",
      "    learn_throughput: 1445.762\n",
      "    learn_time_ms: 691.677\n",
      "    load_throughput: 41324.088\n",
      "    load_time_ms: 24.199\n",
      "    sample_throughput: 45.112\n",
      "    sample_time_ms: 22167.096\n",
      "    update_time_ms: 2.491\n",
      "  timestamp: 1635069001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         6130.91</td><td style=\"text-align: right;\">262000</td><td style=\"text-align: right;\"> -2.9266</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">            292.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 263000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-50-20\n",
      "  done: false\n",
      "  episode_len_mean: 296.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.964199999999981\n",
      "  episode_reward_min: -4.689999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 828\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008342742919921875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.009610546959771\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011680892082561517\n",
      "          policy_loss: -0.0013419845037990147\n",
      "          total_loss: -0.008872084816296895\n",
      "          vf_explained_var: -0.1434735655784607\n",
      "          vf_loss: 0.012556262621688397\n",
      "    num_agent_steps_sampled: 263000\n",
      "    num_agent_steps_trained: 263000\n",
      "    num_steps_sampled: 263000\n",
      "    num_steps_trained: 263000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.09285714285714\n",
      "    ram_util_percent: 38.91071428571428\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038875247296234046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.67217675301312\n",
      "    mean_inference_ms: 1.9308701783856876\n",
      "    mean_raw_obs_processing_ms: 1.9695123246173984\n",
      "  time_since_restore: 6150.1103501319885\n",
      "  time_this_iter_s: 19.20361828804016\n",
      "  time_total_s: 6150.1103501319885\n",
      "  timers:\n",
      "    learn_throughput: 1444.052\n",
      "    learn_time_ms: 692.496\n",
      "    load_throughput: 41347.267\n",
      "    load_time_ms: 24.185\n",
      "    sample_throughput: 45.86\n",
      "    sample_time_ms: 21805.7\n",
      "    update_time_ms: 2.477\n",
      "  timestamp: 1635069020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263000\n",
      "  training_iteration: 263\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         6150.11</td><td style=\"text-align: right;\">263000</td><td style=\"text-align: right;\"> -2.9642</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.69</td><td style=\"text-align: right;\">            296.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-50-38\n",
      "  done: false\n",
      "  episode_len_mean: 299.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.9904999999999804\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 830\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008342742919921875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7542042785220675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011839807452736788\n",
      "          policy_loss: -0.052509689620799486\n",
      "          total_loss: -0.06000116599930657\n",
      "          vf_explained_var: 0.5761417150497437\n",
      "          vf_loss: 0.010040686420527183\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.526923076923076\n",
      "    ram_util_percent: 38.86923076923077\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038874308618691414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.674259797703552\n",
      "    mean_inference_ms: 1.930854627827771\n",
      "    mean_raw_obs_processing_ms: 1.969421133867585\n",
      "  time_since_restore: 6168.219327926636\n",
      "  time_this_iter_s: 18.108977794647217\n",
      "  time_total_s: 6168.219327926636\n",
      "  timers:\n",
      "    learn_throughput: 1443.994\n",
      "    learn_time_ms: 692.524\n",
      "    load_throughput: 41302.114\n",
      "    load_time_ms: 24.212\n",
      "    sample_throughput: 46.651\n",
      "    sample_time_ms: 21435.667\n",
      "    update_time_ms: 2.503\n",
      "  timestamp: 1635069038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 264\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         6168.22</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\"> -2.9905</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            299.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 265000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-50-57\n",
      "  done: false\n",
      "  episode_len_mean: 303.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -3.0313999999999792\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 833\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008342742919921875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7032871696684095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0243787233704002\n",
      "          policy_loss: 0.07880658093425963\n",
      "          total_loss: 0.07122831485337681\n",
      "          vf_explained_var: 0.34951066970825195\n",
      "          vf_loss: 0.009434270427542893\n",
      "    num_agent_steps_sampled: 265000\n",
      "    num_agent_steps_trained: 265000\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.13703703703704\n",
      "    ram_util_percent: 38.825925925925915\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887293544820671\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.676664740853965\n",
      "    mean_inference_ms: 1.930831813497948\n",
      "    mean_raw_obs_processing_ms: 1.9693074424264778\n",
      "  time_since_restore: 6187.0148384571075\n",
      "  time_this_iter_s: 18.7955105304718\n",
      "  time_total_s: 6187.0148384571075\n",
      "  timers:\n",
      "    learn_throughput: 1444.325\n",
      "    learn_time_ms: 692.365\n",
      "    load_throughput: 42171.017\n",
      "    load_time_ms: 23.713\n",
      "    sample_throughput: 47.125\n",
      "    sample_time_ms: 21220.317\n",
      "    update_time_ms: 2.541\n",
      "  timestamp: 1635069057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 265\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         6187.01</td><td style=\"text-align: right;\">265000</td><td style=\"text-align: right;\"> -3.0314</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            303.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 266000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-51-17\n",
      "  done: false\n",
      "  episode_len_mean: 304.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -3.049899999999979\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 835\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0012514114379882815\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6071575654877557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.034309678445562375\n",
      "          policy_loss: -0.18660563876231512\n",
      "          total_loss: -0.1927170784937011\n",
      "          vf_explained_var: 0.687069296836853\n",
      "          vf_loss: 0.009917198695216536\n",
      "    num_agent_steps_sampled: 266000\n",
      "    num_agent_steps_trained: 266000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.628571428571426\n",
      "    ram_util_percent: 38.775\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887201334127476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.67809836922408\n",
      "    mean_inference_ms: 1.9308165848867123\n",
      "    mean_raw_obs_processing_ms: 1.969172825060036\n",
      "  time_since_restore: 6207.2443079948425\n",
      "  time_this_iter_s: 20.229469537734985\n",
      "  time_total_s: 6207.2443079948425\n",
      "  timers:\n",
      "    learn_throughput: 1441.978\n",
      "    learn_time_ms: 693.492\n",
      "    load_throughput: 42521.115\n",
      "    load_time_ms: 23.518\n",
      "    sample_throughput: 47.263\n",
      "    sample_time_ms: 21158.374\n",
      "    update_time_ms: 2.518\n",
      "  timestamp: 1635069077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         6207.24</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\"> -3.0499</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            304.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 267000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-51-38\n",
      "  done: false\n",
      "  episode_len_mean: 306.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -3.0674999999999786\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 839\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1992564479509988\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007466039309543327\n",
      "          policy_loss: -0.017314413603809146\n",
      "          total_loss: -0.015327742613024182\n",
      "          vf_explained_var: 0.4927230775356293\n",
      "          vf_loss: 0.013965223067336613\n",
      "    num_agent_steps_sampled: 267000\n",
      "    num_agent_steps_trained: 267000\n",
      "    num_steps_sampled: 267000\n",
      "    num_steps_trained: 267000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.80000000000001\n",
      "    ram_util_percent: 38.72580645161292\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887026995857627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.680711361785082\n",
      "    mean_inference_ms: 1.9307870071046278\n",
      "    mean_raw_obs_processing_ms: 1.9689408475882013\n",
      "  time_since_restore: 6228.684103012085\n",
      "  time_this_iter_s: 21.43979501724243\n",
      "  time_total_s: 6228.684103012085\n",
      "  timers:\n",
      "    learn_throughput: 1442.618\n",
      "    learn_time_ms: 693.184\n",
      "    load_throughput: 42138.224\n",
      "    load_time_ms: 23.731\n",
      "    sample_throughput: 51.342\n",
      "    sample_time_ms: 19477.07\n",
      "    update_time_ms: 2.51\n",
      "  timestamp: 1635069098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 267000\n",
      "  training_iteration: 267\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         6228.68</td><td style=\"text-align: right;\">267000</td><td style=\"text-align: right;\"> -3.0675</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            306.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-52-19\n",
      "  done: false\n",
      "  episode_len_mean: 306.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -3.0697999999999785\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 842\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0620182500945197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009090799448297639\n",
      "          policy_loss: 0.0634146726793713\n",
      "          total_loss: 0.062212695015801324\n",
      "          vf_explained_var: 0.5825045108795166\n",
      "          vf_loss: 0.009401138701812467\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.61379310344827\n",
      "    ram_util_percent: 38.63448275862069\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886898391830766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.6827141145866\n",
      "    mean_inference_ms: 1.9307646056512167\n",
      "    mean_raw_obs_processing_ms: 1.9706245747118354\n",
      "  time_since_restore: 6269.423447608948\n",
      "  time_this_iter_s: 40.73934459686279\n",
      "  time_total_s: 6269.423447608948\n",
      "  timers:\n",
      "    learn_throughput: 1443.374\n",
      "    learn_time_ms: 692.821\n",
      "    load_throughput: 41950.004\n",
      "    load_time_ms: 23.838\n",
      "    sample_throughput: 46.576\n",
      "    sample_time_ms: 21470.198\n",
      "    update_time_ms: 2.516\n",
      "  timestamp: 1635069139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 268\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         6269.42</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\"> -3.0698</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            306.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 269000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-52-42\n",
      "  done: false\n",
      "  episode_len_mean: 308.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -3.0806999999999776\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 845\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8912000868055555\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00553081294094656\n",
      "          policy_loss: -0.040292333645953075\n",
      "          total_loss: -0.03716717205113835\n",
      "          vf_explained_var: 0.27507346868515015\n",
      "          vf_loss: 0.01202677869134479\n",
      "    num_agent_steps_sampled: 269000\n",
      "    num_agent_steps_trained: 269000\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.15454545454545\n",
      "    ram_util_percent: 38.68181818181818\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886769248861454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.684506549889107\n",
      "    mean_inference_ms: 1.9307408261020371\n",
      "    mean_raw_obs_processing_ms: 1.972252207745564\n",
      "  time_since_restore: 6292.0417540073395\n",
      "  time_this_iter_s: 22.618306398391724\n",
      "  time_total_s: 6292.0417540073395\n",
      "  timers:\n",
      "    learn_throughput: 1443.764\n",
      "    learn_time_ms: 692.634\n",
      "    load_throughput: 42073.679\n",
      "    load_time_ms: 23.768\n",
      "    sample_throughput: 45.847\n",
      "    sample_time_ms: 21811.673\n",
      "    update_time_ms: 2.528\n",
      "  timestamp: 1635069162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         6292.04</td><td style=\"text-align: right;\">269000</td><td style=\"text-align: right;\"> -3.0807</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            308.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-53-05\n",
      "  done: false\n",
      "  episode_len_mean: 309.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -3.0912999999999773\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 849\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8967794842190213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008177200784159193\n",
      "          policy_loss: 0.016769423335790633\n",
      "          total_loss: 0.021828778377837605\n",
      "          vf_explained_var: 0.31060850620269775\n",
      "          vf_loss: 0.014011802648504575\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.97575757575758\n",
      "    ram_util_percent: 38.7969696969697\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886595208899788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.686472117988078\n",
      "    mean_inference_ms: 1.9307087021673903\n",
      "    mean_raw_obs_processing_ms: 1.974449076831725\n",
      "  time_since_restore: 6315.570298671722\n",
      "  time_this_iter_s: 23.528544664382935\n",
      "  time_total_s: 6315.570298671722\n",
      "  timers:\n",
      "    learn_throughput: 1442.651\n",
      "    learn_time_ms: 693.168\n",
      "    load_throughput: 42215.584\n",
      "    load_time_ms: 23.688\n",
      "    sample_throughput: 45.468\n",
      "    sample_time_ms: 21993.294\n",
      "    update_time_ms: 2.534\n",
      "  timestamp: 1635069185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 270\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         6315.57</td><td style=\"text-align: right;\">270000</td><td style=\"text-align: right;\"> -3.0913</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            309.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 271000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 310.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -3.1009999999999773\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 852\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8002208941512637\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006397449932409612\n",
      "          policy_loss: -0.11536857924527592\n",
      "          total_loss: -0.10991485218207041\n",
      "          vf_explained_var: 0.23956073820590973\n",
      "          vf_loss: 0.01344392995039622\n",
      "    num_agent_steps_sampled: 271000\n",
      "    num_agent_steps_trained: 271000\n",
      "    num_steps_sampled: 271000\n",
      "    num_steps_trained: 271000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.057142857142864\n",
      "    ram_util_percent: 38.88571428571429\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886467420478266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.687532501841652\n",
      "    mean_inference_ms: 1.9306839624996672\n",
      "    mean_raw_obs_processing_ms: 1.9740082407720587\n",
      "  time_since_restore: 6340.142611980438\n",
      "  time_this_iter_s: 24.57231330871582\n",
      "  time_total_s: 6340.142611980438\n",
      "  timers:\n",
      "    learn_throughput: 1438.358\n",
      "    learn_time_ms: 695.237\n",
      "    load_throughput: 41163.324\n",
      "    load_time_ms: 24.293\n",
      "    sample_throughput: 44.85\n",
      "    sample_time_ms: 22296.542\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1635069210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271000\n",
      "  training_iteration: 271\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         6340.14</td><td style=\"text-align: right;\">271000</td><td style=\"text-align: right;\">  -3.101</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">             310.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-53-53\n",
      "  done: false\n",
      "  episode_len_mean: 311.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.1150999999999773\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 856\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8543402228090499\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00708658755173032\n",
      "          policy_loss: 0.0483082115650177\n",
      "          total_loss: 0.05106362832917107\n",
      "          vf_explained_var: 0.1758381724357605\n",
      "          vf_loss: 0.01128551717588885\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.70294117647058\n",
      "    ram_util_percent: 38.870588235294115\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886293376168204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.688680235657152\n",
      "    mean_inference_ms: 1.9306495842753002\n",
      "    mean_raw_obs_processing_ms: 1.9733809085721714\n",
      "  time_since_restore: 6363.668856143951\n",
      "  time_this_iter_s: 23.526244163513184\n",
      "  time_total_s: 6363.668856143951\n",
      "  timers:\n",
      "    learn_throughput: 1438.525\n",
      "    learn_time_ms: 695.156\n",
      "    load_throughput: 41211.13\n",
      "    load_time_ms: 24.265\n",
      "    sample_throughput: 44.344\n",
      "    sample_time_ms: 22550.939\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1635069233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         6363.67</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\"> -3.1151</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            311.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 273000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-54-18\n",
      "  done: false\n",
      "  episode_len_mean: 311.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.1190999999999782\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 860\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8405386924743652\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016158462174866075\n",
      "          policy_loss: 0.04513773065474298\n",
      "          total_loss: 0.04988320875498983\n",
      "          vf_explained_var: 0.04122493788599968\n",
      "          vf_loss: 0.01312053182369305\n",
      "    num_agent_steps_sampled: 273000\n",
      "    num_agent_steps_trained: 273000\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.10294117647059\n",
      "    ram_util_percent: 38.88529411764705\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886121281975987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.689581763184766\n",
      "    mean_inference_ms: 1.9306147212570481\n",
      "    mean_raw_obs_processing_ms: 1.9727914061365217\n",
      "  time_since_restore: 6387.870647907257\n",
      "  time_this_iter_s: 24.201791763305664\n",
      "  time_total_s: 6387.870647907257\n",
      "  timers:\n",
      "    learn_throughput: 1439.344\n",
      "    learn_time_ms: 694.761\n",
      "    load_throughput: 41363.17\n",
      "    load_time_ms: 24.176\n",
      "    sample_throughput: 43.382\n",
      "    sample_time_ms: 23051.184\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1635069258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 273\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         6387.87</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\"> -3.1191</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            311.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 274000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-54-43\n",
      "  done: false\n",
      "  episode_len_mean: 311.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1165999999999774\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 863\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7513547857602437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007758527935940091\n",
      "          policy_loss: -0.09515013032489353\n",
      "          total_loss: -0.08735802041159736\n",
      "          vf_explained_var: 0.04386414587497711\n",
      "          vf_loss: 0.01529109411769443\n",
      "    num_agent_steps_sampled: 274000\n",
      "    num_agent_steps_trained: 274000\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.28055555555555\n",
      "    ram_util_percent: 38.85833333333332\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038859923297983684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.690247998557705\n",
      "    mean_inference_ms: 1.9305883017823535\n",
      "    mean_raw_obs_processing_ms: 1.9724260336908201\n",
      "  time_since_restore: 6412.816427230835\n",
      "  time_this_iter_s: 24.94577932357788\n",
      "  time_total_s: 6412.816427230835\n",
      "  timers:\n",
      "    learn_throughput: 1440.576\n",
      "    learn_time_ms: 694.167\n",
      "    load_throughput: 41365.658\n",
      "    load_time_ms: 24.175\n",
      "    sample_throughput: 42.131\n",
      "    sample_time_ms: 23735.463\n",
      "    update_time_ms: 2.485\n",
      "  timestamp: 1635069283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 274\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         6412.82</td><td style=\"text-align: right;\">274000</td><td style=\"text-align: right;\"> -3.1166</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            311.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 275000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-55-08\n",
      "  done: false\n",
      "  episode_len_mean: 311.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -3.114999999999977\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 867\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7170666972796123\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017599598713932272\n",
      "          policy_loss: -0.04255202131138908\n",
      "          total_loss: -0.03305573893917931\n",
      "          vf_explained_var: 0.0642705038189888\n",
      "          vf_loss: 0.016633914881903265\n",
      "    num_agent_steps_sampled: 275000\n",
      "    num_agent_steps_trained: 275000\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.07027027027027\n",
      "    ram_util_percent: 38.82162162162161\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038858237824645286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.691402974735777\n",
      "    mean_inference_ms: 1.9305514219362185\n",
      "    mean_raw_obs_processing_ms: 1.9719013421182379\n",
      "  time_since_restore: 6438.625088214874\n",
      "  time_this_iter_s: 25.808660984039307\n",
      "  time_total_s: 6438.625088214874\n",
      "  timers:\n",
      "    learn_throughput: 1438.986\n",
      "    learn_time_ms: 694.934\n",
      "    load_throughput: 40840.112\n",
      "    load_time_ms: 24.486\n",
      "    sample_throughput: 40.924\n",
      "    sample_time_ms: 24435.735\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1635069308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         6438.63</td><td style=\"text-align: right;\">275000</td><td style=\"text-align: right;\">  -3.115</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">             311.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 310.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.1017999999999777\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 871\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0018771171569824212\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8344167199399736\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00423065267879728\n",
      "          policy_loss: -0.1150497919983334\n",
      "          total_loss: -0.10438452313343684\n",
      "          vf_explained_var: 0.13489045202732086\n",
      "          vf_loss: 0.019001493665079274\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.36065573770492\n",
      "    ram_util_percent: 38.699999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03885654824928779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.69277542272055\n",
      "    mean_inference_ms: 1.9305143220408292\n",
      "    mean_raw_obs_processing_ms: 1.9738325085859372\n",
      "  time_since_restore: 6481.390711307526\n",
      "  time_this_iter_s: 42.76562309265137\n",
      "  time_total_s: 6481.390711307526\n",
      "  timers:\n",
      "    learn_throughput: 1441.365\n",
      "    learn_time_ms: 693.787\n",
      "    load_throughput: 40553.554\n",
      "    load_time_ms: 24.659\n",
      "    sample_throughput: 37.467\n",
      "    sample_time_ms: 26690.289\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1635069351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 276\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         6481.39</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> -3.1018</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            310.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 277000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-56-18\n",
      "  done: false\n",
      "  episode_len_mean: 308.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.087999999999978\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 875\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7306432803471883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0062474405697575406\n",
      "          policy_loss: -0.10526559154192607\n",
      "          total_loss: -0.09584599344266785\n",
      "          vf_explained_var: 0.16280092298984528\n",
      "          vf_loss: 0.01672016933767332\n",
      "    num_agent_steps_sampled: 277000\n",
      "    num_agent_steps_trained: 277000\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.35405405405405\n",
      "    ram_util_percent: 38.664864864864875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038854867924973606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.694538016731222\n",
      "    mean_inference_ms: 1.9304768686663243\n",
      "    mean_raw_obs_processing_ms: 1.975859794044186\n",
      "  time_since_restore: 6507.676557064056\n",
      "  time_this_iter_s: 26.28584575653076\n",
      "  time_total_s: 6507.676557064056\n",
      "  timers:\n",
      "    learn_throughput: 1441.764\n",
      "    learn_time_ms: 693.595\n",
      "    load_throughput: 40798.956\n",
      "    load_time_ms: 24.51\n",
      "    sample_throughput: 36.798\n",
      "    sample_time_ms: 27175.24\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1635069378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         6507.68</td><td style=\"text-align: right;\">277000</td><td style=\"text-align: right;\">  -3.088</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">             308.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 278000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 305.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.0586999999999778\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 880\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7388110909197065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013716297967075599\n",
      "          policy_loss: -0.016139251159297097\n",
      "          total_loss: -0.009412522407041655\n",
      "          vf_explained_var: 0.2772153913974762\n",
      "          vf_loss: 0.014101966046210793\n",
      "    num_agent_steps_sampled: 278000\n",
      "    num_agent_steps_trained: 278000\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.4421052631579\n",
      "    ram_util_percent: 38.77368421052631\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03885283041473825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.697404926289813\n",
      "    mean_inference_ms: 1.9304316617285082\n",
      "    mean_raw_obs_processing_ms: 1.9772221357595305\n",
      "  time_since_restore: 6534.2847101688385\n",
      "  time_this_iter_s: 26.608153104782104\n",
      "  time_total_s: 6534.2847101688385\n",
      "  timers:\n",
      "    learn_throughput: 1443.773\n",
      "    learn_time_ms: 692.63\n",
      "    load_throughput: 40984.134\n",
      "    load_time_ms: 24.4\n",
      "    sample_throughput: 38.815\n",
      "    sample_time_ms: 25763.205\n",
      "    update_time_ms: 2.45\n",
      "  timestamp: 1635069404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 278\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         6534.28</td><td style=\"text-align: right;\">278000</td><td style=\"text-align: right;\"> -3.0587</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            305.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 279000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-57-10\n",
      "  done: false\n",
      "  episode_len_mean: 303.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.0313999999999792\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 884\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.816157865524292\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016815412731005393\n",
      "          policy_loss: 0.0024512574076652528\n",
      "          total_loss: 0.00481951062877973\n",
      "          vf_explained_var: 0.4290279746055603\n",
      "          vf_loss: 0.010514050359941192\n",
      "    num_agent_steps_sampled: 279000\n",
      "    num_agent_steps_trained: 279000\n",
      "    num_steps_sampled: 279000\n",
      "    num_steps_trained: 279000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.7421052631579\n",
      "    ram_util_percent: 38.86052631578948\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03885126200078967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.700192522569253\n",
      "    mean_inference_ms: 1.9303961984855749\n",
      "    mean_raw_obs_processing_ms: 1.9768205105388696\n",
      "  time_since_restore: 6560.37806892395\n",
      "  time_this_iter_s: 26.093358755111694\n",
      "  time_total_s: 6560.37806892395\n",
      "  timers:\n",
      "    learn_throughput: 1444.609\n",
      "    learn_time_ms: 692.229\n",
      "    load_throughput: 40860.602\n",
      "    load_time_ms: 24.473\n",
      "    sample_throughput: 38.298\n",
      "    sample_time_ms: 26111.059\n",
      "    update_time_ms: 2.439\n",
      "  timestamp: 1635069430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279000\n",
      "  training_iteration: 279\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         6560.38</td><td style=\"text-align: right;\">279000</td><td style=\"text-align: right;\"> -3.0314</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            303.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-57-36\n",
      "  done: false\n",
      "  episode_len_mean: 301.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -3.0164999999999793\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 887\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.949845693508784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010443467873205412\n",
      "          policy_loss: -0.09952030728260676\n",
      "          total_loss: -0.09854679356018702\n",
      "          vf_explained_var: 0.48277512192726135\n",
      "          vf_loss: 0.010462168272998599\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24444444444444\n",
      "    ram_util_percent: 38.891666666666666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03885008572544208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.70274323488807\n",
      "    mean_inference_ms: 1.9303693478075579\n",
      "    mean_raw_obs_processing_ms: 1.9765250504925256\n",
      "  time_since_restore: 6585.906830787659\n",
      "  time_this_iter_s: 25.528761863708496\n",
      "  time_total_s: 6585.906830787659\n",
      "  timers:\n",
      "    learn_throughput: 1445.632\n",
      "    learn_time_ms: 691.739\n",
      "    load_throughput: 40791.972\n",
      "    load_time_ms: 24.515\n",
      "    sample_throughput: 38.006\n",
      "    sample_time_ms: 26311.521\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1635069456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         6585.91</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> -3.0165</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            301.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 281000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-58-00\n",
      "  done: false\n",
      "  episode_len_mean: 298.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.98489999999998\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 891\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0375919494363997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015154653253575942\n",
      "          policy_loss: -0.046325044416719016\n",
      "          total_loss: -0.0487883637762732\n",
      "          vf_explained_var: 0.6602362990379333\n",
      "          vf_loss: 0.007898375645486845\n",
      "    num_agent_steps_sampled: 281000\n",
      "    num_agent_steps_trained: 281000\n",
      "    num_steps_sampled: 281000\n",
      "    num_steps_trained: 281000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.05428571428571\n",
      "    ram_util_percent: 38.87142857142857\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03884854644602338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.70683630254108\n",
      "    mean_inference_ms: 1.930334889545783\n",
      "    mean_raw_obs_processing_ms: 1.9761922668888252\n",
      "  time_since_restore: 6610.323164463043\n",
      "  time_this_iter_s: 24.41633367538452\n",
      "  time_total_s: 6610.323164463043\n",
      "  timers:\n",
      "    learn_throughput: 1448.636\n",
      "    learn_time_ms: 690.304\n",
      "    load_throughput: 40857.059\n",
      "    load_time_ms: 24.476\n",
      "    sample_throughput: 38.027\n",
      "    sample_time_ms: 26297.355\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1635069480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281000\n",
      "  training_iteration: 281\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         6610.32</td><td style=\"text-align: right;\">281000</td><td style=\"text-align: right;\"> -2.9849</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            298.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 282000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-58-26\n",
      "  done: false\n",
      "  episode_len_mean: 296.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.9651999999999803\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 895\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0563539107640585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011028968424538653\n",
      "          policy_loss: 0.0034592580050230026\n",
      "          total_loss: 0.0008148627562655343\n",
      "          vf_explained_var: 0.7523831129074097\n",
      "          vf_loss: 0.007908790832799342\n",
      "    num_agent_steps_sampled: 282000\n",
      "    num_agent_steps_trained: 282000\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.008108108108104\n",
      "    ram_util_percent: 38.81891891891891\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038847046887933355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.71153865282996\n",
      "    mean_inference_ms: 1.9303032165037286\n",
      "    mean_raw_obs_processing_ms: 1.9760336652272121\n",
      "  time_since_restore: 6636.043891429901\n",
      "  time_this_iter_s: 25.72072696685791\n",
      "  time_total_s: 6636.043891429901\n",
      "  timers:\n",
      "    learn_throughput: 1448.356\n",
      "    learn_time_ms: 690.438\n",
      "    load_throughput: 41107.852\n",
      "    load_time_ms: 24.326\n",
      "    sample_throughput: 37.712\n",
      "    sample_time_ms: 26516.792\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1635069506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         6636.04</td><td style=\"text-align: right;\">282000</td><td style=\"text-align: right;\"> -2.9652</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            296.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 283000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-58-51\n",
      "  done: false\n",
      "  episode_len_mean: 295.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.9547999999999814\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 899\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.151063428322474\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007338253370726496\n",
      "          policy_loss: 0.0169158602754275\n",
      "          total_loss: 0.012131714324156443\n",
      "          vf_explained_var: 0.8011374473571777\n",
      "          vf_loss: 0.006719600533445677\n",
      "    num_agent_steps_sampled: 283000\n",
      "    num_agent_steps_trained: 283000\n",
      "    num_steps_sampled: 283000\n",
      "    num_steps_trained: 283000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.13428571428572\n",
      "    ram_util_percent: 38.808571428571426\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03884555409253263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.716532600398853\n",
      "    mean_inference_ms: 1.930272393723925\n",
      "    mean_raw_obs_processing_ms: 1.975975468771534\n",
      "  time_since_restore: 6660.9575753211975\n",
      "  time_this_iter_s: 24.913683891296387\n",
      "  time_total_s: 6660.9575753211975\n",
      "  timers:\n",
      "    learn_throughput: 1447.787\n",
      "    learn_time_ms: 690.709\n",
      "    load_throughput: 41178.479\n",
      "    load_time_ms: 24.285\n",
      "    sample_throughput: 37.612\n",
      "    sample_time_ms: 26587.483\n",
      "    update_time_ms: 2.695\n",
      "  timestamp: 1635069531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 283000\n",
      "  training_iteration: 283\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         6660.96</td><td style=\"text-align: right;\">283000</td><td style=\"text-align: right;\"> -2.9548</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            295.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-59-32\n",
      "  done: false\n",
      "  episode_len_mean: 295.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.9509999999999805\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 902\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5066039972835117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011353435243018042\n",
      "          policy_loss: 0.041424186527729036\n",
      "          total_loss: 0.03195735663175583\n",
      "          vf_explained_var: 0.8212405443191528\n",
      "          vf_loss: 0.005588556353985849\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.361016949152535\n",
      "    ram_util_percent: 38.73898305084745\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03884443078981075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.72051343995031\n",
      "    mean_inference_ms: 1.9302498328336772\n",
      "    mean_raw_obs_processing_ms: 1.9777281149347397\n",
      "  time_since_restore: 6701.914119005203\n",
      "  time_this_iter_s: 40.95654368400574\n",
      "  time_total_s: 6701.914119005203\n",
      "  timers:\n",
      "    learn_throughput: 1447.97\n",
      "    learn_time_ms: 690.622\n",
      "    load_throughput: 41005.771\n",
      "    load_time_ms: 24.387\n",
      "    sample_throughput: 35.475\n",
      "    sample_time_ms: 28188.57\n",
      "    update_time_ms: 2.673\n",
      "  timestamp: 1635069572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         6701.91</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">  -2.951</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">             295.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_09-59-57\n",
      "  done: false\n",
      "  episode_len_mean: 293.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.938999999999981\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 906\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4201854798528883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013381430728532262\n",
      "          policy_loss: 0.03394714925024245\n",
      "          total_loss: 0.024798532906505796\n",
      "          vf_explained_var: 0.8881040215492249\n",
      "          vf_loss: 0.005040679313242436\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.48571428571429\n",
      "    ram_util_percent: 38.688571428571436\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03884297860117154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.726344929072503\n",
      "    mean_inference_ms: 1.9302231072522955\n",
      "    mean_raw_obs_processing_ms: 1.980136155536178\n",
      "  time_since_restore: 6726.51699924469\n",
      "  time_this_iter_s: 24.602880239486694\n",
      "  time_total_s: 6726.51699924469\n",
      "  timers:\n",
      "    learn_throughput: 1449.786\n",
      "    learn_time_ms: 689.757\n",
      "    load_throughput: 41312.081\n",
      "    load_time_ms: 24.206\n",
      "    sample_throughput: 35.627\n",
      "    sample_time_ms: 28068.985\n",
      "    update_time_ms: 2.717\n",
      "  timestamp: 1635069597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 285\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         6726.52</td><td style=\"text-align: right;\">285000</td><td style=\"text-align: right;\">  -2.939</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">             293.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 286000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-00-21\n",
      "  done: false\n",
      "  episode_len_mean: 292.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.928399999999981\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 909\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4515428205331167\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018592192185189163\n",
      "          policy_loss: 0.03891281858086586\n",
      "          total_loss: 0.029255757894780902\n",
      "          vf_explained_var: 0.8827378749847412\n",
      "          vf_loss: 0.004840917342031996\n",
      "    num_agent_steps_sampled: 286000\n",
      "    num_agent_steps_trained: 286000\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.64857142857143\n",
      "    ram_util_percent: 38.78\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03884191874948813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.7311272495014\n",
      "    mean_inference_ms: 1.9302048233639622\n",
      "    mean_raw_obs_processing_ms: 1.9806442642304076\n",
      "  time_since_restore: 6751.243536472321\n",
      "  time_this_iter_s: 24.726537227630615\n",
      "  time_total_s: 6751.243536472321\n",
      "  timers:\n",
      "    learn_throughput: 1450.847\n",
      "    learn_time_ms: 689.252\n",
      "    load_throughput: 41480.122\n",
      "    load_time_ms: 24.108\n",
      "    sample_throughput: 38.073\n",
      "    sample_time_ms: 26265.663\n",
      "    update_time_ms: 2.713\n",
      "  timestamp: 1635069621\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         6751.24</td><td style=\"text-align: right;\">286000</td><td style=\"text-align: right;\"> -2.9284</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            292.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 287000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-00-46\n",
      "  done: false\n",
      "  episode_len_mean: 291.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.9189999999999814\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 913\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2437067892816331\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007365719618429692\n",
      "          policy_loss: -0.024110800276199978\n",
      "          total_loss: -0.02670218182934655\n",
      "          vf_explained_var: 0.7789561152458191\n",
      "          vf_loss: 0.009838774686472283\n",
      "    num_agent_steps_sampled: 287000\n",
      "    num_agent_steps_trained: 287000\n",
      "    num_steps_sampled: 287000\n",
      "    num_steps_trained: 287000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.97714285714286\n",
      "    ram_util_percent: 38.871428571428574\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03884048183492816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.73803143168502\n",
      "    mean_inference_ms: 1.9301827596886938\n",
      "    mean_raw_obs_processing_ms: 1.98047737006115\n",
      "  time_since_restore: 6775.9139976501465\n",
      "  time_this_iter_s: 24.670461177825928\n",
      "  time_total_s: 6775.9139976501465\n",
      "  timers:\n",
      "    learn_throughput: 1453.654\n",
      "    learn_time_ms: 687.921\n",
      "    load_throughput: 41392.479\n",
      "    load_time_ms: 24.159\n",
      "    sample_throughput: 38.306\n",
      "    sample_time_ms: 26105.419\n",
      "    update_time_ms: 2.693\n",
      "  timestamp: 1635069646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287000\n",
      "  training_iteration: 287\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         6775.91</td><td style=\"text-align: right;\">287000</td><td style=\"text-align: right;\">  -2.919</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">             291.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-01-11\n",
      "  done: false\n",
      "  episode_len_mean: 289.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.8937999999999824\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 916\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0009385585784912106\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.149012835820516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027725225494963014\n",
      "          policy_loss: -0.0030988075253036286\n",
      "          total_loss: -0.0064649121628867256\n",
      "          vf_explained_var: 0.7285565733909607\n",
      "          vf_loss: 0.008098003012128175\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.45675675675676\n",
      "    ram_util_percent: 38.88648648648648\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883939409417431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.743884538559605\n",
      "    mean_inference_ms: 1.930167807768291\n",
      "    mean_raw_obs_processing_ms: 1.9804261581888931\n",
      "  time_since_restore: 6801.2957553863525\n",
      "  time_this_iter_s: 25.381757736206055\n",
      "  time_total_s: 6801.2957553863525\n",
      "  timers:\n",
      "    learn_throughput: 1450.127\n",
      "    learn_time_ms: 689.595\n",
      "    load_throughput: 41122.361\n",
      "    load_time_ms: 24.318\n",
      "    sample_throughput: 38.49\n",
      "    sample_time_ms: 25980.943\n",
      "    update_time_ms: 2.702\n",
      "  timestamp: 1635069671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 288\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">          6801.3</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\"> -2.8938</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            289.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 289000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 287.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.8786999999999825\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 920\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0014078378677368168\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.038434413406584\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009333409243461904\n",
      "          policy_loss: 0.0064425391455491384\n",
      "          total_loss: 0.005948039972119861\n",
      "          vf_explained_var: 0.640798807144165\n",
      "          vf_loss: 0.009876705954472225\n",
      "    num_agent_steps_sampled: 289000\n",
      "    num_agent_steps_trained: 289000\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.07058823529412\n",
      "    ram_util_percent: 38.83823529411764\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0388379650487136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.75201044952934\n",
      "    mean_inference_ms: 1.9301502795424517\n",
      "    mean_raw_obs_processing_ms: 1.9804603747770682\n",
      "  time_since_restore: 6825.566061973572\n",
      "  time_this_iter_s: 24.27030658721924\n",
      "  time_total_s: 6825.566061973572\n",
      "  timers:\n",
      "    learn_throughput: 1450.158\n",
      "    learn_time_ms: 689.58\n",
      "    load_throughput: 41003.085\n",
      "    load_time_ms: 24.388\n",
      "    sample_throughput: 38.762\n",
      "    sample_time_ms: 25798.578\n",
      "    update_time_ms: 2.712\n",
      "  timestamp: 1635069696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         6825.57</td><td style=\"text-align: right;\">289000</td><td style=\"text-align: right;\"> -2.8787</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            287.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 290000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-01-59\n",
      "  done: false\n",
      "  episode_len_mean: 287.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.8723999999999825\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 923\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0014078378677368168\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8288607292705112\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012778713222464372\n",
      "          policy_loss: 0.05457076869077153\n",
      "          total_loss: 0.052536389893955655\n",
      "          vf_explained_var: 0.42652618885040283\n",
      "          vf_loss: 0.006236242264923122\n",
      "    num_agent_steps_sampled: 290000\n",
      "    num_agent_steps_trained: 290000\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.805882352941175\n",
      "    ram_util_percent: 38.817647058823525\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0388368830364158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.758299158877765\n",
      "    mean_inference_ms: 1.9301377385593972\n",
      "    mean_raw_obs_processing_ms: 1.9805248504826267\n",
      "  time_since_restore: 6848.98631811142\n",
      "  time_this_iter_s: 23.4202561378479\n",
      "  time_total_s: 6848.98631811142\n",
      "  timers:\n",
      "    learn_throughput: 1448.817\n",
      "    learn_time_ms: 690.218\n",
      "    load_throughput: 41111.398\n",
      "    load_time_ms: 24.324\n",
      "    sample_throughput: 39.082\n",
      "    sample_time_ms: 25587.166\n",
      "    update_time_ms: 2.692\n",
      "  timestamp: 1635069719\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 290\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         6848.99</td><td style=\"text-align: right;\">290000</td><td style=\"text-align: right;\"> -2.8724</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            287.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 291000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-02-23\n",
      "  done: false\n",
      "  episode_len_mean: 283.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.8366999999999836\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 927\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0014078378677368168\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8765174965063731\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007890895426256733\n",
      "          policy_loss: -0.001959945178694195\n",
      "          total_loss: -0.002724985736939642\n",
      "          vf_explained_var: 0.5217256546020508\n",
      "          vf_loss: 0.007989026719911231\n",
      "    num_agent_steps_sampled: 291000\n",
      "    num_agent_steps_trained: 291000\n",
      "    num_steps_sampled: 291000\n",
      "    num_steps_trained: 291000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.07647058823529\n",
      "    ram_util_percent: 38.811764705882354\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883543225529174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.767236499395075\n",
      "    mean_inference_ms: 1.9301216548319895\n",
      "    mean_raw_obs_processing_ms: 1.9806863952579425\n",
      "  time_since_restore: 6873.06326341629\n",
      "  time_this_iter_s: 24.076945304870605\n",
      "  time_total_s: 6873.06326341629\n",
      "  timers:\n",
      "    learn_throughput: 1445.922\n",
      "    learn_time_ms: 691.6\n",
      "    load_throughput: 40724.996\n",
      "    load_time_ms: 24.555\n",
      "    sample_throughput: 39.136\n",
      "    sample_time_ms: 25551.682\n",
      "    update_time_ms: 2.676\n",
      "  timestamp: 1635069743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291000\n",
      "  training_iteration: 291\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         6873.06</td><td style=\"text-align: right;\">291000</td><td style=\"text-align: right;\"> -2.8367</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            283.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 278.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7878999999999836\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 931\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0014078378677368168\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8750265214178298\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007846042202929107\n",
      "          policy_loss: -0.019692500390940244\n",
      "          total_loss: -0.016179453374611005\n",
      "          vf_explained_var: 0.23055152595043182\n",
      "          vf_loss: 0.012252267388006052\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.50819672131147\n",
      "    ram_util_percent: 38.72622950819672\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883404399091764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.77742740348325\n",
      "    mean_inference_ms: 1.930106427528316\n",
      "    mean_raw_obs_processing_ms: 1.9832782034406968\n",
      "  time_since_restore: 6915.611022472382\n",
      "  time_this_iter_s: 42.54775905609131\n",
      "  time_total_s: 6915.611022472382\n",
      "  timers:\n",
      "    learn_throughput: 1448.006\n",
      "    learn_time_ms: 690.605\n",
      "    load_throughput: 40669.12\n",
      "    load_time_ms: 24.589\n",
      "    sample_throughput: 36.717\n",
      "    sample_time_ms: 27235.314\n",
      "    update_time_ms: 2.709\n",
      "  timestamp: 1635069786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         6915.61</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\"> -2.7879</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            278.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 293000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-03-32\n",
      "  done: false\n",
      "  episode_len_mean: 273.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7337999999999854\n",
      "  episode_reward_min: -3.609999999999967\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 935\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0014078378677368168\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6885493901040819\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004884144194511684\n",
      "          policy_loss: -0.014779128051466412\n",
      "          total_loss: -0.011132685674561394\n",
      "          vf_explained_var: 0.3448481857776642\n",
      "          vf_loss: 0.010525061593701443\n",
      "    num_agent_steps_sampled: 293000\n",
      "    num_agent_steps_trained: 293000\n",
      "    num_steps_sampled: 293000\n",
      "    num_steps_trained: 293000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.729729729729726\n",
      "    ram_util_percent: 38.74864864864865\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883271882425092\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.788748510575214\n",
      "    mean_inference_ms: 1.930092157394307\n",
      "    mean_raw_obs_processing_ms: 1.9860244337143262\n",
      "  time_since_restore: 6941.752507209778\n",
      "  time_this_iter_s: 26.14148473739624\n",
      "  time_total_s: 6941.752507209778\n",
      "  timers:\n",
      "    learn_throughput: 1448.59\n",
      "    learn_time_ms: 690.327\n",
      "    load_throughput: 40746.716\n",
      "    load_time_ms: 24.542\n",
      "    sample_throughput: 36.551\n",
      "    sample_time_ms: 27358.717\n",
      "    update_time_ms: 2.425\n",
      "  timestamp: 1635069812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 293000\n",
      "  training_iteration: 293\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         6941.75</td><td style=\"text-align: right;\">293000</td><td style=\"text-align: right;\"> -2.7338</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.61</td><td style=\"text-align: right;\">            273.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 294000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-03-58\n",
      "  done: false\n",
      "  episode_len_mean: 270.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7075999999999865\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 938\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007039189338684084\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6733316388395097\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015598066439929425\n",
      "          policy_loss: -0.09658333775069979\n",
      "          total_loss: -0.09194874539971351\n",
      "          vf_explained_var: 0.2363084852695465\n",
      "          vf_loss: 0.011356927785608503\n",
      "    num_agent_steps_sampled: 294000\n",
      "    num_agent_steps_trained: 294000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.96756756756757\n",
      "    ram_util_percent: 38.78648648648648\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038831752163068826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.797711999300414\n",
      "    mean_inference_ms: 1.9300830795972632\n",
      "    mean_raw_obs_processing_ms: 1.9881899454298624\n",
      "  time_since_restore: 6967.311648845673\n",
      "  time_this_iter_s: 25.559141635894775\n",
      "  time_total_s: 6967.311648845673\n",
      "  timers:\n",
      "    learn_throughput: 1447.995\n",
      "    learn_time_ms: 690.61\n",
      "    load_throughput: 40784.555\n",
      "    load_time_ms: 24.519\n",
      "    sample_throughput: 38.732\n",
      "    sample_time_ms: 25818.708\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1635069838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 294\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         6967.31</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\"> -2.7076</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            270.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 295000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-04-22\n",
      "  done: false\n",
      "  episode_len_mean: 269.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.6945999999999857\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 942\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007039189338684084\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7416695449087355\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.036200388656365705\n",
      "          policy_loss: -0.031144380900594924\n",
      "          total_loss: -0.02810153224402004\n",
      "          vf_explained_var: 0.22800792753696442\n",
      "          vf_loss: 0.010434064155237541\n",
      "    num_agent_steps_sampled: 295000\n",
      "    num_agent_steps_trained: 295000\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.27142857142857\n",
      "    ram_util_percent: 38.79714285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883047841843729\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.809767948652297\n",
      "    mean_inference_ms: 1.9300722265278558\n",
      "    mean_raw_obs_processing_ms: 1.9891584428128695\n",
      "  time_since_restore: 6992.012535810471\n",
      "  time_this_iter_s: 24.700886964797974\n",
      "  time_total_s: 6992.012535810471\n",
      "  timers:\n",
      "    learn_throughput: 1447.292\n",
      "    learn_time_ms: 690.946\n",
      "    load_throughput: 40385.376\n",
      "    load_time_ms: 24.761\n",
      "    sample_throughput: 38.718\n",
      "    sample_time_ms: 25827.995\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1635069862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         6992.01</td><td style=\"text-align: right;\">295000</td><td style=\"text-align: right;\"> -2.6946</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            269.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-04-44\n",
      "  done: false\n",
      "  episode_len_mean: 269.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.6916999999999867\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 946\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0010558784008026126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9724959797329373\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008819038462538151\n",
      "          policy_loss: -0.006035883186591996\n",
      "          total_loss: -0.0030544977635145187\n",
      "          vf_explained_var: 0.1919991672039032\n",
      "          vf_loss: 0.012697033201240831\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.10625\n",
      "    ram_util_percent: 38.896875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882921624229261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82152571524481\n",
      "    mean_inference_ms: 1.9300626974444919\n",
      "    mean_raw_obs_processing_ms: 1.9896580423974666\n",
      "  time_since_restore: 7014.045876979828\n",
      "  time_this_iter_s: 22.0333411693573\n",
      "  time_total_s: 7014.045876979828\n",
      "  timers:\n",
      "    learn_throughput: 1445.465\n",
      "    learn_time_ms: 691.819\n",
      "    load_throughput: 40338.185\n",
      "    load_time_ms: 24.79\n",
      "    sample_throughput: 39.127\n",
      "    sample_time_ms: 25557.793\n",
      "    update_time_ms: 2.385\n",
      "  timestamp: 1635069884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 296\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         7014.05</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -2.6917</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            269.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 297000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-05-10\n",
      "  done: false\n",
      "  episode_len_mean: 268.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.683599999999986\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 949\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0010558784008026126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6376617319054074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008066584993360751\n",
      "          policy_loss: -0.09088224768638611\n",
      "          total_loss: -0.08510082587599754\n",
      "          vf_explained_var: 0.10645461827516556\n",
      "          vf_loss: 0.012149518076330423\n",
      "    num_agent_steps_sampled: 297000\n",
      "    num_agent_steps_trained: 297000\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.82972972972973\n",
      "    ram_util_percent: 38.85135135135135\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0388282683676929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.83059053981209\n",
      "    mean_inference_ms: 1.9300557323555745\n",
      "    mean_raw_obs_processing_ms: 1.989996608114065\n",
      "  time_since_restore: 7039.930369853973\n",
      "  time_this_iter_s: 25.884492874145508\n",
      "  time_total_s: 7039.930369853973\n",
      "  timers:\n",
      "    learn_throughput: 1443.718\n",
      "    learn_time_ms: 692.656\n",
      "    load_throughput: 40030.541\n",
      "    load_time_ms: 24.981\n",
      "    sample_throughput: 38.944\n",
      "    sample_time_ms: 25678.123\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1635069910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 297\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         7039.93</td><td style=\"text-align: right;\">297000</td><td style=\"text-align: right;\"> -2.6836</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            268.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 298000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-05-32\n",
      "  done: false\n",
      "  episode_len_mean: 268.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.6895999999999862\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 953\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0010558784008026126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7985936820507049\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012366845965105986\n",
      "          policy_loss: 0.054958919021818375\n",
      "          total_loss: 0.05673690363764763\n",
      "          vf_explained_var: 0.04723800718784332\n",
      "          vf_loss: 0.009750861840115654\n",
      "    num_agent_steps_sampled: 298000\n",
      "    num_agent_steps_trained: 298000\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06451612903226\n",
      "    ram_util_percent: 38.8290322580645\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0388270573147759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.842162233156227\n",
      "    mean_inference_ms: 1.9300466456089325\n",
      "    mean_raw_obs_processing_ms: 1.9905374358863634\n",
      "  time_since_restore: 7061.8026287555695\n",
      "  time_this_iter_s: 21.87225890159607\n",
      "  time_total_s: 7061.8026287555695\n",
      "  timers:\n",
      "    learn_throughput: 1444.861\n",
      "    learn_time_ms: 692.108\n",
      "    load_throughput: 40322.557\n",
      "    load_time_ms: 24.8\n",
      "    sample_throughput: 39.482\n",
      "    sample_time_ms: 25327.884\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1635069932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">          7061.8</td><td style=\"text-align: right;\">298000</td><td style=\"text-align: right;\"> -2.6896</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            268.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 299000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-05-57\n",
      "  done: false\n",
      "  episode_len_mean: 268.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.6801999999999873\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 957\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0010558784008026126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7463649895456103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006482025349649787\n",
      "          policy_loss: 0.014371275073952146\n",
      "          total_loss: 0.02013580103715261\n",
      "          vf_explained_var: 0.06536143273115158\n",
      "          vf_loss: 0.013221334334876803\n",
      "    num_agent_steps_sampled: 299000\n",
      "    num_agent_steps_trained: 299000\n",
      "    num_steps_sampled: 299000\n",
      "    num_steps_trained: 299000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11666666666667\n",
      "    ram_util_percent: 38.788888888888884\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882588367699795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.853857642220415\n",
      "    mean_inference_ms: 1.930037982114573\n",
      "    mean_raw_obs_processing_ms: 1.9911012317842005\n",
      "  time_since_restore: 7086.927287101746\n",
      "  time_this_iter_s: 25.124658346176147\n",
      "  time_total_s: 7086.927287101746\n",
      "  timers:\n",
      "    learn_throughput: 1442.794\n",
      "    learn_time_ms: 693.1\n",
      "    load_throughput: 40366.253\n",
      "    load_time_ms: 24.773\n",
      "    sample_throughput: 39.351\n",
      "    sample_time_ms: 25412.363\n",
      "    update_time_ms: 2.426\n",
      "  timestamp: 1635069957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299000\n",
      "  training_iteration: 299\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         7086.93</td><td style=\"text-align: right;\">299000</td><td style=\"text-align: right;\"> -2.6802</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            268.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-06-40\n",
      "  done: false\n",
      "  episode_len_mean: 267.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.6787999999999874\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 960\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0010558784008026126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7249318874544568\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015739559453733894\n",
      "          policy_loss: -0.10250056154198117\n",
      "          total_loss: -0.09640490619672669\n",
      "          vf_explained_var: 0.09848188608884811\n",
      "          vf_loss: 0.013328351887563865\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.036666666666676\n",
      "    ram_util_percent: 38.74\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882498607469537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.862778812746974\n",
      "    mean_inference_ms: 1.9300307879979086\n",
      "    mean_raw_obs_processing_ms: 1.9931841274243076\n",
      "  time_since_restore: 7129.388780832291\n",
      "  time_this_iter_s: 42.461493730545044\n",
      "  time_total_s: 7129.388780832291\n",
      "  timers:\n",
      "    learn_throughput: 1442.006\n",
      "    learn_time_ms: 693.478\n",
      "    load_throughput: 40268.359\n",
      "    load_time_ms: 24.833\n",
      "    sample_throughput: 36.609\n",
      "    sample_time_ms: 27316.011\n",
      "    update_time_ms: 2.443\n",
      "  timestamp: 1635070000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 300\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         7129.39</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\"> -2.6788</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            267.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 301000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-07-01\n",
      "  done: false\n",
      "  episode_len_mean: 269.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.6950999999999867\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 964\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0010558784008026126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.053720157676273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006820351853287102\n",
      "          policy_loss: 0.009491446365912756\n",
      "          total_loss: 0.013143770313925213\n",
      "          vf_explained_var: 0.040692251175642014\n",
      "          vf_loss: 0.014182323093215625\n",
      "    num_agent_steps_sampled: 301000\n",
      "    num_agent_steps_trained: 301000\n",
      "    num_steps_sampled: 301000\n",
      "    num_steps_trained: 301000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.538709677419355\n",
      "    ram_util_percent: 38.65161290322582\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038823778069063036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.87398051505785\n",
      "    mean_inference_ms: 1.9300209301107907\n",
      "    mean_raw_obs_processing_ms: 1.9960403724911924\n",
      "  time_since_restore: 7150.885870218277\n",
      "  time_this_iter_s: 21.497089385986328\n",
      "  time_total_s: 7150.885870218277\n",
      "  timers:\n",
      "    learn_throughput: 1443.256\n",
      "    learn_time_ms: 692.878\n",
      "    load_throughput: 40660.959\n",
      "    load_time_ms: 24.594\n",
      "    sample_throughput: 36.957\n",
      "    sample_time_ms: 27058.836\n",
      "    update_time_ms: 2.469\n",
      "  timestamp: 1635070021\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 301000\n",
      "  training_iteration: 301\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         7150.89</td><td style=\"text-align: right;\">301000</td><td style=\"text-align: right;\"> -2.6951</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            269.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 302000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-07-22\n",
      "  done: false\n",
      "  episode_len_mean: 271.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.239999999999996\n",
      "  episode_reward_mean: -2.7147999999999866\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 967\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0010558784008026126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0624422828356426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020313788085901764\n",
      "          policy_loss: 0.059706243044800225\n",
      "          total_loss: 0.05880995152725114\n",
      "          vf_explained_var: -0.23608729243278503\n",
      "          vf_loss: 0.009706680856955549\n",
      "    num_agent_steps_sampled: 302000\n",
      "    num_agent_steps_trained: 302000\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.87333333333333\n",
      "    ram_util_percent: 38.75666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882285521491442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.88193433854357\n",
      "    mean_inference_ms: 1.9300133518259728\n",
      "    mean_raw_obs_processing_ms: 1.9981417899020832\n",
      "  time_since_restore: 7171.7146961688995\n",
      "  time_this_iter_s: 20.82882595062256\n",
      "  time_total_s: 7171.7146961688995\n",
      "  timers:\n",
      "    learn_throughput: 1440.161\n",
      "    learn_time_ms: 694.367\n",
      "    load_throughput: 40567.046\n",
      "    load_time_ms: 24.651\n",
      "    sample_throughput: 40.184\n",
      "    sample_time_ms: 24885.396\n",
      "    update_time_ms: 2.473\n",
      "  timestamp: 1635070042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         7171.71</td><td style=\"text-align: right;\">302000</td><td style=\"text-align: right;\"> -2.7148</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            271.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 303000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-07-44\n",
      "  done: false\n",
      "  episode_len_mean: 273.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.7318999999999853\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 970\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015838176012039178\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.102066460582945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005899731583368073\n",
      "          policy_loss: 0.06601527929306031\n",
      "          total_loss: 0.0656431860393948\n",
      "          vf_explained_var: 0.023311885073781013\n",
      "          vf_loss: 0.010639226474126594\n",
      "    num_agent_steps_sampled: 303000\n",
      "    num_agent_steps_trained: 303000\n",
      "    num_steps_sampled: 303000\n",
      "    num_steps_trained: 303000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.92580645161289\n",
      "    ram_util_percent: 38.79032258064515\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038821862517864215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.889438468510896\n",
      "    mean_inference_ms: 1.9300034001603334\n",
      "    mean_raw_obs_processing_ms: 1.9984356133122776\n",
      "  time_since_restore: 7193.881264925003\n",
      "  time_this_iter_s: 22.166568756103516\n",
      "  time_total_s: 7193.881264925003\n",
      "  timers:\n",
      "    learn_throughput: 1443.148\n",
      "    learn_time_ms: 692.93\n",
      "    load_throughput: 40039.445\n",
      "    load_time_ms: 24.975\n",
      "    sample_throughput: 40.835\n",
      "    sample_time_ms: 24489.029\n",
      "    update_time_ms: 2.461\n",
      "  timestamp: 1635070064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303000\n",
      "  training_iteration: 303\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         7193.88</td><td style=\"text-align: right;\">303000</td><td style=\"text-align: right;\"> -2.7319</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            273.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-08-04\n",
      "  done: false\n",
      "  episode_len_mean: 275.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.7590999999999855\n",
      "  episode_reward_min: -3.759999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 973\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015838176012039178\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2314035223590003\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00700600856204553\n",
      "          policy_loss: 0.05978641543123457\n",
      "          total_loss: 0.058050600356525844\n",
      "          vf_explained_var: -0.2666687071323395\n",
      "          vf_loss: 0.010567123928598852\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.671428571428564\n",
      "    ram_util_percent: 38.85714285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038820840144629463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.896381403821238\n",
      "    mean_inference_ms: 1.929992209573747\n",
      "    mean_raw_obs_processing_ms: 1.9986796582233968\n",
      "  time_since_restore: 7213.4686596393585\n",
      "  time_this_iter_s: 19.58739471435547\n",
      "  time_total_s: 7213.4686596393585\n",
      "  timers:\n",
      "    learn_throughput: 1442.993\n",
      "    learn_time_ms: 693.004\n",
      "    load_throughput: 39884.331\n",
      "    load_time_ms: 25.073\n",
      "    sample_throughput: 41.856\n",
      "    sample_time_ms: 23891.684\n",
      "    update_time_ms: 2.477\n",
      "  timestamp: 1635070084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         7213.47</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\"> -2.7591</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.76</td><td style=\"text-align: right;\">            275.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 305000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-08-23\n",
      "  done: false\n",
      "  episode_len_mean: 279.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.794399999999984\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 976\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015838176012039178\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2861849281522963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011881724895991791\n",
      "          policy_loss: 0.09512430048651166\n",
      "          total_loss: 0.09006282852755652\n",
      "          vf_explained_var: 0.2968853712081909\n",
      "          vf_loss: 0.007781562277361647\n",
      "    num_agent_steps_sampled: 305000\n",
      "    num_agent_steps_trained: 305000\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.69642857142858\n",
      "    ram_util_percent: 38.89285714285713\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881978404608171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.902727673430668\n",
      "    mean_inference_ms: 1.9299798702976716\n",
      "    mean_raw_obs_processing_ms: 1.9988729873800155\n",
      "  time_since_restore: 7232.767384052277\n",
      "  time_this_iter_s: 19.29872441291809\n",
      "  time_total_s: 7232.767384052277\n",
      "  timers:\n",
      "    learn_throughput: 1444.206\n",
      "    learn_time_ms: 692.422\n",
      "    load_throughput: 40201.896\n",
      "    load_time_ms: 24.874\n",
      "    sample_throughput: 42.822\n",
      "    sample_time_ms: 23352.249\n",
      "    update_time_ms: 2.484\n",
      "  timestamp: 1635070103\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 305\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         7232.77</td><td style=\"text-align: right;\">305000</td><td style=\"text-align: right;\"> -2.7944</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            279.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 306000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-08-43\n",
      "  done: false\n",
      "  episode_len_mean: 281.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.8156999999999837\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 978\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015838176012039178\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2898368848694695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00881104717230661\n",
      "          policy_loss: -0.08301386568281385\n",
      "          total_loss: -0.0837062292628818\n",
      "          vf_explained_var: 0.10130658745765686\n",
      "          vf_loss: 0.012192050388289823\n",
      "    num_agent_steps_sampled: 306000\n",
      "    num_agent_steps_trained: 306000\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.903448275862054\n",
      "    ram_util_percent: 38.86206896551723\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881906297483423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.906607618045445\n",
      "    mean_inference_ms: 1.9299708368320765\n",
      "    mean_raw_obs_processing_ms: 1.998965653952586\n",
      "  time_since_restore: 7252.83200097084\n",
      "  time_this_iter_s: 20.064616918563843\n",
      "  time_total_s: 7252.83200097084\n",
      "  timers:\n",
      "    learn_throughput: 1444.886\n",
      "    learn_time_ms: 692.096\n",
      "    load_throughput: 39880.955\n",
      "    load_time_ms: 25.075\n",
      "    sample_throughput: 43.186\n",
      "    sample_time_ms: 23155.526\n",
      "    update_time_ms: 2.467\n",
      "  timestamp: 1635070123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 306\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         7252.83</td><td style=\"text-align: right;\">306000</td><td style=\"text-align: right;\"> -2.8157</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            281.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 307000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-09-04\n",
      "  done: false\n",
      "  episode_len_mean: 284.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.849899999999983\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 982\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015838176012039178\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0959353970156775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008607398420799032\n",
      "          policy_loss: 0.0305445349878735\n",
      "          total_loss: 0.03389682935343848\n",
      "          vf_explained_var: 0.1710204780101776\n",
      "          vf_loss: 0.0142980194857551\n",
      "    num_agent_steps_sampled: 307000\n",
      "    num_agent_steps_trained: 307000\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.79666666666667\n",
      "    ram_util_percent: 38.776666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038817601082597665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.91394383534566\n",
      "    mean_inference_ms: 1.9299514402796298\n",
      "    mean_raw_obs_processing_ms: 1.9990478102188416\n",
      "  time_since_restore: 7273.898726940155\n",
      "  time_this_iter_s: 21.066725969314575\n",
      "  time_total_s: 7273.898726940155\n",
      "  timers:\n",
      "    learn_throughput: 1445.318\n",
      "    learn_time_ms: 691.889\n",
      "    load_throughput: 39949.9\n",
      "    load_time_ms: 25.031\n",
      "    sample_throughput: 44.103\n",
      "    sample_time_ms: 22674.046\n",
      "    update_time_ms: 2.429\n",
      "  timestamp: 1635070144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">          7273.9</td><td style=\"text-align: right;\">307000</td><td style=\"text-align: right;\"> -2.8499</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            284.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-09-26\n",
      "  done: false\n",
      "  episode_len_mean: 286.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.8636999999999824\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 985\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015838176012039178\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0082478357685938\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007282568403064241\n",
      "          policy_loss: 0.05537277791235182\n",
      "          total_loss: 0.05548918429348204\n",
      "          vf_explained_var: 0.31005343794822693\n",
      "          vf_loss: 0.010187350529142552\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.122580645161285\n",
      "    ram_util_percent: 38.745161290322585\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881648495538683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.91909086693758\n",
      "    mean_inference_ms: 1.9299368250678495\n",
      "    mean_raw_obs_processing_ms: 1.9990942961480784\n",
      "  time_since_restore: 7295.953457355499\n",
      "  time_this_iter_s: 22.05473041534424\n",
      "  time_total_s: 7295.953457355499\n",
      "  timers:\n",
      "    learn_throughput: 1447.876\n",
      "    learn_time_ms: 690.667\n",
      "    load_throughput: 39745.247\n",
      "    load_time_ms: 25.16\n",
      "    sample_throughput: 44.066\n",
      "    sample_time_ms: 22693.399\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1635070166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 308\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         7295.95</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\"> -2.8637</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            286.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 309000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-09-50\n",
      "  done: false\n",
      "  episode_len_mean: 286.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.8665999999999827\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 988\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015838176012039178\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7222506278091007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02313983958033232\n",
      "          policy_loss: -0.10007905165354411\n",
      "          total_loss: -0.09624920040369034\n",
      "          vf_explained_var: 0.3895309865474701\n",
      "          vf_loss: 0.011015707864943478\n",
      "    num_agent_steps_sampled: 309000\n",
      "    num_agent_steps_trained: 309000\n",
      "    num_steps_sampled: 309000\n",
      "    num_steps_trained: 309000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.05294117647059\n",
      "    ram_util_percent: 38.70588235294118\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038815410311452955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.92407213769502\n",
      "    mean_inference_ms: 1.9299228356074911\n",
      "    mean_raw_obs_processing_ms: 1.9991561999705088\n",
      "  time_since_restore: 7319.91627573967\n",
      "  time_this_iter_s: 23.962818384170532\n",
      "  time_total_s: 7319.91627573967\n",
      "  timers:\n",
      "    learn_throughput: 1448.6\n",
      "    learn_time_ms: 690.322\n",
      "    load_throughput: 39572.042\n",
      "    load_time_ms: 25.27\n",
      "    sample_throughput: 44.292\n",
      "    sample_time_ms: 22577.424\n",
      "    update_time_ms: 2.456\n",
      "  timestamp: 1635070190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309000\n",
      "  training_iteration: 309\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         7319.92</td><td style=\"text-align: right;\">309000</td><td style=\"text-align: right;\"> -2.8666</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            286.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 310000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-10-33\n",
      "  done: false\n",
      "  episode_len_mean: 287.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8729999999999825\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 992\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023757264018058784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8940497464603848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010724296195940753\n",
      "          policy_loss: -0.04536381479766634\n",
      "          total_loss: -0.03890109004245864\n",
      "          vf_explained_var: 0.3246677815914154\n",
      "          vf_loss: 0.015377744479984459\n",
      "    num_agent_steps_sampled: 310000\n",
      "    num_agent_steps_trained: 310000\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.5049180327869\n",
      "    ram_util_percent: 38.608196721311465\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881408807037743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.930791251276528\n",
      "    mean_inference_ms: 1.9299037545575453\n",
      "    mean_raw_obs_processing_ms: 2.0014221288863614\n",
      "  time_since_restore: 7362.399662494659\n",
      "  time_this_iter_s: 42.483386754989624\n",
      "  time_total_s: 7362.399662494659\n",
      "  timers:\n",
      "    learn_throughput: 1449.856\n",
      "    learn_time_ms: 689.724\n",
      "    load_throughput: 39523.565\n",
      "    load_time_ms: 25.301\n",
      "    sample_throughput: 44.287\n",
      "    sample_time_ms: 22580.15\n",
      "    update_time_ms: 2.528\n",
      "  timestamp: 1635070233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">          7362.4</td><td style=\"text-align: right;\">310000</td><td style=\"text-align: right;\">  -2.873</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">             287.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 311000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-10-58\n",
      "  done: false\n",
      "  episode_len_mean: 287.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8754999999999824\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 996\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023757264018058784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9660017251968384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009405207536804343\n",
      "          policy_loss: 0.014790188603931003\n",
      "          total_loss: 0.01677316307193703\n",
      "          vf_explained_var: 0.5312321186065674\n",
      "          vf_loss: 0.011620642503516541\n",
      "    num_agent_steps_sampled: 311000\n",
      "    num_agent_steps_trained: 311000\n",
      "    num_steps_sampled: 311000\n",
      "    num_steps_trained: 311000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03888888888889\n",
      "    ram_util_percent: 38.725\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881276495723606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.93737808493603\n",
      "    mean_inference_ms: 1.92988323424932\n",
      "    mean_raw_obs_processing_ms: 2.0037040173127605\n",
      "  time_since_restore: 7387.441014766693\n",
      "  time_this_iter_s: 25.04135227203369\n",
      "  time_total_s: 7387.441014766693\n",
      "  timers:\n",
      "    learn_throughput: 1448.255\n",
      "    learn_time_ms: 690.486\n",
      "    load_throughput: 39445.694\n",
      "    load_time_ms: 25.351\n",
      "    sample_throughput: 43.604\n",
      "    sample_time_ms: 22933.796\n",
      "    update_time_ms: 2.501\n",
      "  timestamp: 1635070258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311000\n",
      "  training_iteration: 311\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         7387.44</td><td style=\"text-align: right;\">311000</td><td style=\"text-align: right;\"> -2.8755</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            287.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 287.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8721999999999825\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1000\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023757264018058784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8330606129434374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005100244580115051\n",
      "          policy_loss: 0.021887709117598003\n",
      "          total_loss: 0.024550952224267855\n",
      "          vf_explained_var: 0.5704025626182556\n",
      "          vf_loss: 0.01098172999918461\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.955555555555556\n",
      "    ram_util_percent: 38.77222222222222\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881148145969397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.943964615281125\n",
      "    mean_inference_ms: 1.9298627567768518\n",
      "    mean_raw_obs_processing_ms: 2.005397621292306\n",
      "  time_since_restore: 7412.532769918442\n",
      "  time_this_iter_s: 25.091755151748657\n",
      "  time_total_s: 7412.532769918442\n",
      "  timers:\n",
      "    learn_throughput: 1448.973\n",
      "    learn_time_ms: 690.144\n",
      "    load_throughput: 39176.219\n",
      "    load_time_ms: 25.526\n",
      "    sample_throughput: 42.808\n",
      "    sample_time_ms: 23360.315\n",
      "    update_time_ms: 2.478\n",
      "  timestamp: 1635070283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 312\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         7412.53</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> -2.8722</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            287.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 313000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-11-49\n",
      "  done: false\n",
      "  episode_len_mean: 285.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8535999999999833\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1004\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023757264018058784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7980451689826118\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0034872184334000323\n",
      "          policy_loss: 0.02794928550720215\n",
      "          total_loss: 0.030624894756409858\n",
      "          vf_explained_var: 0.5855082869529724\n",
      "          vf_loss: 0.010647774332513411\n",
      "    num_agent_steps_sampled: 313000\n",
      "    num_agent_steps_trained: 313000\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.32162162162162\n",
      "    ram_util_percent: 38.87567567567567\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881020809079553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.950652736874286\n",
      "    mean_inference_ms: 1.9298414819491065\n",
      "    mean_raw_obs_processing_ms: 2.0053614539932902\n",
      "  time_since_restore: 7438.509054899216\n",
      "  time_this_iter_s: 25.976284980773926\n",
      "  time_total_s: 7438.509054899216\n",
      "  timers:\n",
      "    learn_throughput: 1445.426\n",
      "    learn_time_ms: 691.838\n",
      "    load_throughput: 39682.6\n",
      "    load_time_ms: 25.2\n",
      "    sample_throughput: 42.123\n",
      "    sample_time_ms: 23739.923\n",
      "    update_time_ms: 2.47\n",
      "  timestamp: 1635070309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         7438.51</td><td style=\"text-align: right;\">313000</td><td style=\"text-align: right;\"> -2.8536</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            285.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 314000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-12-14\n",
      "  done: false\n",
      "  episode_len_mean: 284.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8434999999999833\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1008\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011878632009029392\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9102366752094693\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004773960941620656\n",
      "          policy_loss: 0.017784406493107478\n",
      "          total_loss: 0.01826551407575607\n",
      "          vf_explained_var: 0.672303318977356\n",
      "          vf_loss: 0.009577803117119603\n",
      "    num_agent_steps_sampled: 314000\n",
      "    num_agent_steps_trained: 314000\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.958333333333336\n",
      "    ram_util_percent: 38.89166666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880891361448148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.957367001830743\n",
      "    mean_inference_ms: 1.9298192146147823\n",
      "    mean_raw_obs_processing_ms: 2.0053485505927857\n",
      "  time_since_restore: 7463.837227344513\n",
      "  time_this_iter_s: 25.32817244529724\n",
      "  time_total_s: 7463.837227344513\n",
      "  timers:\n",
      "    learn_throughput: 1444.274\n",
      "    learn_time_ms: 692.389\n",
      "    load_throughput: 40009.654\n",
      "    load_time_ms: 24.994\n",
      "    sample_throughput: 41.129\n",
      "    sample_time_ms: 24313.651\n",
      "    update_time_ms: 2.465\n",
      "  timestamp: 1635070334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 314\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         7463.84</td><td style=\"text-align: right;\">314000</td><td style=\"text-align: right;\"> -2.8435</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            284.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-12-40\n",
      "  done: false\n",
      "  episode_len_mean: 282.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.828199999999984\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1012\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005939316004514696\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8936722808414035\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006846976571173257\n",
      "          policy_loss: 0.002359430988629659\n",
      "          total_loss: 0.002643378617035018\n",
      "          vf_explained_var: 0.6691498756408691\n",
      "          vf_loss: 0.009216600252936283\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.22162162162162\n",
      "    ram_util_percent: 38.881081081081085\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038807607967401576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.96407026971569\n",
      "    mean_inference_ms: 1.9297955799810296\n",
      "    mean_raw_obs_processing_ms: 2.005421699846312\n",
      "  time_since_restore: 7489.698458909988\n",
      "  time_this_iter_s: 25.861231565475464\n",
      "  time_total_s: 7489.698458909988\n",
      "  timers:\n",
      "    learn_throughput: 1442.824\n",
      "    learn_time_ms: 693.085\n",
      "    load_throughput: 40092.108\n",
      "    load_time_ms: 24.943\n",
      "    sample_throughput: 40.049\n",
      "    sample_time_ms: 24969.254\n",
      "    update_time_ms: 2.448\n",
      "  timestamp: 1635070360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 315\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">          7489.7</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\"> -2.8282</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            282.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 283.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.835699999999983\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1015\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005939316004514696\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.336514304081599\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011391683562851461\n",
      "          policy_loss: 0.04625353566888306\n",
      "          total_loss: 0.04067725980033477\n",
      "          vf_explained_var: 0.5781882405281067\n",
      "          vf_loss: 0.007782102937603163\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.38484848484848\n",
      "    ram_util_percent: 38.88787878787878\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038806653830849275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.968866307369066\n",
      "    mean_inference_ms: 1.929777120706095\n",
      "    mean_raw_obs_processing_ms: 2.0054766185238373\n",
      "  time_since_restore: 7512.6157739162445\n",
      "  time_this_iter_s: 22.917315006256104\n",
      "  time_total_s: 7512.6157739162445\n",
      "  timers:\n",
      "    learn_throughput: 1441.309\n",
      "    learn_time_ms: 693.814\n",
      "    load_throughput: 40555.828\n",
      "    load_time_ms: 24.657\n",
      "    sample_throughput: 39.598\n",
      "    sample_time_ms: 25254.072\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1635070383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         7512.62</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\"> -2.8357</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            283.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 317000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-13-27\n",
      "  done: false\n",
      "  episode_len_mean: 283.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8364999999999823\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1018\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005939316004514696\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9477247926923964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011271630893229556\n",
      "          policy_loss: -0.12601536959409715\n",
      "          total_loss: -0.12104318680034744\n",
      "          vf_explained_var: 0.47870945930480957\n",
      "          vf_loss: 0.014442740711900923\n",
      "    num_agent_steps_sampled: 317000\n",
      "    num_agent_steps_trained: 317000\n",
      "    num_steps_sampled: 317000\n",
      "    num_steps_trained: 317000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.088235294117645\n",
      "    ram_util_percent: 38.8470588235294\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038805714634587804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.97357210494832\n",
      "    mean_inference_ms: 1.9297580220203756\n",
      "    mean_raw_obs_processing_ms: 2.0055442231518996\n",
      "  time_since_restore: 7536.682723045349\n",
      "  time_this_iter_s: 24.066949129104614\n",
      "  time_total_s: 7536.682723045349\n",
      "  timers:\n",
      "    learn_throughput: 1441.539\n",
      "    learn_time_ms: 693.703\n",
      "    load_throughput: 40812.175\n",
      "    load_time_ms: 24.502\n",
      "    sample_throughput: 39.132\n",
      "    sample_time_ms: 25554.362\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1635070407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 317000\n",
      "  training_iteration: 317\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         7536.68</td><td style=\"text-align: right;\">317000</td><td style=\"text-align: right;\"> -2.8365</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            283.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 318000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-14-11\n",
      "  done: false\n",
      "  episode_len_mean: 282.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8263999999999836\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1022\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005939316004514696\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9154674132664998\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015882840081210107\n",
      "          policy_loss: -0.11879751988583141\n",
      "          total_loss: -0.1128132849931717\n",
      "          vf_explained_var: 0.5622662305831909\n",
      "          vf_loss: 0.01512947724097305\n",
      "    num_agent_steps_sampled: 318000\n",
      "    num_agent_steps_trained: 318000\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.05161290322581\n",
      "    ram_util_percent: 38.75483870967741\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038804515297994197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.980150683262423\n",
      "    mean_inference_ms: 1.9297330472871215\n",
      "    mean_raw_obs_processing_ms: 2.0077990795221923\n",
      "  time_since_restore: 7580.218553781509\n",
      "  time_this_iter_s: 43.53583073616028\n",
      "  time_total_s: 7580.218553781509\n",
      "  timers:\n",
      "    learn_throughput: 1440.52\n",
      "    learn_time_ms: 694.194\n",
      "    load_throughput: 41054.578\n",
      "    load_time_ms: 24.358\n",
      "    sample_throughput: 36.098\n",
      "    sample_time_ms: 27702.115\n",
      "    update_time_ms: 2.457\n",
      "  timestamp: 1635070451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 318\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         7580.22</td><td style=\"text-align: right;\">318000</td><td style=\"text-align: right;\"> -2.8264</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            282.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 319000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-14-35\n",
      "  done: false\n",
      "  episode_len_mean: 282.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8206999999999836\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1026\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005939316004514696\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2803728342056275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028755125612868446\n",
      "          policy_loss: 0.018847517917553583\n",
      "          total_loss: 0.013782194587919447\n",
      "          vf_explained_var: 0.725818395614624\n",
      "          vf_loss: 0.007721324792752663\n",
      "    num_agent_steps_sampled: 319000\n",
      "    num_agent_steps_trained: 319000\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.26470588235294\n",
      "    ram_util_percent: 38.741176470588236\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880336937421423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.986633150478568\n",
      "    mean_inference_ms: 1.9297098909896613\n",
      "    mean_raw_obs_processing_ms: 2.010127016099591\n",
      "  time_since_restore: 7604.158270597458\n",
      "  time_this_iter_s: 23.939716815948486\n",
      "  time_total_s: 7604.158270597458\n",
      "  timers:\n",
      "    learn_throughput: 1438.82\n",
      "    learn_time_ms: 695.014\n",
      "    load_throughput: 41554.835\n",
      "    load_time_ms: 24.065\n",
      "    sample_throughput: 36.102\n",
      "    sample_time_ms: 27699.248\n",
      "    update_time_ms: 2.47\n",
      "  timestamp: 1635070475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         7604.16</td><td style=\"text-align: right;\">319000</td><td style=\"text-align: right;\"> -2.8207</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            282.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 282.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.825099999999984\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1030\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0053914719157748\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009451595267986768\n",
      "          policy_loss: 0.006758826474348704\n",
      "          total_loss: 0.00364161878824234\n",
      "          vf_explained_var: 0.8498336672782898\n",
      "          vf_loss: 0.006928287146406041\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24571428571429\n",
      "    ram_util_percent: 38.82\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880227407966677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.992936656259154\n",
      "    mean_inference_ms: 1.9296877043028229\n",
      "    mean_raw_obs_processing_ms: 2.0107678188072704\n",
      "  time_since_restore: 7628.780961275101\n",
      "  time_this_iter_s: 24.622690677642822\n",
      "  time_total_s: 7628.780961275101\n",
      "  timers:\n",
      "    learn_throughput: 1436.402\n",
      "    learn_time_ms: 696.184\n",
      "    load_throughput: 41534.465\n",
      "    load_time_ms: 24.076\n",
      "    sample_throughput: 38.592\n",
      "    sample_time_ms: 25912.056\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1635070500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 320\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         7628.78</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> -2.8251</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            282.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 321000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-15-23\n",
      "  done: false\n",
      "  episode_len_mean: 283.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8343999999999823\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1033\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9009231236245897\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005882508084124143\n",
      "          policy_loss: 0.08313528117206362\n",
      "          total_loss: 0.08087713826033804\n",
      "          vf_explained_var: 0.8014566898345947\n",
      "          vf_loss: 0.006745848070649017\n",
      "    num_agent_steps_sampled: 321000\n",
      "    num_agent_steps_trained: 321000\n",
      "    num_steps_sampled: 321000\n",
      "    num_steps_trained: 321000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.93529411764706\n",
      "    ram_util_percent: 38.891176470588235\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880145861349521\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.997432666352466\n",
      "    mean_inference_ms: 1.929671395395306\n",
      "    mean_raw_obs_processing_ms: 2.010819774609497\n",
      "  time_since_restore: 7652.550095081329\n",
      "  time_this_iter_s: 23.769133806228638\n",
      "  time_total_s: 7652.550095081329\n",
      "  timers:\n",
      "    learn_throughput: 1439.198\n",
      "    learn_time_ms: 694.831\n",
      "    load_throughput: 41346.004\n",
      "    load_time_ms: 24.186\n",
      "    sample_throughput: 38.781\n",
      "    sample_time_ms: 25786.055\n",
      "    update_time_ms: 2.421\n",
      "  timestamp: 1635070523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 321000\n",
      "  training_iteration: 321\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         7652.55</td><td style=\"text-align: right;\">321000</td><td style=\"text-align: right;\"> -2.8344</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            283.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 322000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-15-48\n",
      "  done: false\n",
      "  episode_len_mean: 283.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.838299999999982\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1037\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7910535898473527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007911890646914483\n",
      "          policy_loss: 0.05659096240997315\n",
      "          total_loss: 0.05613264267643293\n",
      "          vf_explained_var: 0.7560557723045349\n",
      "          vf_loss: 0.007445166585966945\n",
      "    num_agent_steps_sampled: 322000\n",
      "    num_agent_steps_trained: 322000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.775\n",
      "    ram_util_percent: 38.91666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038800367136835344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.003318070837636\n",
      "    mean_inference_ms: 1.9296489563857546\n",
      "    mean_raw_obs_processing_ms: 2.0108718548818025\n",
      "  time_since_restore: 7677.298415899277\n",
      "  time_this_iter_s: 24.748320817947388\n",
      "  time_total_s: 7677.298415899277\n",
      "  timers:\n",
      "    learn_throughput: 1437.894\n",
      "    learn_time_ms: 695.462\n",
      "    load_throughput: 41797.334\n",
      "    load_time_ms: 23.925\n",
      "    sample_throughput: 38.833\n",
      "    sample_time_ms: 25751.358\n",
      "    update_time_ms: 2.398\n",
      "  timestamp: 1635070548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">          7677.3</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\"> -2.8383</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            283.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 323000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 284.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.842799999999983\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1040\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9224457442760468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010128601857293676\n",
      "          policy_loss: -0.10282689481973648\n",
      "          total_loss: -0.09912344184186724\n",
      "          vf_explained_var: 0.5054841637611389\n",
      "          vf_loss: 0.012918888849930631\n",
      "    num_agent_steps_sampled: 323000\n",
      "    num_agent_steps_trained: 323000\n",
      "    num_steps_sampled: 323000\n",
      "    num_steps_trained: 323000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.01470588235294\n",
      "    ram_util_percent: 38.91176470588236\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038799551585561226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.007533095363154\n",
      "    mean_inference_ms: 1.9296309554842668\n",
      "    mean_raw_obs_processing_ms: 2.0109558352944363\n",
      "  time_since_restore: 7701.220352888107\n",
      "  time_this_iter_s: 23.921936988830566\n",
      "  time_total_s: 7701.220352888107\n",
      "  timers:\n",
      "    learn_throughput: 1440.916\n",
      "    learn_time_ms: 694.003\n",
      "    load_throughput: 41708.306\n",
      "    load_time_ms: 23.976\n",
      "    sample_throughput: 39.143\n",
      "    sample_time_ms: 25547.321\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1635070572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323000\n",
      "  training_iteration: 323\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         7701.22</td><td style=\"text-align: right;\">323000</td><td style=\"text-align: right;\"> -2.8428</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            284.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 284.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.845599999999983\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1044\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7730143825213115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0077112477663806404\n",
      "          policy_loss: 0.03733788463804457\n",
      "          total_loss: 0.041530891507864\n",
      "          vf_explained_var: 0.36676928400993347\n",
      "          vf_loss: 0.011916280703412161\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.29411764705883\n",
      "    ram_util_percent: 38.88529411764706\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879844280421923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01324835485269\n",
      "    mean_inference_ms: 1.9296064193996516\n",
      "    mean_raw_obs_processing_ms: 2.0110487255886387\n",
      "  time_since_restore: 7725.122390031815\n",
      "  time_this_iter_s: 23.902037143707275\n",
      "  time_total_s: 7725.122390031815\n",
      "  timers:\n",
      "    learn_throughput: 1442.943\n",
      "    learn_time_ms: 693.028\n",
      "    load_throughput: 41732.458\n",
      "    load_time_ms: 23.962\n",
      "    sample_throughput: 39.361\n",
      "    sample_time_ms: 25405.711\n",
      "    update_time_ms: 2.393\n",
      "  timestamp: 1635070596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 324\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         7725.12</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\"> -2.8456</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            284.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 325000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-17-01\n",
      "  done: false\n",
      "  episode_len_mean: 284.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.841699999999983\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1048\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.685366051726871\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012902184183521598\n",
      "          policy_loss: 0.018216562188333934\n",
      "          total_loss: 0.023747729096147748\n",
      "          vf_explained_var: 0.3294861912727356\n",
      "          vf_loss: 0.012373332461963097\n",
      "    num_agent_steps_sampled: 325000\n",
      "    num_agent_steps_trained: 325000\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.95833333333334\n",
      "    ram_util_percent: 38.88611111111111\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879735260841361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01908547095923\n",
      "    mean_inference_ms: 1.9295817718941362\n",
      "    mean_raw_obs_processing_ms: 2.01116513481544\n",
      "  time_since_restore: 7750.430418014526\n",
      "  time_this_iter_s: 25.308027982711792\n",
      "  time_total_s: 7750.430418014526\n",
      "  timers:\n",
      "    learn_throughput: 1443.663\n",
      "    learn_time_ms: 692.682\n",
      "    load_throughput: 41579.84\n",
      "    load_time_ms: 24.05\n",
      "    sample_throughput: 39.447\n",
      "    sample_time_ms: 25350.627\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1635070621\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         7750.43</td><td style=\"text-align: right;\">325000</td><td style=\"text-align: right;\"> -2.8417</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            284.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 326000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-17-43\n",
      "  done: false\n",
      "  episode_len_mean: 283.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8316999999999832\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1052\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.747075006696913\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011107101230263242\n",
      "          policy_loss: 0.02350622796350055\n",
      "          total_loss: 0.027559363345305125\n",
      "          vf_explained_var: 0.2744276821613312\n",
      "          vf_loss: 0.011513989791274071\n",
      "    num_agent_steps_sampled: 326000\n",
      "    num_agent_steps_trained: 326000\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.89500000000001\n",
      "    ram_util_percent: 38.79833333333332\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038796257268810114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.025006741812675\n",
      "    mean_inference_ms: 1.9295582832893134\n",
      "    mean_raw_obs_processing_ms: 2.0134735816653158\n",
      "  time_since_restore: 7792.528922796249\n",
      "  time_this_iter_s: 42.09850478172302\n",
      "  time_total_s: 7792.528922796249\n",
      "  timers:\n",
      "    learn_throughput: 1446.077\n",
      "    learn_time_ms: 691.526\n",
      "    load_throughput: 41633.66\n",
      "    load_time_ms: 24.019\n",
      "    sample_throughput: 36.67\n",
      "    sample_time_ms: 27269.931\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1635070663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 326\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         7792.53</td><td style=\"text-align: right;\">326000</td><td style=\"text-align: right;\"> -2.8317</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            283.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 327000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-18-09\n",
      "  done: false\n",
      "  episode_len_mean: 282.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.824099999999983\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1056\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7060695654816098\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010550471810026826\n",
      "          policy_loss: -0.015607431448168224\n",
      "          total_loss: -0.011261071181959577\n",
      "          vf_explained_var: 0.4016490876674652\n",
      "          vf_loss: 0.011397656229221158\n",
      "    num_agent_steps_sampled: 327000\n",
      "    num_agent_steps_trained: 327000\n",
      "    num_steps_sampled: 327000\n",
      "    num_steps_trained: 327000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.10270270270271\n",
      "    ram_util_percent: 38.72162162162163\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879515954689842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.03104174623103\n",
      "    mean_inference_ms: 1.929535448758036\n",
      "    mean_raw_obs_processing_ms: 2.0157945808720075\n",
      "  time_since_restore: 7818.141418933868\n",
      "  time_this_iter_s: 25.61249613761902\n",
      "  time_total_s: 7818.141418933868\n",
      "  timers:\n",
      "    learn_throughput: 1443.48\n",
      "    learn_time_ms: 692.77\n",
      "    load_throughput: 41512.062\n",
      "    load_time_ms: 24.089\n",
      "    sample_throughput: 36.466\n",
      "    sample_time_ms: 27423.151\n",
      "    update_time_ms: 2.415\n",
      "  timestamp: 1635070689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327000\n",
      "  training_iteration: 327\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         7818.14</td><td style=\"text-align: right;\">327000</td><td style=\"text-align: right;\"> -2.8241</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            282.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-18-33\n",
      "  done: false\n",
      "  episode_len_mean: 283.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.834799999999983\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1059\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9310413281122843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008510235333716152\n",
      "          policy_loss: 0.051346834831767614\n",
      "          total_loss: 0.05157992541790009\n",
      "          vf_explained_var: 0.3629157543182373\n",
      "          vf_loss: 0.009535920594094529\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.900000000000006\n",
      "    ram_util_percent: 38.838235294117645\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879435066815596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.035406734025365\n",
      "    mean_inference_ms: 1.9295185054371626\n",
      "    mean_raw_obs_processing_ms: 2.0163995451706485\n",
      "  time_since_restore: 7841.929790496826\n",
      "  time_this_iter_s: 23.788371562957764\n",
      "  time_total_s: 7841.929790496826\n",
      "  timers:\n",
      "    learn_throughput: 1442.739\n",
      "    learn_time_ms: 693.126\n",
      "    load_throughput: 41379.289\n",
      "    load_time_ms: 24.167\n",
      "    sample_throughput: 39.296\n",
      "    sample_time_ms: 25447.867\n",
      "    update_time_ms: 2.51\n",
      "  timestamp: 1635070713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         7841.93</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\"> -2.8348</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            283.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 329000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-18-56\n",
      "  done: false\n",
      "  episode_len_mean: 282.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8231999999999835\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1063\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008908974006772042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1065880715847016\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.038061410249862995\n",
      "          policy_loss: 0.009464953260289299\n",
      "          total_loss: 0.011273640725347731\n",
      "          vf_explained_var: 0.3875941038131714\n",
      "          vf_loss: 0.01284065731904573\n",
      "    num_agent_steps_sampled: 329000\n",
      "    num_agent_steps_trained: 329000\n",
      "    num_steps_sampled: 329000\n",
      "    num_steps_trained: 329000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.9969696969697\n",
      "    ram_util_percent: 38.9030303030303\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879331446521437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.041260380777814\n",
      "    mean_inference_ms: 1.9294967883169425\n",
      "    mean_raw_obs_processing_ms: 2.016491657626343\n",
      "  time_since_restore: 7865.241118431091\n",
      "  time_this_iter_s: 23.311327934265137\n",
      "  time_total_s: 7865.241118431091\n",
      "  timers:\n",
      "    learn_throughput: 1445.318\n",
      "    learn_time_ms: 691.889\n",
      "    load_throughput: 41007.455\n",
      "    load_time_ms: 24.386\n",
      "    sample_throughput: 39.392\n",
      "    sample_time_ms: 25386.087\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1635070736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329000\n",
      "  training_iteration: 329\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         7865.24</td><td style=\"text-align: right;\">329000</td><td style=\"text-align: right;\"> -2.8232</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            282.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-19-19\n",
      "  done: false\n",
      "  episode_len_mean: 282.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8212999999999835\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1066\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.143167488442527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0077366285154403\n",
      "          policy_loss: 0.01880914626850022\n",
      "          total_loss: 0.018457599315378402\n",
      "          vf_explained_var: 0.3762354254722595\n",
      "          vf_loss: 0.011069790993092788\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.959374999999994\n",
      "    ram_util_percent: 38.925000000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879249193358176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.04577949852152\n",
      "    mean_inference_ms: 1.9294802017654056\n",
      "    mean_raw_obs_processing_ms: 2.0165609832215985\n",
      "  time_since_restore: 7887.795031070709\n",
      "  time_this_iter_s: 22.55391263961792\n",
      "  time_total_s: 7887.795031070709\n",
      "  timers:\n",
      "    learn_throughput: 1449.262\n",
      "    learn_time_ms: 690.007\n",
      "    load_throughput: 41087.717\n",
      "    load_time_ms: 24.338\n",
      "    sample_throughput: 39.712\n",
      "    sample_time_ms: 25181.152\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1635070759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 330\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">          7887.8</td><td style=\"text-align: right;\">330000</td><td style=\"text-align: right;\"> -2.8213</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            282.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 331000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-19-40\n",
      "  done: false\n",
      "  episode_len_mean: 282.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8263999999999836\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1069\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4395073983404372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01673873498944434\n",
      "          policy_loss: 0.028619059258037143\n",
      "          total_loss: 0.02597505831056171\n",
      "          vf_explained_var: 0.2025834172964096\n",
      "          vf_loss: 0.011728703736379329\n",
      "    num_agent_steps_sampled: 331000\n",
      "    num_agent_steps_trained: 331000\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.769999999999996\n",
      "    ram_util_percent: 38.933333333333344\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038791691049328995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.05021900530404\n",
      "    mean_inference_ms: 1.9294636352467505\n",
      "    mean_raw_obs_processing_ms: 2.0166463158959362\n",
      "  time_since_restore: 7908.76705121994\n",
      "  time_this_iter_s: 20.972020149230957\n",
      "  time_total_s: 7908.76705121994\n",
      "  timers:\n",
      "    learn_throughput: 1449.703\n",
      "    learn_time_ms: 689.797\n",
      "    load_throughput: 41278.416\n",
      "    load_time_ms: 24.226\n",
      "    sample_throughput: 40.158\n",
      "    sample_time_ms: 24901.753\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1635070780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         7908.77</td><td style=\"text-align: right;\">331000</td><td style=\"text-align: right;\"> -2.8264</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            282.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-19-59\n",
      "  done: false\n",
      "  episode_len_mean: 283.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8307999999999827\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1072\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3361236559020149\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009240266902977782\n",
      "          policy_loss: 0.010999402734968397\n",
      "          total_loss: 0.008508448468314276\n",
      "          vf_explained_var: -0.18830062448978424\n",
      "          vf_loss: 0.010857936051777668\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.54642857142858\n",
      "    ram_util_percent: 38.910714285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879090591100056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.054570115678807\n",
      "    mean_inference_ms: 1.9294469913418326\n",
      "    mean_raw_obs_processing_ms: 2.0167460279993015\n",
      "  time_since_restore: 7928.236746311188\n",
      "  time_this_iter_s: 19.46969509124756\n",
      "  time_total_s: 7928.236746311188\n",
      "  timers:\n",
      "    learn_throughput: 1453.489\n",
      "    learn_time_ms: 688.0\n",
      "    load_throughput: 40833.472\n",
      "    load_time_ms: 24.49\n",
      "    sample_throughput: 41.025\n",
      "    sample_time_ms: 24375.352\n",
      "    update_time_ms: 2.535\n",
      "  timestamp: 1635070799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 332\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         7928.24</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> -2.8308</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            283.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 333000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-20-20\n",
      "  done: false\n",
      "  episode_len_mean: 282.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8275999999999835\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1075\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3225482940673827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00868103246651199\n",
      "          policy_loss: 0.04905705708596442\n",
      "          total_loss: 0.046400895218054454\n",
      "          vf_explained_var: 0.12350820749998093\n",
      "          vf_loss: 0.010557722584861848\n",
      "    num_agent_steps_sampled: 333000\n",
      "    num_agent_steps_trained: 333000\n",
      "    num_steps_sampled: 333000\n",
      "    num_steps_trained: 333000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.95172413793103\n",
      "    ram_util_percent: 38.93103448275862\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879014802310756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.059036886078047\n",
      "    mean_inference_ms: 1.9294306031271922\n",
      "    mean_raw_obs_processing_ms: 2.0168598827765183\n",
      "  time_since_restore: 7948.816899299622\n",
      "  time_this_iter_s: 20.580152988433838\n",
      "  time_total_s: 7948.816899299622\n",
      "  timers:\n",
      "    learn_throughput: 1453.238\n",
      "    learn_time_ms: 688.118\n",
      "    load_throughput: 40670.54\n",
      "    load_time_ms: 24.588\n",
      "    sample_throughput: 41.596\n",
      "    sample_time_ms: 24040.959\n",
      "    update_time_ms: 2.538\n",
      "  timestamp: 1635070820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333000\n",
      "  training_iteration: 333\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         7948.82</td><td style=\"text-align: right;\">333000</td><td style=\"text-align: right;\"> -2.8276</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            282.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 334000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-20-40\n",
      "  done: false\n",
      "  episode_len_mean: 282.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8231999999999844\n",
      "  episode_reward_min: -3.8299999999999623\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1078\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2591666062672933\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008428384464431421\n",
      "          policy_loss: 0.04832050783766641\n",
      "          total_loss: 0.04704178886281119\n",
      "          vf_explained_var: 0.1041119247674942\n",
      "          vf_loss: 0.011301684126050936\n",
      "    num_agent_steps_sampled: 334000\n",
      "    num_agent_steps_trained: 334000\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.906896551724145\n",
      "    ram_util_percent: 38.89999999999999\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038789400643401584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.063503345332073\n",
      "    mean_inference_ms: 1.9294142861033121\n",
      "    mean_raw_obs_processing_ms: 2.0169900176272133\n",
      "  time_since_restore: 7968.60840177536\n",
      "  time_this_iter_s: 19.791502475738525\n",
      "  time_total_s: 7968.60840177536\n",
      "  timers:\n",
      "    learn_throughput: 1449.812\n",
      "    learn_time_ms: 689.745\n",
      "    load_throughput: 40660.92\n",
      "    load_time_ms: 24.594\n",
      "    sample_throughput: 42.322\n",
      "    sample_time_ms: 23628.276\n",
      "    update_time_ms: 2.538\n",
      "  timestamp: 1635070840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         7968.61</td><td style=\"text-align: right;\">334000</td><td style=\"text-align: right;\"> -2.8232</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.83</td><td style=\"text-align: right;\">            282.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 335000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-21-18\n",
      "  done: false\n",
      "  episode_len_mean: 282.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.821399999999984\n",
      "  episode_reward_min: -3.8299999999999623\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1081\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2592759529749553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00973834957024665\n",
      "          policy_loss: 0.04615991794400745\n",
      "          total_loss: 0.04414347501264678\n",
      "          vf_explained_var: 0.06161157786846161\n",
      "          vf_loss: 0.010563300565182645\n",
      "    num_agent_steps_sampled: 335000\n",
      "    num_agent_steps_trained: 335000\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.158181818181816\n",
      "    ram_util_percent: 38.74727272727272\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878868727699178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0681094971992\n",
      "    mean_inference_ms: 1.9293995800147996\n",
      "    mean_raw_obs_processing_ms: 2.0186821273965263\n",
      "  time_since_restore: 8007.299524307251\n",
      "  time_this_iter_s: 38.69112253189087\n",
      "  time_total_s: 8007.299524307251\n",
      "  timers:\n",
      "    learn_throughput: 1447.988\n",
      "    learn_time_ms: 690.613\n",
      "    load_throughput: 40517.747\n",
      "    load_time_ms: 24.681\n",
      "    sample_throughput: 40.055\n",
      "    sample_time_ms: 24965.65\n",
      "    update_time_ms: 2.521\n",
      "  timestamp: 1635070878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 335\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">          8007.3</td><td style=\"text-align: right;\">335000</td><td style=\"text-align: right;\"> -2.8214</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -3.83</td><td style=\"text-align: right;\">            282.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-21-36\n",
      "  done: false\n",
      "  episode_len_mean: 283.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.834599999999983\n",
      "  episode_reward_min: -4.019999999999959\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1083\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4362934072812399\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016361776684405912\n",
      "          policy_loss: -0.05238017299109035\n",
      "          total_loss: -0.06037904396653175\n",
      "          vf_explained_var: 0.22034505009651184\n",
      "          vf_loss: 0.006342196007446748\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.224000000000004\n",
      "    ram_util_percent: 38.760000000000005\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878819265542102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.070927781377637\n",
      "    mean_inference_ms: 1.9293892912613528\n",
      "    mean_raw_obs_processing_ms: 2.019756653157896\n",
      "  time_since_restore: 8025.164279222488\n",
      "  time_this_iter_s: 17.864754915237427\n",
      "  time_total_s: 8025.164279222488\n",
      "  timers:\n",
      "    learn_throughput: 1445.37\n",
      "    learn_time_ms: 691.864\n",
      "    load_throughput: 40340.901\n",
      "    load_time_ms: 24.789\n",
      "    sample_throughput: 44.364\n",
      "    sample_time_ms: 22540.93\n",
      "    update_time_ms: 2.514\n",
      "  timestamp: 1635070896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 336\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         8025.16</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> -2.8346</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -4.02</td><td style=\"text-align: right;\">            283.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 337000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-21-55\n",
      "  done: false\n",
      "  episode_len_mean: 286.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8616999999999835\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1086\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.357860869831509\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073992709128898725\n",
      "          policy_loss: -0.09094295253356298\n",
      "          total_loss: -0.08625168022182253\n",
      "          vf_explained_var: 0.011041659861803055\n",
      "          vf_loss: 0.018259994292424784\n",
      "    num_agent_steps_sampled: 337000\n",
      "    num_agent_steps_trained: 337000\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.528571428571425\n",
      "    ram_util_percent: 38.810714285714276\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878746220841858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.074819651915004\n",
      "    mean_inference_ms: 1.9293734431209975\n",
      "    mean_raw_obs_processing_ms: 2.0213496669719477\n",
      "  time_since_restore: 8044.273930311203\n",
      "  time_this_iter_s: 19.1096510887146\n",
      "  time_total_s: 8044.273930311203\n",
      "  timers:\n",
      "    learn_throughput: 1445.465\n",
      "    learn_time_ms: 691.819\n",
      "    load_throughput: 40397.473\n",
      "    load_time_ms: 24.754\n",
      "    sample_throughput: 45.681\n",
      "    sample_time_ms: 21890.701\n",
      "    update_time_ms: 2.533\n",
      "  timestamp: 1635070915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         8044.27</td><td style=\"text-align: right;\">337000</td><td style=\"text-align: right;\"> -2.8617</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            286.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 338000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-22-14\n",
      "  done: false\n",
      "  episode_len_mean: 287.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.8769999999999825\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1089\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1569264882140688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010576679596852854\n",
      "          policy_loss: -0.02686149858766132\n",
      "          total_loss: -0.02514772050910526\n",
      "          vf_explained_var: -0.010645383968949318\n",
      "          vf_loss: 0.013268912454239196\n",
      "    num_agent_steps_sampled: 338000\n",
      "    num_agent_steps_trained: 338000\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.74814814814815\n",
      "    ram_util_percent: 38.86666666666666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038786663239586454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.078202217968972\n",
      "    mean_inference_ms: 1.9293571701787458\n",
      "    mean_raw_obs_processing_ms: 2.022405831326436\n",
      "  time_since_restore: 8063.376201868057\n",
      "  time_this_iter_s: 19.102271556854248\n",
      "  time_total_s: 8063.376201868057\n",
      "  timers:\n",
      "    learn_throughput: 1446.536\n",
      "    learn_time_ms: 691.307\n",
      "    load_throughput: 40427.65\n",
      "    load_time_ms: 24.736\n",
      "    sample_throughput: 46.679\n",
      "    sample_time_ms: 21422.735\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1635070934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 338\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         8063.38</td><td style=\"text-align: right;\">338000</td><td style=\"text-align: right;\">  -2.877</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">             287.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 339000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-22-34\n",
      "  done: false\n",
      "  episode_len_mean: 290.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.9082999999999815\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1092\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.309127974510193\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01254172224081979\n",
      "          policy_loss: 0.047811378869745465\n",
      "          total_loss: 0.047217609898911585\n",
      "          vf_explained_var: 0.0019292877987027168\n",
      "          vf_loss: 0.012480754586350586\n",
      "    num_agent_steps_sampled: 339000\n",
      "    num_agent_steps_trained: 339000\n",
      "    num_steps_sampled: 339000\n",
      "    num_steps_trained: 339000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11481481481482\n",
      "    ram_util_percent: 38.92962962962962\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038785777095629546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.081096005591192\n",
      "    mean_inference_ms: 1.929339374129314\n",
      "    mean_raw_obs_processing_ms: 2.0223216954535994\n",
      "  time_since_restore: 8082.515048265457\n",
      "  time_this_iter_s: 19.138846397399902\n",
      "  time_total_s: 8082.515048265457\n",
      "  timers:\n",
      "    learn_throughput: 1443.828\n",
      "    learn_time_ms: 692.603\n",
      "    load_throughput: 40637.283\n",
      "    load_time_ms: 24.608\n",
      "    sample_throughput: 47.609\n",
      "    sample_time_ms: 21004.255\n",
      "    update_time_ms: 2.482\n",
      "  timestamp: 1635070954\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339000\n",
      "  training_iteration: 339\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         8082.52</td><td style=\"text-align: right;\">339000</td><td style=\"text-align: right;\"> -2.9083</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            290.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 294.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.9399999999999813\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1095\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4797249952952067\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006930501107167765\n",
      "          policy_loss: 0.042119664864407644\n",
      "          total_loss: 0.03931037916077508\n",
      "          vf_explained_var: -0.17955128848552704\n",
      "          vf_loss: 0.01197870429346545\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.34285714285714\n",
      "    ram_util_percent: 38.92857142857143\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878488044870761\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.083448876739357\n",
      "    mean_inference_ms: 1.9293217223051582\n",
      "    mean_raw_obs_processing_ms: 2.022248765169175\n",
      "  time_since_restore: 8101.756751537323\n",
      "  time_this_iter_s: 19.241703271865845\n",
      "  time_total_s: 8101.756751537323\n",
      "  timers:\n",
      "    learn_throughput: 1442.528\n",
      "    learn_time_ms: 693.227\n",
      "    load_throughput: 40696.901\n",
      "    load_time_ms: 24.572\n",
      "    sample_throughput: 48.374\n",
      "    sample_time_ms: 20672.45\n",
      "    update_time_ms: 2.486\n",
      "  timestamp: 1635070973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         8101.76</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">   -2.94</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">               294</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 341000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-23-14\n",
      "  done: false\n",
      "  episode_len_mean: 295.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.958499999999981\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1098\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4507539391517639\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009862798266082566\n",
      "          policy_loss: 0.034671066453059514\n",
      "          total_loss: 0.03238337304857042\n",
      "          vf_explained_var: -0.0749402716755867\n",
      "          vf_loss: 0.012206665218238615\n",
      "    num_agent_steps_sampled: 341000\n",
      "    num_agent_steps_trained: 341000\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.7\n",
      "    ram_util_percent: 38.91333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038783947669037686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.08554585699185\n",
      "    mean_inference_ms: 1.9293033084198021\n",
      "    mean_raw_obs_processing_ms: 2.022131618902145\n",
      "  time_since_restore: 8123.130032300949\n",
      "  time_this_iter_s: 21.3732807636261\n",
      "  time_total_s: 8123.130032300949\n",
      "  timers:\n",
      "    learn_throughput: 1441.777\n",
      "    learn_time_ms: 693.588\n",
      "    load_throughput: 40477.628\n",
      "    load_time_ms: 24.705\n",
      "    sample_throughput: 48.281\n",
      "    sample_time_ms: 20712.096\n",
      "    update_time_ms: 2.478\n",
      "  timestamp: 1635070994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 341\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         8123.13</td><td style=\"text-align: right;\">341000</td><td style=\"text-align: right;\"> -2.9585</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            295.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 342000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-23-33\n",
      "  done: false\n",
      "  episode_len_mean: 298.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.98009999999998\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1100\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5537692493862576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016916027133718595\n",
      "          policy_loss: -0.09651948461929957\n",
      "          total_loss: -0.10085399465428459\n",
      "          vf_explained_var: 0.06421569734811783\n",
      "          vf_loss: 0.011180575921510656\n",
      "    num_agent_steps_sampled: 342000\n",
      "    num_agent_steps_trained: 342000\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.396428571428565\n",
      "    ram_util_percent: 38.910714285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038783309586596065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.08670931990179\n",
      "    mean_inference_ms: 1.929290446969512\n",
      "    mean_raw_obs_processing_ms: 2.0219844959715743\n",
      "  time_since_restore: 8142.224947690964\n",
      "  time_this_iter_s: 19.09491539001465\n",
      "  time_total_s: 8142.224947690964\n",
      "  timers:\n",
      "    learn_throughput: 1440.24\n",
      "    learn_time_ms: 694.329\n",
      "    load_throughput: 41855.938\n",
      "    load_time_ms: 23.891\n",
      "    sample_throughput: 48.368\n",
      "    sample_time_ms: 20674.776\n",
      "    update_time_ms: 2.41\n",
      "  timestamp: 1635071013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 342\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         8142.22</td><td style=\"text-align: right;\">342000</td><td style=\"text-align: right;\"> -2.9801</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            298.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 343000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-23-54\n",
      "  done: false\n",
      "  episode_len_mean: 300.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.00869999999998\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1103\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013363461010158062\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5665979425112406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025348337537620443\n",
      "          policy_loss: -0.12525448732905917\n",
      "          total_loss: -0.12340998243954447\n",
      "          vf_explained_var: 0.23257768154144287\n",
      "          vf_loss: 0.017476608448972305\n",
      "    num_agent_steps_sampled: 343000\n",
      "    num_agent_steps_trained: 343000\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.85172413793103\n",
      "    ram_util_percent: 38.88620689655173\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878236109929812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.087939613802092\n",
      "    mean_inference_ms: 1.929270777182223\n",
      "    mean_raw_obs_processing_ms: 2.0217766412687466\n",
      "  time_since_restore: 8162.718815088272\n",
      "  time_this_iter_s: 20.49386739730835\n",
      "  time_total_s: 8162.718815088272\n",
      "  timers:\n",
      "    learn_throughput: 1437.974\n",
      "    learn_time_ms: 695.423\n",
      "    load_throughput: 41889.589\n",
      "    load_time_ms: 23.872\n",
      "    sample_throughput: 48.391\n",
      "    sample_time_ms: 20665.013\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1635071034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         8162.72</td><td style=\"text-align: right;\">343000</td><td style=\"text-align: right;\"> -3.0087</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            300.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-24-15\n",
      "  done: false\n",
      "  episode_len_mean: 302.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.0277999999999796\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1106\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00200451915152371\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3805603656503889\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06091581403259857\n",
      "          policy_loss: -0.11598247182038095\n",
      "          total_loss: -0.11401829967896143\n",
      "          vf_explained_var: 0.34699127078056335\n",
      "          vf_loss: 0.015647668795039257\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.71\n",
      "    ram_util_percent: 38.86333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878143658618725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.08889068960265\n",
      "    mean_inference_ms: 1.9292509004994738\n",
      "    mean_raw_obs_processing_ms: 2.021524884001603\n",
      "  time_since_restore: 8183.950345277786\n",
      "  time_this_iter_s: 21.23153018951416\n",
      "  time_total_s: 8183.950345277786\n",
      "  timers:\n",
      "    learn_throughput: 1439.058\n",
      "    learn_time_ms: 694.899\n",
      "    load_throughput: 41912.695\n",
      "    load_time_ms: 23.859\n",
      "    sample_throughput: 48.055\n",
      "    sample_time_ms: 20809.552\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1635071055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 344\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         8183.95</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> -3.0278</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            302.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-24-54\n",
      "  done: false\n",
      "  episode_len_mean: 304.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.0445999999999795\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1110\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0030067787272855633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4720883144272698\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006811514481350874\n",
      "          policy_loss: 0.016649006803830465\n",
      "          total_loss: 0.017328734281990263\n",
      "          vf_explained_var: 0.2957550585269928\n",
      "          vf_loss: 0.01538012744858861\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.425000000000004\n",
      "    ram_util_percent: 38.81428571428571\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387802091371535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.089823586454145\n",
      "    mean_inference_ms: 1.929225012010248\n",
      "    mean_raw_obs_processing_ms: 2.0230958860107164\n",
      "  time_since_restore: 8223.00686788559\n",
      "  time_this_iter_s: 39.056522607803345\n",
      "  time_total_s: 8223.00686788559\n",
      "  timers:\n",
      "    learn_throughput: 1441.185\n",
      "    learn_time_ms: 693.873\n",
      "    load_throughput: 43336.616\n",
      "    load_time_ms: 23.075\n",
      "    sample_throughput: 47.967\n",
      "    sample_time_ms: 20847.874\n",
      "    update_time_ms: 2.474\n",
      "  timestamp: 1635071094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 345\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         8223.01</td><td style=\"text-align: right;\">345000</td><td style=\"text-align: right;\"> -3.0446</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            304.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 346000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-25-20\n",
      "  done: false\n",
      "  episode_len_mean: 304.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.0445999999999787\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1113\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0030067787272855633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0764250808291964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016467692595304662\n",
      "          policy_loss: -0.0926640117333995\n",
      "          total_loss: -0.08938225913378928\n",
      "          vf_explained_var: 0.2287052571773529\n",
      "          vf_loss: 0.013996489076978631\n",
      "    num_agent_steps_sampled: 346000\n",
      "    num_agent_steps_trained: 346000\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11621621621621\n",
      "    ram_util_percent: 38.754054054054066\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877933413166206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09065653723173\n",
      "    mean_inference_ms: 1.9292060196383463\n",
      "    mean_raw_obs_processing_ms: 2.0242554997778073\n",
      "  time_since_restore: 8248.995126485825\n",
      "  time_this_iter_s: 25.988258600234985\n",
      "  time_total_s: 8248.995126485825\n",
      "  timers:\n",
      "    learn_throughput: 1443.401\n",
      "    learn_time_ms: 692.808\n",
      "    load_throughput: 43279.333\n",
      "    load_time_ms: 23.106\n",
      "    sample_throughput: 46.166\n",
      "    sample_time_ms: 21661.187\n",
      "    update_time_ms: 2.511\n",
      "  timestamp: 1635071120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">            8249</td><td style=\"text-align: right;\">346000</td><td style=\"text-align: right;\"> -3.0446</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            304.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 347000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-25-45\n",
      "  done: false\n",
      "  episode_len_mean: 303.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.0340999999999787\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1117\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0030067787272855633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9640512122048273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0062926564411030644\n",
      "          policy_loss: -0.02762243648370107\n",
      "          total_loss: -0.021999779178036583\n",
      "          vf_explained_var: 0.11327671259641647\n",
      "          vf_loss: 0.015244250713537136\n",
      "    num_agent_steps_sampled: 347000\n",
      "    num_agent_steps_trained: 347000\n",
      "    num_steps_sampled: 347000\n",
      "    num_steps_trained: 347000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.82222222222223\n",
      "    ram_util_percent: 38.880555555555546\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877815586325088\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09190133017393\n",
      "    mean_inference_ms: 1.9291808310866356\n",
      "    mean_raw_obs_processing_ms: 2.0258578732794597\n",
      "  time_since_restore: 8274.027611255646\n",
      "  time_this_iter_s: 25.032484769821167\n",
      "  time_total_s: 8274.027611255646\n",
      "  timers:\n",
      "    learn_throughput: 1445.319\n",
      "    learn_time_ms: 691.889\n",
      "    load_throughput: 43450.242\n",
      "    load_time_ms: 23.015\n",
      "    sample_throughput: 44.935\n",
      "    sample_time_ms: 22254.52\n",
      "    update_time_ms: 2.482\n",
      "  timestamp: 1635071145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347000\n",
      "  training_iteration: 347\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         8274.03</td><td style=\"text-align: right;\">347000</td><td style=\"text-align: right;\"> -3.0341</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            303.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-26-10\n",
      "  done: false\n",
      "  episode_len_mean: 303.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0363999999999796\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1121\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0030067787272855633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8943458947870466\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007734228683246553\n",
      "          policy_loss: 0.02309383720987373\n",
      "          total_loss: 0.027231112950377993\n",
      "          vf_explained_var: 0.2410207986831665\n",
      "          vf_loss: 0.013057478693210417\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.022857142857156\n",
      "    ram_util_percent: 38.87428571428571\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877696400443375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.092935832950708\n",
      "    mean_inference_ms: 1.9291563309896373\n",
      "    mean_raw_obs_processing_ms: 2.025894410998741\n",
      "  time_since_restore: 8298.821162223816\n",
      "  time_this_iter_s: 24.793550968170166\n",
      "  time_total_s: 8298.821162223816\n",
      "  timers:\n",
      "    learn_throughput: 1443.285\n",
      "    learn_time_ms: 692.864\n",
      "    load_throughput: 43449.567\n",
      "    load_time_ms: 23.015\n",
      "    sample_throughput: 43.816\n",
      "    sample_time_ms: 22822.695\n",
      "    update_time_ms: 2.475\n",
      "  timestamp: 1635071170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 348\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         8298.82</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\"> -3.0364</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            303.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 349000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-26-35\n",
      "  done: false\n",
      "  episode_len_mean: 303.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0344999999999795\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1125\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0030067787272855633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1283861729833815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00648777474189205\n",
      "          policy_loss: 0.0009210258722305298\n",
      "          total_loss: 0.0009224475257926517\n",
      "          vf_explained_var: 0.4282835125923157\n",
      "          vf_loss: 0.01126577714458108\n",
      "    num_agent_steps_sampled: 349000\n",
      "    num_agent_steps_trained: 349000\n",
      "    num_steps_sampled: 349000\n",
      "    num_steps_trained: 349000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.075\n",
      "    ram_util_percent: 38.93888888888889\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038775753167988244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.093988213721126\n",
      "    mean_inference_ms: 1.9291313040544145\n",
      "    mean_raw_obs_processing_ms: 2.0254049447164357\n",
      "  time_since_restore: 8323.726217508316\n",
      "  time_this_iter_s: 24.905055284500122\n",
      "  time_total_s: 8323.726217508316\n",
      "  timers:\n",
      "    learn_throughput: 1446.064\n",
      "    learn_time_ms: 691.532\n",
      "    load_throughput: 43478.798\n",
      "    load_time_ms: 23.0\n",
      "    sample_throughput: 42.734\n",
      "    sample_time_ms: 23400.715\n",
      "    update_time_ms: 2.414\n",
      "  timestamp: 1635071195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349000\n",
      "  training_iteration: 349\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         8323.73</td><td style=\"text-align: right;\">349000</td><td style=\"text-align: right;\"> -3.0345</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            303.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 350000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-27-00\n",
      "  done: false\n",
      "  episode_len_mean: 303.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0334999999999788\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1128\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0030067787272855633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1525980194409688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00661943989000946\n",
      "          policy_loss: -0.08472971899641885\n",
      "          total_loss: -0.0862397301528189\n",
      "          vf_explained_var: 0.43453800678253174\n",
      "          vf_loss: 0.009996070091923078\n",
      "    num_agent_steps_sampled: 350000\n",
      "    num_agent_steps_trained: 350000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.10285714285714\n",
      "    ram_util_percent: 38.93428571428572\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038774804392466077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09481777231072\n",
      "    mean_inference_ms: 1.929112081645412\n",
      "    mean_raw_obs_processing_ms: 2.025036861212837\n",
      "  time_since_restore: 8348.62029671669\n",
      "  time_this_iter_s: 24.894079208374023\n",
      "  time_total_s: 8348.62029671669\n",
      "  timers:\n",
      "    learn_throughput: 1445.824\n",
      "    learn_time_ms: 691.647\n",
      "    load_throughput: 43290.858\n",
      "    load_time_ms: 23.1\n",
      "    sample_throughput: 41.726\n",
      "    sample_time_ms: 23965.712\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1635071220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         8348.62</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\"> -3.0335</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            303.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 351000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-27-23\n",
      "  done: false\n",
      "  episode_len_mean: 303.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.030999999999978\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1132\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0030067787272855633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2941466291745505\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016094043424451185\n",
      "          policy_loss: 0.0038829803466796874\n",
      "          total_loss: -0.0010198024411996207\n",
      "          vf_explained_var: 0.6802845597267151\n",
      "          vf_loss: 0.007990293023693893\n",
      "    num_agent_steps_sampled: 351000\n",
      "    num_agent_steps_trained: 351000\n",
      "    num_steps_sampled: 351000\n",
      "    num_steps_trained: 351000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.07272727272727\n",
      "    ram_util_percent: 38.88484848484848\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877355355036808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.095817097300536\n",
      "    mean_inference_ms: 1.9290853856492907\n",
      "    mean_raw_obs_processing_ms: 2.024531238897319\n",
      "  time_since_restore: 8371.647311925888\n",
      "  time_this_iter_s: 23.027015209197998\n",
      "  time_total_s: 8371.647311925888\n",
      "  timers:\n",
      "    learn_throughput: 1444.752\n",
      "    learn_time_ms: 692.16\n",
      "    load_throughput: 42780.337\n",
      "    load_time_ms: 23.375\n",
      "    sample_throughput: 41.442\n",
      "    sample_time_ms: 24130.298\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1635071243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351000\n",
      "  training_iteration: 351\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         8371.65</td><td style=\"text-align: right;\">351000</td><td style=\"text-align: right;\">  -3.031</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">             303.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-27-46\n",
      "  done: false\n",
      "  episode_len_mean: 303.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0367999999999786\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1135\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0030067787272855633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1391815490192838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018872981255098332\n",
      "          policy_loss: -0.07668863170676761\n",
      "          total_loss: -0.0810183208849695\n",
      "          vf_explained_var: 0.7939908504486084\n",
      "          vf_loss: 0.007005379483517673\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.023529411764706\n",
      "    ram_util_percent: 38.88823529411764\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038772602061524164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.096425196127704\n",
      "    mean_inference_ms: 1.9290649795569614\n",
      "    mean_raw_obs_processing_ms: 2.0241937039478626\n",
      "  time_since_restore: 8394.939029216766\n",
      "  time_this_iter_s: 23.291717290878296\n",
      "  time_total_s: 8394.939029216766\n",
      "  timers:\n",
      "    learn_throughput: 1445.554\n",
      "    learn_time_ms: 691.776\n",
      "    load_throughput: 41882.813\n",
      "    load_time_ms: 23.876\n",
      "    sample_throughput: 40.734\n",
      "    sample_time_ms: 24549.763\n",
      "    update_time_ms: 2.48\n",
      "  timestamp: 1635071266\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 352\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         8394.94</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\"> -3.0368</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            303.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 353000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-28-05\n",
      "  done: false\n",
      "  episode_len_mean: 307.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.077199999999978\n",
      "  episode_reward_min: -4.999999999999938\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1138\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0030067787272855633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2881443487273323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022606034436713194\n",
      "          policy_loss: 0.0026759799155924055\n",
      "          total_loss: -0.01218173743949996\n",
      "          vf_explained_var: 0.3292110562324524\n",
      "          vf_loss: 0.00795575343977867\n",
      "    num_agent_steps_sampled: 353000\n",
      "    num_agent_steps_trained: 353000\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.90384615384615\n",
      "    ram_util_percent: 38.86153846153846\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038771632157061114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.096584034645268\n",
      "    mean_inference_ms: 1.9290448423632724\n",
      "    mean_raw_obs_processing_ms: 2.0238158085904856\n",
      "  time_since_restore: 8413.391202926636\n",
      "  time_this_iter_s: 18.452173709869385\n",
      "  time_total_s: 8413.391202926636\n",
      "  timers:\n",
      "    learn_throughput: 1446.026\n",
      "    learn_time_ms: 691.551\n",
      "    load_throughput: 41930.67\n",
      "    load_time_ms: 23.849\n",
      "    sample_throughput: 41.075\n",
      "    sample_time_ms: 24345.893\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1635071285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         8413.39</td><td style=\"text-align: right;\">353000</td><td style=\"text-align: right;\"> -3.0772</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            307.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 354000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 309.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.099199999999978\n",
      "  episode_reward_min: -4.999999999999938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1140\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004510168090928346\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9364219056235419\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0365048194381535\n",
      "          policy_loss: -0.052829202223155236\n",
      "          total_loss: -0.06536846334735552\n",
      "          vf_explained_var: 0.7053176164627075\n",
      "          vf_loss: 0.0066603186873382784\n",
      "    num_agent_steps_sampled: 354000\n",
      "    num_agent_steps_trained: 354000\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.658\n",
      "    ram_util_percent: 38.772\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038770956296385134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.096414700730826\n",
      "    mean_inference_ms: 1.929031799348387\n",
      "    mean_raw_obs_processing_ms: 2.0244808223221016\n",
      "  time_since_restore: 8448.650648117065\n",
      "  time_this_iter_s: 35.25944519042969\n",
      "  time_total_s: 8448.650648117065\n",
      "  timers:\n",
      "    learn_throughput: 1447.564\n",
      "    learn_time_ms: 690.816\n",
      "    load_throughput: 41551.912\n",
      "    load_time_ms: 24.066\n",
      "    sample_throughput: 38.836\n",
      "    sample_time_ms: 25749.152\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1635071320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 354\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         8448.65</td><td style=\"text-align: right;\">354000</td><td style=\"text-align: right;\"> -3.0992</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            309.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 355000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-28-58\n",
      "  done: false\n",
      "  episode_len_mean: 315.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.1546999999999765\n",
      "  episode_reward_min: -5.919999999999918\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1142\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006765252136392522\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.29910847875807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015059669423518263\n",
      "          policy_loss: -0.23217036674420038\n",
      "          total_loss: -0.2467207959956593\n",
      "          vf_explained_var: 0.3102521598339081\n",
      "          vf_loss: 0.008338769567975154\n",
      "    num_agent_steps_sampled: 355000\n",
      "    num_agent_steps_trained: 355000\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.57037037037038\n",
      "    ram_util_percent: 38.53333333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038770300413417356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.095923383066413\n",
      "    mean_inference_ms: 1.9290196629766525\n",
      "    mean_raw_obs_processing_ms: 2.0251533098104373\n",
      "  time_since_restore: 8467.031442403793\n",
      "  time_this_iter_s: 18.380794286727905\n",
      "  time_total_s: 8467.031442403793\n",
      "  timers:\n",
      "    learn_throughput: 1445.333\n",
      "    learn_time_ms: 691.882\n",
      "    load_throughput: 40416.47\n",
      "    load_time_ms: 24.742\n",
      "    sample_throughput: 42.23\n",
      "    sample_time_ms: 23679.867\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1635071338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         8467.03</td><td style=\"text-align: right;\">355000</td><td style=\"text-align: right;\"> -3.1547</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -5.92</td><td style=\"text-align: right;\">            315.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-29-16\n",
      "  done: false\n",
      "  episode_len_mean: 320.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.327899999999974\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1144\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006765252136392522\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.251120360692342\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.041186749121993986\n",
      "          policy_loss: 0.07802250550852882\n",
      "          total_loss: 0.11698386039998797\n",
      "          vf_explained_var: -0.41683441400527954\n",
      "          vf_loss: 0.061193921864873525\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.895999999999994\n",
      "    ram_util_percent: 38.763999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038769610421353004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.095196236024766\n",
      "    mean_inference_ms: 1.9290080311183022\n",
      "    mean_raw_obs_processing_ms: 2.0257248580526106\n",
      "  time_since_restore: 8484.89366555214\n",
      "  time_this_iter_s: 17.862223148345947\n",
      "  time_total_s: 8484.89366555214\n",
      "  timers:\n",
      "    learn_throughput: 1443.031\n",
      "    learn_time_ms: 692.986\n",
      "    load_throughput: 40517.943\n",
      "    load_time_ms: 24.68\n",
      "    sample_throughput: 43.733\n",
      "    sample_time_ms: 22866.268\n",
      "    update_time_ms: 2.441\n",
      "  timestamp: 1635071356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 356\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         8484.89</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> -3.3279</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            320.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 357000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-29-33\n",
      "  done: false\n",
      "  episode_len_mean: 323.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.3576999999999737\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1145\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010147878204588776\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.28265380859375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022898812668670387\n",
      "          policy_loss: -0.2157151503695382\n",
      "          total_loss: -0.22774095171027714\n",
      "          vf_explained_var: 0.8711476922035217\n",
      "          vf_loss: 0.010568369730996589\n",
      "    num_agent_steps_sampled: 357000\n",
      "    num_agent_steps_trained: 357000\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.80416666666667\n",
      "    ram_util_percent: 38.80833333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038769264421797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.094592932218113\n",
      "    mean_inference_ms: 1.9290024235095211\n",
      "    mean_raw_obs_processing_ms: 2.026012816712089\n",
      "  time_since_restore: 8501.842078447342\n",
      "  time_this_iter_s: 16.948412895202637\n",
      "  time_total_s: 8501.842078447342\n",
      "  timers:\n",
      "    learn_throughput: 1444.577\n",
      "    learn_time_ms: 692.244\n",
      "    load_throughput: 42027.01\n",
      "    load_time_ms: 23.794\n",
      "    sample_throughput: 45.332\n",
      "    sample_time_ms: 22059.494\n",
      "    update_time_ms: 2.436\n",
      "  timestamp: 1635071373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 357\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         8501.84</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\"> -3.3577</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            323.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 358000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-29-51\n",
      "  done: false\n",
      "  episode_len_mean: 329.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.409299999999972\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1147\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.015221817306883167\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.292079869906108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01942466064097163\n",
      "          policy_loss: -0.054011364612314434\n",
      "          total_loss: -0.05750845596194267\n",
      "          vf_explained_var: 0.43328502774238586\n",
      "          vf_loss: 0.019128023385484187\n",
      "    num_agent_steps_sampled: 358000\n",
      "    num_agent_steps_trained: 358000\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.36\n",
      "    ram_util_percent: 38.88\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876856436149854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09313040353109\n",
      "    mean_inference_ms: 1.9289914234904029\n",
      "    mean_raw_obs_processing_ms: 2.0264883122032855\n",
      "  time_since_restore: 8519.269379138947\n",
      "  time_this_iter_s: 17.427300691604614\n",
      "  time_total_s: 8519.269379138947\n",
      "  timers:\n",
      "    learn_throughput: 1446.668\n",
      "    learn_time_ms: 691.244\n",
      "    load_throughput: 42149.488\n",
      "    load_time_ms: 23.725\n",
      "    sample_throughput: 46.896\n",
      "    sample_time_ms: 21323.917\n",
      "    update_time_ms: 2.443\n",
      "  timestamp: 1635071391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         8519.27</td><td style=\"text-align: right;\">358000</td><td style=\"text-align: right;\"> -3.4093</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            329.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 359000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-30-11\n",
      "  done: false\n",
      "  episode_len_mean: 334.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.4648999999999703\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1150\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.015221817306883167\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.12266092300415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020501604753484424\n",
      "          policy_loss: -0.017833680576748317\n",
      "          total_loss: -0.019111416902807023\n",
      "          vf_explained_var: 0.6432809829711914\n",
      "          vf_loss: 0.01963680312037468\n",
      "    num_agent_steps_sampled: 359000\n",
      "    num_agent_steps_trained: 359000\n",
      "    num_steps_sampled: 359000\n",
      "    num_steps_trained: 359000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.939285714285724\n",
      "    ram_util_percent: 38.92142857142857\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876749827965072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.090582371049198\n",
      "    mean_inference_ms: 1.9289749856035934\n",
      "    mean_raw_obs_processing_ms: 2.0261031375653844\n",
      "  time_since_restore: 8539.093031167984\n",
      "  time_this_iter_s: 19.823652029037476\n",
      "  time_total_s: 8539.093031167984\n",
      "  timers:\n",
      "    learn_throughput: 1445.248\n",
      "    learn_time_ms: 691.923\n",
      "    load_throughput: 41989.733\n",
      "    load_time_ms: 23.815\n",
      "    sample_throughput: 48.042\n",
      "    sample_time_ms: 20815.034\n",
      "    update_time_ms: 2.454\n",
      "  timestamp: 1635071411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359000\n",
      "  training_iteration: 359\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         8539.09</td><td style=\"text-align: right;\">359000</td><td style=\"text-align: right;\"> -3.4649</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            334.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-30-28\n",
      "  done: false\n",
      "  episode_len_mean: 338.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.5086999999999704\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1152\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9112542192141215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011737863442984173\n",
      "          policy_loss: 0.08180246866411633\n",
      "          total_loss: 0.07074563023116853\n",
      "          vf_explained_var: 0.38212355971336365\n",
      "          vf_loss: 0.007787695702831519\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.861538461538466\n",
      "    ram_util_percent: 38.91153846153846\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038766763644370934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.08859275952938\n",
      "    mean_inference_ms: 1.9289637218006555\n",
      "    mean_raw_obs_processing_ms: 2.0254305491624303\n",
      "  time_since_restore: 8556.860412836075\n",
      "  time_this_iter_s: 17.76738166809082\n",
      "  time_total_s: 8556.860412836075\n",
      "  timers:\n",
      "    learn_throughput: 1446.441\n",
      "    learn_time_ms: 691.352\n",
      "    load_throughput: 43750.733\n",
      "    load_time_ms: 22.857\n",
      "    sample_throughput: 49.742\n",
      "    sample_time_ms: 20103.937\n",
      "    update_time_ms: 2.414\n",
      "  timestamp: 1635071428\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 360\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         8556.86</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\"> -3.5087</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            338.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 361000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-30-48\n",
      "  done: false\n",
      "  episode_len_mean: 343.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.551599999999971\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1155\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8583545472886827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011043982204820483\n",
      "          policy_loss: -0.057153527583513\n",
      "          total_loss: -0.06275763743453555\n",
      "          vf_explained_var: 0.650975227355957\n",
      "          vf_loss: 0.01272727082315315\n",
      "    num_agent_steps_sampled: 361000\n",
      "    num_agent_steps_trained: 361000\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.017857142857146\n",
      "    ram_util_percent: 38.77142857142858\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876565199163462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.085098739969105\n",
      "    mean_inference_ms: 1.9289467501477362\n",
      "    mean_raw_obs_processing_ms: 2.0244392838898317\n",
      "  time_since_restore: 8576.590891838074\n",
      "  time_this_iter_s: 19.7304790019989\n",
      "  time_total_s: 8576.590891838074\n",
      "  timers:\n",
      "    learn_throughput: 1446.484\n",
      "    learn_time_ms: 691.332\n",
      "    load_throughput: 44273.275\n",
      "    load_time_ms: 22.587\n",
      "    sample_throughput: 50.57\n",
      "    sample_time_ms: 19774.579\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1635071448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         8576.59</td><td style=\"text-align: right;\">361000</td><td style=\"text-align: right;\"> -3.5516</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            343.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 362000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-31-09\n",
      "  done: false\n",
      "  episode_len_mean: 345.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.574399999999969\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1157\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.76775787141588\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008831418926323161\n",
      "          policy_loss: -0.059604850825336245\n",
      "          total_loss: -0.06255695985423194\n",
      "          vf_explained_var: 0.5253308415412903\n",
      "          vf_loss: 0.01452382160609381\n",
      "    num_agent_steps_sampled: 362000\n",
      "    num_agent_steps_trained: 362000\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.44827586206898\n",
      "    ram_util_percent: 38.53793103448276\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876491728801927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.082640272523975\n",
      "    mean_inference_ms: 1.9289354306540722\n",
      "    mean_raw_obs_processing_ms: 2.023733706573193\n",
      "  time_since_restore: 8597.122717142105\n",
      "  time_this_iter_s: 20.531825304031372\n",
      "  time_total_s: 8597.122717142105\n",
      "  timers:\n",
      "    learn_throughput: 1446.844\n",
      "    learn_time_ms: 691.16\n",
      "    load_throughput: 44703.051\n",
      "    load_time_ms: 22.37\n",
      "    sample_throughput: 51.285\n",
      "    sample_time_ms: 19499.049\n",
      "    update_time_ms: 2.39\n",
      "  timestamp: 1635071469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 362\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         8597.12</td><td style=\"text-align: right;\">362000</td><td style=\"text-align: right;\"> -3.5744</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            345.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 363000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-31-29\n",
      "  done: false\n",
      "  episode_len_mean: 346.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.586399999999969\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1160\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6249454763200548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008597235882537134\n",
      "          policy_loss: -0.0954784746799204\n",
      "          total_loss: -0.09486814878053135\n",
      "          vf_explained_var: 0.4114640951156616\n",
      "          vf_loss: 0.016663483964900177\n",
      "    num_agent_steps_sampled: 363000\n",
      "    num_agent_steps_trained: 363000\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.76551724137931\n",
      "    ram_util_percent: 38.27241379310345\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876376164130285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.07870140155096\n",
      "    mean_inference_ms: 1.9289185425346331\n",
      "    mean_raw_obs_processing_ms: 2.0226670121711687\n",
      "  time_since_restore: 8617.453583955765\n",
      "  time_this_iter_s: 20.330866813659668\n",
      "  time_total_s: 8617.453583955765\n",
      "  timers:\n",
      "    learn_throughput: 1445.653\n",
      "    learn_time_ms: 691.729\n",
      "    load_throughput: 44713.535\n",
      "    load_time_ms: 22.365\n",
      "    sample_throughput: 50.797\n",
      "    sample_time_ms: 19686.366\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1635071489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 363\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         8617.45</td><td style=\"text-align: right;\">363000</td><td style=\"text-align: right;\"> -3.5864</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            346.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-31-49\n",
      "  done: false\n",
      "  episode_len_mean: 349.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.6122999999999688\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1163\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.548232834868961\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007539276008606559\n",
      "          policy_loss: 0.061828930675983426\n",
      "          total_loss: 0.05761768710282114\n",
      "          vf_explained_var: 0.17105159163475037\n",
      "          vf_loss: 0.011098941384504239\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.46206896551725\n",
      "    ram_util_percent: 38.24137931034482\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876260583181733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.07453670778446\n",
      "    mean_inference_ms: 1.9289015600269799\n",
      "    mean_raw_obs_processing_ms: 2.021562922387972\n",
      "  time_since_restore: 8637.558532953262\n",
      "  time_this_iter_s: 20.10494899749756\n",
      "  time_total_s: 8637.558532953262\n",
      "  timers:\n",
      "    learn_throughput: 1445.23\n",
      "    learn_time_ms: 691.931\n",
      "    load_throughput: 44985.355\n",
      "    load_time_ms: 22.229\n",
      "    sample_throughput: 55.033\n",
      "    sample_time_ms: 18170.904\n",
      "    update_time_ms: 2.365\n",
      "  timestamp: 1635071509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         8637.56</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\"> -3.6123</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            349.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 365000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-32-10\n",
      "  done: false\n",
      "  episode_len_mean: 350.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.618799999999969\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1166\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4460579805903964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008578966971919568\n",
      "          policy_loss: 0.009335400660832723\n",
      "          total_loss: 0.007181955046123929\n",
      "          vf_explained_var: 0.393452912569046\n",
      "          vf_loss: 0.01211125442577112\n",
      "    num_agent_steps_sampled: 365000\n",
      "    num_agent_steps_trained: 365000\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.609677419354824\n",
      "    ram_util_percent: 38.196774193548386\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876150867337576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.070274103392954\n",
      "    mean_inference_ms: 1.9288855405226055\n",
      "    mean_raw_obs_processing_ms: 2.0204781350169525\n",
      "  time_since_restore: 8658.851975440979\n",
      "  time_this_iter_s: 21.293442487716675\n",
      "  time_total_s: 8658.851975440979\n",
      "  timers:\n",
      "    learn_throughput: 1447.393\n",
      "    learn_time_ms: 690.898\n",
      "    load_throughput: 44900.742\n",
      "    load_time_ms: 22.271\n",
      "    sample_throughput: 54.162\n",
      "    sample_time_ms: 18463.167\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1635071530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 365\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         8658.85</td><td style=\"text-align: right;\">365000</td><td style=\"text-align: right;\"> -3.6188</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">               350</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 366000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-32-30\n",
      "  done: false\n",
      "  episode_len_mean: 350.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.6250999999999687\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1169\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4761518624093797\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008818602813211503\n",
      "          policy_loss: 0.05123998522758484\n",
      "          total_loss: 0.048093357847796545\n",
      "          vf_explained_var: 0.30586737394332886\n",
      "          vf_loss: 0.01141353721678671\n",
      "    num_agent_steps_sampled: 366000\n",
      "    num_agent_steps_trained: 366000\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.87499999999999\n",
      "    ram_util_percent: 38.189285714285724\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038760409408606304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.065945582658347\n",
      "    mean_inference_ms: 1.9288698769165178\n",
      "    mean_raw_obs_processing_ms: 2.0194117493861823\n",
      "  time_since_restore: 8678.772867918015\n",
      "  time_this_iter_s: 19.920892477035522\n",
      "  time_total_s: 8678.772867918015\n",
      "  timers:\n",
      "    learn_throughput: 1447.727\n",
      "    learn_time_ms: 690.738\n",
      "    load_throughput: 44788.355\n",
      "    load_time_ms: 22.327\n",
      "    sample_throughput: 53.564\n",
      "    sample_time_ms: 18669.154\n",
      "    update_time_ms: 2.351\n",
      "  timestamp: 1635071550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 366\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         8678.77</td><td style=\"text-align: right;\">366000</td><td style=\"text-align: right;\"> -3.6251</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            350.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 367000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 350.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.626199999999969\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1172\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5687852144241332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010858670649060227\n",
      "          policy_loss: 0.06173736287487878\n",
      "          total_loss: 0.05781555275122325\n",
      "          vf_explained_var: 0.18979117274284363\n",
      "          vf_loss: 0.011518108886149195\n",
      "    num_agent_steps_sampled: 367000\n",
      "    num_agent_steps_trained: 367000\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.72692307692308\n",
      "    ram_util_percent: 38.21538461538462\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875930489715971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.06171954009335\n",
      "    mean_inference_ms: 1.9288545921032958\n",
      "    mean_raw_obs_processing_ms: 2.0196582604540083\n",
      "  time_since_restore: 8714.923573255539\n",
      "  time_this_iter_s: 36.150705337524414\n",
      "  time_total_s: 8714.923573255539\n",
      "  timers:\n",
      "    learn_throughput: 1444.923\n",
      "    learn_time_ms: 692.078\n",
      "    load_throughput: 43105.509\n",
      "    load_time_ms: 23.199\n",
      "    sample_throughput: 48.574\n",
      "    sample_time_ms: 20587.092\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1635071587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         8714.92</td><td style=\"text-align: right;\">367000</td><td style=\"text-align: right;\"> -3.6262</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            350.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-33-27\n",
      "  done: false\n",
      "  episode_len_mean: 350.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.6248999999999683\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1175\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6701716992590163\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01567717003283185\n",
      "          policy_loss: 0.05613201856613159\n",
      "          total_loss: 0.051225509577327306\n",
      "          vf_explained_var: -0.013083009980618954\n",
      "          vf_loss: 0.011437256954377518\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.314285714285724\n",
      "    ram_util_percent: 38.58928571428572\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038758170779024666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0574901843667\n",
      "    mean_inference_ms: 1.928838890117046\n",
      "    mean_raw_obs_processing_ms: 2.019919333185219\n",
      "  time_since_restore: 8735.169193983078\n",
      "  time_this_iter_s: 20.245620727539062\n",
      "  time_total_s: 8735.169193983078\n",
      "  timers:\n",
      "    learn_throughput: 1443.843\n",
      "    learn_time_ms: 692.596\n",
      "    load_throughput: 42866.119\n",
      "    load_time_ms: 23.328\n",
      "    sample_throughput: 47.92\n",
      "    sample_time_ms: 20867.912\n",
      "    update_time_ms: 2.728\n",
      "  timestamp: 1635071607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 368\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         8735.17</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\"> -3.6249</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            350.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 369000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 349.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.617299999999969\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1178\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5277404851383634\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0060338944160186255\n",
      "          policy_loss: 0.06845672296153174\n",
      "          total_loss: 0.0648275117079417\n",
      "          vf_explained_var: 0.1390380561351776\n",
      "          vf_loss: 0.011510420121097317\n",
      "    num_agent_steps_sampled: 369000\n",
      "    num_agent_steps_trained: 369000\n",
      "    num_steps_sampled: 369000\n",
      "    num_steps_trained: 369000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.648387096774194\n",
      "    ram_util_percent: 38.751612903225805\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038757030692484734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.053411996151073\n",
      "    mean_inference_ms: 1.9288233601076652\n",
      "    mean_raw_obs_processing_ms: 2.0201949960139434\n",
      "  time_since_restore: 8756.428480625153\n",
      "  time_this_iter_s: 21.259286642074585\n",
      "  time_total_s: 8756.428480625153\n",
      "  timers:\n",
      "    learn_throughput: 1444.579\n",
      "    learn_time_ms: 692.243\n",
      "    load_throughput: 43132.239\n",
      "    load_time_ms: 23.185\n",
      "    sample_throughput: 47.592\n",
      "    sample_time_ms: 21011.956\n",
      "    update_time_ms: 2.728\n",
      "  timestamp: 1635071628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369000\n",
      "  training_iteration: 369\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         8756.43</td><td style=\"text-align: right;\">369000</td><td style=\"text-align: right;\"> -3.6173</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            349.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 370000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-34-08\n",
      "  done: false\n",
      "  episode_len_mean: 350.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.622099999999968\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1181\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5493040641148885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012647258930729264\n",
      "          policy_loss: 0.05061031720704502\n",
      "          total_loss: 0.047704532245794934\n",
      "          vf_explained_var: -0.25147882103919983\n",
      "          vf_loss: 0.01229848012379888\n",
      "    num_agent_steps_sampled: 370000\n",
      "    num_agent_steps_trained: 370000\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.939285714285724\n",
      "    ram_util_percent: 38.88214285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387558667810748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.049175801134997\n",
      "    mean_inference_ms: 1.9288068761815576\n",
      "    mean_raw_obs_processing_ms: 2.01899359443878\n",
      "  time_since_restore: 8776.352640151978\n",
      "  time_this_iter_s: 19.92415952682495\n",
      "  time_total_s: 8776.352640151978\n",
      "  timers:\n",
      "    learn_throughput: 1445.1\n",
      "    learn_time_ms: 691.993\n",
      "    load_throughput: 41272.648\n",
      "    load_time_ms: 24.229\n",
      "    sample_throughput: 47.11\n",
      "    sample_time_ms: 21226.82\n",
      "    update_time_ms: 2.738\n",
      "  timestamp: 1635071648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         8776.35</td><td style=\"text-align: right;\">370000</td><td style=\"text-align: right;\"> -3.6221</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            350.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 371000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-34-30\n",
      "  done: false\n",
      "  episode_len_mean: 348.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.6007999999999685\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1184\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4331888026661344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006506663359978325\n",
      "          policy_loss: 0.049524897502528295\n",
      "          total_loss: 0.046954257869058186\n",
      "          vf_explained_var: 0.04616687074303627\n",
      "          vf_loss: 0.011612681465016471\n",
      "    num_agent_steps_sampled: 371000\n",
      "    num_agent_steps_trained: 371000\n",
      "    num_steps_sampled: 371000\n",
      "    num_steps_trained: 371000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.81290322580646\n",
      "    ram_util_percent: 38.87741935483869\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875472073347331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.04536024548435\n",
      "    mean_inference_ms: 1.928791007975096\n",
      "    mean_raw_obs_processing_ms: 2.01786765842006\n",
      "  time_since_restore: 8797.787014484406\n",
      "  time_this_iter_s: 21.43437433242798\n",
      "  time_total_s: 8797.787014484406\n",
      "  timers:\n",
      "    learn_throughput: 1446.874\n",
      "    learn_time_ms: 691.145\n",
      "    load_throughput: 41483.035\n",
      "    load_time_ms: 24.106\n",
      "    sample_throughput: 46.733\n",
      "    sample_time_ms: 21398.036\n",
      "    update_time_ms: 2.855\n",
      "  timestamp: 1635071670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 371000\n",
      "  training_iteration: 371\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         8797.79</td><td style=\"text-align: right;\">371000</td><td style=\"text-align: right;\"> -3.6008</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">             348.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-34-50\n",
      "  done: false\n",
      "  episode_len_mean: 347.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.591399999999969\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1187\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.474637766679128\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00828453920536227\n",
      "          policy_loss: 0.01702123342288865\n",
      "          total_loss: 0.015067121883233388\n",
      "          vf_explained_var: 0.0936809554696083\n",
      "          vf_loss: 0.01260310608583192\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.35172413793104\n",
      "    ram_util_percent: 38.91724137931034\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387535732909904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.04170461997398\n",
      "    mean_inference_ms: 1.9287750607113336\n",
      "    mean_raw_obs_processing_ms: 2.0167595086082297\n",
      "  time_since_restore: 8818.429951190948\n",
      "  time_this_iter_s: 20.64293670654297\n",
      "  time_total_s: 8818.429951190948\n",
      "  timers:\n",
      "    learn_throughput: 1447.738\n",
      "    learn_time_ms: 690.733\n",
      "    load_throughput: 40853.04\n",
      "    load_time_ms: 24.478\n",
      "    sample_throughput: 46.709\n",
      "    sample_time_ms: 21409.207\n",
      "    update_time_ms: 2.842\n",
      "  timestamp: 1635071690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 372\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         8818.43</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\"> -3.5914</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            347.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 373000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-35-10\n",
      "  done: false\n",
      "  episode_len_mean: 346.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.5809999999999693\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1190\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3546791315078734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011968499941419786\n",
      "          policy_loss: 0.019316781643364166\n",
      "          total_loss: 0.019376705669694478\n",
      "          vf_explained_var: -0.019277093932032585\n",
      "          vf_loss: 0.013333440767989183\n",
      "    num_agent_steps_sampled: 373000\n",
      "    num_agent_steps_trained: 373000\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.955172413793115\n",
      "    ram_util_percent: 38.91379310344828\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038752456519101156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.038175291877028\n",
      "    mean_inference_ms: 1.928759494937719\n",
      "    mean_raw_obs_processing_ms: 2.015670040727501\n",
      "  time_since_restore: 8838.69773364067\n",
      "  time_this_iter_s: 20.26778244972229\n",
      "  time_total_s: 8838.69773364067\n",
      "  timers:\n",
      "    learn_throughput: 1450.736\n",
      "    learn_time_ms: 689.305\n",
      "    load_throughput: 40798.083\n",
      "    load_time_ms: 24.511\n",
      "    sample_throughput: 46.72\n",
      "    sample_time_ms: 21404.298\n",
      "    update_time_ms: 2.825\n",
      "  timestamp: 1635071710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">          8838.7</td><td style=\"text-align: right;\">373000</td><td style=\"text-align: right;\">  -3.581</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            346.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 374000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-35-33\n",
      "  done: false\n",
      "  episode_len_mean: 345.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.570399999999969\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1193\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2793922781944276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011152405046622663\n",
      "          policy_loss: -0.10454105113943418\n",
      "          total_loss: -0.10029058166676097\n",
      "          vf_explained_var: 0.1009816825389862\n",
      "          vf_loss: 0.016789753300448258\n",
      "    num_agent_steps_sampled: 374000\n",
      "    num_agent_steps_trained: 374000\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.88125\n",
      "    ram_util_percent: 38.903125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875135566539476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0349227401996\n",
      "    mean_inference_ms: 1.928744543482101\n",
      "    mean_raw_obs_processing_ms: 2.014599241308638\n",
      "  time_since_restore: 8860.915021419525\n",
      "  time_this_iter_s: 22.21728777885437\n",
      "  time_total_s: 8860.915021419525\n",
      "  timers:\n",
      "    learn_throughput: 1451.285\n",
      "    learn_time_ms: 689.044\n",
      "    load_throughput: 40903.398\n",
      "    load_time_ms: 24.448\n",
      "    sample_throughput: 46.262\n",
      "    sample_time_ms: 21615.785\n",
      "    update_time_ms: 2.865\n",
      "  timestamp: 1635071733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 374\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         8860.92</td><td style=\"text-align: right;\">374000</td><td style=\"text-align: right;\"> -3.5704</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            345.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-35-54\n",
      "  done: false\n",
      "  episode_len_mean: 343.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.554599999999971\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1197\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5335740592744616\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009421716670915398\n",
      "          policy_loss: -0.005725705375274022\n",
      "          total_loss: -0.003269569410218133\n",
      "          vf_explained_var: 0.10007870197296143\n",
      "          vf_loss: 0.01757675034718381\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.86774193548386\n",
      "    ram_util_percent: 38.87419354838708\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874992162003055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.030809892046022\n",
      "    mean_inference_ms: 1.928724748664845\n",
      "    mean_raw_obs_processing_ms: 2.013233351384832\n",
      "  time_since_restore: 8882.560460805893\n",
      "  time_this_iter_s: 21.645439386367798\n",
      "  time_total_s: 8882.560460805893\n",
      "  timers:\n",
      "    learn_throughput: 1450.229\n",
      "    learn_time_ms: 689.546\n",
      "    load_throughput: 40994.87\n",
      "    load_time_ms: 24.393\n",
      "    sample_throughput: 46.188\n",
      "    sample_time_ms: 21650.528\n",
      "    update_time_ms: 2.864\n",
      "  timestamp: 1635071754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 375\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         8882.56</td><td style=\"text-align: right;\">375000</td><td style=\"text-align: right;\"> -3.5546</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            343.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 341.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.53749999999997\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1200\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3157340897454155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008597456511154098\n",
      "          policy_loss: 0.07914530734221141\n",
      "          total_loss: 0.07374041146702237\n",
      "          vf_explained_var: 0.26172247529029846\n",
      "          vf_loss: 0.007556141548169156\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.896491228070175\n",
      "    ram_util_percent: 38.8140350877193\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874886189032624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.027923895996327\n",
      "    mean_inference_ms: 1.928710492811602\n",
      "    mean_raw_obs_processing_ms: 2.0136612938153973\n",
      "  time_since_restore: 8922.147115468979\n",
      "  time_this_iter_s: 39.58665466308594\n",
      "  time_total_s: 8922.147115468979\n",
      "  timers:\n",
      "    learn_throughput: 1449.295\n",
      "    learn_time_ms: 689.991\n",
      "    load_throughput: 41117.403\n",
      "    load_time_ms: 24.321\n",
      "    sample_throughput: 42.343\n",
      "    sample_time_ms: 23616.731\n",
      "    update_time_ms: 2.87\n",
      "  timestamp: 1635071794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         8922.15</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\"> -3.5375</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            341.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 377000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 339.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.510299999999971\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1204\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1770325700441997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009486709422645213\n",
      "          policy_loss: 0.0064751219004392626\n",
      "          total_loss: 0.012822258182697825\n",
      "          vf_explained_var: 0.05410384014248848\n",
      "          vf_loss: 0.017900856460134187\n",
      "    num_agent_steps_sampled: 377000\n",
      "    num_agent_steps_trained: 377000\n",
      "    num_steps_sampled: 377000\n",
      "    num_steps_trained: 377000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.74285714285714\n",
      "    ram_util_percent: 38.7\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874751699383182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.02465914059196\n",
      "    mean_inference_ms: 1.9286929319679564\n",
      "    mean_raw_obs_processing_ms: 2.0143764919560896\n",
      "  time_since_restore: 8946.674770593643\n",
      "  time_this_iter_s: 24.527655124664307\n",
      "  time_total_s: 8946.674770593643\n",
      "  timers:\n",
      "    learn_throughput: 1450.402\n",
      "    learn_time_ms: 689.464\n",
      "    load_throughput: 40417.755\n",
      "    load_time_ms: 24.742\n",
      "    sample_throughput: 44.534\n",
      "    sample_time_ms: 22454.603\n",
      "    update_time_ms: 2.833\n",
      "  timestamp: 1635071819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 377000\n",
      "  training_iteration: 377\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         8946.67</td><td style=\"text-align: right;\">377000</td><td style=\"text-align: right;\"> -3.5103</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            339.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 378000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-37-21\n",
      "  done: false\n",
      "  episode_len_mean: 338.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.500099999999971\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1207\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2412475943565369\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010533478671599757\n",
      "          policy_loss: 0.03345991919438044\n",
      "          total_loss: 0.03357616803712315\n",
      "          vf_explained_var: 0.021135399118065834\n",
      "          vf_loss: 0.012288220676903923\n",
      "    num_agent_steps_sampled: 378000\n",
      "    num_agent_steps_trained: 378000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.984375\n",
      "    ram_util_percent: 38.753125000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038746513981964464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.022292297580346\n",
      "    mean_inference_ms: 1.928679805900769\n",
      "    mean_raw_obs_processing_ms: 2.014456553758808\n",
      "  time_since_restore: 8969.154813051224\n",
      "  time_this_iter_s: 22.480042457580566\n",
      "  time_total_s: 8969.154813051224\n",
      "  timers:\n",
      "    learn_throughput: 1449.524\n",
      "    learn_time_ms: 689.882\n",
      "    load_throughput: 40584.043\n",
      "    load_time_ms: 24.64\n",
      "    sample_throughput: 44.095\n",
      "    sample_time_ms: 22678.097\n",
      "    update_time_ms: 2.502\n",
      "  timestamp: 1635071841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 378\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         8969.15</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\"> -3.5001</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            338.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 379000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 337.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.4955999999999716\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1211\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.113202722205056\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009012257304499135\n",
      "          policy_loss: 0.001210806311832534\n",
      "          total_loss: 0.00562921373380555\n",
      "          vf_explained_var: 0.17117281258106232\n",
      "          vf_loss: 0.015344662198589907\n",
      "    num_agent_steps_sampled: 379000\n",
      "    num_agent_steps_trained: 379000\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18529411764706\n",
      "    ram_util_percent: 38.83235294117646\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038745198788796786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01922391366164\n",
      "    mean_inference_ms: 1.928661590154677\n",
      "    mean_raw_obs_processing_ms: 2.0132817729259993\n",
      "  time_since_restore: 8993.345133066177\n",
      "  time_this_iter_s: 24.190320014953613\n",
      "  time_total_s: 8993.345133066177\n",
      "  timers:\n",
      "    learn_throughput: 1448.122\n",
      "    learn_time_ms: 690.549\n",
      "    load_throughput: 40495.997\n",
      "    load_time_ms: 24.694\n",
      "    sample_throughput: 43.534\n",
      "    sample_time_ms: 22970.499\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1635071865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         8993.35</td><td style=\"text-align: right;\">379000</td><td style=\"text-align: right;\"> -3.4956</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            337.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-38-08\n",
      "  done: false\n",
      "  episode_len_mean: 338.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.4998999999999705\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1214\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.357342733277215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007830012121810902\n",
      "          policy_loss: 0.04138098789585961\n",
      "          total_loss: 0.036495355930593276\n",
      "          vf_explained_var: 0.28518566489219666\n",
      "          vf_loss: 0.008509016055743106\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.842424242424244\n",
      "    ram_util_percent: 38.89090909090908\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874417028443169\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01665006437762\n",
      "    mean_inference_ms: 1.9286476989091448\n",
      "    mean_raw_obs_processing_ms: 2.0124287810076953\n",
      "  time_since_restore: 9016.198807001114\n",
      "  time_this_iter_s: 22.853673934936523\n",
      "  time_total_s: 9016.198807001114\n",
      "  timers:\n",
      "    learn_throughput: 1446.279\n",
      "    learn_time_ms: 691.43\n",
      "    load_throughput: 40331.63\n",
      "    load_time_ms: 24.794\n",
      "    sample_throughput: 42.988\n",
      "    sample_time_ms: 23262.482\n",
      "    update_time_ms: 2.482\n",
      "  timestamp: 1635071888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 380\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">          9016.2</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\"> -3.4999</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            338.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 381000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-38-32\n",
      "  done: false\n",
      "  episode_len_mean: 338.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.507999999999971\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1218\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2422680947515699\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006346627390229307\n",
      "          policy_loss: 0.02303129037221273\n",
      "          total_loss: 0.024141193512413235\n",
      "          vf_explained_var: 0.38744282722473145\n",
      "          vf_loss: 0.0133876729135712\n",
      "    num_agent_steps_sampled: 381000\n",
      "    num_agent_steps_trained: 381000\n",
      "    num_steps_sampled: 381000\n",
      "    num_steps_trained: 381000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.89411764705882\n",
      "    ram_util_percent: 38.90882352941176\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387428103744609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.013117971174964\n",
      "    mean_inference_ms: 1.9286294177927095\n",
      "    mean_raw_obs_processing_ms: 2.0112947635982024\n",
      "  time_since_restore: 9040.140349388123\n",
      "  time_this_iter_s: 23.941542387008667\n",
      "  time_total_s: 9040.140349388123\n",
      "  timers:\n",
      "    learn_throughput: 1443.881\n",
      "    learn_time_ms: 692.578\n",
      "    load_throughput: 40134.845\n",
      "    load_time_ms: 24.916\n",
      "    sample_throughput: 42.531\n",
      "    sample_time_ms: 23512.064\n",
      "    update_time_ms: 2.379\n",
      "  timestamp: 1635071912\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 381000\n",
      "  training_iteration: 381\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         9040.14</td><td style=\"text-align: right;\">381000</td><td style=\"text-align: right;\">  -3.508</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            338.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 382000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-38-56\n",
      "  done: false\n",
      "  episode_len_mean: 339.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.514899999999971\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1221\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3148472706476848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010304873472559786\n",
      "          policy_loss: 0.02972487070494228\n",
      "          total_loss: 0.02390822395682335\n",
      "          vf_explained_var: 0.6598025560379028\n",
      "          vf_loss: 0.007096535936903415\n",
      "    num_agent_steps_sampled: 382000\n",
      "    num_agent_steps_trained: 382000\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.02058823529411\n",
      "    ram_util_percent: 38.88823529411765\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038741792053427974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.010414956506594\n",
      "    mean_inference_ms: 1.9286151937740978\n",
      "    mean_raw_obs_processing_ms: 2.0104193008875204\n",
      "  time_since_restore: 9063.628987312317\n",
      "  time_this_iter_s: 23.488637924194336\n",
      "  time_total_s: 9063.628987312317\n",
      "  timers:\n",
      "    learn_throughput: 1442.437\n",
      "    learn_time_ms: 693.271\n",
      "    load_throughput: 40208.139\n",
      "    load_time_ms: 24.871\n",
      "    sample_throughput: 42.024\n",
      "    sample_time_ms: 23795.954\n",
      "    update_time_ms: 2.377\n",
      "  timestamp: 1635071936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         9063.63</td><td style=\"text-align: right;\">382000</td><td style=\"text-align: right;\"> -3.5149</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            339.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 383000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-39-18\n",
      "  done: false\n",
      "  episode_len_mean: 340.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.520799999999971\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1224\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2326981796158685\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00571344682034203\n",
      "          policy_loss: -0.11557037565443251\n",
      "          total_loss: -0.1199652729762925\n",
      "          vf_explained_var: 0.7386038899421692\n",
      "          vf_loss: 0.007801626027665204\n",
      "    num_agent_steps_sampled: 383000\n",
      "    num_agent_steps_trained: 383000\n",
      "    num_steps_sampled: 383000\n",
      "    num_steps_trained: 383000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.106249999999996\n",
      "    ram_util_percent: 38.89375\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874079071055037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.007537209691776\n",
      "    mean_inference_ms: 1.9285999462533716\n",
      "    mean_raw_obs_processing_ms: 2.0095569580445303\n",
      "  time_since_restore: 9086.456083774567\n",
      "  time_this_iter_s: 22.827096462249756\n",
      "  time_total_s: 9086.456083774567\n",
      "  timers:\n",
      "    learn_throughput: 1439.495\n",
      "    learn_time_ms: 694.688\n",
      "    load_throughput: 39990.504\n",
      "    load_time_ms: 25.006\n",
      "    sample_throughput: 41.58\n",
      "    sample_time_ms: 24050.304\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1635071958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383000\n",
      "  training_iteration: 383\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         9086.46</td><td style=\"text-align: right;\">383000</td><td style=\"text-align: right;\"> -3.5208</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">             340.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-39-41\n",
      "  done: false\n",
      "  episode_len_mean: 341.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.5345999999999695\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1228\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2252609180079566\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011222031435229398\n",
      "          policy_loss: 0.04036870565679338\n",
      "          total_loss: 0.038070771015352674\n",
      "          vf_explained_var: 0.6773551106452942\n",
      "          vf_loss: 0.009698443896033698\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.821875000000006\n",
      "    ram_util_percent: 38.828125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873944997361873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.003466305145896\n",
      "    mean_inference_ms: 1.9285790844429016\n",
      "    mean_raw_obs_processing_ms: 2.0083796085733265\n",
      "  time_since_restore: 9108.517058372498\n",
      "  time_this_iter_s: 22.060974597930908\n",
      "  time_total_s: 9108.517058372498\n",
      "  timers:\n",
      "    learn_throughput: 1439.309\n",
      "    learn_time_ms: 694.778\n",
      "    load_throughput: 39839.816\n",
      "    load_time_ms: 25.101\n",
      "    sample_throughput: 41.607\n",
      "    sample_time_ms: 24034.553\n",
      "    update_time_ms: 2.363\n",
      "  timestamp: 1635071981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 384\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         9108.52</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\"> -3.5346</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            341.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 385000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-40-23\n",
      "  done: false\n",
      "  episode_len_mean: 341.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -3.53369999999997\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1231\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1860169245137109\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008482191067091962\n",
      "          policy_loss: -0.14559485846095616\n",
      "          total_loss: -0.14807996534638934\n",
      "          vf_explained_var: 0.6504514217376709\n",
      "          vf_loss: 0.009181392668849892\n",
      "    num_agent_steps_sampled: 385000\n",
      "    num_agent_steps_trained: 385000\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.24\n",
      "    ram_util_percent: 38.745000000000005\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873840116061312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.000532652074085\n",
      "    mean_inference_ms: 1.928563419169576\n",
      "    mean_raw_obs_processing_ms: 2.008846756878968\n",
      "  time_since_restore: 9150.474611997604\n",
      "  time_this_iter_s: 41.95755362510681\n",
      "  time_total_s: 9150.474611997604\n",
      "  timers:\n",
      "    learn_throughput: 1440.06\n",
      "    learn_time_ms: 694.415\n",
      "    load_throughput: 39516.304\n",
      "    load_time_ms: 25.306\n",
      "    sample_throughput: 38.364\n",
      "    sample_time_ms: 26065.928\n",
      "    update_time_ms: 2.356\n",
      "  timestamp: 1635072023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         9150.47</td><td style=\"text-align: right;\">385000</td><td style=\"text-align: right;\"> -3.5337</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            341.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 386000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-40-46\n",
      "  done: false\n",
      "  episode_len_mean: 341.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -3.5312999999999697\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1235\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.198632585340076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009560490422610791\n",
      "          policy_loss: 0.003515670821070671\n",
      "          total_loss: 0.005463684350252152\n",
      "          vf_explained_var: 0.2960973381996155\n",
      "          vf_loss: 0.013716048198855586\n",
      "    num_agent_steps_sampled: 386000\n",
      "    num_agent_steps_trained: 386000\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.521212121212116\n",
      "    ram_util_percent: 38.79393939393939\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387369988092866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.996659580839918\n",
      "    mean_inference_ms: 1.9285417995295473\n",
      "    mean_raw_obs_processing_ms: 2.0094384004849695\n",
      "  time_since_restore: 9173.853640794754\n",
      "  time_this_iter_s: 23.379028797149658\n",
      "  time_total_s: 9173.853640794754\n",
      "  timers:\n",
      "    learn_throughput: 1441.042\n",
      "    learn_time_ms: 693.942\n",
      "    load_throughput: 39279.608\n",
      "    load_time_ms: 25.459\n",
      "    sample_throughput: 40.907\n",
      "    sample_time_ms: 24445.495\n",
      "    update_time_ms: 2.341\n",
      "  timestamp: 1635072046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 386\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         9173.85</td><td style=\"text-align: right;\">386000</td><td style=\"text-align: right;\"> -3.5313</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            341.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 387000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-41-10\n",
      "  done: false\n",
      "  episode_len_mean: 336.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -3.4886999999999717\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1239\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0479785495334202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007520575822564446\n",
      "          policy_loss: 0.06653613986240493\n",
      "          total_loss: 0.0659932264023357\n",
      "          vf_explained_var: 0.39495787024497986\n",
      "          vf_loss: 0.009765155856601065\n",
      "    num_agent_steps_sampled: 387000\n",
      "    num_agent_steps_trained: 387000\n",
      "    num_steps_sampled: 387000\n",
      "    num_steps_trained: 387000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.84\n",
      "    ram_util_percent: 38.811428571428564\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038735634785677854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99351027149489\n",
      "    mean_inference_ms: 1.9285200477572582\n",
      "    mean_raw_obs_processing_ms: 2.009677944384094\n",
      "  time_since_restore: 9198.279184818268\n",
      "  time_this_iter_s: 24.425544023513794\n",
      "  time_total_s: 9198.279184818268\n",
      "  timers:\n",
      "    learn_throughput: 1438.346\n",
      "    learn_time_ms: 695.243\n",
      "    load_throughput: 39824.383\n",
      "    load_time_ms: 25.11\n",
      "    sample_throughput: 40.926\n",
      "    sample_time_ms: 24434.312\n",
      "    update_time_ms: 2.355\n",
      "  timestamp: 1635072070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 387000\n",
      "  training_iteration: 387\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         9198.28</td><td style=\"text-align: right;\">387000</td><td style=\"text-align: right;\"> -3.4887</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            336.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-41-34\n",
      "  done: false\n",
      "  episode_len_mean: 329.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -3.4127999999999723\n",
      "  episode_reward_min: -11.39999999999989\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1242\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5538839419682822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014417134861107113\n",
      "          policy_loss: 0.013062058885892232\n",
      "          total_loss: 0.007429778741465674\n",
      "          vf_explained_var: 0.2860516309738159\n",
      "          vf_loss: 0.009577378928143945\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24545454545455\n",
      "    ram_util_percent: 38.86969696969697\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873462211958742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.991681266888996\n",
      "    mean_inference_ms: 1.928502327794302\n",
      "    mean_raw_obs_processing_ms: 2.008871821395678\n",
      "  time_since_restore: 9221.561931610107\n",
      "  time_this_iter_s: 23.2827467918396\n",
      "  time_total_s: 9221.561931610107\n",
      "  timers:\n",
      "    learn_throughput: 1440.613\n",
      "    learn_time_ms: 694.149\n",
      "    load_throughput: 39728.607\n",
      "    load_time_ms: 25.171\n",
      "    sample_throughput: 40.79\n",
      "    sample_time_ms: 24515.528\n",
      "    update_time_ms: 2.448\n",
      "  timestamp: 1635072094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         9221.56</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\"> -3.4128</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">             329.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 389000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-41-56\n",
      "  done: false\n",
      "  episode_len_mean: 319.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -3.1944999999999752\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1246\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5623878147866992\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009887298067677679\n",
      "          policy_loss: 0.03209809685746829\n",
      "          total_loss: 0.028902112940947213\n",
      "          vf_explained_var: 0.39261001348495483\n",
      "          vf_loss: 0.012202142520497243\n",
      "    num_agent_steps_sampled: 389000\n",
      "    num_agent_steps_trained: 389000\n",
      "    num_steps_sampled: 389000\n",
      "    num_steps_trained: 389000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.672727272727265\n",
      "    ram_util_percent: 38.91212121212121\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873331681313059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99033741340518\n",
      "    mean_inference_ms: 1.9284763183563074\n",
      "    mean_raw_obs_processing_ms: 2.008039231684523\n",
      "  time_since_restore: 9244.284353733063\n",
      "  time_this_iter_s: 22.722422122955322\n",
      "  time_total_s: 9244.284353733063\n",
      "  timers:\n",
      "    learn_throughput: 1443.317\n",
      "    learn_time_ms: 692.848\n",
      "    load_throughput: 39697.586\n",
      "    load_time_ms: 25.19\n",
      "    sample_throughput: 41.034\n",
      "    sample_time_ms: 24370.012\n",
      "    update_time_ms: 2.465\n",
      "  timestamp: 1635072116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389000\n",
      "  training_iteration: 389\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         9244.28</td><td style=\"text-align: right;\">389000</td><td style=\"text-align: right;\"> -3.1945</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            319.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-42-21\n",
      "  done: false\n",
      "  episode_len_mean: 312.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -3.129599999999977\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1249\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2745037900076972\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00507316433683244\n",
      "          policy_loss: 0.02194795235991478\n",
      "          total_loss: 0.018044008480177984\n",
      "          vf_explained_var: 0.47242382168769836\n",
      "          vf_loss: 0.00872525899645148\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.07142857142857\n",
      "    ram_util_percent: 38.89714285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038732349684754336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.990146683036908\n",
      "    mean_inference_ms: 1.928456144230218\n",
      "    mean_raw_obs_processing_ms: 2.0075678807453756\n",
      "  time_since_restore: 9268.81973028183\n",
      "  time_this_iter_s: 24.53537654876709\n",
      "  time_total_s: 9268.81973028183\n",
      "  timers:\n",
      "    learn_throughput: 1444.0\n",
      "    learn_time_ms: 692.521\n",
      "    load_throughput: 40137.649\n",
      "    load_time_ms: 24.914\n",
      "    sample_throughput: 40.752\n",
      "    sample_time_ms: 24538.724\n",
      "    update_time_ms: 2.499\n",
      "  timestamp: 1635072141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 390\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         9268.82</td><td style=\"text-align: right;\">390000</td><td style=\"text-align: right;\"> -3.1296</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            312.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 391000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 308.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -3.080599999999978\n",
      "  episode_reward_min: -4.869999999999941\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1253\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4201789816220602\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010338987450693003\n",
      "          policy_loss: -0.019576314008898206\n",
      "          total_loss: -0.019273383584287433\n",
      "          vf_explained_var: 0.331036239862442\n",
      "          vf_loss: 0.014268652546323007\n",
      "    num_agent_steps_sampled: 391000\n",
      "    num_agent_steps_trained: 391000\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.41612903225806\n",
      "    ram_util_percent: 38.88709677419355\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038731123011585045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.990410131812418\n",
      "    mean_inference_ms: 1.9284288944511272\n",
      "    mean_raw_obs_processing_ms: 2.0070249742101085\n",
      "  time_since_restore: 9291.060988903046\n",
      "  time_this_iter_s: 22.24125862121582\n",
      "  time_total_s: 9291.060988903046\n",
      "  timers:\n",
      "    learn_throughput: 1442.596\n",
      "    learn_time_ms: 693.195\n",
      "    load_throughput: 40338.108\n",
      "    load_time_ms: 24.79\n",
      "    sample_throughput: 41.037\n",
      "    sample_time_ms: 24368.123\n",
      "    update_time_ms: 2.51\n",
      "  timestamp: 1635072163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         9291.06</td><td style=\"text-align: right;\">391000</td><td style=\"text-align: right;\"> -3.0806</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -4.87</td><td style=\"text-align: right;\">            308.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-43-07\n",
      "  done: false\n",
      "  episode_len_mean: 303.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -3.0378999999999787\n",
      "  episode_reward_min: -3.699999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1256\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0623759388923646\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008898711544934202\n",
      "          policy_loss: -0.048281978526049193\n",
      "          total_loss: -0.048395389152897726\n",
      "          vf_explained_var: 0.5037849545478821\n",
      "          vf_loss: 0.010307170347207122\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.12\n",
      "    ram_util_percent: 38.82571428571428\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873023617206992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.991046254530808\n",
      "    mean_inference_ms: 1.9284088257098375\n",
      "    mean_raw_obs_processing_ms: 2.0066792475961712\n",
      "  time_since_restore: 9315.26838350296\n",
      "  time_this_iter_s: 24.20739459991455\n",
      "  time_total_s: 9315.26838350296\n",
      "  timers:\n",
      "    learn_throughput: 1444.288\n",
      "    learn_time_ms: 692.383\n",
      "    load_throughput: 40073.606\n",
      "    load_time_ms: 24.954\n",
      "    sample_throughput: 40.916\n",
      "    sample_time_ms: 24440.549\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1635072187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 392\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         9315.27</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\"> -3.0379</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -3.7</td><td style=\"text-align: right;\">            303.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 393000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-43-50\n",
      "  done: false\n",
      "  episode_len_mean: 300.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.209999999999997\n",
      "  episode_reward_mean: -3.00319999999998\n",
      "  episode_reward_min: -3.699999999999965\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1260\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9421282907327017\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004905544235713756\n",
      "          policy_loss: -0.02594340153866344\n",
      "          total_loss: -0.024194377660751342\n",
      "          vf_explained_var: 0.4233262240886688\n",
      "          vf_loss: 0.01105829994711611\n",
      "    num_agent_steps_sampled: 393000\n",
      "    num_agent_steps_trained: 393000\n",
      "    num_steps_sampled: 393000\n",
      "    num_steps_trained: 393000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.29672131147541\n",
      "    ram_util_percent: 38.78360655737704\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038729156664146926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.992521856470212\n",
      "    mean_inference_ms: 1.9283822989380628\n",
      "    mean_raw_obs_processing_ms: 2.0080236979696053\n",
      "  time_since_restore: 9357.838822841644\n",
      "  time_this_iter_s: 42.57043933868408\n",
      "  time_total_s: 9357.838822841644\n",
      "  timers:\n",
      "    learn_throughput: 1444.392\n",
      "    learn_time_ms: 692.333\n",
      "    load_throughput: 40448.508\n",
      "    load_time_ms: 24.723\n",
      "    sample_throughput: 37.857\n",
      "    sample_time_ms: 26415.194\n",
      "    update_time_ms: 2.515\n",
      "  timestamp: 1635072230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 393000\n",
      "  training_iteration: 393\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         9357.84</td><td style=\"text-align: right;\">393000</td><td style=\"text-align: right;\"> -3.0032</td><td style=\"text-align: right;\">               -2.21</td><td style=\"text-align: right;\">                -3.7</td><td style=\"text-align: right;\">            300.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 394000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-44-17\n",
      "  done: false\n",
      "  episode_len_mean: 296.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.209999999999997\n",
      "  episode_reward_mean: -2.9649999999999808\n",
      "  episode_reward_min: -3.659999999999966\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1264\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.011416362980162381\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9082021547688378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00336151984793776\n",
      "          policy_loss: -0.039151544206672245\n",
      "          total_loss: -0.03658765595820215\n",
      "          vf_explained_var: 0.38924577832221985\n",
      "          vf_loss: 0.011607533486353027\n",
      "    num_agent_steps_sampled: 394000\n",
      "    num_agent_steps_trained: 394000\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.28461538461539\n",
      "    ram_util_percent: 38.75897435897434\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872813570045585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.994815121445807\n",
      "    mean_inference_ms: 1.9283561756422811\n",
      "    mean_raw_obs_processing_ms: 2.009479072104253\n",
      "  time_since_restore: 9385.286574840546\n",
      "  time_this_iter_s: 27.447751998901367\n",
      "  time_total_s: 9385.286574840546\n",
      "  timers:\n",
      "    learn_throughput: 1444.068\n",
      "    learn_time_ms: 692.488\n",
      "    load_throughput: 40185.988\n",
      "    load_time_ms: 24.884\n",
      "    sample_throughput: 37.101\n",
      "    sample_time_ms: 26953.554\n",
      "    update_time_ms: 2.517\n",
      "  timestamp: 1635072257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         9385.29</td><td style=\"text-align: right;\">394000</td><td style=\"text-align: right;\">  -2.965</td><td style=\"text-align: right;\">               -2.21</td><td style=\"text-align: right;\">               -3.66</td><td style=\"text-align: right;\">             296.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 395000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-44-41\n",
      "  done: false\n",
      "  episode_len_mean: 293.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.209999999999997\n",
      "  episode_reward_mean: -2.937099999999981\n",
      "  episode_reward_min: -3.659999999999966\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1268\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9980975084834629\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011451098553479689\n",
      "          policy_loss: 0.03555857166647911\n",
      "          total_loss: 0.03753305930230352\n",
      "          vf_explained_var: 0.3960656523704529\n",
      "          vf_loss: 0.01189010012894869\n",
      "    num_agent_steps_sampled: 395000\n",
      "    num_agent_steps_trained: 395000\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.13529411764707\n",
      "    ram_util_percent: 38.8735294117647\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038727155416843996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99744547629024\n",
      "    mean_inference_ms: 1.9283301982593002\n",
      "    mean_raw_obs_processing_ms: 2.010996334857515\n",
      "  time_since_restore: 9409.020328998566\n",
      "  time_this_iter_s: 23.73375415802002\n",
      "  time_total_s: 9409.020328998566\n",
      "  timers:\n",
      "    learn_throughput: 1444.015\n",
      "    learn_time_ms: 692.514\n",
      "    load_throughput: 40544.576\n",
      "    load_time_ms: 24.664\n",
      "    sample_throughput: 39.791\n",
      "    sample_time_ms: 25131.354\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1635072281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 395\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         9409.02</td><td style=\"text-align: right;\">395000</td><td style=\"text-align: right;\"> -2.9371</td><td style=\"text-align: right;\">               -2.21</td><td style=\"text-align: right;\">               -3.66</td><td style=\"text-align: right;\">            293.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-45-08\n",
      "  done: false\n",
      "  episode_len_mean: 289.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.209999999999997\n",
      "  episode_reward_mean: -2.8980999999999826\n",
      "  episode_reward_min: -3.659999999999966\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1272\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7524757570690579\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014033644733464514\n",
      "          policy_loss: 0.03570864109529389\n",
      "          total_loss: 0.0393332721458541\n",
      "          vf_explained_var: 0.40345221757888794\n",
      "          vf_loss: 0.011069284317394098\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.27894736842105\n",
      "    ram_util_percent: 38.886842105263156\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872620726925811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.000818495064497\n",
      "    mean_inference_ms: 1.9283057752235553\n",
      "    mean_raw_obs_processing_ms: 2.011279986038588\n",
      "  time_since_restore: 9436.012833833694\n",
      "  time_this_iter_s: 26.992504835128784\n",
      "  time_total_s: 9436.012833833694\n",
      "  timers:\n",
      "    learn_throughput: 1442.827\n",
      "    learn_time_ms: 693.084\n",
      "    load_throughput: 40761.804\n",
      "    load_time_ms: 24.533\n",
      "    sample_throughput: 39.228\n",
      "    sample_time_ms: 25492.253\n",
      "    update_time_ms: 2.54\n",
      "  timestamp: 1635072308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 396\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         9436.01</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> -2.8981</td><td style=\"text-align: right;\">               -2.21</td><td style=\"text-align: right;\">               -3.66</td><td style=\"text-align: right;\">            289.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 397000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 287.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.209999999999997\n",
      "  episode_reward_mean: -2.8732999999999835\n",
      "  episode_reward_min: -3.659999999999966\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1276\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9557187729411655\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012752629020399158\n",
      "          policy_loss: 0.04535578162305885\n",
      "          total_loss: 0.053272219250599544\n",
      "          vf_explained_var: 0.1033080443739891\n",
      "          vf_loss: 0.017400829390519196\n",
      "    num_agent_steps_sampled: 397000\n",
      "    num_agent_steps_trained: 397000\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.7\n",
      "    ram_util_percent: 38.917142857142856\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038725315842686986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.004644051302602\n",
      "    mean_inference_ms: 1.9282831251955503\n",
      "    mean_raw_obs_processing_ms: 2.011246578863338\n",
      "  time_since_restore: 9460.120691537857\n",
      "  time_this_iter_s: 24.107857704162598\n",
      "  time_total_s: 9460.120691537857\n",
      "  timers:\n",
      "    learn_throughput: 1445.379\n",
      "    learn_time_ms: 691.86\n",
      "    load_throughput: 40669.278\n",
      "    load_time_ms: 24.589\n",
      "    sample_throughput: 39.275\n",
      "    sample_time_ms: 25461.646\n",
      "    update_time_ms: 2.529\n",
      "  timestamp: 1635072332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         9460.12</td><td style=\"text-align: right;\">397000</td><td style=\"text-align: right;\"> -2.8733</td><td style=\"text-align: right;\">               -2.21</td><td style=\"text-align: right;\">               -3.66</td><td style=\"text-align: right;\">            287.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 398000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-45-59\n",
      "  done: false\n",
      "  episode_len_mean: 283.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.209999999999997\n",
      "  episode_reward_mean: -2.836799999999984\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1280\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6357402834627364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005119095726114489\n",
      "          policy_loss: 0.018175813721285926\n",
      "          total_loss: 0.023778522594107523\n",
      "          vf_explained_var: 0.2870316207408905\n",
      "          vf_loss: 0.011930893299480279\n",
      "    num_agent_steps_sampled: 398000\n",
      "    num_agent_steps_trained: 398000\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.16315789473684\n",
      "    ram_util_percent: 38.91842105263158\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872447427786117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.009129807653554\n",
      "    mean_inference_ms: 1.9282623120713946\n",
      "    mean_raw_obs_processing_ms: 2.011279864121407\n",
      "  time_since_restore: 9487.154905080795\n",
      "  time_this_iter_s: 27.034213542938232\n",
      "  time_total_s: 9487.154905080795\n",
      "  timers:\n",
      "    learn_throughput: 1444.029\n",
      "    learn_time_ms: 692.507\n",
      "    load_throughput: 40960.44\n",
      "    load_time_ms: 24.414\n",
      "    sample_throughput: 38.705\n",
      "    sample_time_ms: 25836.324\n",
      "    update_time_ms: 2.472\n",
      "  timestamp: 1635072359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 398\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         9487.15</td><td style=\"text-align: right;\">398000</td><td style=\"text-align: right;\"> -2.8368</td><td style=\"text-align: right;\">               -2.21</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            283.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 399000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-46-25\n",
      "  done: false\n",
      "  episode_len_mean: 281.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.209999999999997\n",
      "  episode_reward_mean: -2.8141999999999836\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1284\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9551870286464691\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006942661850761075\n",
      "          policy_loss: 0.0171478853871425\n",
      "          total_loss: 0.01965763635105557\n",
      "          vf_explained_var: 0.2549528479576111\n",
      "          vf_loss: 0.012021994249274333\n",
      "    num_agent_steps_sampled: 399000\n",
      "    num_agent_steps_trained: 399000\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.00277777777778\n",
      "    ram_util_percent: 38.875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038723699356193256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0140793710477\n",
      "    mean_inference_ms: 1.9282426184962997\n",
      "    mean_raw_obs_processing_ms: 2.0113785304943854\n",
      "  time_since_restore: 9512.398586034775\n",
      "  time_this_iter_s: 25.243680953979492\n",
      "  time_total_s: 9512.398586034775\n",
      "  timers:\n",
      "    learn_throughput: 1442.052\n",
      "    learn_time_ms: 693.456\n",
      "    load_throughput: 40976.847\n",
      "    load_time_ms: 24.404\n",
      "    sample_throughput: 38.333\n",
      "    sample_time_ms: 26087.509\n",
      "    update_time_ms: 2.472\n",
      "  timestamp: 1635072385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 399\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          9512.4</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\"> -2.8142</td><td style=\"text-align: right;\">               -2.21</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            281.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-46-49\n",
      "  done: false\n",
      "  episode_len_mean: 279.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.209999999999997\n",
      "  episode_reward_mean: -2.797299999999985\n",
      "  episode_reward_min: -3.519999999999969\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1287\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0239967829651302\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005546492264605263\n",
      "          policy_loss: -0.04009590182039473\n",
      "          total_loss: -0.04044719744059774\n",
      "          vf_explained_var: 0.4420629143714905\n",
      "          vf_loss: 0.009857007896708738\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03055555555556\n",
      "    ram_util_percent: 38.886111111111106\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387231318806784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.018093409755814\n",
      "    mean_inference_ms: 1.9282284609450273\n",
      "    mean_raw_obs_processing_ms: 2.011498877116751\n",
      "  time_since_restore: 9536.983956575394\n",
      "  time_this_iter_s: 24.585370540618896\n",
      "  time_total_s: 9536.983956575394\n",
      "  timers:\n",
      "    learn_throughput: 1443.542\n",
      "    learn_time_ms: 692.741\n",
      "    load_throughput: 40771.828\n",
      "    load_time_ms: 24.527\n",
      "    sample_throughput: 38.324\n",
      "    sample_time_ms: 26093.154\n",
      "    update_time_ms: 2.445\n",
      "  timestamp: 1635072409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         9536.98</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\"> -2.7973</td><td style=\"text-align: right;\">               -2.21</td><td style=\"text-align: right;\">               -3.52</td><td style=\"text-align: right;\">            279.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 401000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-47-33\n",
      "  done: false\n",
      "  episode_len_mean: 275.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.759299999999984\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1291\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7705541524622176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005829027665943916\n",
      "          policy_loss: -0.09693006575107574\n",
      "          total_loss: -0.08624402284622193\n",
      "          vf_explained_var: 0.11370327323675156\n",
      "          vf_loss: 0.018358306545350287\n",
      "    num_agent_steps_sampled: 401000\n",
      "    num_agent_steps_trained: 401000\n",
      "    num_steps_sampled: 401000\n",
      "    num_steps_trained: 401000\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.16290322580646\n",
      "    ram_util_percent: 38.75645161290321\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872240452184562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.024026255711426\n",
      "    mean_inference_ms: 1.9282111849227692\n",
      "    mean_raw_obs_processing_ms: 2.0134541849726393\n",
      "  time_since_restore: 9580.580585718155\n",
      "  time_this_iter_s: 43.59662914276123\n",
      "  time_total_s: 9580.580585718155\n",
      "  timers:\n",
      "    learn_throughput: 1444.662\n",
      "    learn_time_ms: 692.203\n",
      "    load_throughput: 40714.915\n",
      "    load_time_ms: 24.561\n",
      "    sample_throughput: 35.424\n",
      "    sample_time_ms: 28229.255\n",
      "    update_time_ms: 2.405\n",
      "  timestamp: 1635072453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 401000\n",
      "  training_iteration: 401\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         9580.58</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\"> -2.7593</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            275.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 402000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-47-56\n",
      "  done: false\n",
      "  episode_len_mean: 275.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.7504999999999855\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1295\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1852373785442776\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013742206835495808\n",
      "          policy_loss: 0.008966321332587136\n",
      "          total_loss: 0.008009208738803864\n",
      "          vf_explained_var: 0.5413329005241394\n",
      "          vf_loss: 0.010816816995955176\n",
      "    num_agent_steps_sampled: 402000\n",
      "    num_agent_steps_trained: 402000\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.26363636363636\n",
      "    ram_util_percent: 38.724242424242426\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872169898723613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.030085381537386\n",
      "    mean_inference_ms: 1.9281944522245016\n",
      "    mean_raw_obs_processing_ms: 2.0154652820858194\n",
      "  time_since_restore: 9604.024379491806\n",
      "  time_this_iter_s: 23.443793773651123\n",
      "  time_total_s: 9604.024379491806\n",
      "  timers:\n",
      "    learn_throughput: 1442.058\n",
      "    learn_time_ms: 693.454\n",
      "    load_throughput: 40727.092\n",
      "    load_time_ms: 24.554\n",
      "    sample_throughput: 35.522\n",
      "    sample_time_ms: 28151.788\n",
      "    update_time_ms: 2.375\n",
      "  timestamp: 1635072476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 402\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         9604.02</td><td style=\"text-align: right;\">402000</td><td style=\"text-align: right;\"> -2.7505</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            275.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 403000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 274.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.7406999999999844\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1298\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0762152598963843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018831214560424157\n",
      "          policy_loss: -0.08502395794623428\n",
      "          total_loss: -0.08180487847162618\n",
      "          vf_explained_var: 0.3683890402317047\n",
      "          vf_loss: 0.013873740006238222\n",
      "    num_agent_steps_sampled: 403000\n",
      "    num_agent_steps_trained: 403000\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.817647058823525\n",
      "    ram_util_percent: 38.82941176470588\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872115472241323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.034747809541294\n",
      "    mean_inference_ms: 1.9281818193645746\n",
      "    mean_raw_obs_processing_ms: 2.016485645276398\n",
      "  time_since_restore: 9627.523881196976\n",
      "  time_this_iter_s: 23.499501705169678\n",
      "  time_total_s: 9627.523881196976\n",
      "  timers:\n",
      "    learn_throughput: 1443.003\n",
      "    learn_time_ms: 692.999\n",
      "    load_throughput: 40894.585\n",
      "    load_time_ms: 24.453\n",
      "    sample_throughput: 38.102\n",
      "    sample_time_ms: 26245.236\n",
      "    update_time_ms: 2.382\n",
      "  timestamp: 1635072500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         9627.52</td><td style=\"text-align: right;\">403000</td><td style=\"text-align: right;\"> -2.7407</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            274.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-48-44\n",
      "  done: false\n",
      "  episode_len_mean: 274.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.740399999999985\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1302\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8489760955174764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006955996746215042\n",
      "          policy_loss: 0.034265971183776854\n",
      "          total_loss: 0.03645069640543726\n",
      "          vf_explained_var: 0.33078083395957947\n",
      "          vf_loss: 0.010634781875544124\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.25882352941177\n",
      "    ram_util_percent: 38.891176470588235\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872043649407922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.040998279665395\n",
      "    mean_inference_ms: 1.9281643433574664\n",
      "    mean_raw_obs_processing_ms: 2.0166375737231634\n",
      "  time_since_restore: 9651.353648662567\n",
      "  time_this_iter_s: 23.82976746559143\n",
      "  time_total_s: 9651.353648662567\n",
      "  timers:\n",
      "    learn_throughput: 1443.615\n",
      "    learn_time_ms: 692.705\n",
      "    load_throughput: 40999.438\n",
      "    load_time_ms: 24.391\n",
      "    sample_throughput: 38.634\n",
      "    sample_time_ms: 25883.804\n",
      "    update_time_ms: 2.388\n",
      "  timestamp: 1635072524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 404\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         9651.35</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\"> -2.7404</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            274.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-49-07\n",
      "  done: false\n",
      "  episode_len_mean: 274.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.741499999999985\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1306\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9307263996866014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01249938685624667\n",
      "          policy_loss: -0.015728512530525526\n",
      "          total_loss: -0.01014219613538848\n",
      "          vf_explained_var: 0.11661358922719955\n",
      "          vf_loss: 0.01482223230931494\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.02424242424242\n",
      "    ram_util_percent: 38.945454545454545\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038719720965969005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.047204374099948\n",
      "    mean_inference_ms: 1.928146214335789\n",
      "    mean_raw_obs_processing_ms: 2.0168028046199367\n",
      "  time_since_restore: 9674.664850711823\n",
      "  time_this_iter_s: 23.31120204925537\n",
      "  time_total_s: 9674.664850711823\n",
      "  timers:\n",
      "    learn_throughput: 1445.33\n",
      "    learn_time_ms: 691.883\n",
      "    load_throughput: 41122.925\n",
      "    load_time_ms: 24.317\n",
      "    sample_throughput: 38.696\n",
      "    sample_time_ms: 25842.464\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1635072547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 405\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         9674.66</td><td style=\"text-align: right;\">405000</td><td style=\"text-align: right;\"> -2.7415</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            274.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 406000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-49-33\n",
      "  done: false\n",
      "  episode_len_mean: 272.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.7264999999999855\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1310\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.755102362897661\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00849626818766913\n",
      "          policy_loss: 0.005668088379833434\n",
      "          total_loss: 0.011618792389829954\n",
      "          vf_explained_var: 0.15540118515491486\n",
      "          vf_loss: 0.013453228730294439\n",
      "    num_agent_steps_sampled: 406000\n",
      "    num_agent_steps_trained: 406000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24054054054054\n",
      "    ram_util_percent: 38.94324324324325\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038718978863360405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.053587689623022\n",
      "    mean_inference_ms: 1.9281285593045427\n",
      "    mean_raw_obs_processing_ms: 2.017029863137856\n",
      "  time_since_restore: 9700.679995298386\n",
      "  time_this_iter_s: 26.01514458656311\n",
      "  time_total_s: 9700.679995298386\n",
      "  timers:\n",
      "    learn_throughput: 1446.949\n",
      "    learn_time_ms: 691.109\n",
      "    load_throughput: 41193.928\n",
      "    load_time_ms: 24.275\n",
      "    sample_throughput: 38.842\n",
      "    sample_time_ms: 25745.488\n",
      "    update_time_ms: 2.396\n",
      "  timestamp: 1635072573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         9700.68</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\"> -2.7265</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            272.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 407000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-49-57\n",
      "  done: false\n",
      "  episode_len_mean: 272.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.727099999999985\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1313\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0053657750288645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013627373198473415\n",
      "          policy_loss: 0.02108829559551345\n",
      "          total_loss: 0.020985168798102274\n",
      "          vf_explained_var: 0.17422305047512054\n",
      "          vf_loss: 0.009872741360838214\n",
      "    num_agent_steps_sampled: 407000\n",
      "    num_agent_steps_trained: 407000\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.976470588235294\n",
      "    ram_util_percent: 38.91470588235294\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871841887045463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.058388417931784\n",
      "    mean_inference_ms: 1.9281153501984591\n",
      "    mean_raw_obs_processing_ms: 2.0171968046556565\n",
      "  time_since_restore: 9724.194303274155\n",
      "  time_this_iter_s: 23.514307975769043\n",
      "  time_total_s: 9724.194303274155\n",
      "  timers:\n",
      "    learn_throughput: 1447.195\n",
      "    learn_time_ms: 690.992\n",
      "    load_throughput: 41428.948\n",
      "    load_time_ms: 24.138\n",
      "    sample_throughput: 38.931\n",
      "    sample_time_ms: 25686.409\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1635072597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 407\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         9724.19</td><td style=\"text-align: right;\">407000</td><td style=\"text-align: right;\"> -2.7271</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            272.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-50-20\n",
      "  done: false\n",
      "  episode_len_mean: 272.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.725899999999987\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1317\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1828201439645556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011919181869867372\n",
      "          policy_loss: 0.027631825953722\n",
      "          total_loss: 0.026877569158871968\n",
      "          vf_explained_var: 0.2822227478027344\n",
      "          vf_loss: 0.011005907019393312\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.93333333333333\n",
      "    ram_util_percent: 38.89393939393939\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871773010473131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.064726315014692\n",
      "    mean_inference_ms: 1.928097664138706\n",
      "    mean_raw_obs_processing_ms: 2.017447926026102\n",
      "  time_since_restore: 9747.58582520485\n",
      "  time_this_iter_s: 23.39152193069458\n",
      "  time_total_s: 9747.58582520485\n",
      "  timers:\n",
      "    learn_throughput: 1447.477\n",
      "    learn_time_ms: 690.857\n",
      "    load_throughput: 41331.173\n",
      "    load_time_ms: 24.195\n",
      "    sample_throughput: 39.491\n",
      "    sample_time_ms: 25322.284\n",
      "    update_time_ms: 2.36\n",
      "  timestamp: 1635072620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 408\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         9747.59</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\"> -2.7259</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            272.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 409000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-51-04\n",
      "  done: false\n",
      "  episode_len_mean: 271.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.713599999999986\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1321\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9292523211903042\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014857356896035531\n",
      "          policy_loss: -0.0026714006231890785\n",
      "          total_loss: 0.00026478411422835454\n",
      "          vf_explained_var: 0.35162970423698425\n",
      "          vf_loss: 0.012143901021530231\n",
      "    num_agent_steps_sampled: 409000\n",
      "    num_agent_steps_trained: 409000\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.50793650793651\n",
      "    ram_util_percent: 38.815873015873024\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871707066156702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.071346565739997\n",
      "    mean_inference_ms: 1.9280803269219904\n",
      "    mean_raw_obs_processing_ms: 2.019369771022201\n",
      "  time_since_restore: 9791.32108259201\n",
      "  time_this_iter_s: 43.735257387161255\n",
      "  time_total_s: 9791.32108259201\n",
      "  timers:\n",
      "    learn_throughput: 1447.387\n",
      "    learn_time_ms: 690.9\n",
      "    load_throughput: 41298.86\n",
      "    load_time_ms: 24.214\n",
      "    sample_throughput: 36.803\n",
      "    sample_time_ms: 27171.363\n",
      "    update_time_ms: 2.371\n",
      "  timestamp: 1635072664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         9791.32</td><td style=\"text-align: right;\">409000</td><td style=\"text-align: right;\"> -2.7136</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            271.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 410000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-51-28\n",
      "  done: false\n",
      "  episode_len_mean: 270.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.707799999999986\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1324\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005708181490081191\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.055629732211431\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0048593215059183435\n",
      "          policy_loss: -0.07857259958982468\n",
      "          total_loss: -0.0761948849591944\n",
      "          vf_explained_var: 0.2956690490245819\n",
      "          vf_loss: 0.012906275720645984\n",
      "    num_agent_steps_sampled: 410000\n",
      "    num_agent_steps_trained: 410000\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.64857142857143\n",
      "    ram_util_percent: 38.67142857142857\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871664010492624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.076387883752613\n",
      "    mean_inference_ms: 1.9280676478180032\n",
      "    mean_raw_obs_processing_ms: 2.0208519565606498\n",
      "  time_since_restore: 9815.828392744064\n",
      "  time_this_iter_s: 24.507310152053833\n",
      "  time_total_s: 9815.828392744064\n",
      "  timers:\n",
      "    learn_throughput: 1446.525\n",
      "    learn_time_ms: 691.312\n",
      "    load_throughput: 41370.554\n",
      "    load_time_ms: 24.172\n",
      "    sample_throughput: 36.815\n",
      "    sample_time_ms: 27163.194\n",
      "    update_time_ms: 2.354\n",
      "  timestamp: 1635072688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 410\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">         9815.83</td><td style=\"text-align: right;\">410000</td><td style=\"text-align: right;\"> -2.7078</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            270.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 411000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-51-53\n",
      "  done: false\n",
      "  episode_len_mean: 269.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.6960999999999866\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1328\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0028540907450405953\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2595676316155329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020973426050827883\n",
      "          policy_loss: 0.04815684403810236\n",
      "          total_loss: 0.045618415416942705\n",
      "          vf_explained_var: 0.42378950119018555\n",
      "          vf_loss: 0.009997387752971716\n",
      "    num_agent_steps_sampled: 411000\n",
      "    num_agent_steps_trained: 411000\n",
      "    num_steps_sampled: 411000\n",
      "    num_steps_trained: 411000\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11470588235294\n",
      "    ram_util_percent: 38.76470588235294\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871605931161441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.083304475037398\n",
      "    mean_inference_ms: 1.9280499848698287\n",
      "    mean_raw_obs_processing_ms: 2.022836088330338\n",
      "  time_since_restore: 9840.0573990345\n",
      "  time_this_iter_s: 24.22900629043579\n",
      "  time_total_s: 9840.0573990345\n",
      "  timers:\n",
      "    learn_throughput: 1450.15\n",
      "    learn_time_ms: 689.584\n",
      "    load_throughput: 41329.463\n",
      "    load_time_ms: 24.196\n",
      "    sample_throughput: 39.638\n",
      "    sample_time_ms: 25228.099\n",
      "    update_time_ms: 2.372\n",
      "  timestamp: 1635072713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411000\n",
      "  training_iteration: 411\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         9840.06</td><td style=\"text-align: right;\">411000</td><td style=\"text-align: right;\"> -2.6961</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            269.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-52-17\n",
      "  done: false\n",
      "  episode_len_mean: 269.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.697899999999986\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1332\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0042811361175608895\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4079207075966729\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0202225325389468\n",
      "          policy_loss: 0.005427876611550649\n",
      "          total_loss: 0.00431355159315798\n",
      "          vf_explained_var: 0.25132590532302856\n",
      "          vf_loss: 0.012878306349739433\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.87714285714286\n",
      "    ram_util_percent: 38.88285714285715\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871551907529186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.090058900212796\n",
      "    mean_inference_ms: 1.9280332270225125\n",
      "    mean_raw_obs_processing_ms: 2.023146181501251\n",
      "  time_since_restore: 9864.508443117142\n",
      "  time_this_iter_s: 24.4510440826416\n",
      "  time_total_s: 9864.508443117142\n",
      "  timers:\n",
      "    learn_throughput: 1450.918\n",
      "    learn_time_ms: 689.219\n",
      "    load_throughput: 41426.698\n",
      "    load_time_ms: 24.139\n",
      "    sample_throughput: 39.48\n",
      "    sample_time_ms: 25329.23\n",
      "    update_time_ms: 2.389\n",
      "  timestamp: 1635072737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         9864.51</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\"> -2.6979</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            269.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 413000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 270.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.703999999999986\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1335\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3330050044589572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014344779870698323\n",
      "          policy_loss: 0.05476146737734477\n",
      "          total_loss: 0.050749592648612125\n",
      "          vf_explained_var: 0.2618948519229889\n",
      "          vf_loss: 0.009226054593455046\n",
      "    num_agent_steps_sampled: 413000\n",
      "    num_agent_steps_trained: 413000\n",
      "    num_steps_sampled: 413000\n",
      "    num_steps_trained: 413000\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.96875\n",
      "    ram_util_percent: 38.95\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871513348094208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.095040290819444\n",
      "    mean_inference_ms: 1.9280220355839657\n",
      "    mean_raw_obs_processing_ms: 2.023350508216717\n",
      "  time_since_restore: 9886.593773126602\n",
      "  time_this_iter_s: 22.08533000946045\n",
      "  time_total_s: 9886.593773126602\n",
      "  timers:\n",
      "    learn_throughput: 1453.661\n",
      "    learn_time_ms: 687.918\n",
      "    load_throughput: 41098.829\n",
      "    load_time_ms: 24.332\n",
      "    sample_throughput: 39.7\n",
      "    sample_time_ms: 25188.926\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1635072759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 413000\n",
      "  training_iteration: 413\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         9886.59</td><td style=\"text-align: right;\">413000</td><td style=\"text-align: right;\">  -2.704</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">             270.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 414000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-53-04\n",
      "  done: false\n",
      "  episode_len_mean: 270.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.7016999999999864\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1339\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2078090647856394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073191113138562575\n",
      "          policy_loss: 0.004955859772033161\n",
      "          total_loss: 0.0060559951596789886\n",
      "          vf_explained_var: 0.22718358039855957\n",
      "          vf_loss: 0.013131227209750148\n",
      "    num_agent_steps_sampled: 414000\n",
      "    num_agent_steps_trained: 414000\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.919444444444444\n",
      "    ram_util_percent: 38.94444444444445\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871462603080969\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.10171557661785\n",
      "    mean_inference_ms: 1.9280070702820193\n",
      "    mean_raw_obs_processing_ms: 2.0236339872897364\n",
      "  time_since_restore: 9911.680098295212\n",
      "  time_this_iter_s: 25.08632516860962\n",
      "  time_total_s: 9911.680098295212\n",
      "  timers:\n",
      "    learn_throughput: 1454.918\n",
      "    learn_time_ms: 687.324\n",
      "    load_throughput: 41305.449\n",
      "    load_time_ms: 24.21\n",
      "    sample_throughput: 39.502\n",
      "    sample_time_ms: 25315.263\n",
      "    update_time_ms: 2.414\n",
      "  timestamp: 1635072784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 414\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         9911.68</td><td style=\"text-align: right;\">414000</td><td style=\"text-align: right;\"> -2.7017</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            270.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 415000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-53-28\n",
      "  done: false\n",
      "  episode_len_mean: 269.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.698299999999987\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1342\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2651990208360884\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005239903858538695\n",
      "          policy_loss: 0.002348802155918545\n",
      "          total_loss: -0.001782724509636561\n",
      "          vf_explained_var: 0.4236849844455719\n",
      "          vf_loss: 0.008486815399697258\n",
      "    num_agent_steps_sampled: 415000\n",
      "    num_agent_steps_trained: 415000\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.09090909090909\n",
      "    ram_util_percent: 38.92727272727273\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871425440051353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.10671937802404\n",
      "    mean_inference_ms: 1.9279961305982232\n",
      "    mean_raw_obs_processing_ms: 2.02385506413753\n",
      "  time_since_restore: 9935.190916776657\n",
      "  time_this_iter_s: 23.510818481445312\n",
      "  time_total_s: 9935.190916776657\n",
      "  timers:\n",
      "    learn_throughput: 1453.349\n",
      "    learn_time_ms: 688.066\n",
      "    load_throughput: 41420.561\n",
      "    load_time_ms: 24.143\n",
      "    sample_throughput: 39.472\n",
      "    sample_time_ms: 25334.536\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1635072808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         9935.19</td><td style=\"text-align: right;\">415000</td><td style=\"text-align: right;\"> -2.6983</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            269.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-53-53\n",
      "  done: false\n",
      "  episode_len_mean: 269.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.6930999999999856\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1346\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2461420178413392\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008709725542860584\n",
      "          policy_loss: -0.008678105970223745\n",
      "          total_loss: -0.007849818136956956\n",
      "          vf_explained_var: 0.3005574941635132\n",
      "          vf_loss: 0.013233775821410948\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.94722222222222\n",
      "    ram_util_percent: 38.90833333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871379892310665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.113590180904893\n",
      "    mean_inference_ms: 1.9279824634827918\n",
      "    mean_raw_obs_processing_ms: 2.024159979742093\n",
      "  time_since_restore: 9960.210911989212\n",
      "  time_this_iter_s: 25.01999521255493\n",
      "  time_total_s: 9960.210911989212\n",
      "  timers:\n",
      "    learn_throughput: 1453.142\n",
      "    learn_time_ms: 688.164\n",
      "    load_throughput: 41400.364\n",
      "    load_time_ms: 24.154\n",
      "    sample_throughput: 39.628\n",
      "    sample_time_ms: 25234.957\n",
      "    update_time_ms: 2.412\n",
      "  timestamp: 1635072833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 416\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         9960.21</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\"> -2.6931</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            269.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 417000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-54-34\n",
      "  done: false\n",
      "  episode_len_mean: 269.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.6906999999999868\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1350\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5042781286769442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014051988935631322\n",
      "          policy_loss: -0.021880722625388038\n",
      "          total_loss: -0.02497182654009925\n",
      "          vf_explained_var: 0.4688500165939331\n",
      "          vf_loss: 0.011861439576993387\n",
      "    num_agent_steps_sampled: 417000\n",
      "    num_agent_steps_trained: 417000\n",
      "    num_steps_sampled: 417000\n",
      "    num_steps_trained: 417000\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.039655172413795\n",
      "    ram_util_percent: 38.88103448275862\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871338324435698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.12036206234413\n",
      "    mean_inference_ms: 1.9279691710911686\n",
      "    mean_raw_obs_processing_ms: 2.026142744094265\n",
      "  time_since_restore: 10001.0300385952\n",
      "  time_this_iter_s: 40.81912660598755\n",
      "  time_total_s: 10001.0300385952\n",
      "  timers:\n",
      "    learn_throughput: 1452.635\n",
      "    learn_time_ms: 688.404\n",
      "    load_throughput: 41294.794\n",
      "    load_time_ms: 24.216\n",
      "    sample_throughput: 37.085\n",
      "    sample_time_ms: 26965.119\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1635072874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417000\n",
      "  training_iteration: 417\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">           10001</td><td style=\"text-align: right;\">417000</td><td style=\"text-align: right;\"> -2.6907</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            269.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 418000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-55-01\n",
      "  done: false\n",
      "  episode_len_mean: 268.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.682099999999987\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1353\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1865268561575149\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009643996017174474\n",
      "          policy_loss: -0.09903755953742398\n",
      "          total_loss: -0.10214569779733816\n",
      "          vf_explained_var: 0.5368359088897705\n",
      "          vf_loss: 0.008695199599282609\n",
      "    num_agent_steps_sampled: 418000\n",
      "    num_agent_steps_trained: 418000\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18974358974359\n",
      "    ram_util_percent: 38.78205128205129\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038713081311461436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.125776848184888\n",
      "    mean_inference_ms: 1.9279595598725074\n",
      "    mean_raw_obs_processing_ms: 2.027599421211528\n",
      "  time_since_restore: 10028.041088104248\n",
      "  time_this_iter_s: 27.011049509048462\n",
      "  time_total_s: 10028.041088104248\n",
      "  timers:\n",
      "    learn_throughput: 1452.661\n",
      "    learn_time_ms: 688.392\n",
      "    load_throughput: 41134.218\n",
      "    load_time_ms: 24.311\n",
      "    sample_throughput: 36.594\n",
      "    sample_time_ms: 27327.019\n",
      "    update_time_ms: 2.403\n",
      "  timestamp: 1635072901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">           10028</td><td style=\"text-align: right;\">418000</td><td style=\"text-align: right;\"> -2.6821</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            268.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 419000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-55-27\n",
      "  done: false\n",
      "  episode_len_mean: 267.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.671399999999987\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1357\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9693424827522702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008296236714194234\n",
      "          policy_loss: -0.09905553687777784\n",
      "          total_loss: -0.09693645276129245\n",
      "          vf_explained_var: 0.4758948087692261\n",
      "          vf_loss: 0.011759232139835755\n",
      "    num_agent_steps_sampled: 419000\n",
      "    num_agent_steps_trained: 419000\n",
      "    num_steps_sampled: 419000\n",
      "    num_steps_trained: 419000\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.20810810810811\n",
      "    ram_util_percent: 38.88378378378379\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871267373232842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.133040362758575\n",
      "    mean_inference_ms: 1.9279472027224767\n",
      "    mean_raw_obs_processing_ms: 2.0291702541678625\n",
      "  time_since_restore: 10054.10728430748\n",
      "  time_this_iter_s: 26.06619620323181\n",
      "  time_total_s: 10054.10728430748\n",
      "  timers:\n",
      "    learn_throughput: 1452.783\n",
      "    learn_time_ms: 688.334\n",
      "    load_throughput: 40971.843\n",
      "    load_time_ms: 24.407\n",
      "    sample_throughput: 39.124\n",
      "    sample_time_ms: 25560.071\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1635072927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419000\n",
      "  training_iteration: 419\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         10054.1</td><td style=\"text-align: right;\">419000</td><td style=\"text-align: right;\"> -2.6714</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            267.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-55-52\n",
      "  done: false\n",
      "  episode_len_mean: 267.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.677399999999987\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1361\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2644075353940327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007012907579932994\n",
      "          policy_loss: -0.045097239232725565\n",
      "          total_loss: -0.04671759481231372\n",
      "          vf_explained_var: 0.32326653599739075\n",
      "          vf_loss: 0.010978687740862369\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.083333333333336\n",
      "    ram_util_percent: 38.86388888888889\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038712238022515624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.14014977354535\n",
      "    mean_inference_ms: 1.9279353598659759\n",
      "    mean_raw_obs_processing_ms: 2.029471614889877\n",
      "  time_since_restore: 10079.198604106903\n",
      "  time_this_iter_s: 25.091319799423218\n",
      "  time_total_s: 10079.198604106903\n",
      "  timers:\n",
      "    learn_throughput: 1452.374\n",
      "    learn_time_ms: 688.528\n",
      "    load_throughput: 41054.055\n",
      "    load_time_ms: 24.358\n",
      "    sample_throughput: 39.035\n",
      "    sample_time_ms: 25618.328\n",
      "    update_time_ms: 2.406\n",
      "  timestamp: 1635072952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 420\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         10079.2</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\"> -2.6774</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">            267.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 421000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-56-16\n",
      "  done: false\n",
      "  episode_len_mean: 268.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.6839999999999877\n",
      "  episode_reward_min: -3.3499999999999726\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1365\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.310012854470147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009516669091944902\n",
      "          policy_loss: 0.015850423359208637\n",
      "          total_loss: 0.017231497334109412\n",
      "          vf_explained_var: 0.05003175884485245\n",
      "          vf_loss: 0.014420085990180572\n",
      "    num_agent_steps_sampled: 421000\n",
      "    num_agent_steps_trained: 421000\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06470588235294\n",
      "    ram_util_percent: 38.932352941176475\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871181543192003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.146968670524362\n",
      "    mean_inference_ms: 1.927923796443562\n",
      "    mean_raw_obs_processing_ms: 2.029783623236987\n",
      "  time_since_restore: 10103.147252321243\n",
      "  time_this_iter_s: 23.94864821434021\n",
      "  time_total_s: 10103.147252321243\n",
      "  timers:\n",
      "    learn_throughput: 1449.794\n",
      "    learn_time_ms: 689.753\n",
      "    load_throughput: 40949.043\n",
      "    load_time_ms: 24.421\n",
      "    sample_throughput: 39.079\n",
      "    sample_time_ms: 25588.996\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1635072976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         10103.1</td><td style=\"text-align: right;\">421000</td><td style=\"text-align: right;\">  -2.684</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">             268.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 422000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-56-41\n",
      "  done: false\n",
      "  episode_len_mean: 267.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.6786999999999863\n",
      "  episode_reward_min: -3.239999999999975\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1369\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1374890844027201\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0052859463216871445\n",
      "          policy_loss: 0.005059623759653833\n",
      "          total_loss: 0.005407387597693337\n",
      "          vf_explained_var: 0.22983907163143158\n",
      "          vf_loss: 0.011688710428360435\n",
      "    num_agent_steps_sampled: 422000\n",
      "    num_agent_steps_trained: 422000\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.465714285714284\n",
      "    ram_util_percent: 38.980000000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871137293870213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.15377866945885\n",
      "    mean_inference_ms: 1.9279128010416489\n",
      "    mean_raw_obs_processing_ms: 2.030105898587237\n",
      "  time_since_restore: 10127.93590092659\n",
      "  time_this_iter_s: 24.78864860534668\n",
      "  time_total_s: 10127.93590092659\n",
      "  timers:\n",
      "    learn_throughput: 1450.531\n",
      "    learn_time_ms: 689.403\n",
      "    load_throughput: 41018.403\n",
      "    load_time_ms: 24.379\n",
      "    sample_throughput: 39.027\n",
      "    sample_time_ms: 25623.158\n",
      "    update_time_ms: 2.406\n",
      "  timestamp: 1635073001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 422\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         10127.9</td><td style=\"text-align: right;\">422000</td><td style=\"text-align: right;\"> -2.6787</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.24</td><td style=\"text-align: right;\">            267.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 423000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 269.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.6909999999999865\n",
      "  episode_reward_min: -3.239999999999975\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1372\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1733727998203702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010060059003266985\n",
      "          policy_loss: -0.02550011765625742\n",
      "          total_loss: -0.030438835836119123\n",
      "          vf_explained_var: 0.3356328010559082\n",
      "          vf_loss: 0.006730403800288008\n",
      "    num_agent_steps_sampled: 423000\n",
      "    num_agent_steps_trained: 423000\n",
      "    num_steps_sampled: 423000\n",
      "    num_steps_trained: 423000\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.93055555555556\n",
      "    ram_util_percent: 38.927777777777784\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387110350400332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.158787355416358\n",
      "    mean_inference_ms: 1.9279042374592366\n",
      "    mean_raw_obs_processing_ms: 2.0303200143728533\n",
      "  time_since_restore: 10152.68598818779\n",
      "  time_this_iter_s: 24.75008726119995\n",
      "  time_total_s: 10152.68598818779\n",
      "  timers:\n",
      "    learn_throughput: 1448.059\n",
      "    learn_time_ms: 690.58\n",
      "    load_throughput: 41006.212\n",
      "    load_time_ms: 24.387\n",
      "    sample_throughput: 38.627\n",
      "    sample_time_ms: 25888.444\n",
      "    update_time_ms: 2.385\n",
      "  timestamp: 1635073025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423000\n",
      "  training_iteration: 423\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         10152.7</td><td style=\"text-align: right;\">423000</td><td style=\"text-align: right;\">  -2.691</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.24</td><td style=\"text-align: right;\">             269.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 269.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.690999999999986\n",
      "  episode_reward_min: -3.239999999999975\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1376\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0422648045751783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013097119309124829\n",
      "          policy_loss: 0.03291356505619155\n",
      "          total_loss: 0.03601310526331266\n",
      "          vf_explained_var: 0.2705267667770386\n",
      "          vf_loss: 0.01343808039608929\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.13333333333333\n",
      "    ram_util_percent: 38.90909090909091\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871058086331735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.165358601750594\n",
      "    mean_inference_ms: 1.927893241958289\n",
      "    mean_raw_obs_processing_ms: 2.0306163517525393\n",
      "  time_since_restore: 10175.971236467361\n",
      "  time_this_iter_s: 23.285248279571533\n",
      "  time_total_s: 10175.971236467361\n",
      "  timers:\n",
      "    learn_throughput: 1446.926\n",
      "    learn_time_ms: 691.12\n",
      "    load_throughput: 41244.807\n",
      "    load_time_ms: 24.245\n",
      "    sample_throughput: 38.898\n",
      "    sample_time_ms: 25707.952\n",
      "    update_time_ms: 2.367\n",
      "  timestamp: 1635073049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">           10176</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">  -2.691</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.24</td><td style=\"text-align: right;\">             269.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 425000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-58-12\n",
      "  done: false\n",
      "  episode_len_mean: 269.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.693899999999987\n",
      "  episode_reward_min: -3.239999999999975\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1380\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9361915462546878\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008784126088842785\n",
      "          policy_loss: 0.020153066598706776\n",
      "          total_loss: 0.024061126841439143\n",
      "          vf_explained_var: 0.21376043558120728\n",
      "          vf_loss: 0.013213565645532475\n",
      "    num_agent_steps_sampled: 425000\n",
      "    num_agent_steps_trained: 425000\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.587096774193554\n",
      "    ram_util_percent: 38.835483870967735\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038710136936238836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.171801003919764\n",
      "    mean_inference_ms: 1.9278822851404562\n",
      "    mean_raw_obs_processing_ms: 2.0325137595183365\n",
      "  time_since_restore: 10219.043058156967\n",
      "  time_this_iter_s: 43.07182168960571\n",
      "  time_total_s: 10219.043058156967\n",
      "  timers:\n",
      "    learn_throughput: 1444.82\n",
      "    learn_time_ms: 692.128\n",
      "    load_throughput: 41735.946\n",
      "    load_time_ms: 23.96\n",
      "    sample_throughput: 36.149\n",
      "    sample_time_ms: 27663.335\n",
      "    update_time_ms: 2.36\n",
      "  timestamp: 1635073092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 425\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">           10219</td><td style=\"text-align: right;\">425000</td><td style=\"text-align: right;\"> -2.6939</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.24</td><td style=\"text-align: right;\">            269.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 426000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 269.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.692499999999987\n",
      "  episode_reward_min: -3.239999999999975\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1384\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0720935185750327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009229668912337551\n",
      "          policy_loss: 0.029055463605456883\n",
      "          total_loss: 0.03339365306827757\n",
      "          vf_explained_var: 0.17795021831989288\n",
      "          vf_loss: 0.014999854440490405\n",
      "    num_agent_steps_sampled: 426000\n",
      "    num_agent_steps_trained: 426000\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.79459459459461\n",
      "    ram_util_percent: 38.70270270270269\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038709688467388884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.178320298491645\n",
      "    mean_inference_ms: 1.927871389857881\n",
      "    mean_raw_obs_processing_ms: 2.034417604885278\n",
      "  time_since_restore: 10245.480279684067\n",
      "  time_this_iter_s: 26.43722152709961\n",
      "  time_total_s: 10245.480279684067\n",
      "  timers:\n",
      "    learn_throughput: 1444.1\n",
      "    learn_time_ms: 692.473\n",
      "    load_throughput: 41681.282\n",
      "    load_time_ms: 23.992\n",
      "    sample_throughput: 35.965\n",
      "    sample_time_ms: 27804.696\n",
      "    update_time_ms: 2.351\n",
      "  timestamp: 1635073118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 426\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         10245.5</td><td style=\"text-align: right;\">426000</td><td style=\"text-align: right;\"> -2.6925</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.24</td><td style=\"text-align: right;\">            269.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 427000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-59-04\n",
      "  done: false\n",
      "  episode_len_mean: 268.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.6870999999999863\n",
      "  episode_reward_min: -3.239999999999975\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1388\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.963676451974445\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009499573320220042\n",
      "          policy_loss: 0.0035381831228733064\n",
      "          total_loss: 0.008137757082780202\n",
      "          vf_explained_var: 0.25611960887908936\n",
      "          vf_loss: 0.014175336569961575\n",
      "    num_agent_steps_sampled: 427000\n",
      "    num_agent_steps_trained: 427000\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11081081081081\n",
      "    ram_util_percent: 38.81891891891892\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870924868272865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.184821750843188\n",
      "    mean_inference_ms: 1.9278610448455549\n",
      "    mean_raw_obs_processing_ms: 2.035942545785996\n",
      "  time_since_restore: 10271.365066289902\n",
      "  time_this_iter_s: 25.88478660583496\n",
      "  time_total_s: 10271.365066289902\n",
      "  timers:\n",
      "    learn_throughput: 1442.177\n",
      "    learn_time_ms: 693.396\n",
      "    load_throughput: 41987.547\n",
      "    load_time_ms: 23.817\n",
      "    sample_throughput: 38.008\n",
      "    sample_time_ms: 26310.544\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1635073144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         10271.4</td><td style=\"text-align: right;\">427000</td><td style=\"text-align: right;\"> -2.6871</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.24</td><td style=\"text-align: right;\">            268.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-59-30\n",
      "  done: false\n",
      "  episode_len_mean: 269.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.6908999999999863\n",
      "  episode_reward_min: -3.239999999999975\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1392\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9593803677293989\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01586974683723123\n",
      "          policy_loss: 0.0017647960119777256\n",
      "          total_loss: 0.0057004724939664205\n",
      "          vf_explained_var: 0.28173673152923584\n",
      "          vf_loss: 0.013427567513038715\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03783783783785\n",
      "    ram_util_percent: 38.891891891891895\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870876926412196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.191302687493565\n",
      "    mean_inference_ms: 1.9278511529086761\n",
      "    mean_raw_obs_processing_ms: 2.0361758590829844\n",
      "  time_since_restore: 10297.08507847786\n",
      "  time_this_iter_s: 25.720012187957764\n",
      "  time_total_s: 10297.08507847786\n",
      "  timers:\n",
      "    learn_throughput: 1440.739\n",
      "    learn_time_ms: 694.088\n",
      "    load_throughput: 41801.625\n",
      "    load_time_ms: 23.923\n",
      "    sample_throughput: 38.197\n",
      "    sample_time_ms: 26180.379\n",
      "    update_time_ms: 2.377\n",
      "  timestamp: 1635073170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 428\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">         10297.1</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\"> -2.6909</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.24</td><td style=\"text-align: right;\">            269.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 429000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_10-59-55\n",
      "  done: false\n",
      "  episode_len_mean: 268.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.6866999999999863\n",
      "  episode_reward_min: -3.239999999999975\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1395\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006421704176341336\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.181883015235265\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020616875779001495\n",
      "          policy_loss: -0.09948134678933356\n",
      "          total_loss: -0.09623530200786061\n",
      "          vf_explained_var: 0.25823143124580383\n",
      "          vf_loss: 0.01493247886084848\n",
      "    num_agent_steps_sampled: 429000\n",
      "    num_agent_steps_trained: 429000\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.886111111111106\n",
      "    ram_util_percent: 38.925000000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038708403905785206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.196278451087313\n",
      "    mean_inference_ms: 1.9278440230911063\n",
      "    mean_raw_obs_processing_ms: 2.0363238093080365\n",
      "  time_since_restore: 10322.045642852783\n",
      "  time_this_iter_s: 24.960564374923706\n",
      "  time_total_s: 10322.045642852783\n",
      "  timers:\n",
      "    learn_throughput: 1441.218\n",
      "    learn_time_ms: 693.858\n",
      "    load_throughput: 41833.645\n",
      "    load_time_ms: 23.904\n",
      "    sample_throughput: 38.358\n",
      "    sample_time_ms: 26070.098\n",
      "    update_time_ms: 2.368\n",
      "  timestamp: 1635073195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 429\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">           10322</td><td style=\"text-align: right;\">429000</td><td style=\"text-align: right;\"> -2.6867</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.24</td><td style=\"text-align: right;\">            268.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 430000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-00-22\n",
      "  done: false\n",
      "  episode_len_mean: 266.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.669599999999987\n",
      "  episode_reward_min: -3.239999999999975\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1400\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0353571626875135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009420777342105068\n",
      "          policy_loss: -0.01338161937892437\n",
      "          total_loss: -0.006954698719912105\n",
      "          vf_explained_var: 0.17235437035560608\n",
      "          vf_loss: 0.01668974603008893\n",
      "    num_agent_steps_sampled: 430000\n",
      "    num_agent_steps_trained: 430000\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.96578947368422\n",
      "    ram_util_percent: 38.91315789473685\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870781142362184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.20480340978203\n",
      "    mean_inference_ms: 1.9278334916055937\n",
      "    mean_raw_obs_processing_ms: 2.0366792419774726\n",
      "  time_since_restore: 10348.602081298828\n",
      "  time_this_iter_s: 26.556438446044922\n",
      "  time_total_s: 10348.602081298828\n",
      "  timers:\n",
      "    learn_throughput: 1442.487\n",
      "    learn_time_ms: 693.247\n",
      "    load_throughput: 41953.151\n",
      "    load_time_ms: 23.836\n",
      "    sample_throughput: 38.143\n",
      "    sample_time_ms: 26217.255\n",
      "    update_time_ms: 2.371\n",
      "  timestamp: 1635073222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         10348.6</td><td style=\"text-align: right;\">430000</td><td style=\"text-align: right;\"> -2.6696</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.24</td><td style=\"text-align: right;\">            266.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 431000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-00-46\n",
      "  done: false\n",
      "  episode_len_mean: 266.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.664799999999987\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1403\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2009052581257291\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010117729025984406\n",
      "          policy_loss: 0.030436041785611045\n",
      "          total_loss: 0.028332024812698364\n",
      "          vf_explained_var: 0.32355421781539917\n",
      "          vf_loss: 0.009807574257461562\n",
      "    num_agent_steps_sampled: 431000\n",
      "    num_agent_steps_trained: 431000\n",
      "    num_steps_sampled: 431000\n",
      "    num_steps_trained: 431000\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.46571428571428\n",
      "    ram_util_percent: 38.94285714285714\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870744690680483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.209977535096392\n",
      "    mean_inference_ms: 1.927827836870646\n",
      "    mean_raw_obs_processing_ms: 2.0368893739082425\n",
      "  time_since_restore: 10373.246535301208\n",
      "  time_this_iter_s: 24.64445400238037\n",
      "  time_total_s: 10373.246535301208\n",
      "  timers:\n",
      "    learn_throughput: 1441.99\n",
      "    learn_time_ms: 693.486\n",
      "    load_throughput: 42064.902\n",
      "    load_time_ms: 23.773\n",
      "    sample_throughput: 38.042\n",
      "    sample_time_ms: 26286.682\n",
      "    update_time_ms: 2.356\n",
      "  timestamp: 1635073246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431000\n",
      "  training_iteration: 431\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         10373.2</td><td style=\"text-align: right;\">431000</td><td style=\"text-align: right;\"> -2.6648</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            266.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-01-10\n",
      "  done: false\n",
      "  episode_len_mean: 267.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.672899999999987\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1407\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2657782753308615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009293251195437453\n",
      "          policy_loss: 0.01249583868516816\n",
      "          total_loss: 0.01507862044705285\n",
      "          vf_explained_var: 0.23422150313854218\n",
      "          vf_loss: 0.01515104521272911\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06060606060606\n",
      "    ram_util_percent: 38.92727272727272\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870697900080812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.216816084315013\n",
      "    mean_inference_ms: 1.9278205354859848\n",
      "    mean_raw_obs_processing_ms: 2.03716539246751\n",
      "  time_since_restore: 10396.782015562057\n",
      "  time_this_iter_s: 23.535480260849\n",
      "  time_total_s: 10396.782015562057\n",
      "  timers:\n",
      "    learn_throughput: 1441.392\n",
      "    learn_time_ms: 693.774\n",
      "    load_throughput: 41906.832\n",
      "    load_time_ms: 23.862\n",
      "    sample_throughput: 38.225\n",
      "    sample_time_ms: 26160.99\n",
      "    update_time_ms: 2.354\n",
      "  timestamp: 1635073270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 432\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         10396.8</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\"> -2.6729</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            267.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 433000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-01-53\n",
      "  done: false\n",
      "  episode_len_mean: 267.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.6709999999999865\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1411\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0581918060779572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006341261601736884\n",
      "          policy_loss: -0.005738539662626055\n",
      "          total_loss: -0.0027822504027022257\n",
      "          vf_explained_var: 0.33525604009628296\n",
      "          vf_loss: 0.013477125401712127\n",
      "    num_agent_steps_sampled: 433000\n",
      "    num_agent_steps_trained: 433000\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.650000000000006\n",
      "    ram_util_percent: 38.86774193548386\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038706545346861965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.22361775525623\n",
      "    mean_inference_ms: 1.9278133754438678\n",
      "    mean_raw_obs_processing_ms: 2.0390785959659197\n",
      "  time_since_restore: 10439.788651943207\n",
      "  time_this_iter_s: 43.00663638114929\n",
      "  time_total_s: 10439.788651943207\n",
      "  timers:\n",
      "    learn_throughput: 1441.053\n",
      "    learn_time_ms: 693.937\n",
      "    load_throughput: 41947.948\n",
      "    load_time_ms: 23.839\n",
      "    sample_throughput: 35.732\n",
      "    sample_time_ms: 27986.496\n",
      "    update_time_ms: 2.369\n",
      "  timestamp: 1635073313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         10439.8</td><td style=\"text-align: right;\">433000</td><td style=\"text-align: right;\">  -2.671</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">             267.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 434000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-02-19\n",
      "  done: false\n",
      "  episode_len_mean: 265.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.656299999999987\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1415\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0055079367425708\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005401467146768053\n",
      "          policy_loss: 0.00840427064233356\n",
      "          total_loss: 0.01191828010810746\n",
      "          vf_explained_var: 0.24538210034370422\n",
      "          vf_loss: 0.013517062345312702\n",
      "    num_agent_steps_sampled: 434000\n",
      "    num_agent_steps_trained: 434000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.36052631578948\n",
      "    ram_util_percent: 38.76578947368421\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870611690737812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.2306328733848\n",
      "    mean_inference_ms: 1.9278070317719906\n",
      "    mean_raw_obs_processing_ms: 2.0410449823307464\n",
      "  time_since_restore: 10466.247159957886\n",
      "  time_this_iter_s: 26.458508014678955\n",
      "  time_total_s: 10466.247159957886\n",
      "  timers:\n",
      "    learn_throughput: 1438.589\n",
      "    learn_time_ms: 695.126\n",
      "    load_throughput: 41777.559\n",
      "    load_time_ms: 23.936\n",
      "    sample_throughput: 35.333\n",
      "    sample_time_ms: 28302.46\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1635073339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 434\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         10466.2</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\"> -2.6563</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            265.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-02-44\n",
      "  done: false\n",
      "  episode_len_mean: 265.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.6531999999999876\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1418\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.104421619574229\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006422603557109636\n",
      "          policy_loss: -0.07563715875148773\n",
      "          total_loss: -0.07485998906195164\n",
      "          vf_explained_var: 0.3094368278980255\n",
      "          vf_loss: 0.011759519049276908\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.63142857142857\n",
      "    ram_util_percent: 38.81999999999999\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870578943702628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.23592172585556\n",
      "    mean_inference_ms: 1.9278025240108025\n",
      "    mean_raw_obs_processing_ms: 2.0420864398587026\n",
      "  time_since_restore: 10491.248478412628\n",
      "  time_this_iter_s: 25.00131845474243\n",
      "  time_total_s: 10491.248478412628\n",
      "  timers:\n",
      "    learn_throughput: 1440.808\n",
      "    learn_time_ms: 694.055\n",
      "    load_throughput: 40879.917\n",
      "    load_time_ms: 24.462\n",
      "    sample_throughput: 37.742\n",
      "    sample_time_ms: 26495.949\n",
      "    update_time_ms: 2.422\n",
      "  timestamp: 1635073364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 435\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         10491.2</td><td style=\"text-align: right;\">435000</td><td style=\"text-align: right;\"> -2.6532</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            265.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-03-10\n",
      "  done: false\n",
      "  episode_len_mean: 265.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.6529999999999876\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1422\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0128214657306671\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005628751103272853\n",
      "          policy_loss: -0.0313149774654044\n",
      "          total_loss: -0.028717990302377278\n",
      "          vf_explained_var: 0.3463852107524872\n",
      "          vf_loss: 0.012670982784281174\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.92368421052631\n",
      "    ram_util_percent: 38.91578947368421\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038705319867926254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.242964683809628\n",
      "    mean_inference_ms: 1.9277969107160486\n",
      "    mean_raw_obs_processing_ms: 2.042364045658788\n",
      "  time_since_restore: 10517.423590421677\n",
      "  time_this_iter_s: 26.175112009048462\n",
      "  time_total_s: 10517.423590421677\n",
      "  timers:\n",
      "    learn_throughput: 1441.25\n",
      "    learn_time_ms: 693.842\n",
      "    load_throughput: 40932.578\n",
      "    load_time_ms: 24.43\n",
      "    sample_throughput: 37.779\n",
      "    sample_time_ms: 26469.991\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1635073390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         10517.4</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">  -2.653</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">             265.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-03-36\n",
      "  done: false\n",
      "  episode_len_mean: 264.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.6452999999999873\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1426\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9421915451685587\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005714780781108825\n",
      "          policy_loss: -0.055938495364454055\n",
      "          total_loss: -0.05239166220029195\n",
      "          vf_explained_var: 0.3740311861038208\n",
      "          vf_loss: 0.012913703587320116\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.13055555555556\n",
      "    ram_util_percent: 38.986111111111114\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387048067341075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.25003426183962\n",
      "    mean_inference_ms: 1.9277921763729318\n",
      "    mean_raw_obs_processing_ms: 2.042697265050532\n",
      "  time_since_restore: 10543.069269180298\n",
      "  time_this_iter_s: 25.645678758621216\n",
      "  time_total_s: 10543.069269180298\n",
      "  timers:\n",
      "    learn_throughput: 1442.836\n",
      "    learn_time_ms: 693.079\n",
      "    load_throughput: 40572.933\n",
      "    load_time_ms: 24.647\n",
      "    sample_throughput: 37.812\n",
      "    sample_time_ms: 26446.611\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1635073416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         10543.1</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\"> -2.6453</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            264.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 438000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-04-03\n",
      "  done: false\n",
      "  episode_len_mean: 263.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.638999999999988\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1430\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8271106600761413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007752968425970409\n",
      "          policy_loss: -0.024316264357831744\n",
      "          total_loss: -0.02016445476975706\n",
      "          vf_explained_var: 0.3629644513130188\n",
      "          vf_loss: 0.012348236629946364\n",
      "    num_agent_steps_sampled: 438000\n",
      "    num_agent_steps_trained: 438000\n",
      "    num_steps_sampled: 438000\n",
      "    num_steps_trained: 438000\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.735897435897435\n",
      "    ram_util_percent: 39.01282051282051\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870429494542024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.257299356957013\n",
      "    mean_inference_ms: 1.9277880275085573\n",
      "    mean_raw_obs_processing_ms: 2.043040183555557\n",
      "  time_since_restore: 10569.878970384598\n",
      "  time_this_iter_s: 26.809701204299927\n",
      "  time_total_s: 10569.878970384598\n",
      "  timers:\n",
      "    learn_throughput: 1444.365\n",
      "    learn_time_ms: 692.346\n",
      "    load_throughput: 40530.159\n",
      "    load_time_ms: 24.673\n",
      "    sample_throughput: 37.656\n",
      "    sample_time_ms: 26556.53\n",
      "    update_time_ms: 2.401\n",
      "  timestamp: 1635073443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 438000\n",
      "  training_iteration: 438\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         10569.9</td><td style=\"text-align: right;\">438000</td><td style=\"text-align: right;\">  -2.639</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">             263.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 439000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-04-30\n",
      "  done: false\n",
      "  episode_len_mean: 261.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.613099999999988\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1435\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8384015652868483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007663769204943015\n",
      "          policy_loss: -0.03618830218911171\n",
      "          total_loss: -0.029551327642467286\n",
      "          vf_explained_var: 0.3509426712989807\n",
      "          vf_loss: 0.014947170195066266\n",
      "    num_agent_steps_sampled: 439000\n",
      "    num_agent_steps_trained: 439000\n",
      "    num_steps_sampled: 439000\n",
      "    num_steps_trained: 439000\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.252631578947366\n",
      "    ram_util_percent: 38.95000000000001\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870365216091683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.2667994978998\n",
      "    mean_inference_ms: 1.9277818900429367\n",
      "    mean_raw_obs_processing_ms: 2.0435081081656272\n",
      "  time_since_restore: 10597.125339269638\n",
      "  time_this_iter_s: 27.246368885040283\n",
      "  time_total_s: 10597.125339269638\n",
      "  timers:\n",
      "    learn_throughput: 1442.8\n",
      "    learn_time_ms: 693.097\n",
      "    load_throughput: 40870.715\n",
      "    load_time_ms: 24.467\n",
      "    sample_throughput: 37.335\n",
      "    sample_time_ms: 26784.571\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1635073470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439000\n",
      "  training_iteration: 439\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">         10597.1</td><td style=\"text-align: right;\">439000</td><td style=\"text-align: right;\"> -2.6131</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            261.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-04-56\n",
      "  done: false\n",
      "  episode_len_mean: 260.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1799999999999975\n",
      "  episode_reward_mean: -2.608199999999988\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1438\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009632556264512006\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9258170518610213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.039827111434905284\n",
      "          policy_loss: -0.02095820465021663\n",
      "          total_loss: -0.021589102264907626\n",
      "          vf_explained_var: 0.49105212092399597\n",
      "          vf_loss: 0.008243637149118715\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.178378378378376\n",
      "    ram_util_percent: 38.948648648648664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038703262320071895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.272523367878016\n",
      "    mean_inference_ms: 1.9277782089995443\n",
      "    mean_raw_obs_processing_ms: 2.0438470173659526\n",
      "  time_since_restore: 10622.911881923676\n",
      "  time_this_iter_s: 25.786542654037476\n",
      "  time_total_s: 10622.911881923676\n",
      "  timers:\n",
      "    learn_throughput: 1441.852\n",
      "    learn_time_ms: 693.553\n",
      "    load_throughput: 40720.252\n",
      "    load_time_ms: 24.558\n",
      "    sample_throughput: 37.443\n",
      "    sample_time_ms: 26707.069\n",
      "    update_time_ms: 2.384\n",
      "  timestamp: 1635073496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 440\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">         10622.9</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\"> -2.6082</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            260.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 441000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-05-41\n",
      "  done: false\n",
      "  episode_len_mean: 258.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.5875999999999886\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1443\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014448834396768001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6832957082324558\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00742496445228961\n",
      "          policy_loss: -0.031557680004172854\n",
      "          total_loss: -0.025018209922644828\n",
      "          vf_explained_var: 0.41583681106567383\n",
      "          vf_loss: 0.013265146118485265\n",
      "    num_agent_steps_sampled: 441000\n",
      "    num_agent_steps_trained: 441000\n",
      "    num_steps_sampled: 441000\n",
      "    num_steps_trained: 441000\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.118750000000006\n",
      "    ram_util_percent: 38.771874999999994\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870261065133368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.28251314330463\n",
      "    mean_inference_ms: 1.927774535692283\n",
      "    mean_raw_obs_processing_ms: 2.0463229128326903\n",
      "  time_since_restore: 10667.777049779892\n",
      "  time_this_iter_s: 44.86516785621643\n",
      "  time_total_s: 10667.777049779892\n",
      "  timers:\n",
      "    learn_throughput: 1441.251\n",
      "    learn_time_ms: 693.842\n",
      "    load_throughput: 40336.052\n",
      "    load_time_ms: 24.792\n",
      "    sample_throughput: 34.809\n",
      "    sample_time_ms: 28728.499\n",
      "    update_time_ms: 2.496\n",
      "  timestamp: 1635073541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 441000\n",
      "  training_iteration: 441\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         10667.8</td><td style=\"text-align: right;\">441000</td><td style=\"text-align: right;\"> -2.5876</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            258.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 442000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 258.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.583099999999989\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1447\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014448834396768001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7829208407137129\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015485314853629372\n",
      "          policy_loss: -0.028226326985491646\n",
      "          total_loss: -0.025214219838380812\n",
      "          vf_explained_var: 0.24869593977928162\n",
      "          vf_loss: 0.010617573554110195\n",
      "    num_agent_steps_sampled: 442000\n",
      "    num_agent_steps_trained: 442000\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.29736842105264\n",
      "    ram_util_percent: 38.81842105263157\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870211770955838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.2905730743779\n",
      "    mean_inference_ms: 1.9277723626841827\n",
      "    mean_raw_obs_processing_ms: 2.047948669392019\n",
      "  time_since_restore: 10694.100084066391\n",
      "  time_this_iter_s: 26.323034286499023\n",
      "  time_total_s: 10694.100084066391\n",
      "  timers:\n",
      "    learn_throughput: 1440.558\n",
      "    learn_time_ms: 694.175\n",
      "    load_throughput: 41596.17\n",
      "    load_time_ms: 24.041\n",
      "    sample_throughput: 34.474\n",
      "    sample_time_ms: 29007.652\n",
      "    update_time_ms: 2.499\n",
      "  timestamp: 1635073567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 442\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">         10694.1</td><td style=\"text-align: right;\">442000</td><td style=\"text-align: right;\"> -2.5831</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            258.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 443000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-06-33\n",
      "  done: false\n",
      "  episode_len_mean: 257.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.5714999999999884\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1451\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014448834396768001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7464255538251665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006303346220536494\n",
      "          policy_loss: 0.020414423859781688\n",
      "          total_loss: 0.024505347427394655\n",
      "          vf_explained_var: 0.19436033070087433\n",
      "          vf_loss: 0.011464105194641484\n",
      "    num_agent_steps_sampled: 443000\n",
      "    num_agent_steps_trained: 443000\n",
      "    num_steps_sampled: 443000\n",
      "    num_steps_trained: 443000\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.50540540540541\n",
      "    ram_util_percent: 38.870270270270275\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038701636478267305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.29872654986843\n",
      "    mean_inference_ms: 1.9277708256347363\n",
      "    mean_raw_obs_processing_ms: 2.048368349597267\n",
      "  time_since_restore: 10720.244903087616\n",
      "  time_this_iter_s: 26.144819021224976\n",
      "  time_total_s: 10720.244903087616\n",
      "  timers:\n",
      "    learn_throughput: 1439.289\n",
      "    learn_time_ms: 694.788\n",
      "    load_throughput: 41487.671\n",
      "    load_time_ms: 24.104\n",
      "    sample_throughput: 36.602\n",
      "    sample_time_ms: 27320.825\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1635073593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 443000\n",
      "  training_iteration: 443\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         10720.2</td><td style=\"text-align: right;\">443000</td><td style=\"text-align: right;\"> -2.5715</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            257.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 257.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.572599999999989\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1454\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014448834396768001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7898966279294756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004689969934914097\n",
      "          policy_loss: -0.09804830518033769\n",
      "          total_loss: -0.09435506181584465\n",
      "          vf_explained_var: 0.1690034717321396\n",
      "          vf_loss: 0.011524442045225038\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.188571428571436\n",
      "    ram_util_percent: 38.96\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038701265665556185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.304641471237474\n",
      "    mean_inference_ms: 1.9277692058932672\n",
      "    mean_raw_obs_processing_ms: 2.04870067704263\n",
      "  time_since_restore: 10744.884582281113\n",
      "  time_this_iter_s: 24.639679193496704\n",
      "  time_total_s: 10744.884582281113\n",
      "  timers:\n",
      "    learn_throughput: 1440.102\n",
      "    learn_time_ms: 694.395\n",
      "    load_throughput: 41458.719\n",
      "    load_time_ms: 24.12\n",
      "    sample_throughput: 36.847\n",
      "    sample_time_ms: 27139.394\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1635073618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 444\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         10744.9</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\"> -2.5726</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            257.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 445000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-07-23\n",
      "  done: false\n",
      "  episode_len_mean: 258.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.5802999999999887\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1458\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0072244171983840005\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8293249547481537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008531553176302964\n",
      "          policy_loss: 0.007500806947549185\n",
      "          total_loss: 0.009745542539490594\n",
      "          vf_explained_var: 0.2903570532798767\n",
      "          vf_loss: 0.010476350722213586\n",
      "    num_agent_steps_sampled: 445000\n",
      "    num_agent_steps_trained: 445000\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.08\n",
      "    ram_util_percent: 38.96857142857144\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870076976615309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.31240659496112\n",
      "    mean_inference_ms: 1.9277664340519294\n",
      "    mean_raw_obs_processing_ms: 2.049138166584049\n",
      "  time_since_restore: 10769.421632051468\n",
      "  time_this_iter_s: 24.537049770355225\n",
      "  time_total_s: 10769.421632051468\n",
      "  timers:\n",
      "    learn_throughput: 1439.93\n",
      "    learn_time_ms: 694.478\n",
      "    load_throughput: 41496.743\n",
      "    load_time_ms: 24.098\n",
      "    sample_throughput: 36.91\n",
      "    sample_time_ms: 27092.862\n",
      "    update_time_ms: 2.476\n",
      "  timestamp: 1635073643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 445\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         10769.4</td><td style=\"text-align: right;\">445000</td><td style=\"text-align: right;\"> -2.5803</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            258.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 446000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-07-48\n",
      "  done: false\n",
      "  episode_len_mean: 258.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.581699999999989\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1462\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0072244171983840005\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7339989099237654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005715797298457264\n",
      "          policy_loss: 0.022842247949706185\n",
      "          total_loss: 0.027088950408829582\n",
      "          vf_explained_var: 0.2831706702709198\n",
      "          vf_loss: 0.011545397186030945\n",
      "    num_agent_steps_sampled: 446000\n",
      "    num_agent_steps_trained: 446000\n",
      "    num_steps_sampled: 446000\n",
      "    num_steps_trained: 446000\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24722222222223\n",
      "    ram_util_percent: 38.925000000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870025417476593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.320176558819302\n",
      "    mean_inference_ms: 1.927763401582657\n",
      "    mean_raw_obs_processing_ms: 2.049584003217224\n",
      "  time_since_restore: 10794.621947526932\n",
      "  time_this_iter_s: 25.200315475463867\n",
      "  time_total_s: 10794.621947526932\n",
      "  timers:\n",
      "    learn_throughput: 1441.099\n",
      "    learn_time_ms: 693.915\n",
      "    load_throughput: 41443.849\n",
      "    load_time_ms: 24.129\n",
      "    sample_throughput: 37.043\n",
      "    sample_time_ms: 26995.924\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1635073668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 446000\n",
      "  training_iteration: 446\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">         10794.6</td><td style=\"text-align: right;\">446000</td><td style=\"text-align: right;\"> -2.5817</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            258.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 447000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-08-13\n",
      "  done: false\n",
      "  episode_len_mean: 257.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.578699999999989\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1466\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0072244171983840005\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6856080998977025\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007472757629937219\n",
      "          policy_loss: 0.023116258283456165\n",
      "          total_loss: 0.027861135204633077\n",
      "          vf_explained_var: 0.28443029522895813\n",
      "          vf_loss: 0.011546974153154426\n",
      "    num_agent_steps_sampled: 447000\n",
      "    num_agent_steps_trained: 447000\n",
      "    num_steps_sampled: 447000\n",
      "    num_steps_trained: 447000\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.067567567567565\n",
      "    ram_util_percent: 38.881081081081085\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869970832784731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.328043356639583\n",
      "    mean_inference_ms: 1.927760795642795\n",
      "    mean_raw_obs_processing_ms: 2.0500390103675206\n",
      "  time_since_restore: 10820.17768740654\n",
      "  time_this_iter_s: 25.555739879608154\n",
      "  time_total_s: 10820.17768740654\n",
      "  timers:\n",
      "    learn_throughput: 1441.904\n",
      "    learn_time_ms: 693.527\n",
      "    load_throughput: 41492.596\n",
      "    load_time_ms: 24.101\n",
      "    sample_throughput: 37.054\n",
      "    sample_time_ms: 26987.363\n",
      "    update_time_ms: 2.456\n",
      "  timestamp: 1635073693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447000\n",
      "  training_iteration: 447\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         10820.2</td><td style=\"text-align: right;\">447000</td><td style=\"text-align: right;\"> -2.5787</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            257.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 257.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.573599999999989\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1470\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0072244171983840005\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9427243159876929\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009756990991275197\n",
      "          policy_loss: -0.007911561263932122\n",
      "          total_loss: -0.005886436502138773\n",
      "          vf_explained_var: 0.3788664937019348\n",
      "          vf_loss: 0.01138187704814805\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.348333333333336\n",
      "    ram_util_percent: 38.85333333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869919292441933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.335885068398447\n",
      "    mean_inference_ms: 1.9277586348930265\n",
      "    mean_raw_obs_processing_ms: 2.0520103491402093\n",
      "  time_since_restore: 10861.928770542145\n",
      "  time_this_iter_s: 41.75108313560486\n",
      "  time_total_s: 10861.928770542145\n",
      "  timers:\n",
      "    learn_throughput: 1440.517\n",
      "    learn_time_ms: 694.195\n",
      "    load_throughput: 41969.607\n",
      "    load_time_ms: 23.827\n",
      "    sample_throughput: 35.111\n",
      "    sample_time_ms: 28481.018\n",
      "    update_time_ms: 2.547\n",
      "  timestamp: 1635073735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 448\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         10861.9</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\"> -2.5736</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            257.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 449000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-09-23\n",
      "  done: false\n",
      "  episode_len_mean: 256.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.563899999999989\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1474\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0072244171983840005\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8878736264175839\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012023402887734057\n",
      "          policy_loss: -0.017523412075307634\n",
      "          total_loss: -0.013051020436816746\n",
      "          vf_explained_var: 0.19423805177211761\n",
      "          vf_loss: 0.013264265884127881\n",
      "    num_agent_steps_sampled: 449000\n",
      "    num_agent_steps_trained: 449000\n",
      "    num_steps_sampled: 449000\n",
      "    num_steps_trained: 449000\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.567499999999995\n",
      "    ram_util_percent: 38.614999999999995\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038698687470470804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.343990781995597\n",
      "    mean_inference_ms: 1.9277577003392037\n",
      "    mean_raw_obs_processing_ms: 2.0540313672751704\n",
      "  time_since_restore: 10889.924269914627\n",
      "  time_this_iter_s: 27.9954993724823\n",
      "  time_total_s: 10889.924269914627\n",
      "  timers:\n",
      "    learn_throughput: 1443.67\n",
      "    learn_time_ms: 692.679\n",
      "    load_throughput: 41218.299\n",
      "    load_time_ms: 24.261\n",
      "    sample_throughput: 35.018\n",
      "    sample_time_ms: 28557.01\n",
      "    update_time_ms: 2.549\n",
      "  timestamp: 1635073763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 449000\n",
      "  training_iteration: 449\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         10889.9</td><td style=\"text-align: right;\">449000</td><td style=\"text-align: right;\"> -2.5639</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            256.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-09-50\n",
      "  done: false\n",
      "  episode_len_mean: 255.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.5523999999999893\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1478\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0072244171983840005\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6996509518888262\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005851807947710529\n",
      "          policy_loss: 0.01552795527709855\n",
      "          total_loss: 0.018175602704286576\n",
      "          vf_explained_var: 0.4560015797615051\n",
      "          vf_loss: 0.009601882472634316\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_agent_steps_trained: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.13243243243244\n",
      "    ram_util_percent: 38.78918918918919\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038698186021027155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.352210491767586\n",
      "    mean_inference_ms: 1.9277566056017184\n",
      "    mean_raw_obs_processing_ms: 2.0552626987236726\n",
      "  time_since_restore: 10916.294470787048\n",
      "  time_this_iter_s: 26.370200872421265\n",
      "  time_total_s: 10916.294470787048\n",
      "  timers:\n",
      "    learn_throughput: 1443.412\n",
      "    learn_time_ms: 692.803\n",
      "    load_throughput: 41364.842\n",
      "    load_time_ms: 24.175\n",
      "    sample_throughput: 34.946\n",
      "    sample_time_ms: 28615.341\n",
      "    update_time_ms: 2.543\n",
      "  timestamp: 1635073790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 450\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">         10916.3</td><td style=\"text-align: right;\">450000</td><td style=\"text-align: right;\"> -2.5524</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            255.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 451000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-10-15\n",
      "  done: false\n",
      "  episode_len_mean: 256.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.560999999999989\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1482\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0072244171983840005\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9917182975345188\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.030373340718925394\n",
      "          policy_loss: 0.016796064666575856\n",
      "          total_loss: 0.01716876510116789\n",
      "          vf_explained_var: 0.5460103154182434\n",
      "          vf_loss: 0.010070454639693102\n",
      "    num_agent_steps_sampled: 451000\n",
      "    num_agent_steps_trained: 451000\n",
      "    num_steps_sampled: 451000\n",
      "    num_steps_trained: 451000\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.09444444444444\n",
      "    ram_util_percent: 38.91944444444445\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869769162387092\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.36026819443889\n",
      "    mean_inference_ms: 1.9277557601183704\n",
      "    mean_raw_obs_processing_ms: 2.05570506910674\n",
      "  time_since_restore: 10941.206568479538\n",
      "  time_this_iter_s: 24.912097692489624\n",
      "  time_total_s: 10941.206568479538\n",
      "  timers:\n",
      "    learn_throughput: 1446.157\n",
      "    learn_time_ms: 691.488\n",
      "    load_throughput: 41852.346\n",
      "    load_time_ms: 23.894\n",
      "    sample_throughput: 37.563\n",
      "    sample_time_ms: 26621.718\n",
      "    update_time_ms: 2.443\n",
      "  timestamp: 1635073815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 451000\n",
      "  training_iteration: 451\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         10941.2</td><td style=\"text-align: right;\">451000</td><td style=\"text-align: right;\">  -2.561</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">             256.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 255.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.5532999999999895\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1486\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010836625797576008\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7065065655443403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009454060611101264\n",
      "          policy_loss: -0.021768378673328294\n",
      "          total_loss: -0.0189791036148866\n",
      "          vf_explained_var: 0.48224231600761414\n",
      "          vf_loss: 0.009751887598799334\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.23157894736842\n",
      "    ram_util_percent: 38.96842105263158\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038697172262316525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.36831936743287\n",
      "    mean_inference_ms: 1.9277555118623761\n",
      "    mean_raw_obs_processing_ms: 2.056155675127728\n",
      "  time_since_restore: 10967.731041431427\n",
      "  time_this_iter_s: 26.524472951889038\n",
      "  time_total_s: 10967.731041431427\n",
      "  timers:\n",
      "    learn_throughput: 1446.208\n",
      "    learn_time_ms: 691.464\n",
      "    load_throughput: 40504.561\n",
      "    load_time_ms: 24.689\n",
      "    sample_throughput: 37.536\n",
      "    sample_time_ms: 26640.982\n",
      "    update_time_ms: 2.558\n",
      "  timestamp: 1635073841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 452\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">         10967.7</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\"> -2.5533</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            255.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 453000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-11-08\n",
      "  done: false\n",
      "  episode_len_mean: 255.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.5502999999999894\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1490\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010836625797576008\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7044594082567427\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008622107438429635\n",
      "          policy_loss: 0.04337243148022228\n",
      "          total_loss: 0.04713549431827333\n",
      "          vf_explained_var: 0.22546230256557465\n",
      "          vf_loss: 0.010714221311112245\n",
      "    num_agent_steps_sampled: 453000\n",
      "    num_agent_steps_trained: 453000\n",
      "    num_steps_sampled: 453000\n",
      "    num_steps_trained: 453000\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.26153846153845\n",
      "    ram_util_percent: 38.958974358974366\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869667371109787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.37646815903227\n",
      "    mean_inference_ms: 1.9277559454092301\n",
      "    mean_raw_obs_processing_ms: 2.056615025031169\n",
      "  time_since_restore: 10995.064984798431\n",
      "  time_this_iter_s: 27.333943367004395\n",
      "  time_total_s: 10995.064984798431\n",
      "  timers:\n",
      "    learn_throughput: 1446.508\n",
      "    learn_time_ms: 691.32\n",
      "    load_throughput: 40748.26\n",
      "    load_time_ms: 24.541\n",
      "    sample_throughput: 37.369\n",
      "    sample_time_ms: 26760.186\n",
      "    update_time_ms: 2.547\n",
      "  timestamp: 1635073868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 453000\n",
      "  training_iteration: 453\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         10995.1</td><td style=\"text-align: right;\">453000</td><td style=\"text-align: right;\"> -2.5503</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            255.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 454000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-11-36\n",
      "  done: false\n",
      "  episode_len_mean: 253.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.5397999999999894\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1494\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010836625797576008\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6899041950702667\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007721575950615842\n",
      "          policy_loss: 0.016423273583253226\n",
      "          total_loss: 0.02104494170182281\n",
      "          vf_explained_var: 0.23832456767559052\n",
      "          vf_loss: 0.011437034596585564\n",
      "    num_agent_steps_sampled: 454000\n",
      "    num_agent_steps_trained: 454000\n",
      "    num_steps_sampled: 454000\n",
      "    num_steps_trained: 454000\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.358974358974365\n",
      "    ram_util_percent: 38.93076923076924\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386962200358134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.384782925059987\n",
      "    mean_inference_ms: 1.927756864767498\n",
      "    mean_raw_obs_processing_ms: 2.0570842019571476\n",
      "  time_since_restore: 11022.69902920723\n",
      "  time_this_iter_s: 27.634044408798218\n",
      "  time_total_s: 11022.69902920723\n",
      "  timers:\n",
      "    learn_throughput: 1446.311\n",
      "    learn_time_ms: 691.414\n",
      "    load_throughput: 40764.062\n",
      "    load_time_ms: 24.531\n",
      "    sample_throughput: 36.956\n",
      "    sample_time_ms: 27059.534\n",
      "    update_time_ms: 2.552\n",
      "  timestamp: 1635073896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 454000\n",
      "  training_iteration: 454\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         11022.7</td><td style=\"text-align: right;\">454000</td><td style=\"text-align: right;\"> -2.5398</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            253.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 455000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-12-04\n",
      "  done: false\n",
      "  episode_len_mean: 253.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.53099999999999\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1498\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010836625797576008\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6809104813469781\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02864227219690447\n",
      "          policy_loss: 0.011089325116740333\n",
      "          total_loss: 0.01465009285344018\n",
      "          vf_explained_var: 0.3591906726360321\n",
      "          vf_loss: 0.010059487322966259\n",
      "    num_agent_steps_sampled: 455000\n",
      "    num_agent_steps_trained: 455000\n",
      "    num_steps_sampled: 455000\n",
      "    num_steps_trained: 455000\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.045\n",
      "    ram_util_percent: 38.9\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386957650853057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.39314039832799\n",
      "    mean_inference_ms: 1.9277582863415115\n",
      "    mean_raw_obs_processing_ms: 2.0576027148744718\n",
      "  time_since_restore: 11050.487821817398\n",
      "  time_this_iter_s: 27.788792610168457\n",
      "  time_total_s: 11050.487821817398\n",
      "  timers:\n",
      "    learn_throughput: 1447.737\n",
      "    learn_time_ms: 690.733\n",
      "    load_throughput: 40862.194\n",
      "    load_time_ms: 24.472\n",
      "    sample_throughput: 36.516\n",
      "    sample_time_ms: 27385.491\n",
      "    update_time_ms: 2.518\n",
      "  timestamp: 1635073924\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455000\n",
      "  training_iteration: 455\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">         11050.5</td><td style=\"text-align: right;\">455000</td><td style=\"text-align: right;\">  -2.531</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">             253.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-12-49\n",
      "  done: false\n",
      "  episode_len_mean: 252.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.52349999999999\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1502\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01625493869636401\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7283923765023549\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028854469636895096\n",
      "          policy_loss: -0.10930883404281404\n",
      "          total_loss: -0.09824405825800366\n",
      "          vf_explained_var: 0.12245792895555496\n",
      "          vf_loss: 0.01787966900608606\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.09538461538462\n",
      "    ram_util_percent: 38.78461538461538\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038695335807801115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.40179840459384\n",
      "    mean_inference_ms: 1.9277603376022758\n",
      "    mean_raw_obs_processing_ms: 2.059579239337057\n",
      "  time_since_restore: 11095.787864923477\n",
      "  time_this_iter_s: 45.3000431060791\n",
      "  time_total_s: 11095.787864923477\n",
      "  timers:\n",
      "    learn_throughput: 1448.664\n",
      "    learn_time_ms: 690.291\n",
      "    load_throughput: 40881.072\n",
      "    load_time_ms: 24.461\n",
      "    sample_throughput: 34.018\n",
      "    sample_time_ms: 29395.846\n",
      "    update_time_ms: 2.559\n",
      "  timestamp: 1635073969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 456\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         11095.8</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\"> -2.5235</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            252.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 457000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-13-15\n",
      "  done: false\n",
      "  episode_len_mean: 251.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.5132999999999903\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1506\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024382408044546007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8278193493684133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016107976402323566\n",
      "          policy_loss: -0.1042579495244556\n",
      "          total_loss: -0.09274282885922326\n",
      "          vf_explained_var: 0.1223599761724472\n",
      "          vf_loss: 0.019400561114566194\n",
      "    num_agent_steps_sampled: 457000\n",
      "    num_agent_steps_trained: 457000\n",
      "    num_steps_sampled: 457000\n",
      "    num_steps_trained: 457000\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.564864864864866\n",
      "    ram_util_percent: 38.797297297297284\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869492067911856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.410580753448162\n",
      "    mean_inference_ms: 1.9277630266018457\n",
      "    mean_raw_obs_processing_ms: 2.061603489676686\n",
      "  time_since_restore: 11121.981185436249\n",
      "  time_this_iter_s: 26.193320512771606\n",
      "  time_total_s: 11121.981185436249\n",
      "  timers:\n",
      "    learn_throughput: 1447.45\n",
      "    learn_time_ms: 690.87\n",
      "    load_throughput: 41000.56\n",
      "    load_time_ms: 24.39\n",
      "    sample_throughput: 33.945\n",
      "    sample_time_ms: 29459.056\n",
      "    update_time_ms: 2.575\n",
      "  timestamp: 1635073995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 457000\n",
      "  training_iteration: 457\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">           11122</td><td style=\"text-align: right;\">457000</td><td style=\"text-align: right;\"> -2.5133</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            251.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 458000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-13-43\n",
      "  done: false\n",
      "  episode_len_mean: 249.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4972999999999907\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1511\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024382408044546007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8234800193044874\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06785499922843227\n",
      "          policy_loss: 0.011246869423323208\n",
      "          total_loss: 0.01795299156672425\n",
      "          vf_explained_var: 0.36862555146217346\n",
      "          vf_loss: 0.013286453816625807\n",
      "    num_agent_steps_sampled: 458000\n",
      "    num_agent_steps_trained: 458000\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.166666666666664\n",
      "    ram_util_percent: 38.87179487179487\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869443409425381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.421788219676568\n",
      "    mean_inference_ms: 1.9277668055841604\n",
      "    mean_raw_obs_processing_ms: 2.062526272237429\n",
      "  time_since_restore: 11149.58672618866\n",
      "  time_this_iter_s: 27.60554075241089\n",
      "  time_total_s: 11149.58672618866\n",
      "  timers:\n",
      "    learn_throughput: 1446.329\n",
      "    learn_time_ms: 691.406\n",
      "    load_throughput: 40847.351\n",
      "    load_time_ms: 24.481\n",
      "    sample_throughput: 35.658\n",
      "    sample_time_ms: 28043.954\n",
      "    update_time_ms: 2.485\n",
      "  timestamp: 1635074023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 458\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">         11149.6</td><td style=\"text-align: right;\">458000</td><td style=\"text-align: right;\"> -2.4973</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            249.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 459000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-14-11\n",
      "  done: false\n",
      "  episode_len_mean: 249.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.492499999999991\n",
      "  episode_reward_min: -3.1299999999999772\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1515\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03657361206681903\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4947149912516276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0035615169069864627\n",
      "          policy_loss: 0.056418691906664105\n",
      "          total_loss: 0.06017272281977865\n",
      "          vf_explained_var: 0.506807804107666\n",
      "          vf_loss: 0.00857092361483309\n",
      "    num_agent_steps_sampled: 459000\n",
      "    num_agent_steps_trained: 459000\n",
      "    num_steps_sampled: 459000\n",
      "    num_steps_trained: 459000\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.940000000000005\n",
      "    ram_util_percent: 38.972500000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869404901433928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.43074307892989\n",
      "    mean_inference_ms: 1.927770110660008\n",
      "    mean_raw_obs_processing_ms: 2.0629796040949584\n",
      "  time_since_restore: 11177.025482416153\n",
      "  time_this_iter_s: 27.438756227493286\n",
      "  time_total_s: 11177.025482416153\n",
      "  timers:\n",
      "    learn_throughput: 1444.774\n",
      "    learn_time_ms: 692.15\n",
      "    load_throughput: 41742.509\n",
      "    load_time_ms: 23.956\n",
      "    sample_throughput: 35.73\n",
      "    sample_time_ms: 27988.05\n",
      "    update_time_ms: 2.5\n",
      "  timestamp: 1635074051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 459000\n",
      "  training_iteration: 459\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">           11177</td><td style=\"text-align: right;\">459000</td><td style=\"text-align: right;\"> -2.4925</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.13</td><td style=\"text-align: right;\">            249.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-14-38\n",
      "  done: false\n",
      "  episode_len_mean: 248.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.480599999999991\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1519\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.018286806033409514\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5356109645631578\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012972228078346701\n",
      "          policy_loss: 0.03409268980224927\n",
      "          total_loss: 0.03810761736498939\n",
      "          vf_explained_var: 0.3351089656352997\n",
      "          vf_loss: 0.009133816816999266\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.61025641025641\n",
      "    ram_util_percent: 38.971794871794884\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869367618562267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.439806729345968\n",
      "    mean_inference_ms: 1.9277742489369394\n",
      "    mean_raw_obs_processing_ms: 2.0634858268737846\n",
      "  time_since_restore: 11204.724863767624\n",
      "  time_this_iter_s: 27.699381351470947\n",
      "  time_total_s: 11204.724863767624\n",
      "  timers:\n",
      "    learn_throughput: 1444.457\n",
      "    learn_time_ms: 692.301\n",
      "    load_throughput: 42102.778\n",
      "    load_time_ms: 23.751\n",
      "    sample_throughput: 35.561\n",
      "    sample_time_ms: 28121.013\n",
      "    update_time_ms: 2.502\n",
      "  timestamp: 1635074078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 460\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">         11204.7</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\"> -2.4806</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            248.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 461000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-15-06\n",
      "  done: false\n",
      "  episode_len_mean: 247.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.475399999999991\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1523\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.018286806033409514\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4314116640223397\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00348984439592876\n",
      "          policy_loss: -0.044532242996825115\n",
      "          total_loss: -0.035915689046184224\n",
      "          vf_explained_var: 0.1518448144197464\n",
      "          vf_loss: 0.012866853829473257\n",
      "    num_agent_steps_sampled: 461000\n",
      "    num_agent_steps_trained: 461000\n",
      "    num_steps_sampled: 461000\n",
      "    num_steps_trained: 461000\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.214999999999996\n",
      "    ram_util_percent: 38.9825\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869332232315024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.44898031067623\n",
      "    mean_inference_ms: 1.9277783885829372\n",
      "    mean_raw_obs_processing_ms: 2.0640001418270493\n",
      "  time_since_restore: 11232.468487262726\n",
      "  time_this_iter_s: 27.74362349510193\n",
      "  time_total_s: 11232.468487262726\n",
      "  timers:\n",
      "    learn_throughput: 1443.307\n",
      "    learn_time_ms: 692.853\n",
      "    load_throughput: 42129.928\n",
      "    load_time_ms: 23.736\n",
      "    sample_throughput: 35.207\n",
      "    sample_time_ms: 28403.586\n",
      "    update_time_ms: 2.539\n",
      "  timestamp: 1635074106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 461000\n",
      "  training_iteration: 461\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         11232.5</td><td style=\"text-align: right;\">461000</td><td style=\"text-align: right;\"> -2.4754</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            247.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 462000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-15-33\n",
      "  done: false\n",
      "  episode_len_mean: 246.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.464899999999991\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1528\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009143403016704757\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5440368771553039\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004496205140947356\n",
      "          policy_loss: -0.011662734382682376\n",
      "          total_loss: -0.0013505728708373176\n",
      "          vf_explained_var: 0.19741782546043396\n",
      "          vf_loss: 0.015711421395341554\n",
      "    num_agent_steps_sampled: 462000\n",
      "    num_agent_steps_trained: 462000\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.36578947368422\n",
      "    ram_util_percent: 38.915789473684214\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038692873042062814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.460452308841187\n",
      "    mean_inference_ms: 1.9277841671329816\n",
      "    mean_raw_obs_processing_ms: 2.064687277739759\n",
      "  time_since_restore: 11259.644381284714\n",
      "  time_this_iter_s: 27.175894021987915\n",
      "  time_total_s: 11259.644381284714\n",
      "  timers:\n",
      "    learn_throughput: 1442.032\n",
      "    learn_time_ms: 693.466\n",
      "    load_throughput: 42198.934\n",
      "    load_time_ms: 23.697\n",
      "    sample_throughput: 35.127\n",
      "    sample_time_ms: 28468.249\n",
      "    update_time_ms: 2.44\n",
      "  timestamp: 1635074133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 462\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   462</td><td style=\"text-align: right;\">         11259.6</td><td style=\"text-align: right;\">462000</td><td style=\"text-align: right;\"> -2.4649</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            246.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 463000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-16-19\n",
      "  done: false\n",
      "  episode_len_mean: 245.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4585999999999917\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1532\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0045717015083523785\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.511597767803404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008328980363798956\n",
      "          policy_loss: 0.026598722570472293\n",
      "          total_loss: 0.03535189314021005\n",
      "          vf_explained_var: 0.1057569608092308\n",
      "          vf_loss: 0.01383106837876969\n",
      "    num_agent_steps_sampled: 463000\n",
      "    num_agent_steps_trained: 463000\n",
      "    num_steps_sampled: 463000\n",
      "    num_steps_trained: 463000\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.38461538461539\n",
      "    ram_util_percent: 38.816923076923075\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869253363959161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.46974289118626\n",
      "    mean_inference_ms: 1.9277895134062022\n",
      "    mean_raw_obs_processing_ms: 2.0666596706292357\n",
      "  time_since_restore: 11305.149595022202\n",
      "  time_this_iter_s: 45.50521373748779\n",
      "  time_total_s: 11305.149595022202\n",
      "  timers:\n",
      "    learn_throughput: 1442.274\n",
      "    learn_time_ms: 693.349\n",
      "    load_throughput: 42166.056\n",
      "    load_time_ms: 23.716\n",
      "    sample_throughput: 33.019\n",
      "    sample_time_ms: 30285.454\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1635074179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463000\n",
      "  training_iteration: 463\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   463</td><td style=\"text-align: right;\">         11305.1</td><td style=\"text-align: right;\">463000</td><td style=\"text-align: right;\"> -2.4586</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            245.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-16-47\n",
      "  done: false\n",
      "  episode_len_mean: 245.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.453599999999992\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1536\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0045717015083523785\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5037715206543605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004508917467950211\n",
      "          policy_loss: -0.011919149094157748\n",
      "          total_loss: -0.004350397570265664\n",
      "          vf_explained_var: 0.16708742082118988\n",
      "          vf_loss: 0.012585854375114043\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.495\n",
      "    ram_util_percent: 38.822500000000005\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869216773610734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.47919433969647\n",
      "    mean_inference_ms: 1.9277952460139292\n",
      "    mean_raw_obs_processing_ms: 2.0685950027740847\n",
      "  time_since_restore: 11333.091197252274\n",
      "  time_this_iter_s: 27.94160223007202\n",
      "  time_total_s: 11333.091197252274\n",
      "  timers:\n",
      "    learn_throughput: 1444.118\n",
      "    learn_time_ms: 692.464\n",
      "    load_throughput: 42155.461\n",
      "    load_time_ms: 23.722\n",
      "    sample_throughput: 32.985\n",
      "    sample_time_ms: 30317.03\n",
      "    update_time_ms: 2.515\n",
      "  timestamp: 1635074207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 464\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   464</td><td style=\"text-align: right;\">         11333.1</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\"> -2.4536</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            245.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-17-14\n",
      "  done: false\n",
      "  episode_len_mean: 245.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4502999999999915\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1540\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0022858507541761892\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4998779508802626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0062916860894918925\n",
      "          policy_loss: -0.12058846867746777\n",
      "          total_loss: -0.1101001624431875\n",
      "          vf_explained_var: 0.21542085707187653\n",
      "          vf_loss: 0.015472705558770233\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_agent_steps_trained: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.01025641025641\n",
      "    ram_util_percent: 38.92820512820513\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038691802417856296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.488516093538298\n",
      "    mean_inference_ms: 1.9278001445767137\n",
      "    mean_raw_obs_processing_ms: 2.0698186592381322\n",
      "  time_since_restore: 11359.949786186218\n",
      "  time_this_iter_s: 26.858588933944702\n",
      "  time_total_s: 11359.949786186218\n",
      "  timers:\n",
      "    learn_throughput: 1442.8\n",
      "    learn_time_ms: 693.097\n",
      "    load_throughput: 42224.976\n",
      "    load_time_ms: 23.683\n",
      "    sample_throughput: 33.087\n",
      "    sample_time_ms: 30223.393\n",
      "    update_time_ms: 2.515\n",
      "  timestamp: 1635074234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 465\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">         11359.9</td><td style=\"text-align: right;\">465000</td><td style=\"text-align: right;\"> -2.4503</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            245.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 466000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-17-40\n",
      "  done: false\n",
      "  episode_len_mean: 244.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4468999999999914\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1545\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0022858507541761892\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6106579787201352\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0050211155722431995\n",
      "          policy_loss: -0.00814091128607591\n",
      "          total_loss: -0.0013729749040471183\n",
      "          vf_explained_var: 0.2904452979564667\n",
      "          vf_loss: 0.012863039722045262\n",
      "    num_agent_steps_sampled: 466000\n",
      "    num_agent_steps_trained: 466000\n",
      "    num_steps_sampled: 466000\n",
      "    num_steps_trained: 466000\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.349999999999994\n",
      "    ram_util_percent: 38.96578947368421\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038691339436040806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.500095573225508\n",
      "    mean_inference_ms: 1.927805045485298\n",
      "    mean_raw_obs_processing_ms: 2.0703895271805632\n",
      "  time_since_restore: 11386.597771167755\n",
      "  time_this_iter_s: 26.647984981536865\n",
      "  time_total_s: 11386.597771167755\n",
      "  timers:\n",
      "    learn_throughput: 1441.902\n",
      "    learn_time_ms: 693.528\n",
      "    load_throughput: 42320.966\n",
      "    load_time_ms: 23.629\n",
      "    sample_throughput: 35.264\n",
      "    sample_time_ms: 28357.877\n",
      "    update_time_ms: 2.478\n",
      "  timestamp: 1635074260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 466000\n",
      "  training_iteration: 466\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   466</td><td style=\"text-align: right;\">         11386.6</td><td style=\"text-align: right;\">466000</td><td style=\"text-align: right;\"> -2.4469</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            244.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 467000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-18-06\n",
      "  done: false\n",
      "  episode_len_mean: 244.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4467999999999916\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1549\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0022858507541761892\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6071074515581131\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011544455431649206\n",
      "          policy_loss: 0.029212177875969143\n",
      "          total_loss: 0.033688525441620085\n",
      "          vf_explained_var: 0.20244884490966797\n",
      "          vf_loss: 0.010521033152730928\n",
      "    num_agent_steps_sampled: 467000\n",
      "    num_agent_steps_trained: 467000\n",
      "    num_steps_sampled: 467000\n",
      "    num_steps_trained: 467000\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11621621621623\n",
      "    ram_util_percent: 38.983783783783785\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869093656440801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.509266898446782\n",
      "    mean_inference_ms: 1.9278080807421225\n",
      "    mean_raw_obs_processing_ms: 2.070869619549127\n",
      "  time_since_restore: 11412.59688949585\n",
      "  time_this_iter_s: 25.999118328094482\n",
      "  time_total_s: 11412.59688949585\n",
      "  timers:\n",
      "    learn_throughput: 1441.884\n",
      "    learn_time_ms: 693.537\n",
      "    load_throughput: 42330.961\n",
      "    load_time_ms: 23.623\n",
      "    sample_throughput: 35.288\n",
      "    sample_time_ms: 28338.45\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1635074286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 467000\n",
      "  training_iteration: 467\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   467</td><td style=\"text-align: right;\">         11412.6</td><td style=\"text-align: right;\">467000</td><td style=\"text-align: right;\"> -2.4468</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            244.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-18-32\n",
      "  done: false\n",
      "  episode_len_mean: 244.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.448899999999991\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1553\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0022858507541761892\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4313333921962314\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.029675304535453507\n",
      "          policy_loss: 0.03695117558042208\n",
      "          total_loss: 0.045251416663328804\n",
      "          vf_explained_var: 0.10378678888082504\n",
      "          vf_loss: 0.012545740832057265\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.348648648648656\n",
      "    ram_util_percent: 38.97027027027027\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869052707744772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.518458884012674\n",
      "    mean_inference_ms: 1.9278112449167173\n",
      "    mean_raw_obs_processing_ms: 2.0713581139837047\n",
      "  time_since_restore: 11438.647772312164\n",
      "  time_this_iter_s: 26.050882816314697\n",
      "  time_total_s: 11438.647772312164\n",
      "  timers:\n",
      "    learn_throughput: 1440.808\n",
      "    learn_time_ms: 694.055\n",
      "    load_throughput: 42437.694\n",
      "    load_time_ms: 23.564\n",
      "    sample_throughput: 35.483\n",
      "    sample_time_ms: 28182.481\n",
      "    update_time_ms: 2.493\n",
      "  timestamp: 1635074312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 468\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   468</td><td style=\"text-align: right;\">         11438.6</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\"> -2.4489</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            244.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 469000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-19-00\n",
      "  done: false\n",
      "  episode_len_mean: 243.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4353999999999916\n",
      "  episode_reward_min: -2.9699999999999807\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1557\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0034287761312642834\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5336385677258174\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007603027412489875\n",
      "          policy_loss: 0.04248811420467165\n",
      "          total_loss: 0.04898253215683831\n",
      "          vf_explained_var: 0.161097452044487\n",
      "          vf_loss: 0.011804735174195634\n",
      "    num_agent_steps_sampled: 469000\n",
      "    num_agent_steps_trained: 469000\n",
      "    num_steps_sampled: 469000\n",
      "    num_steps_trained: 469000\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.47435897435897\n",
      "    ram_util_percent: 38.94615384615387\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869009196289754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.52779287793796\n",
      "    mean_inference_ms: 1.9278149211746176\n",
      "    mean_raw_obs_processing_ms: 2.0718966021256433\n",
      "  time_since_restore: 11465.843452453613\n",
      "  time_this_iter_s: 27.195680141448975\n",
      "  time_total_s: 11465.843452453613\n",
      "  timers:\n",
      "    learn_throughput: 1441.247\n",
      "    learn_time_ms: 693.844\n",
      "    load_throughput: 42050.985\n",
      "    load_time_ms: 23.781\n",
      "    sample_throughput: 35.514\n",
      "    sample_time_ms: 28158.15\n",
      "    update_time_ms: 2.5\n",
      "  timestamp: 1635074340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 469000\n",
      "  training_iteration: 469\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         11465.8</td><td style=\"text-align: right;\">469000</td><td style=\"text-align: right;\"> -2.4354</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">            243.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 470000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-19-42\n",
      "  done: false\n",
      "  episode_len_mean: 243.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4374999999999925\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1561\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0034287761312642834\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7539206246534983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015981233656581394\n",
      "          policy_loss: 0.023796176165342332\n",
      "          total_loss: 0.029311712582906088\n",
      "          vf_explained_var: 0.13857999444007874\n",
      "          vf_loss: 0.012999943395455679\n",
      "    num_agent_steps_sampled: 470000\n",
      "    num_agent_steps_trained: 470000\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.044262295081964\n",
      "    ram_util_percent: 38.821311475409836\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386896810789614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.53712966706205\n",
      "    mean_inference_ms: 1.9278186851171728\n",
      "    mean_raw_obs_processing_ms: 2.073890713971919\n",
      "  time_since_restore: 11508.315004110336\n",
      "  time_this_iter_s: 42.47155165672302\n",
      "  time_total_s: 11508.315004110336\n",
      "  timers:\n",
      "    learn_throughput: 1441.237\n",
      "    learn_time_ms: 693.849\n",
      "    load_throughput: 41293.168\n",
      "    load_time_ms: 24.217\n",
      "    sample_throughput: 33.744\n",
      "    sample_time_ms: 29634.914\n",
      "    update_time_ms: 2.517\n",
      "  timestamp: 1635074382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 470\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   470</td><td style=\"text-align: right;\">         11508.3</td><td style=\"text-align: right;\">470000</td><td style=\"text-align: right;\"> -2.4375</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            243.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 471000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-20-07\n",
      "  done: false\n",
      "  episode_len_mean: 243.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4358999999999913\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1564\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0034287761312642834\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6404309302568436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013497200776084127\n",
      "          policy_loss: -0.006227126883135902\n",
      "          total_loss: -0.002898943962322341\n",
      "          vf_explained_var: 0.21916639804840088\n",
      "          vf_loss: 0.009686213561023276\n",
      "    num_agent_steps_sampled: 471000\n",
      "    num_agent_steps_trained: 471000\n",
      "    num_steps_sampled: 471000\n",
      "    num_steps_trained: 471000\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.962857142857146\n",
      "    ram_util_percent: 38.88571428571429\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868941207096111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.544103216258502\n",
      "    mean_inference_ms: 1.9278221140595502\n",
      "    mean_raw_obs_processing_ms: 2.0753772333107015\n",
      "  time_since_restore: 11533.428020954132\n",
      "  time_this_iter_s: 25.113016843795776\n",
      "  time_total_s: 11533.428020954132\n",
      "  timers:\n",
      "    learn_throughput: 1441.725\n",
      "    learn_time_ms: 693.614\n",
      "    load_throughput: 41023.699\n",
      "    load_time_ms: 24.376\n",
      "    sample_throughput: 34.046\n",
      "    sample_time_ms: 29371.949\n",
      "    update_time_ms: 2.503\n",
      "  timestamp: 1635074407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471000\n",
      "  training_iteration: 471\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   471</td><td style=\"text-align: right;\">         11533.4</td><td style=\"text-align: right;\">471000</td><td style=\"text-align: right;\"> -2.4359</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            243.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-20-34\n",
      "  done: false\n",
      "  episode_len_mean: 242.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4280999999999917\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1568\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0034287761312642834\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.49535256557994417\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0069641625690025554\n",
      "          policy_loss: -0.1121337184475528\n",
      "          total_loss: -0.10159673591454824\n",
      "          vf_explained_var: 0.11578579246997833\n",
      "          vf_loss: 0.015466629103240039\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.46410256410256\n",
      "    ram_util_percent: 38.902564102564114\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038689064876114794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.553552618873617\n",
      "    mean_inference_ms: 1.927826510165953\n",
      "    mean_raw_obs_processing_ms: 2.076583024231752\n",
      "  time_since_restore: 11560.253339290619\n",
      "  time_this_iter_s: 26.825318336486816\n",
      "  time_total_s: 11560.253339290619\n",
      "  timers:\n",
      "    learn_throughput: 1444.199\n",
      "    learn_time_ms: 692.426\n",
      "    load_throughput: 41037.346\n",
      "    load_time_ms: 24.368\n",
      "    sample_throughput: 34.085\n",
      "    sample_time_ms: 29338.078\n",
      "    update_time_ms: 2.498\n",
      "  timestamp: 1635074434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 472\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">         11560.3</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\"> -2.4281</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            242.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 473000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-21-01\n",
      "  done: false\n",
      "  episode_len_mean: 242.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.4251999999999923\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1573\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0034287761312642834\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4053583976295259\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00582465417397739\n",
      "          policy_loss: 0.005617159729202588\n",
      "          total_loss: 0.01572328367167049\n",
      "          vf_explained_var: 0.17749932408332825\n",
      "          vf_loss: 0.014139739403294192\n",
      "    num_agent_steps_sampled: 473000\n",
      "    num_agent_steps_trained: 473000\n",
      "    num_steps_sampled: 473000\n",
      "    num_steps_trained: 473000\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.09487179487179\n",
      "    ram_util_percent: 38.925641025641035\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868864475338678\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.565325640047313\n",
      "    mean_inference_ms: 1.9278313024181835\n",
      "    mean_raw_obs_processing_ms: 2.0771794442428893\n",
      "  time_since_restore: 11587.402798175812\n",
      "  time_this_iter_s: 27.14945888519287\n",
      "  time_total_s: 11587.402798175812\n",
      "  timers:\n",
      "    learn_throughput: 1443.306\n",
      "    learn_time_ms: 692.854\n",
      "    load_throughput: 40805.029\n",
      "    load_time_ms: 24.507\n",
      "    sample_throughput: 36.361\n",
      "    sample_time_ms: 27501.947\n",
      "    update_time_ms: 2.486\n",
      "  timestamp: 1635074461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 473000\n",
      "  training_iteration: 473\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         11587.4</td><td style=\"text-align: right;\">473000</td><td style=\"text-align: right;\"> -2.4252</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            242.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 474000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 241.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.419799999999992\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1577\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0034287761312642834\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.35360413591066997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003433067286140733\n",
      "          policy_loss: 0.0347694123784701\n",
      "          total_loss: 0.04290841387377845\n",
      "          vf_explained_var: 0.16618283092975616\n",
      "          vf_loss: 0.011663274891260598\n",
      "    num_agent_steps_sampled: 474000\n",
      "    num_agent_steps_trained: 474000\n",
      "    num_steps_sampled: 474000\n",
      "    num_steps_trained: 474000\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.38717948717949\n",
      "    ram_util_percent: 38.97435897435898\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868832293338076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.574690857751598\n",
      "    mean_inference_ms: 1.9278357142420313\n",
      "    mean_raw_obs_processing_ms: 2.0776877062159507\n",
      "  time_since_restore: 11614.91119647026\n",
      "  time_this_iter_s: 27.508398294448853\n",
      "  time_total_s: 11614.91119647026\n",
      "  timers:\n",
      "    learn_throughput: 1440.607\n",
      "    learn_time_ms: 694.152\n",
      "    load_throughput: 40786.181\n",
      "    load_time_ms: 24.518\n",
      "    sample_throughput: 36.42\n",
      "    sample_time_ms: 27457.384\n",
      "    update_time_ms: 2.426\n",
      "  timestamp: 1635074489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 474000\n",
      "  training_iteration: 474\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   474</td><td style=\"text-align: right;\">         11614.9</td><td style=\"text-align: right;\">474000</td><td style=\"text-align: right;\"> -2.4198</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            241.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 475000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-21-56\n",
      "  done: false\n",
      "  episode_len_mean: 241.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.4113999999999924\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1581\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0017143880656321417\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.37613723542955185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001997949553281507\n",
      "          policy_loss: -0.014846131039990319\n",
      "          total_loss: -0.00582318181792895\n",
      "          vf_explained_var: 0.09102614969015121\n",
      "          vf_loss: 0.012780897298620806\n",
      "    num_agent_steps_sampled: 475000\n",
      "    num_agent_steps_trained: 475000\n",
      "    num_steps_sampled: 475000\n",
      "    num_steps_trained: 475000\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.45384615384615\n",
      "    ram_util_percent: 38.97435897435899\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868800381599331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.584220436458345\n",
      "    mean_inference_ms: 1.9278406096287626\n",
      "    mean_raw_obs_processing_ms: 2.078202961341698\n",
      "  time_since_restore: 11642.538842439651\n",
      "  time_this_iter_s: 27.62764596939087\n",
      "  time_total_s: 11642.538842439651\n",
      "  timers:\n",
      "    learn_throughput: 1439.266\n",
      "    learn_time_ms: 694.799\n",
      "    load_throughput: 41025.143\n",
      "    load_time_ms: 24.375\n",
      "    sample_throughput: 36.319\n",
      "    sample_time_ms: 27533.81\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1635074516\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 475000\n",
      "  training_iteration: 475\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   475</td><td style=\"text-align: right;\">         11642.5</td><td style=\"text-align: right;\">475000</td><td style=\"text-align: right;\"> -2.4114</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            241.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-22-23\n",
      "  done: false\n",
      "  episode_len_mean: 240.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.4071999999999925\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1585\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008571940328160709\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4811196943124135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0038778427562814145\n",
      "          policy_loss: -0.06339124275578392\n",
      "          total_loss: -0.05626857289009624\n",
      "          vf_explained_var: 0.1916920393705368\n",
      "          vf_loss: 0.011930544612308342\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.01578947368421\n",
      "    ram_util_percent: 38.91315789473684\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868767832165299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.593738015524323\n",
      "    mean_inference_ms: 1.9278450194253915\n",
      "    mean_raw_obs_processing_ms: 2.0787259062695678\n",
      "  time_since_restore: 11668.955207586288\n",
      "  time_this_iter_s: 26.416365146636963\n",
      "  time_total_s: 11668.955207586288\n",
      "  timers:\n",
      "    learn_throughput: 1438.656\n",
      "    learn_time_ms: 695.093\n",
      "    load_throughput: 40701.64\n",
      "    load_time_ms: 24.569\n",
      "    sample_throughput: 36.35\n",
      "    sample_time_ms: 27510.157\n",
      "    update_time_ms: 2.444\n",
      "  timestamp: 1635074543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 476\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   476</td><td style=\"text-align: right;\">           11669</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\"> -2.4072</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            240.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 477000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 240.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.406599999999993\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1590\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00042859701640803543\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4278231352567673\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005238458748608268\n",
      "          policy_loss: -0.029760701374875173\n",
      "          total_loss: -0.018157261030541525\n",
      "          vf_explained_var: 0.17452532052993774\n",
      "          vf_loss: 0.015879427124228743\n",
      "    num_agent_steps_sampled: 477000\n",
      "    num_agent_steps_trained: 477000\n",
      "    num_steps_sampled: 477000\n",
      "    num_steps_trained: 477000\n",
      "  iterations_since_restore: 477\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.651612903225804\n",
      "    ram_util_percent: 38.908064516129045\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868725067717188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.605516160965934\n",
      "    mean_inference_ms: 1.9278496531692093\n",
      "    mean_raw_obs_processing_ms: 2.0811790675376227\n",
      "  time_since_restore: 11712.678765535355\n",
      "  time_this_iter_s: 43.72355794906616\n",
      "  time_total_s: 11712.678765535355\n",
      "  timers:\n",
      "    learn_throughput: 1437.134\n",
      "    learn_time_ms: 695.829\n",
      "    load_throughput: 40536.465\n",
      "    load_time_ms: 24.669\n",
      "    sample_throughput: 34.151\n",
      "    sample_time_ms: 29281.808\n",
      "    update_time_ms: 2.44\n",
      "  timestamp: 1635074587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 477000\n",
      "  training_iteration: 477\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   477</td><td style=\"text-align: right;\">         11712.7</td><td style=\"text-align: right;\">477000</td><td style=\"text-align: right;\"> -2.4066</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            240.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 478000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-23-26\n",
      "  done: false\n",
      "  episode_len_mean: 244.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.4426999999999923\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1592\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00042859701640803543\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.167068295346366\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028468615493654505\n",
      "          policy_loss: 0.05486786746316486\n",
      "          total_loss: 0.048718947834438746\n",
      "          vf_explained_var: 0.2801617383956909\n",
      "          vf_loss: 0.0055095552085226195\n",
      "    num_agent_steps_sampled: 478000\n",
      "    num_agent_steps_trained: 478000\n",
      "    num_steps_sampled: 478000\n",
      "    num_steps_trained: 478000\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.88928571428571\n",
      "    ram_util_percent: 38.72500000000001\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868706112486141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.609820905232\n",
      "    mean_inference_ms: 1.9278511583768896\n",
      "    mean_raw_obs_processing_ms: 2.082175921504832\n",
      "  time_since_restore: 11732.071733474731\n",
      "  time_this_iter_s: 19.39296793937683\n",
      "  time_total_s: 11732.071733474731\n",
      "  timers:\n",
      "    learn_throughput: 1439.134\n",
      "    learn_time_ms: 694.862\n",
      "    load_throughput: 40154.479\n",
      "    load_time_ms: 24.904\n",
      "    sample_throughput: 34.944\n",
      "    sample_time_ms: 28616.824\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1635074606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 478000\n",
      "  training_iteration: 478\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   478</td><td style=\"text-align: right;\">         11732.1</td><td style=\"text-align: right;\">478000</td><td style=\"text-align: right;\"> -2.4427</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            244.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 479000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-23-49\n",
      "  done: false\n",
      "  episode_len_mean: 245.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.4585999999999917\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1596\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.000642895524612053\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7090266399913364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007998660106419524\n",
      "          policy_loss: 0.018046481410662334\n",
      "          total_loss: 0.022887261791361704\n",
      "          vf_explained_var: 0.214222252368927\n",
      "          vf_loss: 0.01192590349043409\n",
      "    num_agent_steps_sampled: 479000\n",
      "    num_agent_steps_trained: 479000\n",
      "    num_steps_sampled: 479000\n",
      "    num_steps_trained: 479000\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18181818181817\n",
      "    ram_util_percent: 38.803030303030305\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868671991941422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.61824577995121\n",
      "    mean_inference_ms: 1.9278538602884303\n",
      "    mean_raw_obs_processing_ms: 2.084093761794668\n",
      "  time_since_restore: 11755.347751617432\n",
      "  time_this_iter_s: 23.276018142700195\n",
      "  time_total_s: 11755.347751617432\n",
      "  timers:\n",
      "    learn_throughput: 1439.452\n",
      "    learn_time_ms: 694.709\n",
      "    load_throughput: 40065.606\n",
      "    load_time_ms: 24.959\n",
      "    sample_throughput: 35.43\n",
      "    sample_time_ms: 28224.969\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1635074629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479000\n",
      "  training_iteration: 479\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   479</td><td style=\"text-align: right;\">         11755.3</td><td style=\"text-align: right;\">479000</td><td style=\"text-align: right;\"> -2.4586</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            245.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-24-14\n",
      "  done: false\n",
      "  episode_len_mean: 246.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.4690999999999916\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1599\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.000642895524612053\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.625228034125434\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006116358015042482\n",
      "          policy_loss: -0.08631967935297225\n",
      "          total_loss: -0.08222464389271206\n",
      "          vf_explained_var: 0.18065688014030457\n",
      "          vf_loss: 0.010343383330230911\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.28888888888889\n",
      "    ram_util_percent: 38.872222222222234\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038686460782386085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.62441716661681\n",
      "    mean_inference_ms: 1.92785535955455\n",
      "    mean_raw_obs_processing_ms: 2.0851411494098846\n",
      "  time_since_restore: 11780.507437944412\n",
      "  time_this_iter_s: 25.15968632698059\n",
      "  time_total_s: 11780.507437944412\n",
      "  timers:\n",
      "    learn_throughput: 1439.205\n",
      "    learn_time_ms: 694.828\n",
      "    load_throughput: 40305.818\n",
      "    load_time_ms: 24.81\n",
      "    sample_throughput: 37.745\n",
      "    sample_time_ms: 26493.775\n",
      "    update_time_ms: 2.425\n",
      "  timestamp: 1635074654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 480\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   480</td><td style=\"text-align: right;\">         11780.5</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\"> -2.4691</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            246.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 481000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-24-39\n",
      "  done: false\n",
      "  episode_len_mean: 247.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.4752999999999914\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1603\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.000642895524612053\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5379651602771547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0020797805768907945\n",
      "          policy_loss: -0.09064122579163975\n",
      "          total_loss: -0.08180041681561205\n",
      "          vf_explained_var: 0.10819332301616669\n",
      "          vf_loss: 0.014219125867303875\n",
      "    num_agent_steps_sampled: 481000\n",
      "    num_agent_steps_trained: 481000\n",
      "    num_steps_sampled: 481000\n",
      "    num_steps_trained: 481000\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.344444444444456\n",
      "    ram_util_percent: 38.96666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868608919264504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.632409517453553\n",
      "    mean_inference_ms: 1.9278564972223904\n",
      "    mean_raw_obs_processing_ms: 2.0855364682104747\n",
      "  time_since_restore: 11805.431238412857\n",
      "  time_this_iter_s: 24.923800468444824\n",
      "  time_total_s: 11805.431238412857\n",
      "  timers:\n",
      "    learn_throughput: 1438.931\n",
      "    learn_time_ms: 694.96\n",
      "    load_throughput: 40649.807\n",
      "    load_time_ms: 24.6\n",
      "    sample_throughput: 37.772\n",
      "    sample_time_ms: 26474.963\n",
      "    update_time_ms: 2.4\n",
      "  timestamp: 1635074679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 481000\n",
      "  training_iteration: 481\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   481</td><td style=\"text-align: right;\">         11805.4</td><td style=\"text-align: right;\">481000</td><td style=\"text-align: right;\"> -2.4753</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            247.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 482000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-25-06\n",
      "  done: false\n",
      "  episode_len_mean: 247.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.4781999999999913\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1607\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003214477623060265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5531416555245717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0043515479048641245\n",
      "          policy_loss: -0.025485532813602024\n",
      "          total_loss: -0.018066103259722393\n",
      "          vf_explained_var: 0.09993097186088562\n",
      "          vf_loss: 0.012949447106156085\n",
      "    num_agent_steps_sampled: 482000\n",
      "    num_agent_steps_trained: 482000\n",
      "    num_steps_sampled: 482000\n",
      "    num_steps_trained: 482000\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.340540540540545\n",
      "    ram_util_percent: 38.994594594594595\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038685718850244674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.640336253923607\n",
      "    mean_inference_ms: 1.9278573758574669\n",
      "    mean_raw_obs_processing_ms: 2.0859386636083275\n",
      "  time_since_restore: 11831.599653720856\n",
      "  time_this_iter_s: 26.168415307998657\n",
      "  time_total_s: 11831.599653720856\n",
      "  timers:\n",
      "    learn_throughput: 1436.422\n",
      "    learn_time_ms: 696.174\n",
      "    load_throughput: 40905.233\n",
      "    load_time_ms: 24.447\n",
      "    sample_throughput: 37.867\n",
      "    sample_time_ms: 26408.243\n",
      "    update_time_ms: 2.388\n",
      "  timestamp: 1635074706\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 482000\n",
      "  training_iteration: 482\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   482</td><td style=\"text-align: right;\">         11831.6</td><td style=\"text-align: right;\">482000</td><td style=\"text-align: right;\"> -2.4782</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            247.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 483000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-25-31\n",
      "  done: false\n",
      "  episode_len_mean: 248.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.486899999999991\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1611\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00016072388115301325\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5301475054687924\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03141088604557933\n",
      "          policy_loss: -0.03818185677131017\n",
      "          total_loss: -0.031019145250320436\n",
      "          vf_explained_var: 0.18600669503211975\n",
      "          vf_loss: 0.012459138304822974\n",
      "    num_agent_steps_sampled: 483000\n",
      "    num_agent_steps_trained: 483000\n",
      "    num_steps_sampled: 483000\n",
      "    num_steps_trained: 483000\n",
      "  iterations_since_restore: 483\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.041666666666664\n",
      "    ram_util_percent: 38.92222222222222\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386853112486305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.648143672049493\n",
      "    mean_inference_ms: 1.9278578013697176\n",
      "    mean_raw_obs_processing_ms: 2.0863077768835603\n",
      "  time_since_restore: 11856.879429578781\n",
      "  time_this_iter_s: 25.279775857925415\n",
      "  time_total_s: 11856.879429578781\n",
      "  timers:\n",
      "    learn_throughput: 1438.335\n",
      "    learn_time_ms: 695.248\n",
      "    load_throughput: 41255.192\n",
      "    load_time_ms: 24.239\n",
      "    sample_throughput: 38.135\n",
      "    sample_time_ms: 26222.43\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1635074731\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 483000\n",
      "  training_iteration: 483\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   483</td><td style=\"text-align: right;\">         11856.9</td><td style=\"text-align: right;\">483000</td><td style=\"text-align: right;\"> -2.4869</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            248.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-25-59\n",
      "  done: false\n",
      "  episode_len_mean: 248.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.486599999999991\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1616\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00024108582172951992\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.270597102244695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005049798542290852\n",
      "          policy_loss: -0.028436937265925936\n",
      "          total_loss: -0.017146163351005977\n",
      "          vf_explained_var: 0.22691872715950012\n",
      "          vf_loss: 0.013995527651988797\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.2575\n",
      "    ram_util_percent: 38.875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038684807135656814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.65776651115085\n",
      "    mean_inference_ms: 1.9278585525935876\n",
      "    mean_raw_obs_processing_ms: 2.086821476916772\n",
      "  time_since_restore: 11884.583441019058\n",
      "  time_this_iter_s: 27.7040114402771\n",
      "  time_total_s: 11884.583441019058\n",
      "  timers:\n",
      "    learn_throughput: 1440.952\n",
      "    learn_time_ms: 693.986\n",
      "    load_throughput: 41323.64\n",
      "    load_time_ms: 24.199\n",
      "    sample_throughput: 38.105\n",
      "    sample_time_ms: 26243.281\n",
      "    update_time_ms: 2.385\n",
      "  timestamp: 1635074759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 484\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   484</td><td style=\"text-align: right;\">         11884.6</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\"> -2.4866</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            248.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 485000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 248.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.4821999999999904\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1620\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00024108582172951992\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5276336683167352\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06659235464985581\n",
      "          policy_loss: 0.014644413855340746\n",
      "          total_loss: 0.019840757507416935\n",
      "          vf_explained_var: 0.23416535556316376\n",
      "          vf_loss: 0.010456625962009032\n",
      "    num_agent_steps_sampled: 485000\n",
      "    num_agent_steps_trained: 485000\n",
      "    num_steps_sampled: 485000\n",
      "    num_steps_trained: 485000\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.98730158730159\n",
      "    ram_util_percent: 38.780952380952385\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038684394303997754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.66535611480792\n",
      "    mean_inference_ms: 1.9278589703643099\n",
      "    mean_raw_obs_processing_ms: 2.0886431444609808\n",
      "  time_since_restore: 11928.64632821083\n",
      "  time_this_iter_s: 44.06288719177246\n",
      "  time_total_s: 11928.64632821083\n",
      "  timers:\n",
      "    learn_throughput: 1442.428\n",
      "    learn_time_ms: 693.276\n",
      "    load_throughput: 40998.556\n",
      "    load_time_ms: 24.391\n",
      "    sample_throughput: 35.859\n",
      "    sample_time_ms: 27887.305\n",
      "    update_time_ms: 2.375\n",
      "  timestamp: 1635074803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485000\n",
      "  training_iteration: 485\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   485</td><td style=\"text-align: right;\">         11928.6</td><td style=\"text-align: right;\">485000</td><td style=\"text-align: right;\"> -2.4822</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            248.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 486000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 249.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.4981999999999904\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1624\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003616287325942798\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8226053529315525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012039683160204914\n",
      "          policy_loss: -0.010865830298927095\n",
      "          total_loss: -0.005337354458040662\n",
      "          vf_explained_var: 0.0760117918252945\n",
      "          vf_loss: 0.01375017466230525\n",
      "    num_agent_steps_sampled: 486000\n",
      "    num_agent_steps_trained: 486000\n",
      "    num_steps_sampled: 486000\n",
      "    num_steps_trained: 486000\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.52162162162162\n",
      "    ram_util_percent: 38.71891891891891\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038683976871658136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.672768565422338\n",
      "    mean_inference_ms: 1.9278594228231056\n",
      "    mean_raw_obs_processing_ms: 2.090470380282273\n",
      "  time_since_restore: 11954.577591896057\n",
      "  time_this_iter_s: 25.93126368522644\n",
      "  time_total_s: 11954.577591896057\n",
      "  timers:\n",
      "    learn_throughput: 1442.242\n",
      "    learn_time_ms: 693.365\n",
      "    load_throughput: 40983.173\n",
      "    load_time_ms: 24.4\n",
      "    sample_throughput: 35.921\n",
      "    sample_time_ms: 27838.679\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1635074829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 486000\n",
      "  training_iteration: 486\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   486</td><td style=\"text-align: right;\">         11954.6</td><td style=\"text-align: right;\">486000</td><td style=\"text-align: right;\"> -2.4982</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            249.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 487000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-27-33\n",
      "  done: false\n",
      "  episode_len_mean: 250.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.50899999999999\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1627\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003616287325942798\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9059902191162109\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013896343767451608\n",
      "          policy_loss: -0.00018787475095854864\n",
      "          total_loss: 0.000705305321349038\n",
      "          vf_explained_var: 0.04639450088143349\n",
      "          vf_loss: 0.009948060696478933\n",
      "    num_agent_steps_sampled: 487000\n",
      "    num_agent_steps_trained: 487000\n",
      "    num_steps_sampled: 487000\n",
      "    num_steps_trained: 487000\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.21764705882353\n",
      "    ram_util_percent: 38.805882352941175\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386836825429985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.678188326265595\n",
      "    mean_inference_ms: 1.9278595448322602\n",
      "    mean_raw_obs_processing_ms: 2.0918114230988407\n",
      "  time_since_restore: 11978.63926076889\n",
      "  time_this_iter_s: 24.061668872833252\n",
      "  time_total_s: 11978.63926076889\n",
      "  timers:\n",
      "    learn_throughput: 1445.041\n",
      "    learn_time_ms: 692.022\n",
      "    load_throughput: 41115.871\n",
      "    load_time_ms: 24.322\n",
      "    sample_throughput: 38.649\n",
      "    sample_time_ms: 25873.821\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1635074853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487000\n",
      "  training_iteration: 487\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   487</td><td style=\"text-align: right;\">         11978.6</td><td style=\"text-align: right;\">487000</td><td style=\"text-align: right;\">  -2.509</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">             250.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-27-57\n",
      "  done: false\n",
      "  episode_len_mean: 252.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.5280999999999896\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1631\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003616287325942798\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.743389939599567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01737351756193806\n",
      "          policy_loss: -0.00615583856900533\n",
      "          total_loss: -7.198916541205512e-06\n",
      "          vf_explained_var: 0.10820574313402176\n",
      "          vf_loss: 0.013576255604210828\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.21142857142858\n",
      "    ram_util_percent: 38.82857142857143\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868328806606395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.685104399975486\n",
      "    mean_inference_ms: 1.927859301325006\n",
      "    mean_raw_obs_processing_ms: 2.0925157017883085\n",
      "  time_since_restore: 12002.915371894836\n",
      "  time_this_iter_s: 24.276111125946045\n",
      "  time_total_s: 12002.915371894836\n",
      "  timers:\n",
      "    learn_throughput: 1448.288\n",
      "    learn_time_ms: 690.47\n",
      "    load_throughput: 41203.681\n",
      "    load_time_ms: 24.27\n",
      "    sample_throughput: 37.931\n",
      "    sample_time_ms: 26363.683\n",
      "    update_time_ms: 2.478\n",
      "  timestamp: 1635074877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 488\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   488</td><td style=\"text-align: right;\">         12002.9</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\"> -2.5281</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            252.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 489000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 252.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.5284999999999895\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1635\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003616287325942798\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3274990412924025\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0038824450331764937\n",
      "          policy_loss: 0.02538892858558231\n",
      "          total_loss: 0.03333424015177621\n",
      "          vf_explained_var: 0.17112454771995544\n",
      "          vf_loss: 0.011218897346407176\n",
      "    num_agent_steps_sampled: 489000\n",
      "    num_agent_steps_trained: 489000\n",
      "    num_steps_sampled: 489000\n",
      "    num_steps_trained: 489000\n",
      "  iterations_since_restore: 489\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.0475\n",
      "    ram_util_percent: 38.8725\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868292817673146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.691955350479137\n",
      "    mean_inference_ms: 1.9278594015384447\n",
      "    mean_raw_obs_processing_ms: 2.09287698917671\n",
      "  time_since_restore: 12030.888701438904\n",
      "  time_this_iter_s: 27.973329544067383\n",
      "  time_total_s: 12030.888701438904\n",
      "  timers:\n",
      "    learn_throughput: 1446.556\n",
      "    learn_time_ms: 691.297\n",
      "    load_throughput: 41276.954\n",
      "    load_time_ms: 24.227\n",
      "    sample_throughput: 37.268\n",
      "    sample_time_ms: 26832.627\n",
      "    update_time_ms: 2.471\n",
      "  timestamp: 1635074905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 489000\n",
      "  training_iteration: 489\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   489</td><td style=\"text-align: right;\">         12030.9</td><td style=\"text-align: right;\">489000</td><td style=\"text-align: right;\"> -2.5285</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            252.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 490000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-28-53\n",
      "  done: false\n",
      "  episode_len_mean: 252.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.5289999999999897\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1639\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001808143662971399\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3640635096364551\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0015576908569343967\n",
      "          policy_loss: -0.009696588416894277\n",
      "          total_loss: -0.0015429151554902396\n",
      "          vf_explained_var: 0.18658307194709778\n",
      "          vf_loss: 0.011794029134843084\n",
      "    num_agent_steps_sampled: 490000\n",
      "    num_agent_steps_trained: 490000\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.45641025641026\n",
      "    ram_util_percent: 38.90769230769232\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868257021366325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.698810390958073\n",
      "    mean_inference_ms: 1.9278597777943765\n",
      "    mean_raw_obs_processing_ms: 2.093246175184112\n",
      "  time_since_restore: 12058.495658159256\n",
      "  time_this_iter_s: 27.606956720352173\n",
      "  time_total_s: 12058.495658159256\n",
      "  timers:\n",
      "    learn_throughput: 1447.084\n",
      "    learn_time_ms: 691.045\n",
      "    load_throughput: 41380.472\n",
      "    load_time_ms: 24.166\n",
      "    sample_throughput: 36.931\n",
      "    sample_time_ms: 27077.684\n",
      "    update_time_ms: 2.464\n",
      "  timestamp: 1635074933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 490\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   490</td><td style=\"text-align: right;\">         12058.5</td><td style=\"text-align: right;\">490000</td><td style=\"text-align: right;\">  -2.529</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">             252.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 491000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-29-20\n",
      "  done: false\n",
      "  episode_len_mean: 252.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.52529999999999\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1644\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.040718314856995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3862751437558068\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0043995018731814006\n",
      "          policy_loss: -0.02577675994899538\n",
      "          total_loss: -0.014154808057679071\n",
      "          vf_explained_var: 0.16607670485973358\n",
      "          vf_loss: 0.015484304415682952\n",
      "    num_agent_steps_sampled: 491000\n",
      "    num_agent_steps_trained: 491000\n",
      "    num_steps_sampled: 491000\n",
      "    num_steps_trained: 491000\n",
      "  iterations_since_restore: 491\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.202564102564104\n",
      "    ram_util_percent: 38.87948717948719\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868212246575373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.707358627257506\n",
      "    mean_inference_ms: 1.9278608097032892\n",
      "    mean_raw_obs_processing_ms: 2.093730088740971\n",
      "  time_since_restore: 12085.648561000824\n",
      "  time_this_iter_s: 27.152902841567993\n",
      "  time_total_s: 12085.648561000824\n",
      "  timers:\n",
      "    learn_throughput: 1445.905\n",
      "    learn_time_ms: 691.608\n",
      "    load_throughput: 41408.457\n",
      "    load_time_ms: 24.15\n",
      "    sample_throughput: 36.63\n",
      "    sample_time_ms: 27300.067\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1635074960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 491000\n",
      "  training_iteration: 491\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   491</td><td style=\"text-align: right;\">         12085.6</td><td style=\"text-align: right;\">491000</td><td style=\"text-align: right;\"> -2.5253</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            252.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-29-47\n",
      "  done: false\n",
      "  episode_len_mean: 252.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.52059999999999\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1648\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.5203591574284975e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3695612142483393\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0018421178422645963\n",
      "          policy_loss: 0.03066441458132532\n",
      "          total_loss: 0.03850139213932885\n",
      "          vf_explained_var: 0.12509416043758392\n",
      "          vf_loss: 0.011532508105867438\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.05128205128205\n",
      "    ram_util_percent: 38.83333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868177889131442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.714285128501253\n",
      "    mean_inference_ms: 1.9278621949342978\n",
      "    mean_raw_obs_processing_ms: 2.094116458251766\n",
      "  time_since_restore: 12113.054683446884\n",
      "  time_this_iter_s: 27.40612244606018\n",
      "  time_total_s: 12113.054683446884\n",
      "  timers:\n",
      "    learn_throughput: 1445.914\n",
      "    learn_time_ms: 691.604\n",
      "    load_throughput: 41288.819\n",
      "    load_time_ms: 24.22\n",
      "    sample_throughput: 36.465\n",
      "    sample_time_ms: 27423.765\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1635074987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 492\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   492</td><td style=\"text-align: right;\">         12113.1</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\"> -2.5206</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            252.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 493000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-30-31\n",
      "  done: false\n",
      "  episode_len_mean: 252.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.52069999999999\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1652\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2601795787142487e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.513493122988277\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0025179942921127463\n",
      "          policy_loss: 0.03321757217248281\n",
      "          total_loss: 0.04150960784819391\n",
      "          vf_explained_var: 0.1631402224302292\n",
      "          vf_loss: 0.01342691309336159\n",
      "    num_agent_steps_sampled: 493000\n",
      "    num_agent_steps_trained: 493000\n",
      "    num_steps_sampled: 493000\n",
      "    num_steps_trained: 493000\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.71428571428572\n",
      "    ram_util_percent: 38.75555555555557\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038681458480385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.721283736270134\n",
      "    mean_inference_ms: 1.927863877528135\n",
      "    mean_raw_obs_processing_ms: 2.095883388756821\n",
      "  time_since_restore: 12157.258121490479\n",
      "  time_this_iter_s: 44.20343804359436\n",
      "  time_total_s: 12157.258121490479\n",
      "  timers:\n",
      "    learn_throughput: 1446.287\n",
      "    learn_time_ms: 691.426\n",
      "    load_throughput: 41183.291\n",
      "    load_time_ms: 24.282\n",
      "    sample_throughput: 34.111\n",
      "    sample_time_ms: 29316.202\n",
      "    update_time_ms: 2.497\n",
      "  timestamp: 1635075031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 493000\n",
      "  training_iteration: 493\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   493</td><td style=\"text-align: right;\">         12157.3</td><td style=\"text-align: right;\">493000</td><td style=\"text-align: right;\"> -2.5207</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            252.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 494000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 252.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.52689999999999\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1656\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1300897893571244e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7238335059748755\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0041476560978174525\n",
      "          policy_loss: 0.02003611135813925\n",
      "          total_loss: 0.028577569872140884\n",
      "          vf_explained_var: 0.02938631922006607\n",
      "          vf_loss: 0.015779750007722113\n",
      "    num_agent_steps_sampled: 494000\n",
      "    num_agent_steps_trained: 494000\n",
      "    num_steps_sampled: 494000\n",
      "    num_steps_trained: 494000\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.775\n",
      "    ram_util_percent: 38.822222222222216\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038681145566209246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.72807426642751\n",
      "    mean_inference_ms: 1.9278656864819204\n",
      "    mean_raw_obs_processing_ms: 2.0976541520006653\n",
      "  time_since_restore: 12182.002986431122\n",
      "  time_this_iter_s: 24.74486494064331\n",
      "  time_total_s: 12182.002986431122\n",
      "  timers:\n",
      "    learn_throughput: 1444.649\n",
      "    learn_time_ms: 692.21\n",
      "    load_throughput: 41173.265\n",
      "    load_time_ms: 24.288\n",
      "    sample_throughput: 34.46\n",
      "    sample_time_ms: 29019.442\n",
      "    update_time_ms: 2.516\n",
      "  timestamp: 1635075056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 494000\n",
      "  training_iteration: 494\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   494</td><td style=\"text-align: right;\">           12182</td><td style=\"text-align: right;\">494000</td><td style=\"text-align: right;\"> -2.5269</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            252.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-31-22\n",
      "  done: false\n",
      "  episode_len_mean: 253.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.53269999999999\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1659\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.650448946785622e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.63135128153695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04510686756144982\n",
      "          policy_loss: -0.108363186650806\n",
      "          total_loss: -0.09846898333893882\n",
      "          vf_explained_var: 0.03574206680059433\n",
      "          vf_loss: 0.016207461452318564\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_agent_steps_trained: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 495\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.41388888888889\n",
      "    ram_util_percent: 38.83055555555555\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868089597189293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.733140887135917\n",
      "    mean_inference_ms: 1.9278674876423696\n",
      "    mean_raw_obs_processing_ms: 2.0982501282599473\n",
      "  time_since_restore: 12207.536666631699\n",
      "  time_this_iter_s: 25.533680200576782\n",
      "  time_total_s: 12207.536666631699\n",
      "  timers:\n",
      "    learn_throughput: 1444.195\n",
      "    learn_time_ms: 692.427\n",
      "    load_throughput: 41177.266\n",
      "    load_time_ms: 24.285\n",
      "    sample_throughput: 36.81\n",
      "    sample_time_ms: 27166.295\n",
      "    update_time_ms: 2.522\n",
      "  timestamp: 1635075082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 495\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   495</td><td style=\"text-align: right;\">         12207.5</td><td style=\"text-align: right;\">495000</td><td style=\"text-align: right;\"> -2.5327</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            253.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-31-46\n",
      "  done: false\n",
      "  episode_len_mean: 252.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.52779999999999\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1663\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.475673420178432e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7836781515015496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021704054228527208\n",
      "          policy_loss: -0.041098588539494406\n",
      "          total_loss: -0.037282721449931465\n",
      "          vf_explained_var: 0.11610429733991623\n",
      "          vf_loss: 0.011652462468999956\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.220000000000006\n",
      "    ram_util_percent: 38.88285714285714\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038680563218055374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.739853696295576\n",
      "    mean_inference_ms: 1.927870069468482\n",
      "    mean_raw_obs_processing_ms: 2.0985446099681044\n",
      "  time_since_restore: 12232.148201704025\n",
      "  time_this_iter_s: 24.61153507232666\n",
      "  time_total_s: 12232.148201704025\n",
      "  timers:\n",
      "    learn_throughput: 1443.874\n",
      "    learn_time_ms: 692.581\n",
      "    load_throughput: 40748.775\n",
      "    load_time_ms: 24.541\n",
      "    sample_throughput: 36.991\n",
      "    sample_time_ms: 27033.922\n",
      "    update_time_ms: 2.527\n",
      "  timestamp: 1635075106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 496\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   496</td><td style=\"text-align: right;\">         12232.1</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\"> -2.5278</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            252.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 497000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-32-08\n",
      "  done: false\n",
      "  episode_len_mean: 253.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.53979999999999\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1666\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2713510130267645e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7912097563346226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011756736065396348\n",
      "          policy_loss: -0.11565790623426438\n",
      "          total_loss: -0.10755742175711526\n",
      "          vf_explained_var: 0.16326633095741272\n",
      "          vf_loss: 0.01601243561340703\n",
      "    num_agent_steps_sampled: 497000\n",
      "    num_agent_steps_trained: 497000\n",
      "    num_steps_sampled: 497000\n",
      "    num_steps_trained: 497000\n",
      "  iterations_since_restore: 497\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.01935483870968\n",
      "    ram_util_percent: 38.92903225806453\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868029493575678\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.744553347864475\n",
      "    mean_inference_ms: 1.9278714752112003\n",
      "    mean_raw_obs_processing_ms: 2.0987898083500203\n",
      "  time_since_restore: 12253.628351211548\n",
      "  time_this_iter_s: 21.480149507522583\n",
      "  time_total_s: 12253.628351211548\n",
      "  timers:\n",
      "    learn_throughput: 1443.638\n",
      "    learn_time_ms: 692.694\n",
      "    load_throughput: 40720.568\n",
      "    load_time_ms: 24.558\n",
      "    sample_throughput: 37.347\n",
      "    sample_time_ms: 26775.715\n",
      "    update_time_ms: 2.43\n",
      "  timestamp: 1635075128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 497000\n",
      "  training_iteration: 497\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   497</td><td style=\"text-align: right;\">         12253.6</td><td style=\"text-align: right;\">497000</td><td style=\"text-align: right;\"> -2.5398</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            253.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 498000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-32-32\n",
      "  done: false\n",
      "  episode_len_mean: 256.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.559999999999989\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1670\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2713510130267645e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6667993525664012\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005383929788650911\n",
      "          policy_loss: 0.05317534117235078\n",
      "          total_loss: 0.05725832308332125\n",
      "          vf_explained_var: 0.2415134608745575\n",
      "          vf_loss: 0.010750909335911274\n",
      "    num_agent_steps_sampled: 498000\n",
      "    num_agent_steps_trained: 498000\n",
      "    num_steps_sampled: 498000\n",
      "    num_steps_trained: 498000\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.182352941176475\n",
      "    ram_util_percent: 38.882352941176464\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867993933795954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.750578721709836\n",
      "    mean_inference_ms: 1.9278734209924908\n",
      "    mean_raw_obs_processing_ms: 2.099096706757385\n",
      "  time_since_restore: 12277.258743286133\n",
      "  time_this_iter_s: 23.63039207458496\n",
      "  time_total_s: 12277.258743286133\n",
      "  timers:\n",
      "    learn_throughput: 1440.505\n",
      "    learn_time_ms: 694.201\n",
      "    load_throughput: 40869.401\n",
      "    load_time_ms: 24.468\n",
      "    sample_throughput: 37.439\n",
      "    sample_time_ms: 26709.764\n",
      "    update_time_ms: 2.404\n",
      "  timestamp: 1635075152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 498000\n",
      "  training_iteration: 498\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   498</td><td style=\"text-align: right;\">         12277.3</td><td style=\"text-align: right;\">498000</td><td style=\"text-align: right;\">   -2.56</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">               256</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 499000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-32-55\n",
      "  done: false\n",
      "  episode_len_mean: 257.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.574699999999989\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1674\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2713510130267645e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7592899726496802\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012445688505486174\n",
      "          policy_loss: 0.03978385221627023\n",
      "          total_loss: 0.043647872739368015\n",
      "          vf_explained_var: 0.06408434361219406\n",
      "          vf_loss: 0.01145676261641913\n",
      "    num_agent_steps_sampled: 499000\n",
      "    num_agent_steps_trained: 499000\n",
      "    num_steps_sampled: 499000\n",
      "    num_steps_trained: 499000\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.582352941176474\n",
      "    ram_util_percent: 38.87647058823529\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867958258059755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.75639368392443\n",
      "    mean_inference_ms: 1.927875017806519\n",
      "    mean_raw_obs_processing_ms: 2.0993705517238337\n",
      "  time_since_restore: 12301.057035923004\n",
      "  time_this_iter_s: 23.798292636871338\n",
      "  time_total_s: 12301.057035923004\n",
      "  timers:\n",
      "    learn_throughput: 1441.157\n",
      "    learn_time_ms: 693.887\n",
      "    load_throughput: 40957.12\n",
      "    load_time_ms: 24.416\n",
      "    sample_throughput: 38.033\n",
      "    sample_time_ms: 26292.627\n",
      "    update_time_ms: 2.412\n",
      "  timestamp: 1635075175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 499000\n",
      "  training_iteration: 499\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         12301.1</td><td style=\"text-align: right;\">499000</td><td style=\"text-align: right;\"> -2.5747</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            257.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-33-15\n",
      "  done: false\n",
      "  episode_len_mean: 260.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.6058999999999886\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1677\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2713510130267645e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8684615174929301\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00806296405758127\n",
      "          policy_loss: 0.05993977040052414\n",
      "          total_loss: 0.06168365544743008\n",
      "          vf_explained_var: -0.18221229314804077\n",
      "          vf_loss: 0.010428400063473318\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.003571428571426\n",
      "    ram_util_percent: 38.896428571428565\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867929501054133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.76033892057383\n",
      "    mean_inference_ms: 1.9278752627653708\n",
      "    mean_raw_obs_processing_ms: 2.09955071194254\n",
      "  time_since_restore: 12320.727913856506\n",
      "  time_this_iter_s: 19.670877933502197\n",
      "  time_total_s: 12320.727913856506\n",
      "  timers:\n",
      "    learn_throughput: 1440.069\n",
      "    learn_time_ms: 694.411\n",
      "    load_throughput: 41066.918\n",
      "    load_time_ms: 24.351\n",
      "    sample_throughput: 39.219\n",
      "    sample_time_ms: 25497.664\n",
      "    update_time_ms: 2.403\n",
      "  timestamp: 1635075195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 500\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         12320.7</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\"> -2.6059</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            260.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 501000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 263.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.638599999999988\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1680\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2713510130267645e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9804502911037869\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013850341936119845\n",
      "          policy_loss: 0.039657434903913075\n",
      "          total_loss: 0.04150902670290735\n",
      "          vf_explained_var: 0.11783306300640106\n",
      "          vf_loss: 0.011655918243599848\n",
      "    num_agent_steps_sampled: 501000\n",
      "    num_agent_steps_trained: 501000\n",
      "    num_steps_sampled: 501000\n",
      "    num_steps_trained: 501000\n",
      "  iterations_since_restore: 501\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.05294117647058\n",
      "    ram_util_percent: 38.84313725490196\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038678979900123804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.76375345817832\n",
      "    mean_inference_ms: 1.927874452715202\n",
      "    mean_raw_obs_processing_ms: 2.100741383578679\n",
      "  time_since_restore: 12356.656360149384\n",
      "  time_this_iter_s: 35.9284462928772\n",
      "  time_total_s: 12356.656360149384\n",
      "  timers:\n",
      "    learn_throughput: 1440.776\n",
      "    learn_time_ms: 694.07\n",
      "    load_throughput: 41015.676\n",
      "    load_time_ms: 24.381\n",
      "    sample_throughput: 37.914\n",
      "    sample_time_ms: 26375.455\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1635075231\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 501000\n",
      "  training_iteration: 501\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   501</td><td style=\"text-align: right;\">         12356.7</td><td style=\"text-align: right;\">501000</td><td style=\"text-align: right;\"> -2.6386</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            263.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 502000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-34-15\n",
      "  done: false\n",
      "  episode_len_mean: 266.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.6602999999999866\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1683\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2713510130267645e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8962983131408692\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01163709409116866\n",
      "          policy_loss: 0.05483721693356832\n",
      "          total_loss: 0.05654948403437932\n",
      "          vf_explained_var: -0.11528098583221436\n",
      "          vf_loss: 0.010675102066145175\n",
      "    num_agent_steps_sampled: 502000\n",
      "    num_agent_steps_trained: 502000\n",
      "    num_steps_sampled: 502000\n",
      "    num_steps_trained: 502000\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.311764705882354\n",
      "    ram_util_percent: 38.726470588235294\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038678668344901974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.767081789958226\n",
      "    mean_inference_ms: 1.9278738713932442\n",
      "    mean_raw_obs_processing_ms: 2.101895079562328\n",
      "  time_since_restore: 12380.59088897705\n",
      "  time_this_iter_s: 23.934528827667236\n",
      "  time_total_s: 12380.59088897705\n",
      "  timers:\n",
      "    learn_throughput: 1444.488\n",
      "    learn_time_ms: 692.287\n",
      "    load_throughput: 40888.924\n",
      "    load_time_ms: 24.457\n",
      "    sample_throughput: 38.417\n",
      "    sample_time_ms: 26029.996\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1635075255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 502000\n",
      "  training_iteration: 502\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   502</td><td style=\"text-align: right;\">         12380.6</td><td style=\"text-align: right;\">502000</td><td style=\"text-align: right;\"> -2.6603</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            266.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 503000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-34-40\n",
      "  done: false\n",
      "  episode_len_mean: 266.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.663999999999987\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1687\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2713510130267645e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6228230552540885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01591613856150889\n",
      "          policy_loss: 0.035038256976339555\n",
      "          total_loss: 0.04235021074612935\n",
      "          vf_explained_var: 0.20821140706539154\n",
      "          vf_loss: 0.013539986291693316\n",
      "    num_agent_steps_sampled: 503000\n",
      "    num_agent_steps_trained: 503000\n",
      "    num_steps_sampled: 503000\n",
      "    num_steps_trained: 503000\n",
      "  iterations_since_restore: 503\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.27499999999999\n",
      "    ram_util_percent: 38.888888888888886\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867829661042059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.771423462485075\n",
      "    mean_inference_ms: 1.9278729732230573\n",
      "    mean_raw_obs_processing_ms: 2.1026999679495915\n",
      "  time_since_restore: 12405.6088950634\n",
      "  time_this_iter_s: 25.018006086349487\n",
      "  time_total_s: 12405.6088950634\n",
      "  timers:\n",
      "    learn_throughput: 1446.23\n",
      "    learn_time_ms: 691.453\n",
      "    load_throughput: 40615.522\n",
      "    load_time_ms: 24.621\n",
      "    sample_throughput: 41.473\n",
      "    sample_time_ms: 24112.144\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1635075280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503000\n",
      "  training_iteration: 503\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   503</td><td style=\"text-align: right;\">         12405.6</td><td style=\"text-align: right;\">503000</td><td style=\"text-align: right;\">  -2.664</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">             266.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 266.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.660899999999988\n",
      "  episode_reward_min: -4.389999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1691\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2713510130267645e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7206743856271108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008989674922938364\n",
      "          policy_loss: 0.0026183542278077868\n",
      "          total_loss: 0.007646736461255285\n",
      "          vf_explained_var: 0.38230377435684204\n",
      "          vf_loss: 0.012235014357914527\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.829729729729735\n",
      "    ram_util_percent: 38.84324324324324\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867793128917767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.775902905706918\n",
      "    mean_inference_ms: 1.9278718387198859\n",
      "    mean_raw_obs_processing_ms: 2.1027606437309725\n",
      "  time_since_restore: 12431.408057689667\n",
      "  time_this_iter_s: 25.79916262626648\n",
      "  time_total_s: 12431.408057689667\n",
      "  timers:\n",
      "    learn_throughput: 1447.213\n",
      "    learn_time_ms: 690.983\n",
      "    load_throughput: 40665.177\n",
      "    load_time_ms: 24.591\n",
      "    sample_throughput: 41.292\n",
      "    sample_time_ms: 24218.011\n",
      "    update_time_ms: 2.527\n",
      "  timestamp: 1635075306\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 504\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   504</td><td style=\"text-align: right;\">         12431.4</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\"> -2.6609</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">               -4.39</td><td style=\"text-align: right;\">            266.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 505000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-35-20\n",
      "  done: false\n",
      "  episode_len_mean: 268.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.688999999999987\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1692\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2713510130267645e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7575121137830947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028482704303800076\n",
      "          policy_loss: -0.023727636535962424\n",
      "          total_loss: -0.03575711051623027\n",
      "          vf_explained_var: 0.2982446253299713\n",
      "          vf_loss: 0.005545286584593769\n",
      "    num_agent_steps_sampled: 505000\n",
      "    num_agent_steps_trained: 505000\n",
      "    num_steps_sampled: 505000\n",
      "    num_steps_trained: 505000\n",
      "  iterations_since_restore: 505\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.075\n",
      "    ram_util_percent: 38.95\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038677844277171955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.776907197109217\n",
      "    mean_inference_ms: 1.927871783323523\n",
      "    mean_raw_obs_processing_ms: 2.10276753304278\n",
      "  time_since_restore: 12445.732169151306\n",
      "  time_this_iter_s: 14.324111461639404\n",
      "  time_total_s: 12445.732169151306\n",
      "  timers:\n",
      "    learn_throughput: 1447.691\n",
      "    learn_time_ms: 690.755\n",
      "    load_throughput: 43094.526\n",
      "    load_time_ms: 23.205\n",
      "    sample_throughput: 43.292\n",
      "    sample_time_ms: 23098.699\n",
      "    update_time_ms: 2.52\n",
      "  timestamp: 1635075320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 505000\n",
      "  training_iteration: 505\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   505</td><td style=\"text-align: right;\">         12445.7</td><td style=\"text-align: right;\">505000</td><td style=\"text-align: right;\">  -2.689</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">             268.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 506000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-35-35\n",
      "  done: false\n",
      "  episode_len_mean: 276.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.7606999999999853\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1694\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9070265195401472e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7476452231407165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014709408031757753\n",
      "          policy_loss: -0.004378050400151147\n",
      "          total_loss: -0.018520673281616634\n",
      "          vf_explained_var: 0.24476563930511475\n",
      "          vf_loss: 0.0033335462354847955\n",
      "    num_agent_steps_sampled: 506000\n",
      "    num_agent_steps_trained: 506000\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.96818181818182\n",
      "    ram_util_percent: 38.97272727272727\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867764646485454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.77858085405303\n",
      "    mean_inference_ms: 1.9278720120543504\n",
      "    mean_raw_obs_processing_ms: 2.102786244366445\n",
      "  time_since_restore: 12460.68423628807\n",
      "  time_this_iter_s: 14.952067136764526\n",
      "  time_total_s: 12460.68423628807\n",
      "  timers:\n",
      "    learn_throughput: 1447.254\n",
      "    learn_time_ms: 690.964\n",
      "    load_throughput: 46232.338\n",
      "    load_time_ms: 21.63\n",
      "    sample_throughput: 45.179\n",
      "    sample_time_ms: 22134.138\n",
      "    update_time_ms: 2.502\n",
      "  timestamp: 1635075335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 506\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   506</td><td style=\"text-align: right;\">         12460.7</td><td style=\"text-align: right;\">506000</td><td style=\"text-align: right;\"> -2.7607</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            276.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 507000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-35-54\n",
      "  done: false\n",
      "  episode_len_mean: 279.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.7925999999999847\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1696\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9070265195401472e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4848910186025832\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03472979668779771\n",
      "          policy_loss: 0.08933109490511318\n",
      "          total_loss: 0.07917247445633015\n",
      "          vf_explained_var: 0.6118830442428589\n",
      "          vf_loss: 0.004689627270434155\n",
      "    num_agent_steps_sampled: 507000\n",
      "    num_agent_steps_trained: 507000\n",
      "    num_steps_sampled: 507000\n",
      "    num_steps_trained: 507000\n",
      "  iterations_since_restore: 507\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.58518518518519\n",
      "    ram_util_percent: 38.87777777777779\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867743472748544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.780118809488446\n",
      "    mean_inference_ms: 1.9278727494327277\n",
      "    mean_raw_obs_processing_ms: 2.1027299939965522\n",
      "  time_since_restore: 12479.981616973877\n",
      "  time_this_iter_s: 19.297380685806274\n",
      "  time_total_s: 12479.981616973877\n",
      "  timers:\n",
      "    learn_throughput: 1445.795\n",
      "    learn_time_ms: 691.661\n",
      "    load_throughput: 45991.789\n",
      "    load_time_ms: 21.743\n",
      "    sample_throughput: 45.631\n",
      "    sample_time_ms: 21915.037\n",
      "    update_time_ms: 2.537\n",
      "  timestamp: 1635075354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 507000\n",
      "  training_iteration: 507\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   507</td><td style=\"text-align: right;\">           12480</td><td style=\"text-align: right;\">507000</td><td style=\"text-align: right;\"> -2.7926</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            279.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-36-19\n",
      "  done: false\n",
      "  episode_len_mean: 279.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.796599999999984\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1700\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8605397793102208e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7381087760130565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008359726452467244\n",
      "          policy_loss: -0.005030946764681074\n",
      "          total_loss: -0.0018191477490795984\n",
      "          vf_explained_var: 0.6327416896820068\n",
      "          vf_loss: 0.010592653043568134\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.900000000000006\n",
      "    ram_util_percent: 38.51428571428571\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386770334671861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.78309206823191\n",
      "    mean_inference_ms: 1.927875020372972\n",
      "    mean_raw_obs_processing_ms: 2.102667766955102\n",
      "  time_since_restore: 12504.683332920074\n",
      "  time_this_iter_s: 24.70171594619751\n",
      "  time_total_s: 12504.683332920074\n",
      "  timers:\n",
      "    learn_throughput: 1449.103\n",
      "    learn_time_ms: 690.082\n",
      "    load_throughput: 45643.147\n",
      "    load_time_ms: 21.909\n",
      "    sample_throughput: 45.406\n",
      "    sample_time_ms: 22023.573\n",
      "    update_time_ms: 2.55\n",
      "  timestamp: 1635075379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 508\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   508</td><td style=\"text-align: right;\">         12504.7</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\"> -2.7966</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            279.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 509000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-36-45\n",
      "  done: false\n",
      "  episode_len_mean: 279.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.7990999999999837\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1703\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8605397793102208e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5837133387724559\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004134289199592602\n",
      "          policy_loss: -0.10773398586445385\n",
      "          total_loss: -0.10500213015410635\n",
      "          vf_explained_var: 0.6796685457229614\n",
      "          vf_loss: 0.008568870421085093\n",
      "    num_agent_steps_sampled: 509000\n",
      "    num_agent_steps_trained: 509000\n",
      "    num_steps_sampled: 509000\n",
      "    num_steps_trained: 509000\n",
      "  iterations_since_restore: 509\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.31052631578948\n",
      "    ram_util_percent: 38.14473684210525\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867673923044807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.78542438941237\n",
      "    mean_inference_ms: 1.9278769420823514\n",
      "    mean_raw_obs_processing_ms: 2.1025967334198277\n",
      "  time_since_restore: 12530.929079771042\n",
      "  time_this_iter_s: 26.245746850967407\n",
      "  time_total_s: 12530.929079771042\n",
      "  timers:\n",
      "    learn_throughput: 1450.271\n",
      "    learn_time_ms: 689.526\n",
      "    load_throughput: 45279.212\n",
      "    load_time_ms: 22.085\n",
      "    sample_throughput: 44.906\n",
      "    sample_time_ms: 22268.705\n",
      "    update_time_ms: 2.539\n",
      "  timestamp: 1635075405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 509000\n",
      "  training_iteration: 509\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   509</td><td style=\"text-align: right;\">         12530.9</td><td style=\"text-align: right;\">509000</td><td style=\"text-align: right;\"> -2.7991</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            279.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 510000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-37-12\n",
      "  done: false\n",
      "  episode_len_mean: 279.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.797699999999985\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1707\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4302698896551104e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.546635823448499\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00461665113557817\n",
      "          policy_loss: -0.1115984102918042\n",
      "          total_loss: -0.1082976236111588\n",
      "          vf_explained_var: 0.6549428105354309\n",
      "          vf_loss: 0.0087670819937355\n",
      "    num_agent_steps_sampled: 510000\n",
      "    num_agent_steps_trained: 510000\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.351351351351354\n",
      "    ram_util_percent: 38.072972972972956\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867635364195098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.788523691122368\n",
      "    mean_inference_ms: 1.9278799135421025\n",
      "    mean_raw_obs_processing_ms: 2.102510892651318\n",
      "  time_since_restore: 12557.250298976898\n",
      "  time_this_iter_s: 26.321219205856323\n",
      "  time_total_s: 12557.250298976898\n",
      "  timers:\n",
      "    learn_throughput: 1452.752\n",
      "    learn_time_ms: 688.349\n",
      "    load_throughput: 44530.862\n",
      "    load_time_ms: 22.456\n",
      "    sample_throughput: 43.601\n",
      "    sample_time_ms: 22935.461\n",
      "    update_time_ms: 2.544\n",
      "  timestamp: 1635075432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 510\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   510</td><td style=\"text-align: right;\">         12557.3</td><td style=\"text-align: right;\">510000</td><td style=\"text-align: right;\"> -2.7977</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            279.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 511000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-37-56\n",
      "  done: false\n",
      "  episode_len_mean: 279.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.7906999999999846\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1712\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.151349448275552e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7470070365402434\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.05650336075123712\n",
      "          policy_loss: -0.03273107194238239\n",
      "          total_loss: -0.03036438851720757\n",
      "          vf_explained_var: 0.6511140465736389\n",
      "          vf_loss: 0.00983635178870625\n",
      "    num_agent_steps_sampled: 511000\n",
      "    num_agent_steps_trained: 511000\n",
      "    num_steps_sampled: 511000\n",
      "    num_steps_trained: 511000\n",
      "  iterations_since_restore: 511\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.8328125\n",
      "    ram_util_percent: 38.11875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867588003729\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.792528519274022\n",
      "    mean_inference_ms: 1.9278841713030455\n",
      "    mean_raw_obs_processing_ms: 2.1040940366903267\n",
      "  time_since_restore: 12601.92837381363\n",
      "  time_this_iter_s: 44.67807483673096\n",
      "  time_total_s: 12601.92837381363\n",
      "  timers:\n",
      "    learn_throughput: 1453.468\n",
      "    learn_time_ms: 688.01\n",
      "    load_throughput: 44481.7\n",
      "    load_time_ms: 22.481\n",
      "    sample_throughput: 41.998\n",
      "    sample_time_ms: 23810.703\n",
      "    update_time_ms: 2.538\n",
      "  timestamp: 1635075476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511000\n",
      "  training_iteration: 511\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   511</td><td style=\"text-align: right;\">         12601.9</td><td style=\"text-align: right;\">511000</td><td style=\"text-align: right;\"> -2.7907</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            279.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-38-18\n",
      "  done: false\n",
      "  episode_len_mean: 281.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.813299999999984\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1715\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0727024172413331e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0401169366306728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.029616741518847967\n",
      "          policy_loss: 0.05671158838603232\n",
      "          total_loss: 0.056049118604924945\n",
      "          vf_explained_var: 0.48103293776512146\n",
      "          vf_loss: 0.009738381293654027\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.43225806451613\n",
      "    ram_util_percent: 38.832258064516125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867560437673407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.794630667800217\n",
      "    mean_inference_ms: 1.927886437720317\n",
      "    mean_raw_obs_processing_ms: 2.105021970800112\n",
      "  time_since_restore: 12623.747990608215\n",
      "  time_this_iter_s: 21.81961679458618\n",
      "  time_total_s: 12623.747990608215\n",
      "  timers:\n",
      "    learn_throughput: 1451.34\n",
      "    learn_time_ms: 689.018\n",
      "    load_throughput: 44948.33\n",
      "    load_time_ms: 22.248\n",
      "    sample_throughput: 42.376\n",
      "    sample_time_ms: 23598.448\n",
      "    update_time_ms: 2.531\n",
      "  timestamp: 1635075498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 512\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   512</td><td style=\"text-align: right;\">         12623.7</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\"> -2.8133</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            281.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 513000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-38-43\n",
      "  done: false\n",
      "  episode_len_mean: 283.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.07\n",
      "  episode_reward_mean: -2.830199999999983\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1719\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6090536258619992e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9091393742296431\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015480545332488226\n",
      "          policy_loss: -0.037102470298608144\n",
      "          total_loss: -0.03662263875206311\n",
      "          vf_explained_var: 0.4660647213459015\n",
      "          vf_loss: 0.009570977619538705\n",
      "    num_agent_steps_sampled: 513000\n",
      "    num_agent_steps_trained: 513000\n",
      "    num_steps_sampled: 513000\n",
      "    num_steps_trained: 513000\n",
      "  iterations_since_restore: 513\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.31428571428572\n",
      "    ram_util_percent: 38.86857142857143\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038675227736172275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.79727295759802\n",
      "    mean_inference_ms: 1.9278891549720458\n",
      "    mean_raw_obs_processing_ms: 2.105180036121666\n",
      "  time_since_restore: 12647.980392217636\n",
      "  time_this_iter_s: 24.232401609420776\n",
      "  time_total_s: 12647.980392217636\n",
      "  timers:\n",
      "    learn_throughput: 1448.88\n",
      "    learn_time_ms: 690.188\n",
      "    load_throughput: 44957.581\n",
      "    load_time_ms: 22.243\n",
      "    sample_throughput: 42.519\n",
      "    sample_time_ms: 23518.731\n",
      "    update_time_ms: 2.54\n",
      "  timestamp: 1635075523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 513000\n",
      "  training_iteration: 513\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   513</td><td style=\"text-align: right;\">           12648</td><td style=\"text-align: right;\">513000</td><td style=\"text-align: right;\"> -2.8302</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            283.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 514000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-39-07\n",
      "  done: false\n",
      "  episode_len_mean: 283.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.8338999999999834\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1722\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6090536258619992e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6442011912663778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027046378949677912\n",
      "          policy_loss: -0.01796595172749625\n",
      "          total_loss: -0.016845795181062488\n",
      "          vf_explained_var: 0.2959069013595581\n",
      "          vf_loss: 0.007561734856830703\n",
      "    num_agent_steps_sampled: 514000\n",
      "    num_agent_steps_trained: 514000\n",
      "    num_steps_sampled: 514000\n",
      "    num_steps_trained: 514000\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06571428571429\n",
      "    ram_util_percent: 38.87714285714286\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038674932698961564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.799168335735693\n",
      "    mean_inference_ms: 1.9278908697418025\n",
      "    mean_raw_obs_processing_ms: 2.105030839714431\n",
      "  time_since_restore: 12672.668715000153\n",
      "  time_this_iter_s: 24.68832278251648\n",
      "  time_total_s: 12672.668715000153\n",
      "  timers:\n",
      "    learn_throughput: 1448.755\n",
      "    learn_time_ms: 690.248\n",
      "    load_throughput: 44795.003\n",
      "    load_time_ms: 22.324\n",
      "    sample_throughput: 42.721\n",
      "    sample_time_ms: 23407.649\n",
      "    update_time_ms: 2.401\n",
      "  timestamp: 1635075547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 514000\n",
      "  training_iteration: 514\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   514</td><td style=\"text-align: right;\">         12672.7</td><td style=\"text-align: right;\">514000</td><td style=\"text-align: right;\"> -2.8339</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            283.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 515000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-39-33\n",
      "  done: false\n",
      "  episode_len_mean: 282.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.8263999999999836\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1726\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4135804387929995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5345854749282201\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008452983001497058\n",
      "          policy_loss: -0.05268642587794198\n",
      "          total_loss: -0.04670993420812819\n",
      "          vf_explained_var: 0.13543978333473206\n",
      "          vf_loss: 0.011322138820671374\n",
      "    num_agent_steps_sampled: 515000\n",
      "    num_agent_steps_trained: 515000\n",
      "    num_steps_sampled: 515000\n",
      "    num_steps_trained: 515000\n",
      "  iterations_since_restore: 515\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.05405405405406\n",
      "    ram_util_percent: 38.88108108108108\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038674530782256085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.801785862616693\n",
      "    mean_inference_ms: 1.9278929839239323\n",
      "    mean_raw_obs_processing_ms: 2.1048155374737654\n",
      "  time_since_restore: 12698.469921588898\n",
      "  time_this_iter_s: 25.801206588745117\n",
      "  time_total_s: 12698.469921588898\n",
      "  timers:\n",
      "    learn_throughput: 1446.972\n",
      "    learn_time_ms: 691.098\n",
      "    load_throughput: 42282.313\n",
      "    load_time_ms: 23.651\n",
      "    sample_throughput: 40.728\n",
      "    sample_time_ms: 24553.13\n",
      "    update_time_ms: 2.43\n",
      "  timestamp: 1635075573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 515000\n",
      "  training_iteration: 515\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   515</td><td style=\"text-align: right;\">         12698.5</td><td style=\"text-align: right;\">515000</td><td style=\"text-align: right;\"> -2.8264</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            282.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-39-58\n",
      "  done: false\n",
      "  episode_len_mean: 281.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.8192999999999837\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1730\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4135804387929995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5613226572672526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006987334078450402\n",
      "          policy_loss: 0.019066004951794942\n",
      "          total_loss: 0.024970272928476332\n",
      "          vf_explained_var: 0.20171453058719635\n",
      "          vf_loss: 0.011517327082239919\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.10857142857143\n",
      "    ram_util_percent: 38.888571428571424\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038674137074119666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.804391920391076\n",
      "    mean_inference_ms: 1.92789490695361\n",
      "    mean_raw_obs_processing_ms: 2.104647175106286\n",
      "  time_since_restore: 12723.04676246643\n",
      "  time_this_iter_s: 24.57684087753296\n",
      "  time_total_s: 12723.04676246643\n",
      "  timers:\n",
      "    learn_throughput: 1449.391\n",
      "    learn_time_ms: 689.945\n",
      "    load_throughput: 40203.591\n",
      "    load_time_ms: 24.873\n",
      "    sample_throughput: 39.192\n",
      "    sample_time_ms: 25515.518\n",
      "    update_time_ms: 2.443\n",
      "  timestamp: 1635075598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 516\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   516</td><td style=\"text-align: right;\">           12723</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\"> -2.8193</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            281.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 517000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-40-23\n",
      "  done: false\n",
      "  episode_len_mean: 282.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.825299999999984\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1734\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4135804387929995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5278113229407204\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006968096166348184\n",
      "          policy_loss: 0.026459303663836586\n",
      "          total_loss: 0.032841997841993965\n",
      "          vf_explained_var: 0.2744468152523041\n",
      "          vf_loss: 0.011660640676402384\n",
      "    num_agent_steps_sampled: 517000\n",
      "    num_agent_steps_trained: 517000\n",
      "    num_steps_sampled: 517000\n",
      "    num_steps_trained: 517000\n",
      "  iterations_since_restore: 517\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.2837837837838\n",
      "    ram_util_percent: 38.88378378378378\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038673746476558496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.806878989868697\n",
      "    mean_inference_ms: 1.9278962432728521\n",
      "    mean_raw_obs_processing_ms: 2.104486813700634\n",
      "  time_since_restore: 12748.850861787796\n",
      "  time_this_iter_s: 25.804099321365356\n",
      "  time_total_s: 12748.850861787796\n",
      "  timers:\n",
      "    learn_throughput: 1450.67\n",
      "    learn_time_ms: 689.337\n",
      "    load_throughput: 40450.068\n",
      "    load_time_ms: 24.722\n",
      "    sample_throughput: 38.216\n",
      "    sample_time_ms: 26166.973\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1635075623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 517000\n",
      "  training_iteration: 517\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   517</td><td style=\"text-align: right;\">         12748.9</td><td style=\"text-align: right;\">517000</td><td style=\"text-align: right;\"> -2.8253</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            282.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 518000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-40-50\n",
      "  done: false\n",
      "  episode_len_mean: 282.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.829099999999984\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1738\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4135804387929995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5880721588929494\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006478734756539714\n",
      "          policy_loss: 0.04943749482433001\n",
      "          total_loss: 0.05352915525436401\n",
      "          vf_explained_var: 0.20379772782325745\n",
      "          vf_loss: 0.009972228823850552\n",
      "    num_agent_steps_sampled: 518000\n",
      "    num_agent_steps_trained: 518000\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.29736842105264\n",
      "    ram_util_percent: 38.85263157894736\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038673390859451165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.809230194240318\n",
      "    mean_inference_ms: 1.9278973377987731\n",
      "    mean_raw_obs_processing_ms: 2.1043344281715273\n",
      "  time_since_restore: 12775.11645936966\n",
      "  time_this_iter_s: 26.265597581863403\n",
      "  time_total_s: 12775.11645936966\n",
      "  timers:\n",
      "    learn_throughput: 1450.345\n",
      "    learn_time_ms: 689.491\n",
      "    load_throughput: 41008.297\n",
      "    load_time_ms: 24.385\n",
      "    sample_throughput: 37.989\n",
      "    sample_time_ms: 26323.56\n",
      "    update_time_ms: 2.405\n",
      "  timestamp: 1635075650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 518\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   518</td><td style=\"text-align: right;\">         12775.1</td><td style=\"text-align: right;\">518000</td><td style=\"text-align: right;\"> -2.8291</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            282.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 519000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-41-34\n",
      "  done: false\n",
      "  episode_len_mean: 283.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.832499999999983\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1742\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4135804387929995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5087800804111693\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005554524547079708\n",
      "          policy_loss: 0.0031841983397801715\n",
      "          total_loss: 0.010487062401241726\n",
      "          vf_explained_var: 0.1700788140296936\n",
      "          vf_loss: 0.01239053150638938\n",
      "    num_agent_steps_sampled: 519000\n",
      "    num_agent_steps_trained: 519000\n",
      "    num_steps_sampled: 519000\n",
      "    num_steps_trained: 519000\n",
      "  iterations_since_restore: 519\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.90967741935486\n",
      "    ram_util_percent: 38.76290322580646\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038673051724785544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.811579348911344\n",
      "    mean_inference_ms: 1.927898159417888\n",
      "    mean_raw_obs_processing_ms: 2.1054466430445897\n",
      "  time_since_restore: 12818.966909646988\n",
      "  time_this_iter_s: 43.85045027732849\n",
      "  time_total_s: 12818.966909646988\n",
      "  timers:\n",
      "    learn_throughput: 1449.258\n",
      "    learn_time_ms: 690.008\n",
      "    load_throughput: 41114.621\n",
      "    load_time_ms: 24.322\n",
      "    sample_throughput: 35.608\n",
      "    sample_time_ms: 28083.546\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1635075694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519000\n",
      "  training_iteration: 519\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   519</td><td style=\"text-align: right;\">           12819</td><td style=\"text-align: right;\">519000</td><td style=\"text-align: right;\"> -2.8325</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            283.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-42-00\n",
      "  done: false\n",
      "  episode_len_mean: 283.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.835899999999983\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1746\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4135804387929995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4914638032515844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0028549972748696147\n",
      "          policy_loss: -0.038887200835678315\n",
      "          total_loss: -0.03123098640806145\n",
      "          vf_explained_var: 0.19321714341640472\n",
      "          vf_loss: 0.012570790211773581\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.26315789473684\n",
      "    ram_util_percent: 38.80789473684211\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867271723485733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81394020715462\n",
      "    mean_inference_ms: 1.927899258205452\n",
      "    mean_raw_obs_processing_ms: 2.1065261103895194\n",
      "  time_since_restore: 12845.541536092758\n",
      "  time_this_iter_s: 26.574626445770264\n",
      "  time_total_s: 12845.541536092758\n",
      "  timers:\n",
      "    learn_throughput: 1448.791\n",
      "    learn_time_ms: 690.231\n",
      "    load_throughput: 40947.964\n",
      "    load_time_ms: 24.421\n",
      "    sample_throughput: 35.576\n",
      "    sample_time_ms: 28108.554\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1635075720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 520\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   520</td><td style=\"text-align: right;\">         12845.5</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\"> -2.8359</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            283.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 521000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-42-27\n",
      "  done: false\n",
      "  episode_len_mean: 284.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1399999999999983\n",
      "  episode_reward_mean: -2.841499999999984\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1750\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2067902193964997e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3937614576684104\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032912605082958952\n",
      "          policy_loss: -0.019486629135078853\n",
      "          total_loss: -0.011488788326581319\n",
      "          vf_explained_var: 0.22730326652526855\n",
      "          vf_loss: 0.011935420779304371\n",
      "    num_agent_steps_sampled: 521000\n",
      "    num_agent_steps_trained: 521000\n",
      "    num_steps_sampled: 521000\n",
      "    num_steps_trained: 521000\n",
      "  iterations_since_restore: 521\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.915789473684214\n",
      "    ram_util_percent: 38.82368421052632\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038672374027546905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.816216371500428\n",
      "    mean_inference_ms: 1.9279003014095832\n",
      "    mean_raw_obs_processing_ms: 2.1069256408398522\n",
      "  time_since_restore: 12872.104299783707\n",
      "  time_this_iter_s: 26.562763690948486\n",
      "  time_total_s: 12872.104299783707\n",
      "  timers:\n",
      "    learn_throughput: 1448.48\n",
      "    learn_time_ms: 690.379\n",
      "    load_throughput: 40799.71\n",
      "    load_time_ms: 24.51\n",
      "    sample_throughput: 38.028\n",
      "    sample_time_ms: 26296.744\n",
      "    update_time_ms: 2.52\n",
      "  timestamp: 1635075747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 521000\n",
      "  training_iteration: 521\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   521</td><td style=\"text-align: right;\">         12872.1</td><td style=\"text-align: right;\">521000</td><td style=\"text-align: right;\"> -2.8415</td><td style=\"text-align: right;\">               -2.14</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            284.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 522000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-42-54\n",
      "  done: false\n",
      "  episode_len_mean: 283.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1399999999999983\n",
      "  episode_reward_mean: -2.832199999999984\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1755\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.033951096982499e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.42976332240634496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0035569313001869002\n",
      "          policy_loss: -0.026712718192074034\n",
      "          total_loss: -0.016163201878468196\n",
      "          vf_explained_var: 0.2447114884853363\n",
      "          vf_loss: 0.01484713399161895\n",
      "    num_agent_steps_sampled: 522000\n",
      "    num_agent_steps_trained: 522000\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.45897435897435\n",
      "    ram_util_percent: 38.92307692307692\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038671944187784135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.819121072587933\n",
      "    mean_inference_ms: 1.9279016271369516\n",
      "    mean_raw_obs_processing_ms: 2.106598643075971\n",
      "  time_since_restore: 12899.21526169777\n",
      "  time_this_iter_s: 27.1109619140625\n",
      "  time_total_s: 12899.21526169777\n",
      "  timers:\n",
      "    learn_throughput: 1449.465\n",
      "    learn_time_ms: 689.91\n",
      "    load_throughput: 40949.323\n",
      "    load_time_ms: 24.42\n",
      "    sample_throughput: 37.277\n",
      "    sample_time_ms: 26826.352\n",
      "    update_time_ms: 2.61\n",
      "  timestamp: 1635075774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 522\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   522</td><td style=\"text-align: right;\">         12899.2</td><td style=\"text-align: right;\">522000</td><td style=\"text-align: right;\"> -2.8322</td><td style=\"text-align: right;\">               -2.14</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            283.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 523000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-43-20\n",
      "  done: false\n",
      "  episode_len_mean: 282.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1399999999999983\n",
      "  episode_reward_mean: -2.8223999999999836\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1759\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.0169755484912494e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4798580762412813\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003567464512506212\n",
      "          policy_loss: 0.0031117899550331964\n",
      "          total_loss: 0.00984689270456632\n",
      "          vf_explained_var: 0.2931298017501831\n",
      "          vf_loss: 0.011533676243076722\n",
      "    num_agent_steps_sampled: 523000\n",
      "    num_agent_steps_trained: 523000\n",
      "    num_steps_sampled: 523000\n",
      "    num_steps_trained: 523000\n",
      "  iterations_since_restore: 523\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.148648648648646\n",
      "    ram_util_percent: 38.95675675675677\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038671608078922406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.821470073135117\n",
      "    mean_inference_ms: 1.9279021370949416\n",
      "    mean_raw_obs_processing_ms: 2.106370076635409\n",
      "  time_since_restore: 12925.414826154709\n",
      "  time_this_iter_s: 26.199564456939697\n",
      "  time_total_s: 12925.414826154709\n",
      "  timers:\n",
      "    learn_throughput: 1449.181\n",
      "    learn_time_ms: 690.045\n",
      "    load_throughput: 40856.224\n",
      "    load_time_ms: 24.476\n",
      "    sample_throughput: 37.006\n",
      "    sample_time_ms: 27022.863\n",
      "    update_time_ms: 2.61\n",
      "  timestamp: 1635075800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 523000\n",
      "  training_iteration: 523\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   523</td><td style=\"text-align: right;\">         12925.4</td><td style=\"text-align: right;\">523000</td><td style=\"text-align: right;\"> -2.8224</td><td style=\"text-align: right;\">               -2.14</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            282.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-43-46\n",
      "  done: false\n",
      "  episode_len_mean: 281.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1399999999999983\n",
      "  episode_reward_mean: -2.8154999999999837\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1763\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5084877742456247e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.43870586786005233\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003848815130185004\n",
      "          policy_loss: 0.023222956765029164\n",
      "          total_loss: 0.030331565770837995\n",
      "          vf_explained_var: 0.2551701068878174\n",
      "          vf_loss: 0.011495662935905987\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18648648648649\n",
      "    ram_util_percent: 38.92702702702704\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867125127316037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.823859271120345\n",
      "    mean_inference_ms: 1.9279019814101026\n",
      "    mean_raw_obs_processing_ms: 2.106187016951564\n",
      "  time_since_restore: 12951.41612458229\n",
      "  time_this_iter_s: 26.001298427581787\n",
      "  time_total_s: 12951.41612458229\n",
      "  timers:\n",
      "    learn_throughput: 1449.07\n",
      "    learn_time_ms: 690.098\n",
      "    load_throughput: 40741.927\n",
      "    load_time_ms: 24.545\n",
      "    sample_throughput: 36.827\n",
      "    sample_time_ms: 27154.031\n",
      "    update_time_ms: 2.627\n",
      "  timestamp: 1635075826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 524\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   524</td><td style=\"text-align: right;\">         12951.4</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\"> -2.8155</td><td style=\"text-align: right;\">               -2.14</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            281.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 525000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 279.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1399999999999983\n",
      "  episode_reward_mean: -2.793899999999984\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1767\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.542438871228123e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.40610863599512315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00709768048187682\n",
      "          policy_loss: 0.021402665393220055\n",
      "          total_loss: 0.029280959235297308\n",
      "          vf_explained_var: 0.16743570566177368\n",
      "          vf_loss: 0.011939376468459766\n",
      "    num_agent_steps_sampled: 525000\n",
      "    num_agent_steps_trained: 525000\n",
      "    num_steps_sampled: 525000\n",
      "    num_steps_trained: 525000\n",
      "  iterations_since_restore: 525\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03947368421053\n",
      "    ram_util_percent: 38.876315789473686\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867090679458998\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.826607520907356\n",
      "    mean_inference_ms: 1.9279023809019367\n",
      "    mean_raw_obs_processing_ms: 2.1060520511045273\n",
      "  time_since_restore: 12977.848742961884\n",
      "  time_this_iter_s: 26.432618379592896\n",
      "  time_total_s: 12977.848742961884\n",
      "  timers:\n",
      "    learn_throughput: 1452.139\n",
      "    learn_time_ms: 688.639\n",
      "    load_throughput: 40824.927\n",
      "    load_time_ms: 24.495\n",
      "    sample_throughput: 36.739\n",
      "    sample_time_ms: 27218.705\n",
      "    update_time_ms: 2.608\n",
      "  timestamp: 1635075853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525000\n",
      "  training_iteration: 525\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   525</td><td style=\"text-align: right;\">         12977.8</td><td style=\"text-align: right;\">525000</td><td style=\"text-align: right;\"> -2.7939</td><td style=\"text-align: right;\">               -2.14</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            279.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 526000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-44-57\n",
      "  done: false\n",
      "  episode_len_mean: 277.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.7731999999999846\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1771\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.542438871228123e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3826733777920405\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003852732967837142\n",
      "          policy_loss: 0.024267968038717905\n",
      "          total_loss: 0.033416472044256\n",
      "          vf_explained_var: 0.21072015166282654\n",
      "          vf_loss: 0.012975231589128574\n",
      "    num_agent_steps_sampled: 526000\n",
      "    num_agent_steps_trained: 526000\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.84126984126984\n",
      "    ram_util_percent: 38.80000000000001\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386705698912464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.829626623725062\n",
      "    mean_inference_ms: 1.9279029337043971\n",
      "    mean_raw_obs_processing_ms: 2.1071879222356036\n",
      "  time_since_restore: 13021.807718515396\n",
      "  time_this_iter_s: 43.95897555351257\n",
      "  time_total_s: 13021.807718515396\n",
      "  timers:\n",
      "    learn_throughput: 1450.54\n",
      "    learn_time_ms: 689.398\n",
      "    load_throughput: 40794.551\n",
      "    load_time_ms: 24.513\n",
      "    sample_throughput: 34.298\n",
      "    sample_time_ms: 29156.092\n",
      "    update_time_ms: 2.655\n",
      "  timestamp: 1635075897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 526\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   526</td><td style=\"text-align: right;\">         13021.8</td><td style=\"text-align: right;\">526000</td><td style=\"text-align: right;\"> -2.7732</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            277.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 527000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-45-22\n",
      "  done: false\n",
      "  episode_len_mean: 277.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.772699999999985\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1775\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7712194356140617e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4909848183393478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006904163815742908\n",
      "          policy_loss: 0.03287467120422257\n",
      "          total_loss: 0.04069381695654657\n",
      "          vf_explained_var: 0.24961568415164948\n",
      "          vf_loss: 0.012728990045272642\n",
      "    num_agent_steps_sampled: 527000\n",
      "    num_agent_steps_trained: 527000\n",
      "    num_steps_sampled: 527000\n",
      "    num_steps_trained: 527000\n",
      "  iterations_since_restore: 527\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.58611111111111\n",
      "    ram_util_percent: 38.84166666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867022213107421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.832841609189256\n",
      "    mean_inference_ms: 1.9279038861108\n",
      "    mean_raw_obs_processing_ms: 2.1083306739111056\n",
      "  time_since_restore: 13047.250086307526\n",
      "  time_this_iter_s: 25.442367792129517\n",
      "  time_total_s: 13047.250086307526\n",
      "  timers:\n",
      "    learn_throughput: 1449.297\n",
      "    learn_time_ms: 689.99\n",
      "    load_throughput: 40366.992\n",
      "    load_time_ms: 24.773\n",
      "    sample_throughput: 34.342\n",
      "    sample_time_ms: 29119.078\n",
      "    update_time_ms: 2.642\n",
      "  timestamp: 1635075922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527000\n",
      "  training_iteration: 527\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   527</td><td style=\"text-align: right;\">         13047.3</td><td style=\"text-align: right;\">527000</td><td style=\"text-align: right;\"> -2.7727</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            277.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 274.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.747999999999985\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1778\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7712194356140617e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4340831786394119\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003859598919983398\n",
      "          policy_loss: -0.06746382663647334\n",
      "          total_loss: -0.06128900249799093\n",
      "          vf_explained_var: 0.33014747500419617\n",
      "          vf_loss: 0.010515658769549595\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.70833333333333\n",
      "    ram_util_percent: 38.85555555555555\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038670005067081646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.83557301657197\n",
      "    mean_inference_ms: 1.9279048330703368\n",
      "    mean_raw_obs_processing_ms: 2.1088658709402868\n",
      "  time_since_restore: 13071.992007255554\n",
      "  time_this_iter_s: 24.741920948028564\n",
      "  time_total_s: 13071.992007255554\n",
      "  timers:\n",
      "    learn_throughput: 1448.27\n",
      "    learn_time_ms: 690.479\n",
      "    load_throughput: 39725.446\n",
      "    load_time_ms: 25.173\n",
      "    sample_throughput: 34.523\n",
      "    sample_time_ms: 28965.806\n",
      "    update_time_ms: 2.66\n",
      "  timestamp: 1635075947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 528\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   528</td><td style=\"text-align: right;\">           13072</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">  -2.748</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">             274.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 529000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-46-14\n",
      "  done: false\n",
      "  episode_len_mean: 271.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.7099999999999858\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1783\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8856097178070309e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.356591812438435\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003763500468180483\n",
      "          policy_loss: -0.021184054182635415\n",
      "          total_loss: -0.010329408198595047\n",
      "          vf_explained_var: 0.31189092993736267\n",
      "          vf_loss: 0.01442056361378895\n",
      "    num_agent_steps_sampled: 529000\n",
      "    num_agent_steps_trained: 529000\n",
      "    num_steps_sampled: 529000\n",
      "    num_steps_trained: 529000\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.26842105263158\n",
      "    ram_util_percent: 38.87894736842105\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038669688470773164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.840693797611443\n",
      "    mean_inference_ms: 1.9279071678959188\n",
      "    mean_raw_obs_processing_ms: 2.1087047014111473\n",
      "  time_since_restore: 13099.11790728569\n",
      "  time_this_iter_s: 27.12590003013611\n",
      "  time_total_s: 13099.11790728569\n",
      "  timers:\n",
      "    learn_throughput: 1450.008\n",
      "    learn_time_ms: 689.651\n",
      "    load_throughput: 39516.193\n",
      "    load_time_ms: 25.306\n",
      "    sample_throughput: 36.638\n",
      "    sample_time_ms: 27294.069\n",
      "    update_time_ms: 2.647\n",
      "  timestamp: 1635075974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 529000\n",
      "  training_iteration: 529\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   529</td><td style=\"text-align: right;\">         13099.1</td><td style=\"text-align: right;\">529000</td><td style=\"text-align: right;\">   -2.71</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">               271</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 530000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-46-40\n",
      "  done: false\n",
      "  episode_len_mean: 270.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.7023999999999857\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1787\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.428048589035154e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.41544548604223464\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017769706343942933\n",
      "          policy_loss: 0.04135315641760826\n",
      "          total_loss: 0.04842228425873651\n",
      "          vf_explained_var: 0.2758355736732483\n",
      "          vf_loss: 0.011223585003366073\n",
      "    num_agent_steps_sampled: 530000\n",
      "    num_agent_steps_trained: 530000\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.35\n",
      "    ram_util_percent: 38.93684210526316\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038669405364423114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.844824406400072\n",
      "    mean_inference_ms: 1.927909140967328\n",
      "    mean_raw_obs_processing_ms: 2.1086454035225586\n",
      "  time_since_restore: 13125.326072216034\n",
      "  time_this_iter_s: 26.208164930343628\n",
      "  time_total_s: 13125.326072216034\n",
      "  timers:\n",
      "    learn_throughput: 1450.644\n",
      "    learn_time_ms: 689.349\n",
      "    load_throughput: 40301.867\n",
      "    load_time_ms: 24.813\n",
      "    sample_throughput: 36.686\n",
      "    sample_time_ms: 27258.216\n",
      "    update_time_ms: 2.659\n",
      "  timestamp: 1635076000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 530\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   530</td><td style=\"text-align: right;\">         13125.3</td><td style=\"text-align: right;\">530000</td><td style=\"text-align: right;\"> -2.7024</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            270.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 531000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-47-07\n",
      "  done: false\n",
      "  episode_len_mean: 269.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.6926999999999857\n",
      "  episode_reward_min: -7.199999999999891\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1791\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.428048589035154e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3492736793226666\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005884064363055093\n",
      "          policy_loss: 0.02551363284389178\n",
      "          total_loss: 0.03242107662889693\n",
      "          vf_explained_var: 0.23813743889331818\n",
      "          vf_loss: 0.010400180611759425\n",
      "    num_agent_steps_sampled: 531000\n",
      "    num_agent_steps_trained: 531000\n",
      "    num_steps_sampled: 531000\n",
      "    num_steps_trained: 531000\n",
      "  iterations_since_restore: 531\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.282051282051285\n",
      "    ram_util_percent: 38.935897435897445\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866911419816507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.849044406649384\n",
      "    mean_inference_ms: 1.9279112626452632\n",
      "    mean_raw_obs_processing_ms: 2.1085939930367448\n",
      "  time_since_restore: 13152.579980134964\n",
      "  time_this_iter_s: 27.253907918930054\n",
      "  time_total_s: 13152.579980134964\n",
      "  timers:\n",
      "    learn_throughput: 1450.222\n",
      "    learn_time_ms: 689.549\n",
      "    load_throughput: 40384.093\n",
      "    load_time_ms: 24.762\n",
      "    sample_throughput: 36.593\n",
      "    sample_time_ms: 27327.268\n",
      "    update_time_ms: 2.57\n",
      "  timestamp: 1635076027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 531000\n",
      "  training_iteration: 531\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   531</td><td style=\"text-align: right;\">         13152.6</td><td style=\"text-align: right;\">531000</td><td style=\"text-align: right;\"> -2.6927</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -7.2</td><td style=\"text-align: right;\">            269.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-47-35\n",
      "  done: false\n",
      "  episode_len_mean: 256.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.5623999999999887\n",
      "  episode_reward_min: -6.099999999999914\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1795\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.428048589035154e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.279927874273724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0011049076717178253\n",
      "          policy_loss: 0.03717425440748533\n",
      "          total_loss: 0.0442720723648866\n",
      "          vf_explained_var: 0.31777575612068176\n",
      "          vf_loss: 0.009897098576443063\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.312820512820515\n",
      "    ram_util_percent: 38.88974358974359\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038668854423281517\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.854799384994468\n",
      "    mean_inference_ms: 1.9279118700337365\n",
      "    mean_raw_obs_processing_ms: 2.1087025807747652\n",
      "  time_since_restore: 13179.952560186386\n",
      "  time_this_iter_s: 27.37258005142212\n",
      "  time_total_s: 13179.952560186386\n",
      "  timers:\n",
      "    learn_throughput: 1448.357\n",
      "    learn_time_ms: 690.438\n",
      "    load_throughput: 40055.772\n",
      "    load_time_ms: 24.965\n",
      "    sample_throughput: 36.56\n",
      "    sample_time_ms: 27352.426\n",
      "    update_time_ms: 2.479\n",
      "  timestamp: 1635076055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 532\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   532</td><td style=\"text-align: right;\">           13180</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\"> -2.5624</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">                -6.1</td><td style=\"text-align: right;\">            256.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 533000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-48-02\n",
      "  done: false\n",
      "  episode_len_mean: 251.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.5174999999999903\n",
      "  episode_reward_min: -3.279999999999974\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1799\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.714024294517577e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3095596821771728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01417159454063371\n",
      "          policy_loss: 0.039041924228270845\n",
      "          total_loss: 0.04681660665406121\n",
      "          vf_explained_var: 0.1595897525548935\n",
      "          vf_loss: 0.010870275563663906\n",
      "    num_agent_steps_sampled: 533000\n",
      "    num_agent_steps_trained: 533000\n",
      "    num_steps_sampled: 533000\n",
      "    num_steps_trained: 533000\n",
      "  iterations_since_restore: 533\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.22368421052632\n",
      "    ram_util_percent: 38.873684210526314\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866862803745856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.861087755766544\n",
      "    mean_inference_ms: 1.9279117558832253\n",
      "    mean_raw_obs_processing_ms: 2.1089290891287567\n",
      "  time_since_restore: 13206.722905635834\n",
      "  time_this_iter_s: 26.770345449447632\n",
      "  time_total_s: 13206.722905635834\n",
      "  timers:\n",
      "    learn_throughput: 1449.353\n",
      "    learn_time_ms: 689.963\n",
      "    load_throughput: 39997.33\n",
      "    load_time_ms: 25.002\n",
      "    sample_throughput: 36.483\n",
      "    sample_time_ms: 27409.932\n",
      "    update_time_ms: 2.476\n",
      "  timestamp: 1635076082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 533000\n",
      "  training_iteration: 533\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   533</td><td style=\"text-align: right;\">         13206.7</td><td style=\"text-align: right;\">533000</td><td style=\"text-align: right;\"> -2.5175</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -3.28</td><td style=\"text-align: right;\">            251.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 534000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-48-46\n",
      "  done: false\n",
      "  episode_len_mean: 250.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5015999999999905\n",
      "  episode_reward_min: -3.279999999999974\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1803\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.714024294517577e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3556648913356993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0043478186774519555\n",
      "          policy_loss: -0.11816834641827477\n",
      "          total_loss: -0.10717567884259754\n",
      "          vf_explained_var: 0.17843949794769287\n",
      "          vf_loss: 0.014549316320982244\n",
      "    num_agent_steps_sampled: 534000\n",
      "    num_agent_steps_trained: 534000\n",
      "    num_steps_sampled: 534000\n",
      "    num_steps_trained: 534000\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.5859375\n",
      "    ram_util_percent: 38.721875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038668402514742385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.86747341591347\n",
      "    mean_inference_ms: 1.9279119127476405\n",
      "    mean_raw_obs_processing_ms: 2.1104429569990053\n",
      "  time_since_restore: 13251.305266857147\n",
      "  time_this_iter_s: 44.58236122131348\n",
      "  time_total_s: 13251.305266857147\n",
      "  timers:\n",
      "    learn_throughput: 1448.043\n",
      "    learn_time_ms: 690.587\n",
      "    load_throughput: 40231.55\n",
      "    load_time_ms: 24.856\n",
      "    sample_throughput: 34.168\n",
      "    sample_time_ms: 29267.542\n",
      "    update_time_ms: 2.475\n",
      "  timestamp: 1635076126\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 534000\n",
      "  training_iteration: 534\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   534</td><td style=\"text-align: right;\">         13251.3</td><td style=\"text-align: right;\">534000</td><td style=\"text-align: right;\"> -2.5016</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.28</td><td style=\"text-align: right;\">            250.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 535000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-49-13\n",
      "  done: false\n",
      "  episode_len_mean: 249.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.495699999999991\n",
      "  episode_reward_min: -3.279999999999974\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1808\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3570121472587886e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.34927790694766575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0021632155923681\n",
      "          policy_loss: -0.005880419909954071\n",
      "          total_loss: 0.0027054035001330906\n",
      "          vf_explained_var: 0.3144417405128479\n",
      "          vf_loss: 0.012078602467146185\n",
      "    num_agent_steps_sampled: 535000\n",
      "    num_agent_steps_trained: 535000\n",
      "    num_steps_sampled: 535000\n",
      "    num_steps_trained: 535000\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.171052631578945\n",
      "    ram_util_percent: 38.768421052631574\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866807676971838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.87526889748228\n",
      "    mean_inference_ms: 1.9279111270609242\n",
      "    mean_raw_obs_processing_ms: 2.112095116010689\n",
      "  time_since_restore: 13277.959842443466\n",
      "  time_this_iter_s: 26.65457558631897\n",
      "  time_total_s: 13277.959842443466\n",
      "  timers:\n",
      "    learn_throughput: 1445.225\n",
      "    learn_time_ms: 691.934\n",
      "    load_throughput: 39992.067\n",
      "    load_time_ms: 25.005\n",
      "    sample_throughput: 34.143\n",
      "    sample_time_ms: 29288.266\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1635076153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535000\n",
      "  training_iteration: 535\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   535</td><td style=\"text-align: right;\">           13278</td><td style=\"text-align: right;\">535000</td><td style=\"text-align: right;\"> -2.4957</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.28</td><td style=\"text-align: right;\">            249.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-49-40\n",
      "  done: false\n",
      "  episode_len_mean: 249.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.4953999999999903\n",
      "  episode_reward_min: -3.279999999999974\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1812\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1785060736293943e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2978777766227722\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002371944386834931\n",
      "          policy_loss: 0.032848734574185474\n",
      "          total_loss: 0.0404470719397068\n",
      "          vf_explained_var: 0.23579542338848114\n",
      "          vf_loss: 0.010577118769288064\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.1\n",
      "    ram_util_percent: 38.86842105263159\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038667802199064065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.88148976014143\n",
      "    mean_inference_ms: 1.9279102173765836\n",
      "    mean_raw_obs_processing_ms: 2.112339554588022\n",
      "  time_since_restore: 13304.956264257431\n",
      "  time_this_iter_s: 26.996421813964844\n",
      "  time_total_s: 13304.956264257431\n",
      "  timers:\n",
      "    learn_throughput: 1447.13\n",
      "    learn_time_ms: 691.023\n",
      "    load_throughput: 39803.899\n",
      "    load_time_ms: 25.123\n",
      "    sample_throughput: 36.241\n",
      "    sample_time_ms: 27592.779\n",
      "    update_time_ms: 2.494\n",
      "  timestamp: 1635076180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 536\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   536</td><td style=\"text-align: right;\">           13305</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\"> -2.4954</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.28</td><td style=\"text-align: right;\">            249.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 537000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-50-08\n",
      "  done: false\n",
      "  episode_len_mean: 247.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.471599999999991\n",
      "  episode_reward_min: -3.0299999999999794\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1816\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.892530368146971e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.38387033840020496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01674960092462467\n",
      "          policy_loss: 0.023553132265806197\n",
      "          total_loss: 0.030994803375667995\n",
      "          vf_explained_var: 0.3006855845451355\n",
      "          vf_loss: 0.011280376826309495\n",
      "    num_agent_steps_sampled: 537000\n",
      "    num_agent_steps_trained: 537000\n",
      "    num_steps_sampled: 537000\n",
      "    num_steps_trained: 537000\n",
      "  iterations_since_restore: 537\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.153846153846146\n",
      "    ram_util_percent: 38.917948717948725\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866752692480382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.888096556927604\n",
      "    mean_inference_ms: 1.927909454640967\n",
      "    mean_raw_obs_processing_ms: 2.112629211661322\n",
      "  time_since_restore: 13332.483450651169\n",
      "  time_this_iter_s: 27.527186393737793\n",
      "  time_total_s: 13332.483450651169\n",
      "  timers:\n",
      "    learn_throughput: 1447.008\n",
      "    learn_time_ms: 691.081\n",
      "    load_throughput: 39955.913\n",
      "    load_time_ms: 25.028\n",
      "    sample_throughput: 35.97\n",
      "    sample_time_ms: 27801.309\n",
      "    update_time_ms: 2.495\n",
      "  timestamp: 1635076208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 537000\n",
      "  training_iteration: 537\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   537</td><td style=\"text-align: right;\">         13332.5</td><td style=\"text-align: right;\">537000</td><td style=\"text-align: right;\"> -2.4716</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.03</td><td style=\"text-align: right;\">            247.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 538000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-50-35\n",
      "  done: false\n",
      "  episode_len_mean: 245.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.456599999999992\n",
      "  episode_reward_min: -3.0199999999999796\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1820\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.892530368146971e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3636863440275192\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009654761817122904\n",
      "          policy_loss: 0.012501424468225903\n",
      "          total_loss: 0.020901565584871502\n",
      "          vf_explained_var: 0.2576420307159424\n",
      "          vf_loss: 0.012037004778782527\n",
      "    num_agent_steps_sampled: 538000\n",
      "    num_agent_steps_trained: 538000\n",
      "    num_steps_sampled: 538000\n",
      "    num_steps_trained: 538000\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.49487179487179\n",
      "    ram_util_percent: 38.925641025641035\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866724656994841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.89489410292179\n",
      "    mean_inference_ms: 1.9279089612313625\n",
      "    mean_raw_obs_processing_ms: 2.1129265941296502\n",
      "  time_since_restore: 13359.65209555626\n",
      "  time_this_iter_s: 27.168644905090332\n",
      "  time_total_s: 13359.65209555626\n",
      "  timers:\n",
      "    learn_throughput: 1446.188\n",
      "    learn_time_ms: 691.473\n",
      "    load_throughput: 40563.986\n",
      "    load_time_ms: 24.652\n",
      "    sample_throughput: 35.658\n",
      "    sample_time_ms: 28043.984\n",
      "    update_time_ms: 2.48\n",
      "  timestamp: 1635076235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 538000\n",
      "  training_iteration: 538\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   538</td><td style=\"text-align: right;\">         13359.7</td><td style=\"text-align: right;\">538000</td><td style=\"text-align: right;\"> -2.4566</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">            245.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 539000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 245.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.4521999999999915\n",
      "  episode_reward_min: -3.0199999999999796\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1824\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.892530368146971e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4174816442860497\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008379686143922843\n",
      "          policy_loss: -0.013994506042864587\n",
      "          total_loss: -0.005993121200137668\n",
      "          vf_explained_var: 0.2703399360179901\n",
      "          vf_loss: 0.012176197953522205\n",
      "    num_agent_steps_sampled: 539000\n",
      "    num_agent_steps_trained: 539000\n",
      "    num_steps_sampled: 539000\n",
      "    num_steps_trained: 539000\n",
      "  iterations_since_restore: 539\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.04054054054054\n",
      "    ram_util_percent: 38.92702702702704\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866699550597977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.90165960671927\n",
      "    mean_inference_ms: 1.9279088281447991\n",
      "    mean_raw_obs_processing_ms: 2.1132675564701016\n",
      "  time_since_restore: 13385.267212629318\n",
      "  time_this_iter_s: 25.615117073059082\n",
      "  time_total_s: 13385.267212629318\n",
      "  timers:\n",
      "    learn_throughput: 1445.081\n",
      "    learn_time_ms: 692.003\n",
      "    load_throughput: 41029.438\n",
      "    load_time_ms: 24.373\n",
      "    sample_throughput: 35.852\n",
      "    sample_time_ms: 27892.613\n",
      "    update_time_ms: 2.477\n",
      "  timestamp: 1635076260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 539000\n",
      "  training_iteration: 539\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   539</td><td style=\"text-align: right;\">         13385.3</td><td style=\"text-align: right;\">539000</td><td style=\"text-align: right;\"> -2.4522</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">            245.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-51-26\n",
      "  done: false\n",
      "  episode_len_mean: 245.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.4542999999999915\n",
      "  episode_reward_min: -3.0199999999999796\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1828\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.892530368146971e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.37938892195622126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06588922350077969\n",
      "          policy_loss: 0.030340205836627217\n",
      "          total_loss: 0.03854890722367499\n",
      "          vf_explained_var: 0.27705681324005127\n",
      "          vf_loss: 0.012002590640137593\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.262162162162156\n",
      "    ram_util_percent: 38.883783783783784\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866674163250814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.90847228828875\n",
      "    mean_inference_ms: 1.9279089402766652\n",
      "    mean_raw_obs_processing_ms: 2.113614519416452\n",
      "  time_since_restore: 13411.381971359253\n",
      "  time_this_iter_s: 26.114758729934692\n",
      "  time_total_s: 13411.381971359253\n",
      "  timers:\n",
      "    learn_throughput: 1443.727\n",
      "    learn_time_ms: 692.652\n",
      "    load_throughput: 40845.879\n",
      "    load_time_ms: 24.482\n",
      "    sample_throughput: 35.865\n",
      "    sample_time_ms: 27882.51\n",
      "    update_time_ms: 2.481\n",
      "  timestamp: 1635076286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 540\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   540</td><td style=\"text-align: right;\">         13411.4</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\"> -2.4543</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">            245.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 541000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-52-11\n",
      "  done: false\n",
      "  episode_len_mean: 243.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.4397999999999915\n",
      "  episode_reward_min: -3.0199999999999796\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1832\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.838795552220456e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.46678643822669985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02260368910233655\n",
      "          policy_loss: -0.02419080932935079\n",
      "          total_loss: -0.015649058669805527\n",
      "          vf_explained_var: 0.17632834613323212\n",
      "          vf_loss: 0.013209615958233674\n",
      "    num_agent_steps_sampled: 541000\n",
      "    num_agent_steps_trained: 541000\n",
      "    num_steps_sampled: 541000\n",
      "    num_steps_trained: 541000\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.43809523809524\n",
      "    ram_util_percent: 38.82857142857144\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866649463260499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.91537250206462\n",
      "    mean_inference_ms: 1.9279100675396625\n",
      "    mean_raw_obs_processing_ms: 2.1152513849362204\n",
      "  time_since_restore: 13455.430543422699\n",
      "  time_this_iter_s: 44.048572063446045\n",
      "  time_total_s: 13455.430543422699\n",
      "  timers:\n",
      "    learn_throughput: 1442.488\n",
      "    learn_time_ms: 693.247\n",
      "    load_throughput: 40831.365\n",
      "    load_time_ms: 24.491\n",
      "    sample_throughput: 33.828\n",
      "    sample_time_ms: 29561.414\n",
      "    update_time_ms: 2.45\n",
      "  timestamp: 1635076331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 541000\n",
      "  training_iteration: 541\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   541</td><td style=\"text-align: right;\">         13455.4</td><td style=\"text-align: right;\">541000</td><td style=\"text-align: right;\"> -2.4398</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">            243.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 542000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-52-32\n",
      "  done: false\n",
      "  episode_len_mean: 246.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.4638999999999913\n",
      "  episode_reward_min: -3.5299999999999687\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1836\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3258193328330684e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7064839508798388\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0703018902031744\n",
      "          policy_loss: 0.002082027701867951\n",
      "          total_loss: -0.0035419565522008473\n",
      "          vf_explained_var: 0.419649213552475\n",
      "          vf_loss: 0.011440856558167272\n",
      "    num_agent_steps_sampled: 542000\n",
      "    num_agent_steps_trained: 542000\n",
      "    num_steps_sampled: 542000\n",
      "    num_steps_trained: 542000\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.91612903225807\n",
      "    ram_util_percent: 38.79677419354838\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866620883314327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.921910724744922\n",
      "    mean_inference_ms: 1.92791029988854\n",
      "    mean_raw_obs_processing_ms: 2.1168929061118216\n",
      "  time_since_restore: 13476.917053461075\n",
      "  time_this_iter_s: 21.486510038375854\n",
      "  time_total_s: 13476.917053461075\n",
      "  timers:\n",
      "    learn_throughput: 1438.924\n",
      "    learn_time_ms: 694.964\n",
      "    load_throughput: 40896.818\n",
      "    load_time_ms: 24.452\n",
      "    sample_throughput: 34.517\n",
      "    sample_time_ms: 28971.142\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1635076352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 542000\n",
      "  training_iteration: 542\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   542</td><td style=\"text-align: right;\">         13476.9</td><td style=\"text-align: right;\">542000</td><td style=\"text-align: right;\"> -2.4639</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">            246.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 543000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-52-52\n",
      "  done: false\n",
      "  episode_len_mean: 248.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.482099999999991\n",
      "  episode_reward_min: -3.5899999999999674\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1838\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9887289992496024e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.432093448109097\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012008428284903654\n",
      "          policy_loss: -0.06298018246889114\n",
      "          total_loss: -0.07154181351264317\n",
      "          vf_explained_var: 0.7925750613212585\n",
      "          vf_loss: 0.0057592994611089425\n",
      "    num_agent_steps_sampled: 543000\n",
      "    num_agent_steps_trained: 543000\n",
      "    num_steps_sampled: 543000\n",
      "    num_steps_trained: 543000\n",
      "  iterations_since_restore: 543\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06428571428571\n",
      "    ram_util_percent: 38.88214285714285\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866605211314916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.924985864832557\n",
      "    mean_inference_ms: 1.9279101211804943\n",
      "    mean_raw_obs_processing_ms: 2.1176776609193757\n",
      "  time_since_restore: 13496.370401620865\n",
      "  time_this_iter_s: 19.45334815979004\n",
      "  time_total_s: 13496.370401620865\n",
      "  timers:\n",
      "    learn_throughput: 1439.887\n",
      "    learn_time_ms: 694.499\n",
      "    load_throughput: 41341.806\n",
      "    load_time_ms: 24.189\n",
      "    sample_throughput: 35.411\n",
      "    sample_time_ms: 28240.144\n",
      "    update_time_ms: 2.471\n",
      "  timestamp: 1635076372\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543000\n",
      "  training_iteration: 543\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   543</td><td style=\"text-align: right;\">         13496.4</td><td style=\"text-align: right;\">543000</td><td style=\"text-align: right;\"> -2.4821</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">            248.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-53-11\n",
      "  done: false\n",
      "  episode_len_mean: 251.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5126999999999904\n",
      "  episode_reward_min: -3.7799999999999634\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1841\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9887289992496024e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5931327435705396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022295348239224595\n",
      "          policy_loss: -0.08277638604243596\n",
      "          total_loss: -0.09169309267567263\n",
      "          vf_explained_var: 0.7621487975120544\n",
      "          vf_loss: 0.007014621597611242\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.73214285714285\n",
      "    ram_util_percent: 38.91071428571428\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866578902850878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.9291616739555\n",
      "    mean_inference_ms: 1.9279090817142566\n",
      "    mean_raw_obs_processing_ms: 2.1179114681943365\n",
      "  time_since_restore: 13516.289420604706\n",
      "  time_this_iter_s: 19.919018983840942\n",
      "  time_total_s: 13516.289420604706\n",
      "  timers:\n",
      "    learn_throughput: 1442.918\n",
      "    learn_time_ms: 693.04\n",
      "    load_throughput: 41067.039\n",
      "    load_time_ms: 24.35\n",
      "    sample_throughput: 38.797\n",
      "    sample_time_ms: 25775.063\n",
      "    update_time_ms: 2.486\n",
      "  timestamp: 1635076391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 544\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   544</td><td style=\"text-align: right;\">         13516.3</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\"> -2.5127</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.78</td><td style=\"text-align: right;\">            251.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 545000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-53-34\n",
      "  done: false\n",
      "  episode_len_mean: 254.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5413999999999897\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1845\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.983093498874404e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1418471104568906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02626226185013919\n",
      "          policy_loss: 0.07172264895505376\n",
      "          total_loss: 0.06597763101259867\n",
      "          vf_explained_var: 0.8326097130775452\n",
      "          vf_loss: 0.005673453527399236\n",
      "    num_agent_steps_sampled: 545000\n",
      "    num_agent_steps_trained: 545000\n",
      "    num_steps_sampled: 545000\n",
      "    num_steps_trained: 545000\n",
      "  iterations_since_restore: 545\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.646875\n",
      "    ram_util_percent: 38.94375\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038665408827085586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.934453127701396\n",
      "    mean_inference_ms: 1.9279064632946936\n",
      "    mean_raw_obs_processing_ms: 2.1181928459638866\n",
      "  time_since_restore: 13538.409545183182\n",
      "  time_this_iter_s: 22.120124578475952\n",
      "  time_total_s: 13538.409545183182\n",
      "  timers:\n",
      "    learn_throughput: 1443.947\n",
      "    learn_time_ms: 692.546\n",
      "    load_throughput: 41334.187\n",
      "    load_time_ms: 24.193\n",
      "    sample_throughput: 39.491\n",
      "    sample_time_ms: 25322.269\n",
      "    update_time_ms: 2.482\n",
      "  timestamp: 1635076414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 545000\n",
      "  training_iteration: 545\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   545</td><td style=\"text-align: right;\">         13538.4</td><td style=\"text-align: right;\">545000</td><td style=\"text-align: right;\"> -2.5414</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            254.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 546000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-53-59\n",
      "  done: false\n",
      "  episode_len_mean: 254.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5439999999999894\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1848\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.474640248311605e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.682203949160046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009956177743134937\n",
      "          policy_loss: -0.11100116819143295\n",
      "          total_loss: -0.11245684151848158\n",
      "          vf_explained_var: 0.83698570728302\n",
      "          vf_loss: 0.005366367986425757\n",
      "    num_agent_steps_sampled: 546000\n",
      "    num_agent_steps_trained: 546000\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.211111111111116\n",
      "    ram_util_percent: 38.9388888888889\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866512639997218\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.93837510836582\n",
      "    mean_inference_ms: 1.927904358773914\n",
      "    mean_raw_obs_processing_ms: 2.1183974300204587\n",
      "  time_since_restore: 13564.07815694809\n",
      "  time_this_iter_s: 25.668611764907837\n",
      "  time_total_s: 13564.07815694809\n",
      "  timers:\n",
      "    learn_throughput: 1441.542\n",
      "    learn_time_ms: 693.702\n",
      "    load_throughput: 41600.873\n",
      "    load_time_ms: 24.038\n",
      "    sample_throughput: 39.701\n",
      "    sample_time_ms: 25188.478\n",
      "    update_time_ms: 2.494\n",
      "  timestamp: 1635076439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 546\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   546</td><td style=\"text-align: right;\">         13564.1</td><td style=\"text-align: right;\">546000</td><td style=\"text-align: right;\">  -2.544</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">             254.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 547000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 255.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.55139999999999\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1852\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.474640248311605e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8785664631260766\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008848443607329809\n",
      "          policy_loss: 0.01970855188038614\n",
      "          total_loss: 0.015604543023639256\n",
      "          vf_explained_var: 0.8395980596542358\n",
      "          vf_loss: 0.00468165589393013\n",
      "    num_agent_steps_sampled: 547000\n",
      "    num_agent_steps_trained: 547000\n",
      "    num_steps_sampled: 547000\n",
      "    num_steps_trained: 547000\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.05277777777778\n",
      "    ram_util_percent: 38.92222222222223\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866475419542939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.94347878205539\n",
      "    mean_inference_ms: 1.9279014976254845\n",
      "    mean_raw_obs_processing_ms: 2.1186513068189083\n",
      "  time_since_restore: 13588.853348493576\n",
      "  time_this_iter_s: 24.77519154548645\n",
      "  time_total_s: 13588.853348493576\n",
      "  timers:\n",
      "    learn_throughput: 1443.848\n",
      "    learn_time_ms: 692.594\n",
      "    load_throughput: 41405.963\n",
      "    load_time_ms: 24.151\n",
      "    sample_throughput: 40.138\n",
      "    sample_time_ms: 24914.242\n",
      "    update_time_ms: 2.507\n",
      "  timestamp: 1635076464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 547000\n",
      "  training_iteration: 547\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   547</td><td style=\"text-align: right;\">         13588.9</td><td style=\"text-align: right;\">547000</td><td style=\"text-align: right;\"> -2.5514</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            255.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-54-49\n",
      "  done: false\n",
      "  episode_len_mean: 255.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5592999999999893\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1856\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.474640248311605e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6515689724021487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005462459130952861\n",
      "          policy_loss: 0.030607139526142014\n",
      "          total_loss: 0.031001874307791392\n",
      "          vf_explained_var: 0.719222366809845\n",
      "          vf_loss: 0.006910422465039624\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.361111111111114\n",
      "    ram_util_percent: 38.886111111111106\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866436846959005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.948522762083783\n",
      "    mean_inference_ms: 1.9278985753956994\n",
      "    mean_raw_obs_processing_ms: 2.1188740194397075\n",
      "  time_since_restore: 13614.099299907684\n",
      "  time_this_iter_s: 25.245951414108276\n",
      "  time_total_s: 13614.099299907684\n",
      "  timers:\n",
      "    learn_throughput: 1445.087\n",
      "    learn_time_ms: 692.0\n",
      "    load_throughput: 40918.283\n",
      "    load_time_ms: 24.439\n",
      "    sample_throughput: 40.449\n",
      "    sample_time_ms: 24722.259\n",
      "    update_time_ms: 2.522\n",
      "  timestamp: 1635076489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 548\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   548</td><td style=\"text-align: right;\">         13614.1</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\"> -2.5593</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            255.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 549000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 256.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.561299999999989\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1860\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.474640248311605e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6216984113057454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073547290177239846\n",
      "          policy_loss: 0.022236015937394566\n",
      "          total_loss: 0.023632401848832765\n",
      "          vf_explained_var: 0.6140222549438477\n",
      "          vf_loss: 0.007613371695495314\n",
      "    num_agent_steps_sampled: 549000\n",
      "    num_agent_steps_trained: 549000\n",
      "    num_steps_sampled: 549000\n",
      "    num_steps_trained: 549000\n",
      "  iterations_since_restore: 549\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.169999999999995\n",
      "    ram_util_percent: 38.839999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038664021731080425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.953484372368894\n",
      "    mean_inference_ms: 1.9278960319369327\n",
      "    mean_raw_obs_processing_ms: 2.120333771417853\n",
      "  time_since_restore: 13656.29966378212\n",
      "  time_this_iter_s: 42.200363874435425\n",
      "  time_total_s: 13656.29966378212\n",
      "  timers:\n",
      "    learn_throughput: 1444.962\n",
      "    learn_time_ms: 692.06\n",
      "    load_throughput: 40592.802\n",
      "    load_time_ms: 24.635\n",
      "    sample_throughput: 37.907\n",
      "    sample_time_ms: 26380.573\n",
      "    update_time_ms: 2.519\n",
      "  timestamp: 1635076532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 549000\n",
      "  training_iteration: 549\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   549</td><td style=\"text-align: right;\">         13656.3</td><td style=\"text-align: right;\">549000</td><td style=\"text-align: right;\"> -2.5613</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            256.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 550000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 256.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.566399999999989\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1864\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.474640248311605e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8887571414311727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004074078619960315\n",
      "          policy_loss: 0.046714900268448725\n",
      "          total_loss: 0.04703953920139207\n",
      "          vf_explained_var: 0.3843154013156891\n",
      "          vf_loss: 0.009212212078273297\n",
      "    num_agent_steps_sampled: 550000\n",
      "    num_agent_steps_trained: 550000\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.386842105263156\n",
      "    ram_util_percent: 38.726315789473674\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866368701879674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.95846736693024\n",
      "    mean_inference_ms: 1.927893989728675\n",
      "    mean_raw_obs_processing_ms: 2.121797512875508\n",
      "  time_since_restore: 13682.994987726212\n",
      "  time_this_iter_s: 26.695323944091797\n",
      "  time_total_s: 13682.994987726212\n",
      "  timers:\n",
      "    learn_throughput: 1444.807\n",
      "    learn_time_ms: 692.134\n",
      "    load_throughput: 40815.035\n",
      "    load_time_ms: 24.501\n",
      "    sample_throughput: 37.823\n",
      "    sample_time_ms: 26438.682\n",
      "    update_time_ms: 2.498\n",
      "  timestamp: 1635076558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 550\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   550</td><td style=\"text-align: right;\">           13683</td><td style=\"text-align: right;\">550000</td><td style=\"text-align: right;\"> -2.5664</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            256.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 551000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 257.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.571599999999989\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1868\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2373201241558026e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8287661850452424\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010545532989194584\n",
      "          policy_loss: 0.024056573522587617\n",
      "          total_loss: 0.024296468123793602\n",
      "          vf_explained_var: 0.5154550075531006\n",
      "          vf_loss: 0.008527555151118173\n",
      "    num_agent_steps_sampled: 551000\n",
      "    num_agent_steps_trained: 551000\n",
      "    num_steps_sampled: 551000\n",
      "    num_steps_trained: 551000\n",
      "  iterations_since_restore: 551\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.04444444444445\n",
      "    ram_util_percent: 38.886111111111106\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038663330403455445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.963322550428078\n",
      "    mean_inference_ms: 1.9278913141424736\n",
      "    mean_raw_obs_processing_ms: 2.1229507892185695\n",
      "  time_since_restore: 13708.226943969727\n",
      "  time_this_iter_s: 25.231956243515015\n",
      "  time_total_s: 13708.226943969727\n",
      "  timers:\n",
      "    learn_throughput: 1445.004\n",
      "    learn_time_ms: 692.04\n",
      "    load_throughput: 40822.9\n",
      "    load_time_ms: 24.496\n",
      "    sample_throughput: 40.721\n",
      "    sample_time_ms: 24557.13\n",
      "    update_time_ms: 2.495\n",
      "  timestamp: 1635076584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551000\n",
      "  training_iteration: 551\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   551</td><td style=\"text-align: right;\">         13708.2</td><td style=\"text-align: right;\">551000</td><td style=\"text-align: right;\"> -2.5716</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            257.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-56-51\n",
      "  done: false\n",
      "  episode_len_mean: 256.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5698999999999894\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1872\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2373201241558026e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4735585242509842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04015854026742976\n",
      "          policy_loss: 0.030096863706906635\n",
      "          total_loss: 0.034673677302069134\n",
      "          vf_explained_var: 0.35856014490127563\n",
      "          vf_loss: 0.009312400221824646\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.42307692307692\n",
      "    ram_util_percent: 38.99999999999999\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866297818749238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.968176825104866\n",
      "    mean_inference_ms: 1.9278887181079063\n",
      "    mean_raw_obs_processing_ms: 2.1231611448004695\n",
      "  time_since_restore: 13735.393052577972\n",
      "  time_this_iter_s: 27.16610860824585\n",
      "  time_total_s: 13735.393052577972\n",
      "  timers:\n",
      "    learn_throughput: 1451.294\n",
      "    learn_time_ms: 689.04\n",
      "    load_throughput: 40125.208\n",
      "    load_time_ms: 24.922\n",
      "    sample_throughput: 39.797\n",
      "    sample_time_ms: 25127.65\n",
      "    update_time_ms: 2.498\n",
      "  timestamp: 1635076611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 552\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   552</td><td style=\"text-align: right;\">         13735.4</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\"> -2.5699</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            256.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 553000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 255.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5569999999999897\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1876\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3559801862337036e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.43757112092441985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0015073330261004337\n",
      "          policy_loss: 0.009423713137706121\n",
      "          total_loss: 0.014293384220865037\n",
      "          vf_explained_var: 0.37402060627937317\n",
      "          vf_loss: 0.009245382042394744\n",
      "    num_agent_steps_sampled: 553000\n",
      "    num_agent_steps_trained: 553000\n",
      "    num_steps_sampled: 553000\n",
      "    num_steps_trained: 553000\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.27368421052632\n",
      "    ram_util_percent: 39.071052631578944\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866263827714531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.973121435961524\n",
      "    mean_inference_ms: 1.9278866071632168\n",
      "    mean_raw_obs_processing_ms: 2.12337966286322\n",
      "  time_since_restore: 13762.178244829178\n",
      "  time_this_iter_s: 26.785192251205444\n",
      "  time_total_s: 13762.178244829178\n",
      "  timers:\n",
      "    learn_throughput: 1450.512\n",
      "    learn_time_ms: 689.412\n",
      "    load_throughput: 39941.682\n",
      "    load_time_ms: 25.037\n",
      "    sample_throughput: 38.669\n",
      "    sample_time_ms: 25860.384\n",
      "    update_time_ms: 2.47\n",
      "  timestamp: 1635076638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 553000\n",
      "  training_iteration: 553\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   553</td><td style=\"text-align: right;\">         13762.2</td><td style=\"text-align: right;\">553000</td><td style=\"text-align: right;\">  -2.557</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">             255.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 554000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-57-44\n",
      "  done: false\n",
      "  episode_len_mean: 255.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5528999999999895\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1880\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6779900931168518e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.525338128540251\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04732078049320852\n",
      "          policy_loss: -0.0849457243250476\n",
      "          total_loss: -0.08099377411935064\n",
      "          vf_explained_var: 0.38728469610214233\n",
      "          vf_loss: 0.009205329418182372\n",
      "    num_agent_steps_sampled: 554000\n",
      "    num_agent_steps_trained: 554000\n",
      "    num_steps_sampled: 554000\n",
      "    num_steps_trained: 554000\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24473684210527\n",
      "    ram_util_percent: 39.07368421052632\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866227723736318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.97807155841149\n",
      "    mean_inference_ms: 1.9278892159880656\n",
      "    mean_raw_obs_processing_ms: 2.1236401793661672\n",
      "  time_since_restore: 13789.028270483017\n",
      "  time_this_iter_s: 26.85002565383911\n",
      "  time_total_s: 13789.028270483017\n",
      "  timers:\n",
      "    learn_throughput: 1447.564\n",
      "    learn_time_ms: 690.816\n",
      "    load_throughput: 39950.394\n",
      "    load_time_ms: 25.031\n",
      "    sample_throughput: 37.662\n",
      "    sample_time_ms: 26552.159\n",
      "    update_time_ms: 2.448\n",
      "  timestamp: 1635076664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 554000\n",
      "  training_iteration: 554\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   554</td><td style=\"text-align: right;\">           13789</td><td style=\"text-align: right;\">554000</td><td style=\"text-align: right;\"> -2.5529</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            255.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 555000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-58-11\n",
      "  done: false\n",
      "  episode_len_mean: 255.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.55179999999999\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1885\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.516985139675278e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4531922909948561\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009700516093441535\n",
      "          policy_loss: -0.0331960620979468\n",
      "          total_loss: -0.026992815732955932\n",
      "          vf_explained_var: 0.44821515679359436\n",
      "          vf_loss: 0.010735169922312101\n",
      "    num_agent_steps_sampled: 555000\n",
      "    num_agent_steps_trained: 555000\n",
      "    num_steps_sampled: 555000\n",
      "    num_steps_trained: 555000\n",
      "  iterations_since_restore: 555\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.17179487179488\n",
      "    ram_util_percent: 39.02051282051282\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866185626143564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.984299375065405\n",
      "    mean_inference_ms: 1.9278922844572157\n",
      "    mean_raw_obs_processing_ms: 2.1239559703357274\n",
      "  time_since_restore: 13816.106699466705\n",
      "  time_this_iter_s: 27.078428983688354\n",
      "  time_total_s: 13816.106699466705\n",
      "  timers:\n",
      "    learn_throughput: 1447.25\n",
      "    learn_time_ms: 690.966\n",
      "    load_throughput: 39927.994\n",
      "    load_time_ms: 25.045\n",
      "    sample_throughput: 36.972\n",
      "    sample_time_ms: 27047.76\n",
      "    update_time_ms: 2.489\n",
      "  timestamp: 1635076691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555000\n",
      "  training_iteration: 555\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   555</td><td style=\"text-align: right;\">         13816.1</td><td style=\"text-align: right;\">555000</td><td style=\"text-align: right;\"> -2.5518</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            255.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 255.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5513999999999895\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1889\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.516985139675278e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5164819406138526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07420342101316414\n",
      "          policy_loss: -0.00827166193889247\n",
      "          total_loss: -0.004716307214564747\n",
      "          vf_explained_var: 0.4201187491416931\n",
      "          vf_loss: 0.008720171311870218\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.547368421052624\n",
      "    ram_util_percent: 38.98947368421053\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866155158881572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.989230165506665\n",
      "    mean_inference_ms: 1.9278947364336827\n",
      "    mean_raw_obs_processing_ms: 2.124228919794648\n",
      "  time_since_restore: 13842.784174203873\n",
      "  time_this_iter_s: 26.67747473716736\n",
      "  time_total_s: 13842.784174203873\n",
      "  timers:\n",
      "    learn_throughput: 1448.775\n",
      "    learn_time_ms: 690.238\n",
      "    load_throughput: 39766.01\n",
      "    load_time_ms: 25.147\n",
      "    sample_throughput: 36.833\n",
      "    sample_time_ms: 27149.38\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1635076718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 556\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   556</td><td style=\"text-align: right;\">         13842.8</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\"> -2.5514</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            255.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 557000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-59-21\n",
      "  done: false\n",
      "  episode_len_mean: 255.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5566999999999895\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1893\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7754777095129166e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6820924205912484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016394527360354714\n",
      "          policy_loss: 0.04664570829934544\n",
      "          total_loss: 0.0491195195251041\n",
      "          vf_explained_var: 0.3460451066493988\n",
      "          vf_loss: 0.009294736271517143\n",
      "    num_agent_steps_sampled: 557000\n",
      "    num_agent_steps_trained: 557000\n",
      "    num_steps_sampled: 557000\n",
      "    num_steps_trained: 557000\n",
      "  iterations_since_restore: 557\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.185245901639355\n",
      "    ram_util_percent: 38.92459016393443\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386612772386289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.994002476124233\n",
      "    mean_inference_ms: 1.9278975467064459\n",
      "    mean_raw_obs_processing_ms: 2.1257310462380477\n",
      "  time_since_restore: 13885.267838954926\n",
      "  time_this_iter_s: 42.483664751052856\n",
      "  time_total_s: 13885.267838954926\n",
      "  timers:\n",
      "    learn_throughput: 1446.898\n",
      "    learn_time_ms: 691.134\n",
      "    load_throughput: 39786.795\n",
      "    load_time_ms: 25.134\n",
      "    sample_throughput: 34.579\n",
      "    sample_time_ms: 28919.359\n",
      "    update_time_ms: 2.379\n",
      "  timestamp: 1635076761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 557000\n",
      "  training_iteration: 557\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   557</td><td style=\"text-align: right;\">         13885.3</td><td style=\"text-align: right;\">557000</td><td style=\"text-align: right;\"> -2.5567</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            255.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 558000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_11-59-46\n",
      "  done: false\n",
      "  episode_len_mean: 256.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0500000000000003\n",
      "  episode_reward_mean: -2.5613999999999892\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1897\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7754777095129166e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7354574667082893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013001473213118212\n",
      "          policy_loss: 0.035805812726418175\n",
      "          total_loss: 0.03653710107836458\n",
      "          vf_explained_var: 0.17765267193317413\n",
      "          vf_loss: 0.008085859848910736\n",
      "    num_agent_steps_sampled: 558000\n",
      "    num_agent_steps_trained: 558000\n",
      "    num_steps_sampled: 558000\n",
      "    num_steps_trained: 558000\n",
      "  iterations_since_restore: 558\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.92972972972973\n",
      "    ram_util_percent: 38.88648648648649\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866099867872884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.998656777389474\n",
      "    mean_inference_ms: 1.9279000958805983\n",
      "    mean_raw_obs_processing_ms: 2.1272368016579035\n",
      "  time_since_restore: 13911.001863718033\n",
      "  time_this_iter_s: 25.7340247631073\n",
      "  time_total_s: 13911.001863718033\n",
      "  timers:\n",
      "    learn_throughput: 1447.7\n",
      "    learn_time_ms: 690.751\n",
      "    load_throughput: 39515.56\n",
      "    load_time_ms: 25.306\n",
      "    sample_throughput: 34.52\n",
      "    sample_time_ms: 28968.398\n",
      "    update_time_ms: 2.362\n",
      "  timestamp: 1635076786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 558000\n",
      "  training_iteration: 558\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   558</td><td style=\"text-align: right;\">           13911</td><td style=\"text-align: right;\">558000</td><td style=\"text-align: right;\"> -2.5614</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            256.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 559000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-00-12\n",
      "  done: false\n",
      "  episode_len_mean: 257.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1299999999999986\n",
      "  episode_reward_mean: -2.570899999999989\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1900\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7754777095129166e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7474357181125217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006759250928676784\n",
      "          policy_loss: -0.09650418501761225\n",
      "          total_loss: -0.09275311157107353\n",
      "          vf_explained_var: 0.0973237007856369\n",
      "          vf_loss: 0.011225434361646573\n",
      "    num_agent_steps_sampled: 559000\n",
      "    num_agent_steps_trained: 559000\n",
      "    num_steps_sampled: 559000\n",
      "    num_steps_trained: 559000\n",
      "  iterations_since_restore: 559\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.23055555555556\n",
      "    ram_util_percent: 38.98888888888889\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866078188658131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.00207891784747\n",
      "    mean_inference_ms: 1.927901687415169\n",
      "    mean_raw_obs_processing_ms: 2.128029914269342\n",
      "  time_since_restore: 13936.352504730225\n",
      "  time_this_iter_s: 25.350641012191772\n",
      "  time_total_s: 13936.352504730225\n",
      "  timers:\n",
      "    learn_throughput: 1446.257\n",
      "    learn_time_ms: 691.44\n",
      "    load_throughput: 39483.792\n",
      "    load_time_ms: 25.327\n",
      "    sample_throughput: 36.653\n",
      "    sample_time_ms: 27282.691\n",
      "    update_time_ms: 2.365\n",
      "  timestamp: 1635076812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559000\n",
      "  training_iteration: 559\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   559</td><td style=\"text-align: right;\">         13936.4</td><td style=\"text-align: right;\">559000</td><td style=\"text-align: right;\"> -2.5709</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            257.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-00-37\n",
      "  done: false\n",
      "  episode_len_mean: 257.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1299999999999986\n",
      "  episode_reward_mean: -2.5774999999999886\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1904\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7754777095129166e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6861522959338294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004306217276189747\n",
      "          policy_loss: -0.040329607865876624\n",
      "          total_loss: -0.035168773949974114\n",
      "          vf_explained_var: 0.10343922674655914\n",
      "          vf_loss: 0.012022358965542582\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 560\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.552777777777784\n",
      "    ram_util_percent: 39.05555555555555\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038660507332971196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.00652927781528\n",
      "    mean_inference_ms: 1.9279038801347412\n",
      "    mean_raw_obs_processing_ms: 2.1282266879148692\n",
      "  time_since_restore: 13961.984148263931\n",
      "  time_this_iter_s: 25.631643533706665\n",
      "  time_total_s: 13961.984148263931\n",
      "  timers:\n",
      "    learn_throughput: 1445.551\n",
      "    learn_time_ms: 691.778\n",
      "    load_throughput: 39191.081\n",
      "    load_time_ms: 25.516\n",
      "    sample_throughput: 36.797\n",
      "    sample_time_ms: 27175.827\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1635076837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 560\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   560</td><td style=\"text-align: right;\">           13962</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\"> -2.5775</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            257.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 561000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-01-03\n",
      "  done: false\n",
      "  episode_len_mean: 258.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1299999999999986\n",
      "  episode_reward_mean: -2.5821999999999887\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1908\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8877388547564583e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6716618822680579\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003090883992428041\n",
      "          policy_loss: -0.027518025868468816\n",
      "          total_loss: -0.021839663262168567\n",
      "          vf_explained_var: 0.1264062523841858\n",
      "          vf_loss: 0.012394981065558062\n",
      "    num_agent_steps_sampled: 561000\n",
      "    num_agent_steps_trained: 561000\n",
      "    num_steps_sampled: 561000\n",
      "    num_steps_trained: 561000\n",
      "  iterations_since_restore: 561\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03783783783784\n",
      "    ram_util_percent: 39.08648648648648\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038660248327564165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.010952584764464\n",
      "    mean_inference_ms: 1.927906017165468\n",
      "    mean_raw_obs_processing_ms: 2.1283932004162924\n",
      "  time_since_restore: 13987.546309232712\n",
      "  time_this_iter_s: 25.562160968780518\n",
      "  time_total_s: 13987.546309232712\n",
      "  timers:\n",
      "    learn_throughput: 1448.114\n",
      "    learn_time_ms: 690.553\n",
      "    load_throughput: 39205.185\n",
      "    load_time_ms: 25.507\n",
      "    sample_throughput: 36.751\n",
      "    sample_time_ms: 27209.975\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1635076863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 561000\n",
      "  training_iteration: 561\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   561</td><td style=\"text-align: right;\">         13987.5</td><td style=\"text-align: right;\">561000</td><td style=\"text-align: right;\"> -2.5822</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            258.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 562000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 259.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1299999999999986\n",
      "  episode_reward_mean: -2.5902999999999885\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1912\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.438694273782292e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6941922995779249\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010349722063098138\n",
      "          policy_loss: -0.002494229707452986\n",
      "          total_loss: 0.003461757302284241\n",
      "          vf_explained_var: 0.09892917424440384\n",
      "          vf_loss: 0.012897908522023094\n",
      "    num_agent_steps_sampled: 562000\n",
      "    num_agent_steps_trained: 562000\n",
      "    num_steps_sampled: 562000\n",
      "    num_steps_trained: 562000\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.15833333333333\n",
      "    ram_util_percent: 39.06388888888888\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038660015047308004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.01521537454935\n",
      "    mean_inference_ms: 1.927908040297172\n",
      "    mean_raw_obs_processing_ms: 2.1285663476885976\n",
      "  time_since_restore: 14012.603829622269\n",
      "  time_this_iter_s: 25.057520389556885\n",
      "  time_total_s: 14012.603829622269\n",
      "  timers:\n",
      "    learn_throughput: 1446.914\n",
      "    learn_time_ms: 691.126\n",
      "    load_throughput: 39656.824\n",
      "    load_time_ms: 25.216\n",
      "    sample_throughput: 37.039\n",
      "    sample_time_ms: 26998.824\n",
      "    update_time_ms: 2.371\n",
      "  timestamp: 1635076888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 562000\n",
      "  training_iteration: 562\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   562</td><td style=\"text-align: right;\">         14012.6</td><td style=\"text-align: right;\">562000</td><td style=\"text-align: right;\"> -2.5903</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            259.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 563000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-01-54\n",
      "  done: false\n",
      "  episode_len_mean: 259.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1299999999999986\n",
      "  episode_reward_mean: -2.5960999999999883\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1916\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.438694273782292e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5737013873126772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00809524835715886\n",
      "          policy_loss: 0.005197364423010085\n",
      "          total_loss: 0.012629632362061077\n",
      "          vf_explained_var: 0.10711085796356201\n",
      "          vf_loss: 0.013169282685137458\n",
      "    num_agent_steps_sampled: 563000\n",
      "    num_agent_steps_trained: 563000\n",
      "    num_steps_sampled: 563000\n",
      "    num_steps_trained: 563000\n",
      "  iterations_since_restore: 563\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24324324324324\n",
      "    ram_util_percent: 39.035135135135135\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865977106350965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.019343097537494\n",
      "    mean_inference_ms: 1.9279098946897693\n",
      "    mean_raw_obs_processing_ms: 2.128745404740645\n",
      "  time_since_restore: 14038.553251028061\n",
      "  time_this_iter_s: 25.949421405792236\n",
      "  time_total_s: 14038.553251028061\n",
      "  timers:\n",
      "    learn_throughput: 1446.175\n",
      "    learn_time_ms: 691.479\n",
      "    load_throughput: 39591.839\n",
      "    load_time_ms: 25.258\n",
      "    sample_throughput: 37.154\n",
      "    sample_time_ms: 26914.86\n",
      "    update_time_ms: 2.378\n",
      "  timestamp: 1635076914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 563000\n",
      "  training_iteration: 563\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   563</td><td style=\"text-align: right;\">         14038.6</td><td style=\"text-align: right;\">563000</td><td style=\"text-align: right;\"> -2.5961</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            259.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-02-37\n",
      "  done: false\n",
      "  episode_len_mean: 259.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1299999999999986\n",
      "  episode_reward_mean: -2.597299999999988\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1920\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.438694273782292e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5553179615073733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012892313556046985\n",
      "          policy_loss: -0.0729796924524837\n",
      "          total_loss: -0.06519451629784372\n",
      "          vf_explained_var: 0.11614947766065598\n",
      "          vf_loss: 0.01333835870027542\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.93666666666666\n",
      "    ram_util_percent: 38.97666666666665\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865953883402587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.023329548660477\n",
      "    mean_inference_ms: 1.9279115517149479\n",
      "    mean_raw_obs_processing_ms: 2.130138107439892\n",
      "  time_since_restore: 14081.071922779083\n",
      "  time_this_iter_s: 42.51867175102234\n",
      "  time_total_s: 14081.071922779083\n",
      "  timers:\n",
      "    learn_throughput: 1445.982\n",
      "    learn_time_ms: 691.571\n",
      "    load_throughput: 39998.055\n",
      "    load_time_ms: 25.001\n",
      "    sample_throughput: 35.11\n",
      "    sample_time_ms: 28481.889\n",
      "    update_time_ms: 2.377\n",
      "  timestamp: 1635076957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 564\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   564</td><td style=\"text-align: right;\">         14081.1</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\"> -2.5973</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            259.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 565000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-03-01\n",
      "  done: false\n",
      "  episode_len_mean: 260.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1299999999999986\n",
      "  episode_reward_mean: -2.6061999999999883\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1924\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.438694273782292e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8524296320146985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009770090049369963\n",
      "          policy_loss: 0.016825306332773633\n",
      "          total_loss: 0.022282702310217752\n",
      "          vf_explained_var: 0.16203758120536804\n",
      "          vf_loss: 0.013981691561639309\n",
      "    num_agent_steps_sampled: 565000\n",
      "    num_agent_steps_trained: 565000\n",
      "    num_steps_sampled: 565000\n",
      "    num_steps_trained: 565000\n",
      "  iterations_since_restore: 565\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.363888888888894\n",
      "    ram_util_percent: 38.766666666666666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038659300770966354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.027232376278185\n",
      "    mean_inference_ms: 1.9279132631243343\n",
      "    mean_raw_obs_processing_ms: 2.131534748510309\n",
      "  time_since_restore: 14105.739617347717\n",
      "  time_this_iter_s: 24.667694568634033\n",
      "  time_total_s: 14105.739617347717\n",
      "  timers:\n",
      "    learn_throughput: 1445.965\n",
      "    learn_time_ms: 691.58\n",
      "    load_throughput: 39799.253\n",
      "    load_time_ms: 25.126\n",
      "    sample_throughput: 35.41\n",
      "    sample_time_ms: 28240.756\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1635076981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 565000\n",
      "  training_iteration: 565\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   565</td><td style=\"text-align: right;\">         14105.7</td><td style=\"text-align: right;\">565000</td><td style=\"text-align: right;\"> -2.6062</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            260.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 566000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-03-28\n",
      "  done: false\n",
      "  episode_len_mean: 260.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1299999999999986\n",
      "  episode_reward_mean: -2.6013999999999884\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1928\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.438694273782292e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.49485656751526724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004959033529778544\n",
      "          policy_loss: 0.031668497870365776\n",
      "          total_loss: 0.03961907318896717\n",
      "          vf_explained_var: 0.17286460101604462\n",
      "          vf_loss: 0.01289914415942298\n",
      "    num_agent_steps_sampled: 566000\n",
      "    num_agent_steps_trained: 566000\n",
      "    num_steps_sampled: 566000\n",
      "    num_steps_trained: 566000\n",
      "  iterations_since_restore: 566\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.078947368421055\n",
      "    ram_util_percent: 38.913157894736855\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865906498412163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.031164363030516\n",
      "    mean_inference_ms: 1.92791532680969\n",
      "    mean_raw_obs_processing_ms: 2.132936572812266\n",
      "  time_since_restore: 14132.542927980423\n",
      "  time_this_iter_s: 26.80331063270569\n",
      "  time_total_s: 14132.542927980423\n",
      "  timers:\n",
      "    learn_throughput: 1444.427\n",
      "    learn_time_ms: 692.316\n",
      "    load_throughput: 39981.66\n",
      "    load_time_ms: 25.011\n",
      "    sample_throughput: 35.395\n",
      "    sample_time_ms: 28252.654\n",
      "    update_time_ms: 2.377\n",
      "  timestamp: 1635077008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 566000\n",
      "  training_iteration: 566\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   566</td><td style=\"text-align: right;\">         14132.5</td><td style=\"text-align: right;\">566000</td><td style=\"text-align: right;\"> -2.6014</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            260.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 567000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-03-55\n",
      "  done: false\n",
      "  episode_len_mean: 260.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.604099999999988\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1932\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.719347136891146e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4171990089946323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010920371326678138\n",
      "          policy_loss: -0.003667505457997322\n",
      "          total_loss: 0.005387097141808934\n",
      "          vf_explained_var: 0.1579742431640625\n",
      "          vf_loss: 0.01322659600733055\n",
      "    num_agent_steps_sampled: 567000\n",
      "    num_agent_steps_trained: 567000\n",
      "    num_steps_sampled: 567000\n",
      "    num_steps_trained: 567000\n",
      "  iterations_since_restore: 567\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.96410256410256\n",
      "    ram_util_percent: 39.01282051282051\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038658824342112805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.03510603045263\n",
      "    mean_inference_ms: 1.9279167265844341\n",
      "    mean_raw_obs_processing_ms: 2.133058325337351\n",
      "  time_since_restore: 14159.624993801117\n",
      "  time_this_iter_s: 27.08206582069397\n",
      "  time_total_s: 14159.624993801117\n",
      "  timers:\n",
      "    learn_throughput: 1444.407\n",
      "    learn_time_ms: 692.325\n",
      "    load_throughput: 40523.502\n",
      "    load_time_ms: 24.677\n",
      "    sample_throughput: 37.435\n",
      "    sample_time_ms: 26712.813\n",
      "    update_time_ms: 2.381\n",
      "  timestamp: 1635077035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 567000\n",
      "  training_iteration: 567\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   567</td><td style=\"text-align: right;\">         14159.6</td><td style=\"text-align: right;\">567000</td><td style=\"text-align: right;\"> -2.6041</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            260.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-04-22\n",
      "  done: false\n",
      "  episode_len_mean: 257.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.5793999999999886\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1936\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.719347136891146e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.424225910504659\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008751034944123623\n",
      "          policy_loss: -0.020509127775828043\n",
      "          total_loss: -0.012161132941643397\n",
      "          vf_explained_var: 0.22433693706989288\n",
      "          vf_loss: 0.012590250714371603\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.107894736842105\n",
      "    ram_util_percent: 39.076315789473675\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865858583350406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.039432849229232\n",
      "    mean_inference_ms: 1.9279193130655727\n",
      "    mean_raw_obs_processing_ms: 2.1331848392176567\n",
      "  time_since_restore: 14186.58427143097\n",
      "  time_this_iter_s: 26.959277629852295\n",
      "  time_total_s: 14186.58427143097\n",
      "  timers:\n",
      "    learn_throughput: 1442.903\n",
      "    learn_time_ms: 693.047\n",
      "    load_throughput: 41048.953\n",
      "    load_time_ms: 24.361\n",
      "    sample_throughput: 37.265\n",
      "    sample_time_ms: 26834.639\n",
      "    update_time_ms: 2.667\n",
      "  timestamp: 1635077062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 568\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   568</td><td style=\"text-align: right;\">         14186.6</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\"> -2.5794</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            257.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 569000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-04-49\n",
      "  done: false\n",
      "  episode_len_mean: 254.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.5431999999999895\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1940\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.719347136891146e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4325216127766503\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0037709548750576184\n",
      "          policy_loss: -0.11081975085867776\n",
      "          total_loss: -0.10030954082806905\n",
      "          vf_explained_var: 0.2290593832731247\n",
      "          vf_loss: 0.01483542485576537\n",
      "    num_agent_steps_sampled: 569000\n",
      "    num_agent_steps_trained: 569000\n",
      "    num_steps_sampled: 569000\n",
      "    num_steps_trained: 569000\n",
      "  iterations_since_restore: 569\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.47631578947368\n",
      "    ram_util_percent: 39.08684210526315\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038658384968993056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.044386680619066\n",
      "    mean_inference_ms: 1.9279228447586525\n",
      "    mean_raw_obs_processing_ms: 2.133391662817275\n",
      "  time_since_restore: 14213.400490045547\n",
      "  time_this_iter_s: 26.816218614578247\n",
      "  time_total_s: 14213.400490045547\n",
      "  timers:\n",
      "    learn_throughput: 1443.353\n",
      "    learn_time_ms: 692.831\n",
      "    load_throughput: 41361.824\n",
      "    load_time_ms: 24.177\n",
      "    sample_throughput: 37.062\n",
      "    sample_time_ms: 26981.633\n",
      "    update_time_ms: 2.649\n",
      "  timestamp: 1635077089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 569000\n",
      "  training_iteration: 569\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   569</td><td style=\"text-align: right;\">         14213.4</td><td style=\"text-align: right;\">569000</td><td style=\"text-align: right;\"> -2.5432</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            254.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 570000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-05-15\n",
      "  done: false\n",
      "  episode_len_mean: 249.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.4997999999999907\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1945\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.359673568445573e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4671149641275406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005537875417540919\n",
      "          policy_loss: -0.015977869969275263\n",
      "          total_loss: -0.006145745515823364\n",
      "          vf_explained_var: 0.1814754754304886\n",
      "          vf_loss: 0.014503276958647702\n",
      "    num_agent_steps_sampled: 570000\n",
      "    num_agent_steps_trained: 570000\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.0921052631579\n",
      "    ram_util_percent: 39.02105263157895\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865817005134245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.05106427121151\n",
      "    mean_inference_ms: 1.927928678079827\n",
      "    mean_raw_obs_processing_ms: 2.133713406398444\n",
      "  time_since_restore: 14239.790363550186\n",
      "  time_this_iter_s: 26.389873504638672\n",
      "  time_total_s: 14239.790363550186\n",
      "  timers:\n",
      "    learn_throughput: 1444.098\n",
      "    learn_time_ms: 692.474\n",
      "    load_throughput: 41463.801\n",
      "    load_time_ms: 24.117\n",
      "    sample_throughput: 36.958\n",
      "    sample_time_ms: 27057.885\n",
      "    update_time_ms: 2.644\n",
      "  timestamp: 1635077115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 570\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   570</td><td style=\"text-align: right;\">         14239.8</td><td style=\"text-align: right;\">570000</td><td style=\"text-align: right;\"> -2.4998</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            249.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 571000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 249.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.494399999999991\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1949\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.359673568445573e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.40743096470832824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003252639442402404\n",
      "          policy_loss: 0.02852627577053176\n",
      "          total_loss: 0.03645797140068478\n",
      "          vf_explained_var: 0.17524351179599762\n",
      "          vf_loss: 0.0120060076419678\n",
      "    num_agent_steps_sampled: 571000\n",
      "    num_agent_steps_trained: 571000\n",
      "    num_steps_sampled: 571000\n",
      "    num_steps_trained: 571000\n",
      "  iterations_since_restore: 571\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.031578947368416\n",
      "    ram_util_percent: 39.01842105263158\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865802030804939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.0564208967203\n",
      "    mean_inference_ms: 1.9279338118326876\n",
      "    mean_raw_obs_processing_ms: 2.134040091880008\n",
      "  time_since_restore: 14266.24804186821\n",
      "  time_this_iter_s: 26.45767831802368\n",
      "  time_total_s: 14266.24804186821\n",
      "  timers:\n",
      "    learn_throughput: 1443.194\n",
      "    learn_time_ms: 692.908\n",
      "    load_throughput: 41862.037\n",
      "    load_time_ms: 23.888\n",
      "    sample_throughput: 36.836\n",
      "    sample_time_ms: 27147.325\n",
      "    update_time_ms: 2.64\n",
      "  timestamp: 1635077142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 571000\n",
      "  training_iteration: 571\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   571</td><td style=\"text-align: right;\">         14266.2</td><td style=\"text-align: right;\">571000</td><td style=\"text-align: right;\"> -2.4944</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            249.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-06-27\n",
      "  done: false\n",
      "  episode_len_mean: 248.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.486499999999991\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1953\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1798367842227865e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6331920428408517\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01342724084166428\n",
      "          policy_loss: 0.02231261647409863\n",
      "          total_loss: 0.029094879743125704\n",
      "          vf_explained_var: 0.21287128329277039\n",
      "          vf_loss: 0.013114185341530376\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 572\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.8515625\n",
      "    ram_util_percent: 38.910937499999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865785717971651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.06197621954389\n",
      "    mean_inference_ms: 1.927939213891369\n",
      "    mean_raw_obs_processing_ms: 2.1355431517402454\n",
      "  time_since_restore: 14310.963304281235\n",
      "  time_this_iter_s: 44.7152624130249\n",
      "  time_total_s: 14310.963304281235\n",
      "  timers:\n",
      "    learn_throughput: 1441.725\n",
      "    learn_time_ms: 693.613\n",
      "    load_throughput: 41562.247\n",
      "    load_time_ms: 24.06\n",
      "    sample_throughput: 34.35\n",
      "    sample_time_ms: 29112.23\n",
      "    update_time_ms: 2.64\n",
      "  timestamp: 1635077187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 572\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   572</td><td style=\"text-align: right;\">           14311</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\"> -2.4865</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            248.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 573000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-06-54\n",
      "  done: false\n",
      "  episode_len_mean: 247.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.4791999999999907\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1957\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1798367842227865e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.36079031725724536\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003038981533372641\n",
      "          policy_loss: 0.033946930203172895\n",
      "          total_loss: 0.0418365048037635\n",
      "          vf_explained_var: 0.16878364980220795\n",
      "          vf_loss: 0.011497475403464503\n",
      "    num_agent_steps_sampled: 573000\n",
      "    num_agent_steps_trained: 573000\n",
      "    num_steps_sampled: 573000\n",
      "    num_steps_trained: 573000\n",
      "  iterations_since_restore: 573\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.59473684210527\n",
      "    ram_util_percent: 38.986842105263165\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865770253533588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.067625142102166\n",
      "    mean_inference_ms: 1.9279447461557289\n",
      "    mean_raw_obs_processing_ms: 2.136741757155655\n",
      "  time_since_restore: 14337.8133289814\n",
      "  time_this_iter_s: 26.850024700164795\n",
      "  time_total_s: 14337.8133289814\n",
      "  timers:\n",
      "    learn_throughput: 1442.39\n",
      "    learn_time_ms: 693.294\n",
      "    load_throughput: 41716.893\n",
      "    load_time_ms: 23.971\n",
      "    sample_throughput: 34.243\n",
      "    sample_time_ms: 29202.675\n",
      "    update_time_ms: 2.65\n",
      "  timestamp: 1635077214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 573000\n",
      "  training_iteration: 573\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   573</td><td style=\"text-align: right;\">         14337.8</td><td style=\"text-align: right;\">573000</td><td style=\"text-align: right;\"> -2.4792</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            247.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 574000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-07-20\n",
      "  done: false\n",
      "  episode_len_mean: 247.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.475699999999991\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1961\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.899183921113932e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.43841793537139895\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016874546112170683\n",
      "          policy_loss: 0.030855158468087514\n",
      "          total_loss: 0.0378107321759065\n",
      "          vf_explained_var: 0.14101988077163696\n",
      "          vf_loss: 0.01133975059621864\n",
      "    num_agent_steps_sampled: 574000\n",
      "    num_agent_steps_trained: 574000\n",
      "    num_steps_sampled: 574000\n",
      "    num_steps_trained: 574000\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.25263157894738\n",
      "    ram_util_percent: 39.015789473684215\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865753278705209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.07334249091125\n",
      "    mean_inference_ms: 1.9279500519380535\n",
      "    mean_raw_obs_processing_ms: 2.1370212680390277\n",
      "  time_since_restore: 14364.71884894371\n",
      "  time_this_iter_s: 26.90551996231079\n",
      "  time_total_s: 14364.71884894371\n",
      "  timers:\n",
      "    learn_throughput: 1442.352\n",
      "    learn_time_ms: 693.312\n",
      "    load_throughput: 41346.615\n",
      "    load_time_ms: 24.186\n",
      "    sample_throughput: 36.178\n",
      "    sample_time_ms: 27641.13\n",
      "    update_time_ms: 2.648\n",
      "  timestamp: 1635077240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 574000\n",
      "  training_iteration: 574\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   574</td><td style=\"text-align: right;\">         14364.7</td><td style=\"text-align: right;\">574000</td><td style=\"text-align: right;\"> -2.4757</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            247.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 575000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-07-48\n",
      "  done: false\n",
      "  episode_len_mean: 246.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.467599999999991\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1965\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.899183921113932e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.35905614925755397\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0030180746277944574\n",
      "          policy_loss: -0.04611441464059883\n",
      "          total_loss: -0.03716706782579422\n",
      "          vf_explained_var: 0.19752106070518494\n",
      "          vf_loss: 0.012537906598299741\n",
      "    num_agent_steps_sampled: 575000\n",
      "    num_agent_steps_trained: 575000\n",
      "    num_steps_sampled: 575000\n",
      "    num_steps_trained: 575000\n",
      "  iterations_since_restore: 575\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.05128205128205\n",
      "    ram_util_percent: 39.07435897435897\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386573441389023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.079093482843533\n",
      "    mean_inference_ms: 1.9279549706844177\n",
      "    mean_raw_obs_processing_ms: 2.1373069503011015\n",
      "  time_since_restore: 14391.826476812363\n",
      "  time_this_iter_s: 27.107627868652344\n",
      "  time_total_s: 14391.826476812363\n",
      "  timers:\n",
      "    learn_throughput: 1443.135\n",
      "    learn_time_ms: 692.936\n",
      "    load_throughput: 41397.708\n",
      "    load_time_ms: 24.156\n",
      "    sample_throughput: 35.861\n",
      "    sample_time_ms: 27885.511\n",
      "    update_time_ms: 2.659\n",
      "  timestamp: 1635077268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 575000\n",
      "  training_iteration: 575\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   575</td><td style=\"text-align: right;\">         14391.8</td><td style=\"text-align: right;\">575000</td><td style=\"text-align: right;\"> -2.4676</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            246.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-08-15\n",
      "  done: false\n",
      "  episode_len_mean: 245.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.459599999999991\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1970\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.949591960556966e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.38261829846435125\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0016950818388494326\n",
      "          policy_loss: -0.02317191883921623\n",
      "          total_loss: -0.01172744772500462\n",
      "          vf_explained_var: 0.2584337592124939\n",
      "          vf_loss: 0.015270654836462604\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 576\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.066666666666656\n",
      "    ram_util_percent: 39.07948717948717\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865714157637921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.08635440581867\n",
      "    mean_inference_ms: 1.9279616427198483\n",
      "    mean_raw_obs_processing_ms: 2.137697605724834\n",
      "  time_since_restore: 14419.31515288353\n",
      "  time_this_iter_s: 27.488676071166992\n",
      "  time_total_s: 14419.31515288353\n",
      "  timers:\n",
      "    learn_throughput: 1443.105\n",
      "    learn_time_ms: 692.95\n",
      "    load_throughput: 41435.251\n",
      "    load_time_ms: 24.134\n",
      "    sample_throughput: 35.773\n",
      "    sample_time_ms: 27954.018\n",
      "    update_time_ms: 2.69\n",
      "  timestamp: 1635077295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 576\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">         14419.3</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\"> -2.4596</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            245.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 577000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-08-42\n",
      "  done: false\n",
      "  episode_len_mean: 245.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.458099999999991\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1974\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.474795980278483e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4580278714497884\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006345702458136613\n",
      "          policy_loss: 0.023733512229389613\n",
      "          total_loss: 0.030621149929033385\n",
      "          vf_explained_var: 0.2294161468744278\n",
      "          vf_loss: 0.011467918381094932\n",
      "    num_agent_steps_sampled: 577000\n",
      "    num_agent_steps_trained: 577000\n",
      "    num_steps_sampled: 577000\n",
      "    num_steps_trained: 577000\n",
      "  iterations_since_restore: 577\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.46410256410257\n",
      "    ram_util_percent: 39.064102564102555\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865698711276161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.092115736199965\n",
      "    mean_inference_ms: 1.9279665168400992\n",
      "    mean_raw_obs_processing_ms: 2.1380282039295566\n",
      "  time_since_restore: 14446.355643987656\n",
      "  time_this_iter_s: 27.040491104125977\n",
      "  time_total_s: 14446.355643987656\n",
      "  timers:\n",
      "    learn_throughput: 1442.657\n",
      "    learn_time_ms: 693.165\n",
      "    load_throughput: 41195.87\n",
      "    load_time_ms: 24.274\n",
      "    sample_throughput: 35.779\n",
      "    sample_time_ms: 27949.502\n",
      "    update_time_ms: 2.69\n",
      "  timestamp: 1635077322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 577000\n",
      "  training_iteration: 577\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   577</td><td style=\"text-align: right;\">         14446.4</td><td style=\"text-align: right;\">577000</td><td style=\"text-align: right;\"> -2.4581</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            245.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 578000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-09-09\n",
      "  done: false\n",
      "  episode_len_mean: 245.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.4579999999999917\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1978\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.474795980278483e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.435955724451277\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0029458148766622117\n",
      "          policy_loss: 0.04543743228746785\n",
      "          total_loss: 0.05298306941986084\n",
      "          vf_explained_var: 0.13853837549686432\n",
      "          vf_loss: 0.011905193515121936\n",
      "    num_agent_steps_sampled: 578000\n",
      "    num_agent_steps_trained: 578000\n",
      "    num_steps_sampled: 578000\n",
      "    num_steps_trained: 578000\n",
      "  iterations_since_restore: 578\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.15384615384615\n",
      "    ram_util_percent: 39.01794871794872\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865681312865951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.09787359097028\n",
      "    mean_inference_ms: 1.9279689374404774\n",
      "    mean_raw_obs_processing_ms: 2.1383634408791683\n",
      "  time_since_restore: 14473.3892827034\n",
      "  time_this_iter_s: 27.03363871574402\n",
      "  time_total_s: 14473.3892827034\n",
      "  timers:\n",
      "    learn_throughput: 1441.942\n",
      "    learn_time_ms: 693.509\n",
      "    load_throughput: 41231.467\n",
      "    load_time_ms: 24.253\n",
      "    sample_throughput: 35.769\n",
      "    sample_time_ms: 27956.891\n",
      "    update_time_ms: 2.409\n",
      "  timestamp: 1635077349\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 578000\n",
      "  training_iteration: 578\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   578</td><td style=\"text-align: right;\">         14473.4</td><td style=\"text-align: right;\">578000</td><td style=\"text-align: right;\">  -2.458</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">             245.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 579000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-09-55\n",
      "  done: false\n",
      "  episode_len_mean: 245.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.4543999999999913\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1982\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.373979901392415e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6354667915238275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020221545073913758\n",
      "          policy_loss: -0.002016909213529693\n",
      "          total_loss: 0.005701409818397628\n",
      "          vf_explained_var: 0.19814838469028473\n",
      "          vf_loss: 0.014072984457015991\n",
      "    num_agent_steps_sampled: 579000\n",
      "    num_agent_steps_trained: 579000\n",
      "    num_steps_sampled: 579000\n",
      "    num_steps_trained: 579000\n",
      "  iterations_since_restore: 579\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.084375\n",
      "    ram_util_percent: 38.8625\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038656628114003805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.103733231729947\n",
      "    mean_inference_ms: 1.9279694545765051\n",
      "    mean_raw_obs_processing_ms: 2.139856694145522\n",
      "  time_since_restore: 14518.794639348984\n",
      "  time_this_iter_s: 45.405356645584106\n",
      "  time_total_s: 14518.794639348984\n",
      "  timers:\n",
      "    learn_throughput: 1444.242\n",
      "    learn_time_ms: 692.405\n",
      "    load_throughput: 40992.946\n",
      "    load_time_ms: 24.394\n",
      "    sample_throughput: 33.538\n",
      "    sample_time_ms: 29816.755\n",
      "    update_time_ms: 2.421\n",
      "  timestamp: 1635077395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 579000\n",
      "  training_iteration: 579\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   579</td><td style=\"text-align: right;\">         14518.8</td><td style=\"text-align: right;\">579000</td><td style=\"text-align: right;\"> -2.4544</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            245.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-10-21\n",
      "  done: false\n",
      "  episode_len_mean: 245.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.4584999999999915\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1986\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1060969852088628e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8680082824495103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025676223340059798\n",
      "          policy_loss: -0.007304254670937856\n",
      "          total_loss: -0.0007324756019645267\n",
      "          vf_explained_var: 0.11800334602594376\n",
      "          vf_loss: 0.01525186499994662\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_agent_steps_trained: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 580\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.35263157894737\n",
      "    ram_util_percent: 38.82631578947369\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038656420528875475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.109580866827777\n",
      "    mean_inference_ms: 1.927970013317551\n",
      "    mean_raw_obs_processing_ms: 2.141318262145481\n",
      "  time_since_restore: 14544.8432431221\n",
      "  time_this_iter_s: 26.048603773117065\n",
      "  time_total_s: 14544.8432431221\n",
      "  timers:\n",
      "    learn_throughput: 1444.955\n",
      "    learn_time_ms: 692.063\n",
      "    load_throughput: 40987.939\n",
      "    load_time_ms: 24.397\n",
      "    sample_throughput: 33.576\n",
      "    sample_time_ms: 29782.924\n",
      "    update_time_ms: 2.428\n",
      "  timestamp: 1635077421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 580\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   580</td><td style=\"text-align: right;\">         14544.8</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\"> -2.4585</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            245.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 581000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-10-48\n",
      "  done: false\n",
      "  episode_len_mean: 246.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.461499999999991\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1990\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6591454778132942e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4794205652342902\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003867693637155551\n",
      "          policy_loss: -0.033958011782831615\n",
      "          total_loss: -0.02598045567671458\n",
      "          vf_explained_var: 0.22780664265155792\n",
      "          vf_loss: 0.012771761883050204\n",
      "    num_agent_steps_sampled: 581000\n",
      "    num_agent_steps_trained: 581000\n",
      "    num_steps_sampled: 581000\n",
      "    num_steps_trained: 581000\n",
      "  iterations_since_restore: 581\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.65526315789474\n",
      "    ram_util_percent: 39.03947368421051\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865619437044191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.115460018300237\n",
      "    mean_inference_ms: 1.9279705573001695\n",
      "    mean_raw_obs_processing_ms: 2.142476770811985\n",
      "  time_since_restore: 14571.965378046036\n",
      "  time_this_iter_s: 27.122134923934937\n",
      "  time_total_s: 14571.965378046036\n",
      "  timers:\n",
      "    learn_throughput: 1443.948\n",
      "    learn_time_ms: 692.545\n",
      "    load_throughput: 40320.58\n",
      "    load_time_ms: 24.801\n",
      "    sample_throughput: 33.503\n",
      "    sample_time_ms: 29848.387\n",
      "    update_time_ms: 2.513\n",
      "  timestamp: 1635077448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 581000\n",
      "  training_iteration: 581\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   581</td><td style=\"text-align: right;\">           14572</td><td style=\"text-align: right;\">581000</td><td style=\"text-align: right;\"> -2.4615</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            246.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 582000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-11-15\n",
      "  done: false\n",
      "  episode_len_mean: 245.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.451799999999992\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1995\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.295727389066471e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5339906026919683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028590124405157902\n",
      "          policy_loss: -0.017584571987390517\n",
      "          total_loss: -0.007254841840929455\n",
      "          vf_explained_var: 0.19832825660705566\n",
      "          vf_loss: 0.01566963577643037\n",
      "    num_agent_steps_sampled: 582000\n",
      "    num_agent_steps_trained: 582000\n",
      "    num_steps_sampled: 582000\n",
      "    num_steps_trained: 582000\n",
      "  iterations_since_restore: 582\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.97692307692308\n",
      "    ram_util_percent: 39.08205128205127\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038655899193543244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.12285860239132\n",
      "    mean_inference_ms: 1.9279712631651467\n",
      "    mean_raw_obs_processing_ms: 2.1428109209769706\n",
      "  time_since_restore: 14598.770854711533\n",
      "  time_this_iter_s: 26.805476665496826\n",
      "  time_total_s: 14598.770854711533\n",
      "  timers:\n",
      "    learn_throughput: 1445.261\n",
      "    learn_time_ms: 691.916\n",
      "    load_throughput: 40777.973\n",
      "    load_time_ms: 24.523\n",
      "    sample_throughput: 35.64\n",
      "    sample_time_ms: 28058.268\n",
      "    update_time_ms: 2.548\n",
      "  timestamp: 1635077475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 582000\n",
      "  training_iteration: 582\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   582</td><td style=\"text-align: right;\">         14598.8</td><td style=\"text-align: right;\">582000</td><td style=\"text-align: right;\"> -2.4518</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            245.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 583000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-11-42\n",
      "  done: false\n",
      "  episode_len_mean: 244.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.443599999999992\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1999\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2443591083599698e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.39046325782934826\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003905647883482288\n",
      "          policy_loss: 0.029347772316800223\n",
      "          total_loss: 0.038024432957172394\n",
      "          vf_explained_var: 0.17193497717380524\n",
      "          vf_loss: 0.012581297051575449\n",
      "    num_agent_steps_sampled: 583000\n",
      "    num_agent_steps_trained: 583000\n",
      "    num_steps_sampled: 583000\n",
      "    num_steps_trained: 583000\n",
      "  iterations_since_restore: 583\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18684210526315\n",
      "    ram_util_percent: 39.07368421052631\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865567210298694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.12882741021322\n",
      "    mean_inference_ms: 1.9279720004101395\n",
      "    mean_raw_obs_processing_ms: 2.1430990547776774\n",
      "  time_since_restore: 14625.600346565247\n",
      "  time_this_iter_s: 26.82949185371399\n",
      "  time_total_s: 14625.600346565247\n",
      "  timers:\n",
      "    learn_throughput: 1446.334\n",
      "    learn_time_ms: 691.403\n",
      "    load_throughput: 40906.869\n",
      "    load_time_ms: 24.446\n",
      "    sample_throughput: 35.642\n",
      "    sample_time_ms: 28056.82\n",
      "    update_time_ms: 2.537\n",
      "  timestamp: 1635077502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 583000\n",
      "  training_iteration: 583\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   583</td><td style=\"text-align: right;\">         14625.6</td><td style=\"text-align: right;\">583000</td><td style=\"text-align: right;\"> -2.4436</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            244.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-12-09\n",
      "  done: false\n",
      "  episode_len_mean: 243.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.435899999999992\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2003\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.221795541799849e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4145759807692634\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01064561889641994\n",
      "          policy_loss: 0.049137947335839274\n",
      "          total_loss: 0.05596739041308562\n",
      "          vf_explained_var: 0.13269931077957153\n",
      "          vf_loss: 0.010975199363504847\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 584\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.16666666666667\n",
      "    ram_util_percent: 39.071794871794864\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865545007457291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.134872213737854\n",
      "    mean_inference_ms: 1.9279727216508138\n",
      "    mean_raw_obs_processing_ms: 2.1434274806486346\n",
      "  time_since_restore: 14653.136634588242\n",
      "  time_this_iter_s: 27.536288022994995\n",
      "  time_total_s: 14653.136634588242\n",
      "  timers:\n",
      "    learn_throughput: 1449.319\n",
      "    learn_time_ms: 689.979\n",
      "    load_throughput: 40936.174\n",
      "    load_time_ms: 24.428\n",
      "    sample_throughput: 35.56\n",
      "    sample_time_ms: 28121.284\n",
      "    update_time_ms: 2.551\n",
      "  timestamp: 1635077529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 584\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   584</td><td style=\"text-align: right;\">         14653.1</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\"> -2.4359</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            243.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 585000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-12-37\n",
      "  done: false\n",
      "  episode_len_mean: 243.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.430999999999992\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2007\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.221795541799849e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5075914690891902\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002817897657348962\n",
      "          policy_loss: 0.007773360030518638\n",
      "          total_loss: 0.017610140558746126\n",
      "          vf_explained_var: 0.06406576186418533\n",
      "          vf_loss: 0.014912694527043236\n",
      "    num_agent_steps_sampled: 585000\n",
      "    num_agent_steps_trained: 585000\n",
      "    num_steps_sampled: 585000\n",
      "    num_steps_trained: 585000\n",
      "  iterations_since_restore: 585\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.2175\n",
      "    ram_util_percent: 39.095\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038655215213837246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.141026065314303\n",
      "    mean_inference_ms: 1.927973614388983\n",
      "    mean_raw_obs_processing_ms: 2.1437606970327128\n",
      "  time_since_restore: 14680.571690559387\n",
      "  time_this_iter_s: 27.43505597114563\n",
      "  time_total_s: 14680.571690559387\n",
      "  timers:\n",
      "    learn_throughput: 1449.735\n",
      "    learn_time_ms: 689.781\n",
      "    load_throughput: 40823.417\n",
      "    load_time_ms: 24.496\n",
      "    sample_throughput: 35.519\n",
      "    sample_time_ms: 28154.158\n",
      "    update_time_ms: 2.54\n",
      "  timestamp: 1635077557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 585000\n",
      "  training_iteration: 585\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   585</td><td style=\"text-align: right;\">         14680.6</td><td style=\"text-align: right;\">585000</td><td style=\"text-align: right;\">  -2.431</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">             243.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 586000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 242.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.4211999999999922\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2011\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1108977708999245e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6544090078936683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0052526710313276544\n",
      "          policy_loss: -0.03858882329530186\n",
      "          total_loss: -0.030771852739983136\n",
      "          vf_explained_var: 0.15760093927383423\n",
      "          vf_loss: 0.01436106013134122\n",
      "    num_agent_steps_sampled: 586000\n",
      "    num_agent_steps_trained: 586000\n",
      "    num_steps_sampled: 586000\n",
      "    num_steps_trained: 586000\n",
      "  iterations_since_restore: 586\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.823437500000004\n",
      "    ram_util_percent: 38.9109375\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865497603331085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.14737738697868\n",
      "    mean_inference_ms: 1.9279749092743583\n",
      "    mean_raw_obs_processing_ms: 2.1452464327369514\n",
      "  time_since_restore: 14725.716064453125\n",
      "  time_this_iter_s: 45.14437389373779\n",
      "  time_total_s: 14725.716064453125\n",
      "  timers:\n",
      "    learn_throughput: 1450.121\n",
      "    learn_time_ms: 689.598\n",
      "    load_throughput: 40904.037\n",
      "    load_time_ms: 24.447\n",
      "    sample_throughput: 33.422\n",
      "    sample_time_ms: 29920.018\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1635077602\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 586000\n",
      "  training_iteration: 586\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   586</td><td style=\"text-align: right;\">         14725.7</td><td style=\"text-align: right;\">586000</td><td style=\"text-align: right;\"> -2.4212</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            242.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 587000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-13-47\n",
      "  done: false\n",
      "  episode_len_mean: 242.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.4211999999999922\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2015\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1108977708999245e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7274345917834176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011137245298146543\n",
      "          policy_loss: -0.022230663647254307\n",
      "          total_loss: -0.014860584007369147\n",
      "          vf_explained_var: 0.12628117203712463\n",
      "          vf_loss: 0.014644428715109825\n",
      "    num_agent_steps_sampled: 587000\n",
      "    num_agent_steps_trained: 587000\n",
      "    num_steps_sampled: 587000\n",
      "    num_steps_trained: 587000\n",
      "  iterations_since_restore: 587\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.36216216216217\n",
      "    ram_util_percent: 38.98918918918919\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038654732561486674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.15371442491162\n",
      "    mean_inference_ms: 1.9279759724652294\n",
      "    mean_raw_obs_processing_ms: 2.1467349478833806\n",
      "  time_since_restore: 14751.48739862442\n",
      "  time_this_iter_s: 25.771334171295166\n",
      "  time_total_s: 14751.48739862442\n",
      "  timers:\n",
      "    learn_throughput: 1452.599\n",
      "    learn_time_ms: 688.421\n",
      "    load_throughput: 40929.343\n",
      "    load_time_ms: 24.432\n",
      "    sample_throughput: 33.563\n",
      "    sample_time_ms: 29794.32\n",
      "    update_time_ms: 2.498\n",
      "  timestamp: 1635077627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 587000\n",
      "  training_iteration: 587\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   587</td><td style=\"text-align: right;\">         14751.5</td><td style=\"text-align: right;\">587000</td><td style=\"text-align: right;\"> -2.4212</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            242.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-14-14\n",
      "  done: false\n",
      "  episode_len_mean: 241.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.4187999999999925\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2020\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1108977708999245e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4808391359117296\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011148158426731274\n",
      "          policy_loss: -0.026873487399684058\n",
      "          total_loss: -0.01650820407602522\n",
      "          vf_explained_var: 0.29230797290802\n",
      "          vf_loss: 0.015173672512173653\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 588\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.197368421052644\n",
      "    ram_util_percent: 38.984210526315785\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865442034564942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.1617175953839\n",
      "    mean_inference_ms: 1.9279774820695263\n",
      "    mean_raw_obs_processing_ms: 2.147401608880136\n",
      "  time_since_restore: 14778.465967178345\n",
      "  time_this_iter_s: 26.97856855392456\n",
      "  time_total_s: 14778.465967178345\n",
      "  timers:\n",
      "    learn_throughput: 1452.362\n",
      "    learn_time_ms: 688.534\n",
      "    load_throughput: 40732.036\n",
      "    load_time_ms: 24.551\n",
      "    sample_throughput: 33.57\n",
      "    sample_time_ms: 29788.581\n",
      "    update_time_ms: 2.506\n",
      "  timestamp: 1635077654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 588\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   588</td><td style=\"text-align: right;\">         14778.5</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\"> -2.4188</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            241.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 589000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-14-41\n",
      "  done: false\n",
      "  episode_len_mean: 240.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.405499999999993\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2024\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1108977708999245e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6147915449407365\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.09692482436575713\n",
      "          policy_loss: -0.0002727766417794757\n",
      "          total_loss: 0.006746742419070668\n",
      "          vf_explained_var: 0.3742962181568146\n",
      "          vf_loss: 0.013167433181984557\n",
      "    num_agent_steps_sampled: 589000\n",
      "    num_agent_steps_trained: 589000\n",
      "    num_steps_sampled: 589000\n",
      "    num_steps_trained: 589000\n",
      "  iterations_since_restore: 589\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.17435897435898\n",
      "    ram_util_percent: 39.06153846153845\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038654159756260784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.168223720162253\n",
      "    mean_inference_ms: 1.9279785300003118\n",
      "    mean_raw_obs_processing_ms: 2.147725730953386\n",
      "  time_since_restore: 14805.401635885239\n",
      "  time_this_iter_s: 26.93566870689392\n",
      "  time_total_s: 14805.401635885239\n",
      "  timers:\n",
      "    learn_throughput: 1450.25\n",
      "    learn_time_ms: 689.536\n",
      "    load_throughput: 41007.054\n",
      "    load_time_ms: 24.386\n",
      "    sample_throughput: 35.79\n",
      "    sample_time_ms: 27940.666\n",
      "    update_time_ms: 2.605\n",
      "  timestamp: 1635077681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 589000\n",
      "  training_iteration: 589\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   589</td><td style=\"text-align: right;\">         14805.4</td><td style=\"text-align: right;\">589000</td><td style=\"text-align: right;\"> -2.4055</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            240.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 590000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-15-04\n",
      "  done: false\n",
      "  episode_len_mean: 242.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.425099999999992\n",
      "  episode_reward_min: -3.7299999999999645\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2027\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.666346656349889e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7969896899329292\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008246259110973694\n",
      "          policy_loss: 0.06567198269897037\n",
      "          total_loss: 0.06807366613712576\n",
      "          vf_explained_var: -0.0062284390442073345\n",
      "          vf_loss: 0.010371579924443116\n",
      "    num_agent_steps_sampled: 590000\n",
      "    num_agent_steps_trained: 590000\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "  iterations_since_restore: 590\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.25483870967742\n",
      "    ram_util_percent: 39.07419354838709\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038653955687623524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.17285045637577\n",
      "    mean_inference_ms: 1.9279786055150039\n",
      "    mean_raw_obs_processing_ms: 2.1479699286368383\n",
      "  time_since_restore: 14827.483496189117\n",
      "  time_this_iter_s: 22.081860303878784\n",
      "  time_total_s: 14827.483496189117\n",
      "  timers:\n",
      "    learn_throughput: 1449.934\n",
      "    learn_time_ms: 689.687\n",
      "    load_throughput: 40994.108\n",
      "    load_time_ms: 24.394\n",
      "    sample_throughput: 36.306\n",
      "    sample_time_ms: 27543.862\n",
      "    update_time_ms: 2.622\n",
      "  timestamp: 1635077704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 590\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   590</td><td style=\"text-align: right;\">         14827.5</td><td style=\"text-align: right;\">590000</td><td style=\"text-align: right;\"> -2.4251</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.73</td><td style=\"text-align: right;\">            242.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 591000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-15-30\n",
      "  done: false\n",
      "  episode_len_mean: 242.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.4266999999999914\n",
      "  episode_reward_min: -3.7299999999999645\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2031\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.666346656349889e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7334346466594273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017331918791820694\n",
      "          policy_loss: 0.021758136401573817\n",
      "          total_loss: 0.02753267354435391\n",
      "          vf_explained_var: 0.18052837252616882\n",
      "          vf_loss: 0.013108884574224551\n",
      "    num_agent_steps_sampled: 591000\n",
      "    num_agent_steps_trained: 591000\n",
      "    num_steps_sampled: 591000\n",
      "    num_steps_trained: 591000\n",
      "  iterations_since_restore: 591\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.08947368421053\n",
      "    ram_util_percent: 39.078947368421055\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038653677957847486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.179014348557175\n",
      "    mean_inference_ms: 1.92797881403521\n",
      "    mean_raw_obs_processing_ms: 2.1482670498825014\n",
      "  time_since_restore: 14853.773274898529\n",
      "  time_this_iter_s: 26.28977870941162\n",
      "  time_total_s: 14853.773274898529\n",
      "  timers:\n",
      "    learn_throughput: 1450.095\n",
      "    learn_time_ms: 689.61\n",
      "    load_throughput: 40948.363\n",
      "    load_time_ms: 24.421\n",
      "    sample_throughput: 36.416\n",
      "    sample_time_ms: 27460.78\n",
      "    update_time_ms: 2.535\n",
      "  timestamp: 1635077730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 591000\n",
      "  training_iteration: 591\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   591</td><td style=\"text-align: right;\">         14853.8</td><td style=\"text-align: right;\">591000</td><td style=\"text-align: right;\"> -2.4267</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.73</td><td style=\"text-align: right;\">            242.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-15-55\n",
      "  done: false\n",
      "  episode_len_mean: 243.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.435499999999992\n",
      "  episode_reward_min: -3.7299999999999645\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2035\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.666346656349889e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8088819762070973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02151621143995399\n",
      "          policy_loss: 0.006058110131157769\n",
      "          total_loss: 0.012688508133093516\n",
      "          vf_explained_var: 0.20114588737487793\n",
      "          vf_loss: 0.014719215790844627\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 592\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.12571428571429\n",
      "    ram_util_percent: 39.088571428571434\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865340375121691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.185007731059436\n",
      "    mean_inference_ms: 1.9279786998602066\n",
      "    mean_raw_obs_processing_ms: 2.148570764153588\n",
      "  time_since_restore: 14878.575224161148\n",
      "  time_this_iter_s: 24.80194926261902\n",
      "  time_total_s: 14878.575224161148\n",
      "  timers:\n",
      "    learn_throughput: 1450.493\n",
      "    learn_time_ms: 689.421\n",
      "    load_throughput: 40903.717\n",
      "    load_time_ms: 24.448\n",
      "    sample_throughput: 36.683\n",
      "    sample_time_ms: 27260.582\n",
      "    update_time_ms: 2.536\n",
      "  timestamp: 1635077755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 592\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   592</td><td style=\"text-align: right;\">         14878.6</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\"> -2.4355</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.73</td><td style=\"text-align: right;\">            243.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 593000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-16-19\n",
      "  done: false\n",
      "  episode_len_mean: 244.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.4493999999999914\n",
      "  episode_reward_min: -3.7299999999999645\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2038\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.999519984524835e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0873972747060987\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027854351416620994\n",
      "          policy_loss: -0.10608340998490652\n",
      "          total_loss: -0.10269075781106948\n",
      "          vf_explained_var: 0.26793497800827026\n",
      "          vf_loss: 0.014266625646915701\n",
      "    num_agent_steps_sampled: 593000\n",
      "    num_agent_steps_trained: 593000\n",
      "    num_steps_sampled: 593000\n",
      "    num_steps_trained: 593000\n",
      "  iterations_since_restore: 593\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.88\n",
      "    ram_util_percent: 39.03142857142858\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865319940935077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.189387095768904\n",
      "    mean_inference_ms: 1.9279787607622498\n",
      "    mean_raw_obs_processing_ms: 2.148792576548913\n",
      "  time_since_restore: 14903.096838474274\n",
      "  time_this_iter_s: 24.52161431312561\n",
      "  time_total_s: 14903.096838474274\n",
      "  timers:\n",
      "    learn_throughput: 1449.962\n",
      "    learn_time_ms: 689.673\n",
      "    load_throughput: 40649.925\n",
      "    load_time_ms: 24.6\n",
      "    sample_throughput: 36.997\n",
      "    sample_time_ms: 27029.402\n",
      "    update_time_ms: 2.536\n",
      "  timestamp: 1635077779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 593000\n",
      "  training_iteration: 593\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   593</td><td style=\"text-align: right;\">         14903.1</td><td style=\"text-align: right;\">593000</td><td style=\"text-align: right;\"> -2.4494</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.73</td><td style=\"text-align: right;\">            244.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 594000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-16-58\n",
      "  done: false\n",
      "  episode_len_mean: 247.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.471699999999991\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2042\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.049927997678725e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1050734096103245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020135576572257858\n",
      "          policy_loss: 0.013966862029499478\n",
      "          total_loss: 0.01728388261463907\n",
      "          vf_explained_var: -0.19213280081748962\n",
      "          vf_loss: 0.014367751286934233\n",
      "    num_agent_steps_sampled: 594000\n",
      "    num_agent_steps_trained: 594000\n",
      "    num_steps_sampled: 594000\n",
      "    num_steps_trained: 594000\n",
      "  iterations_since_restore: 594\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.471428571428575\n",
      "    ram_util_percent: 38.91785714285714\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386529148560341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.194963549862866\n",
      "    mean_inference_ms: 1.9279788624459866\n",
      "    mean_raw_obs_processing_ms: 2.1501997665535386\n",
      "  time_since_restore: 14942.21446943283\n",
      "  time_this_iter_s: 39.11763095855713\n",
      "  time_total_s: 14942.21446943283\n",
      "  timers:\n",
      "    learn_throughput: 1447.012\n",
      "    learn_time_ms: 691.079\n",
      "    load_throughput: 40611.235\n",
      "    load_time_ms: 24.624\n",
      "    sample_throughput: 35.478\n",
      "    sample_time_ms: 28186.15\n",
      "    update_time_ms: 2.528\n",
      "  timestamp: 1635077818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 594000\n",
      "  training_iteration: 594\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   594</td><td style=\"text-align: right;\">         14942.2</td><td style=\"text-align: right;\">594000</td><td style=\"text-align: right;\"> -2.4717</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            247.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 595000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-17-24\n",
      "  done: false\n",
      "  episode_len_mean: 247.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.476999999999991\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2045\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5748919965180868e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.976018018854989\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006590675061787168\n",
      "          policy_loss: -0.11401448340879547\n",
      "          total_loss: -0.11152775651878781\n",
      "          vf_explained_var: 0.3369612693786621\n",
      "          vf_loss: 0.01224690725406011\n",
      "    num_agent_steps_sampled: 595000\n",
      "    num_agent_steps_trained: 595000\n",
      "    num_steps_sampled: 595000\n",
      "    num_steps_trained: 595000\n",
      "  iterations_since_restore: 595\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.55135135135135\n",
      "    ram_util_percent: 38.881081081081085\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865273009698977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.199203298414442\n",
      "    mean_inference_ms: 1.9279795796352133\n",
      "    mean_raw_obs_processing_ms: 2.1512057109667784\n",
      "  time_since_restore: 14968.268533706665\n",
      "  time_this_iter_s: 26.05406427383423\n",
      "  time_total_s: 14968.268533706665\n",
      "  timers:\n",
      "    learn_throughput: 1446.609\n",
      "    learn_time_ms: 691.272\n",
      "    load_throughput: 41128.047\n",
      "    load_time_ms: 24.314\n",
      "    sample_throughput: 35.653\n",
      "    sample_time_ms: 28048.182\n",
      "    update_time_ms: 2.519\n",
      "  timestamp: 1635077844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 595000\n",
      "  training_iteration: 595\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   595</td><td style=\"text-align: right;\">         14968.3</td><td style=\"text-align: right;\">595000</td><td style=\"text-align: right;\">  -2.477</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">             247.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 248.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.482599999999991\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2049\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5748919965180868e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8913013577461243\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006149834362446743\n",
      "          policy_loss: -0.050387876563602024\n",
      "          total_loss: -0.048295865125126305\n",
      "          vf_explained_var: 0.3095908463001251\n",
      "          vf_loss: 0.011005026841950085\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_agent_steps_trained: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 596\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.136111111111106\n",
      "    ram_util_percent: 39.002777777777766\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865244965558177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.204756538694117\n",
      "    mean_inference_ms: 1.9279798573598286\n",
      "    mean_raw_obs_processing_ms: 2.1525503429811783\n",
      "  time_since_restore: 14993.52391409874\n",
      "  time_this_iter_s: 25.255380392074585\n",
      "  time_total_s: 14993.52391409874\n",
      "  timers:\n",
      "    learn_throughput: 1447.178\n",
      "    learn_time_ms: 691.0\n",
      "    load_throughput: 41066.154\n",
      "    load_time_ms: 24.351\n",
      "    sample_throughput: 38.374\n",
      "    sample_time_ms: 26059.471\n",
      "    update_time_ms: 2.55\n",
      "  timestamp: 1635077870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 596\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   596</td><td style=\"text-align: right;\">         14993.5</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\"> -2.4826</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            248.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 597000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-18-15\n",
      "  done: false\n",
      "  episode_len_mean: 248.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.4879999999999907\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2053\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5748919965180868e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9458019693692525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005170070201267802\n",
      "          policy_loss: -0.021342499885294172\n",
      "          total_loss: -0.018001502421167163\n",
      "          vf_explained_var: 0.2572978734970093\n",
      "          vf_loss: 0.01279901655183898\n",
      "    num_agent_steps_sampled: 597000\n",
      "    num_agent_steps_trained: 597000\n",
      "    num_steps_sampled: 597000\n",
      "    num_steps_trained: 597000\n",
      "  iterations_since_restore: 597\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.952777777777776\n",
      "    ram_util_percent: 39.099999999999994\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865216957819693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.210105548541932\n",
      "    mean_inference_ms: 1.9279795991232676\n",
      "    mean_raw_obs_processing_ms: 2.1527267636988636\n",
      "  time_since_restore: 15018.770138025284\n",
      "  time_this_iter_s: 25.24622392654419\n",
      "  time_total_s: 15018.770138025284\n",
      "  timers:\n",
      "    learn_throughput: 1446.769\n",
      "    learn_time_ms: 691.195\n",
      "    load_throughput: 40931.14\n",
      "    load_time_ms: 24.431\n",
      "    sample_throughput: 38.452\n",
      "    sample_time_ms: 26006.674\n",
      "    update_time_ms: 2.558\n",
      "  timestamp: 1635077895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 597000\n",
      "  training_iteration: 597\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   597</td><td style=\"text-align: right;\">         15018.8</td><td style=\"text-align: right;\">597000</td><td style=\"text-align: right;\">  -2.488</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">             248.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 598000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-18-42\n",
      "  done: false\n",
      "  episode_len_mean: 249.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.493399999999991\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2057\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5748919965180868e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8783662352297041\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.032404906299176\n",
      "          policy_loss: -0.02538213311798043\n",
      "          total_loss: -0.021828343719244005\n",
      "          vf_explained_var: 0.26639971137046814\n",
      "          vf_loss: 0.01233745204905669\n",
      "    num_agent_steps_sampled: 598000\n",
      "    num_agent_steps_trained: 598000\n",
      "    num_steps_sampled: 598000\n",
      "    num_steps_trained: 598000\n",
      "  iterations_since_restore: 598\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.473684210526315\n",
      "    ram_util_percent: 39.11052631578947\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038651894013813894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.21541371081327\n",
      "    mean_inference_ms: 1.9279792299959337\n",
      "    mean_raw_obs_processing_ms: 2.1529099454846907\n",
      "  time_since_restore: 15045.32377076149\n",
      "  time_this_iter_s: 26.553632736206055\n",
      "  time_total_s: 15045.32377076149\n",
      "  timers:\n",
      "    learn_throughput: 1447.88\n",
      "    learn_time_ms: 690.665\n",
      "    load_throughput: 41199.998\n",
      "    load_time_ms: 24.272\n",
      "    sample_throughput: 38.514\n",
      "    sample_time_ms: 25964.857\n",
      "    update_time_ms: 2.567\n",
      "  timestamp: 1635077922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 598000\n",
      "  training_iteration: 598\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   598</td><td style=\"text-align: right;\">         15045.3</td><td style=\"text-align: right;\">598000</td><td style=\"text-align: right;\"> -2.4934</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            249.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 599000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-19-04\n",
      "  done: false\n",
      "  episode_len_mean: 251.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.5111999999999903\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2061\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3623379947771303e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0386796156565348\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0076133524140508035\n",
      "          policy_loss: 0.0012565154996183184\n",
      "          total_loss: 0.003702343710594707\n",
      "          vf_explained_var: 0.30372852087020874\n",
      "          vf_loss: 0.012832623088939323\n",
      "    num_agent_steps_sampled: 599000\n",
      "    num_agent_steps_trained: 599000\n",
      "    num_steps_sampled: 599000\n",
      "    num_steps_trained: 599000\n",
      "  iterations_since_restore: 599\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.7875\n",
      "    ram_util_percent: 39.09375\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386516091065301\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.220403686962864\n",
      "    mean_inference_ms: 1.9279784284574149\n",
      "    mean_raw_obs_processing_ms: 2.153099113488472\n",
      "  time_since_restore: 15067.773508548737\n",
      "  time_this_iter_s: 22.449737787246704\n",
      "  time_total_s: 15067.773508548737\n",
      "  timers:\n",
      "    learn_throughput: 1446.346\n",
      "    learn_time_ms: 691.398\n",
      "    load_throughput: 40746.756\n",
      "    load_time_ms: 24.542\n",
      "    sample_throughput: 39.192\n",
      "    sample_time_ms: 25515.387\n",
      "    update_time_ms: 2.464\n",
      "  timestamp: 1635077944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599000\n",
      "  training_iteration: 599\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         15067.8</td><td style=\"text-align: right;\">599000</td><td style=\"text-align: right;\"> -2.5112</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            251.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-19-25\n",
      "  done: false\n",
      "  episode_len_mean: 253.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.53419999999999\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2064\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3623379947771303e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1700080076853434\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023676391427723806\n",
      "          policy_loss: 0.05302559609214465\n",
      "          total_loss: 0.05232046105795436\n",
      "          vf_explained_var: 0.015708820894360542\n",
      "          vf_loss: 0.010994945381147167\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 600\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.045161290322575\n",
      "    ram_util_percent: 39.08709677419355\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865140454120606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.223845470322487\n",
      "    mean_inference_ms: 1.9279774714538491\n",
      "    mean_raw_obs_processing_ms: 2.153242529700045\n",
      "  time_since_restore: 15089.110456943512\n",
      "  time_this_iter_s: 21.33694839477539\n",
      "  time_total_s: 15089.110456943512\n",
      "  timers:\n",
      "    learn_throughput: 1446.909\n",
      "    learn_time_ms: 691.129\n",
      "    load_throughput: 40758.041\n",
      "    load_time_ms: 24.535\n",
      "    sample_throughput: 39.306\n",
      "    sample_time_ms: 25441.16\n",
      "    update_time_ms: 2.455\n",
      "  timestamp: 1635077965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 600\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   600</td><td style=\"text-align: right;\">         15089.1</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\"> -2.5342</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            253.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 601000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-19-47\n",
      "  done: false\n",
      "  episode_len_mean: 255.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.5594999999999892\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2067\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.543506992165697e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.162700852420595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024664668558733005\n",
      "          policy_loss: 0.04884899664256308\n",
      "          total_loss: 0.0486115259428819\n",
      "          vf_explained_var: -0.28047946095466614\n",
      "          vf_loss: 0.01138954082206409\n",
      "    num_agent_steps_sampled: 601000\n",
      "    num_agent_steps_trained: 601000\n",
      "    num_steps_sampled: 601000\n",
      "    num_steps_trained: 601000\n",
      "  iterations_since_restore: 601\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.77419354838711\n",
      "    ram_util_percent: 39.05483870967742\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865118554791613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.22705180214292\n",
      "    mean_inference_ms: 1.927975998643373\n",
      "    mean_raw_obs_processing_ms: 2.1533544251586094\n",
      "  time_since_restore: 15110.621814489365\n",
      "  time_this_iter_s: 21.51135754585266\n",
      "  time_total_s: 15110.621814489365\n",
      "  timers:\n",
      "    learn_throughput: 1445.983\n",
      "    learn_time_ms: 691.571\n",
      "    load_throughput: 40967.561\n",
      "    load_time_ms: 24.41\n",
      "    sample_throughput: 40.059\n",
      "    sample_time_ms: 24963.032\n",
      "    update_time_ms: 2.44\n",
      "  timestamp: 1635077987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 601000\n",
      "  training_iteration: 601\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   601</td><td style=\"text-align: right;\">         15110.6</td><td style=\"text-align: right;\">601000</td><td style=\"text-align: right;\"> -2.5595</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            255.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 602000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-20-29\n",
      "  done: false\n",
      "  episode_len_mean: 257.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.574999999999989\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2071\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.315260488248543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2577932476997375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012469375950242225\n",
      "          policy_loss: 0.02063959530658192\n",
      "          total_loss: 0.022496802856524785\n",
      "          vf_explained_var: 0.12146449834108353\n",
      "          vf_loss: 0.014435141512917148\n",
      "    num_agent_steps_sampled: 602000\n",
      "    num_agent_steps_trained: 602000\n",
      "    num_steps_sampled: 602000\n",
      "    num_steps_trained: 602000\n",
      "  iterations_since_restore: 602\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.626666666666665\n",
      "    ram_util_percent: 38.97833333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865087227646394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.231271739568037\n",
      "    mean_inference_ms: 1.9279735831508718\n",
      "    mean_raw_obs_processing_ms: 2.154585402683221\n",
      "  time_since_restore: 15152.589929103851\n",
      "  time_this_iter_s: 41.968114614486694\n",
      "  time_total_s: 15152.589929103851\n",
      "  timers:\n",
      "    learn_throughput: 1443.868\n",
      "    learn_time_ms: 692.584\n",
      "    load_throughput: 40792.647\n",
      "    load_time_ms: 24.514\n",
      "    sample_throughput: 37.483\n",
      "    sample_time_ms: 26678.589\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1635078029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 602000\n",
      "  training_iteration: 602\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   602</td><td style=\"text-align: right;\">         15152.6</td><td style=\"text-align: right;\">602000</td><td style=\"text-align: right;\">  -2.575</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">             257.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 603000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-20-53\n",
      "  done: false\n",
      "  episode_len_mean: 258.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.588599999999989\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2074\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.315260488248543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3943757613499959\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014816432138289\n",
      "          policy_loss: -0.015306584040323893\n",
      "          total_loss: -0.018399564425150554\n",
      "          vf_explained_var: 0.02991805598139763\n",
      "          vf_loss: 0.010850779351312668\n",
      "    num_agent_steps_sampled: 603000\n",
      "    num_agent_steps_trained: 603000\n",
      "    num_steps_sampled: 603000\n",
      "    num_steps_trained: 603000\n",
      "  iterations_since_restore: 603\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.47647058823529\n",
      "    ram_util_percent: 39.029411764705884\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865062866662854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.234339121490592\n",
      "    mean_inference_ms: 1.9279718199728648\n",
      "    mean_raw_obs_processing_ms: 2.155485380993864\n",
      "  time_since_restore: 15176.903705596924\n",
      "  time_this_iter_s: 24.31377649307251\n",
      "  time_total_s: 15176.903705596924\n",
      "  timers:\n",
      "    learn_throughput: 1445.21\n",
      "    learn_time_ms: 691.941\n",
      "    load_throughput: 40521.779\n",
      "    load_time_ms: 24.678\n",
      "    sample_throughput: 37.513\n",
      "    sample_time_ms: 26657.32\n",
      "    update_time_ms: 3.359\n",
      "  timestamp: 1635078053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 603000\n",
      "  training_iteration: 603\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   603</td><td style=\"text-align: right;\">         15176.9</td><td style=\"text-align: right;\">603000</td><td style=\"text-align: right;\"> -2.5886</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            258.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-21-17\n",
      "  done: false\n",
      "  episode_len_mean: 259.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0899999999999994\n",
      "  episode_reward_mean: -2.5995999999999877\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2078\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.315260488248543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3320763508478801\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010510307446632419\n",
      "          policy_loss: 0.02074172335366408\n",
      "          total_loss: 0.022200654529862935\n",
      "          vf_explained_var: 0.06831227242946625\n",
      "          vf_loss: 0.014779693633317947\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_agent_steps_trained: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 604\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.86857142857143\n",
      "    ram_util_percent: 39.111428571428576\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865031950538736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.238224940654618\n",
      "    mean_inference_ms: 1.9279696555699213\n",
      "    mean_raw_obs_processing_ms: 2.1566888347228685\n",
      "  time_since_restore: 15201.134078025818\n",
      "  time_this_iter_s: 24.230372428894043\n",
      "  time_total_s: 15201.134078025818\n",
      "  timers:\n",
      "    learn_throughput: 1446.433\n",
      "    learn_time_ms: 691.356\n",
      "    load_throughput: 40713.769\n",
      "    load_time_ms: 24.562\n",
      "    sample_throughput: 39.731\n",
      "    sample_time_ms: 25169.231\n",
      "    update_time_ms: 3.389\n",
      "  timestamp: 1635078077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 604\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   604</td><td style=\"text-align: right;\">         15201.1</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\"> -2.5996</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            259.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 605000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-21-42\n",
      "  done: false\n",
      "  episode_len_mean: 261.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.616999999999988\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2082\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.315260488248543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3078791830274794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00669482740642593\n",
      "          policy_loss: 0.005007090419530869\n",
      "          total_loss: 0.006708661963542303\n",
      "          vf_explained_var: 0.150435209274292\n",
      "          vf_loss: 0.014780360087752343\n",
      "    num_agent_steps_sampled: 605000\n",
      "    num_agent_steps_trained: 605000\n",
      "    num_steps_sampled: 605000\n",
      "    num_steps_trained: 605000\n",
      "  iterations_since_restore: 605\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.108823529411765\n",
      "    ram_util_percent: 39.144117647058835\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038650026043200475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.24178425962836\n",
      "    mean_inference_ms: 1.9279671346100997\n",
      "    mean_raw_obs_processing_ms: 2.1567418139301315\n",
      "  time_since_restore: 15225.239604711533\n",
      "  time_this_iter_s: 24.10552668571472\n",
      "  time_total_s: 15225.239604711533\n",
      "  timers:\n",
      "    learn_throughput: 1447.164\n",
      "    learn_time_ms: 691.007\n",
      "    load_throughput: 40060.708\n",
      "    load_time_ms: 24.962\n",
      "    sample_throughput: 40.041\n",
      "    sample_time_ms: 24974.329\n",
      "    update_time_ms: 3.397\n",
      "  timestamp: 1635078102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 605000\n",
      "  training_iteration: 605\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   605</td><td style=\"text-align: right;\">         15225.2</td><td style=\"text-align: right;\">605000</td><td style=\"text-align: right;\">  -2.617</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">             261.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 606000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-22-03\n",
      "  done: false\n",
      "  episode_len_mean: 263.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.634399999999988\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2085\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.315260488248543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.250424975819058\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010453644889710428\n",
      "          policy_loss: 0.06243960890505049\n",
      "          total_loss: 0.06200102037853665\n",
      "          vf_explained_var: 0.051157597452402115\n",
      "          vf_loss: 0.012065661843452188\n",
      "    num_agent_steps_sampled: 606000\n",
      "    num_agent_steps_trained: 606000\n",
      "    num_steps_sampled: 606000\n",
      "    num_steps_trained: 606000\n",
      "  iterations_since_restore: 606\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.78064516129033\n",
      "    ram_util_percent: 39.22580645161291\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038649809365742686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.244230968652793\n",
      "    mean_inference_ms: 1.9279650789586995\n",
      "    mean_raw_obs_processing_ms: 2.156784707918045\n",
      "  time_since_restore: 15246.983720541\n",
      "  time_this_iter_s: 21.744115829467773\n",
      "  time_total_s: 15246.983720541\n",
      "  timers:\n",
      "    learn_throughput: 1446.697\n",
      "    learn_time_ms: 691.23\n",
      "    load_throughput: 40309.691\n",
      "    load_time_ms: 24.808\n",
      "    sample_throughput: 40.612\n",
      "    sample_time_ms: 24623.235\n",
      "    update_time_ms: 3.325\n",
      "  timestamp: 1635078123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 606000\n",
      "  training_iteration: 606\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   606</td><td style=\"text-align: right;\">           15247</td><td style=\"text-align: right;\">606000</td><td style=\"text-align: right;\"> -2.6344</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            263.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 607000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-22-27\n",
      "  done: false\n",
      "  episode_len_mean: 264.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.6412999999999873\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2088\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.315260488248543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2221005055639478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007443922148033898\n",
      "          policy_loss: -0.10642538401815627\n",
      "          total_loss: -0.10284493110246129\n",
      "          vf_explained_var: 0.08027313649654388\n",
      "          vf_loss: 0.015801458826495543\n",
      "    num_agent_steps_sampled: 607000\n",
      "    num_agent_steps_trained: 607000\n",
      "    num_steps_sampled: 607000\n",
      "    num_steps_trained: 607000\n",
      "  iterations_since_restore: 607\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.89428571428572\n",
      "    ram_util_percent: 39.205714285714286\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386495913552476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.246578120126323\n",
      "    mean_inference_ms: 1.9279627928939826\n",
      "    mean_raw_obs_processing_ms: 2.156797299374651\n",
      "  time_since_restore: 15270.924894332886\n",
      "  time_this_iter_s: 23.941173791885376\n",
      "  time_total_s: 15270.924894332886\n",
      "  timers:\n",
      "    learn_throughput: 1446.774\n",
      "    learn_time_ms: 691.193\n",
      "    load_throughput: 40509.725\n",
      "    load_time_ms: 24.685\n",
      "    sample_throughput: 40.828\n",
      "    sample_time_ms: 24492.83\n",
      "    update_time_ms: 3.352\n",
      "  timestamp: 1635078147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 607000\n",
      "  training_iteration: 607\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   607</td><td style=\"text-align: right;\">         15270.9</td><td style=\"text-align: right;\">607000</td><td style=\"text-align: right;\"> -2.6413</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            264.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 265.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.6523999999999863\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2092\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.315260488248543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1908550063769023\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02135262339701656\n",
      "          policy_loss: -0.042011533967322774\n",
      "          total_loss: -0.038491058722138406\n",
      "          vf_explained_var: 0.16601549088954926\n",
      "          vf_loss: 0.015429025267561276\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 608\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.99722222222223\n",
      "    ram_util_percent: 39.20555555555556\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864932723153875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.249659214562637\n",
      "    mean_inference_ms: 1.9279600907308179\n",
      "    mean_raw_obs_processing_ms: 2.156796858132992\n",
      "  time_since_restore: 15296.713766098022\n",
      "  time_this_iter_s: 25.78887176513672\n",
      "  time_total_s: 15296.713766098022\n",
      "  timers:\n",
      "    learn_throughput: 1444.11\n",
      "    learn_time_ms: 692.468\n",
      "    load_throughput: 40631.535\n",
      "    load_time_ms: 24.611\n",
      "    sample_throughput: 40.958\n",
      "    sample_time_ms: 24415.183\n",
      "    update_time_ms: 3.339\n",
      "  timestamp: 1635078173\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 608\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   608</td><td style=\"text-align: right;\">         15296.7</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\"> -2.6524</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            265.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 609000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-23-18\n",
      "  done: false\n",
      "  episode_len_mean: 266.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.662999999999987\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2096\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.972890732372816e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1107832656966314\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008519858664151335\n",
      "          policy_loss: 0.010103808633155293\n",
      "          total_loss: 0.012594006251957682\n",
      "          vf_explained_var: 0.20803555846214294\n",
      "          vf_loss: 0.013598029926005338\n",
      "    num_agent_steps_sampled: 609000\n",
      "    num_agent_steps_trained: 609000\n",
      "    num_steps_sampled: 609000\n",
      "    num_steps_trained: 609000\n",
      "  iterations_since_restore: 609\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.36857142857143\n",
      "    ram_util_percent: 39.15142857142857\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038649071775936086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.252633240808432\n",
      "    mean_inference_ms: 1.9279575867444039\n",
      "    mean_raw_obs_processing_ms: 2.1567687456310836\n",
      "  time_since_restore: 15321.18213224411\n",
      "  time_this_iter_s: 24.468366146087646\n",
      "  time_total_s: 15321.18213224411\n",
      "  timers:\n",
      "    learn_throughput: 1447.41\n",
      "    learn_time_ms: 690.889\n",
      "    load_throughput: 40733.974\n",
      "    load_time_ms: 24.55\n",
      "    sample_throughput: 40.62\n",
      "    sample_time_ms: 24618.675\n",
      "    update_time_ms: 3.348\n",
      "  timestamp: 1635078198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 609000\n",
      "  training_iteration: 609\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   609</td><td style=\"text-align: right;\">         15321.2</td><td style=\"text-align: right;\">609000</td><td style=\"text-align: right;\">  -2.663</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">             266.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 610000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-23-57\n",
      "  done: false\n",
      "  episode_len_mean: 267.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.6773999999999862\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2100\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.972890732372816e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.081985976960924\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009315678127096778\n",
      "          policy_loss: -0.00014061248964733548\n",
      "          total_loss: 0.004899183660745621\n",
      "          vf_explained_var: 0.11387602239847183\n",
      "          vf_loss: 0.01585965331436859\n",
      "    num_agent_steps_sampled: 610000\n",
      "    num_agent_steps_trained: 610000\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "  iterations_since_restore: 610\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.66842105263158\n",
      "    ram_util_percent: 39.078947368421055\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864879276519458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.255321241407287\n",
      "    mean_inference_ms: 1.9279546062630442\n",
      "    mean_raw_obs_processing_ms: 2.1578410907559613\n",
      "  time_since_restore: 15360.760800600052\n",
      "  time_this_iter_s: 39.57866835594177\n",
      "  time_total_s: 15360.760800600052\n",
      "  timers:\n",
      "    learn_throughput: 1447.655\n",
      "    learn_time_ms: 690.772\n",
      "    load_throughput: 41072.026\n",
      "    load_time_ms: 24.347\n",
      "    sample_throughput: 37.817\n",
      "    sample_time_ms: 26443.153\n",
      "    update_time_ms: 3.356\n",
      "  timestamp: 1635078237\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 610\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   610</td><td style=\"text-align: right;\">         15360.8</td><td style=\"text-align: right;\">610000</td><td style=\"text-align: right;\"> -2.6774</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            267.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 611000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-24-22\n",
      "  done: false\n",
      "  episode_len_mean: 269.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.6917999999999864\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2103\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.972890732372816e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2052092128329808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00650533737848718\n",
      "          policy_loss: 0.03190535224146313\n",
      "          total_loss: 0.030993776188956365\n",
      "          vf_explained_var: -0.01983870193362236\n",
      "          vf_loss: 0.011140514006062101\n",
      "    num_agent_steps_sampled: 611000\n",
      "    num_agent_steps_trained: 611000\n",
      "    num_steps_sampled: 611000\n",
      "    num_steps_trained: 611000\n",
      "  iterations_since_restore: 611\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.45277777777778\n",
      "    ram_util_percent: 38.974999999999994\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864857261102145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.257268108929146\n",
      "    mean_inference_ms: 1.9279525484546725\n",
      "    mean_raw_obs_processing_ms: 2.1586221617673087\n",
      "  time_since_restore: 15385.985477209091\n",
      "  time_this_iter_s: 25.224676609039307\n",
      "  time_total_s: 15385.985477209091\n",
      "  timers:\n",
      "    learn_throughput: 1449.737\n",
      "    learn_time_ms: 689.78\n",
      "    load_throughput: 41220.405\n",
      "    load_time_ms: 24.26\n",
      "    sample_throughput: 37.292\n",
      "    sample_time_ms: 26815.538\n",
      "    update_time_ms: 3.369\n",
      "  timestamp: 1635078262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 611000\n",
      "  training_iteration: 611\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   611</td><td style=\"text-align: right;\">           15386</td><td style=\"text-align: right;\">611000</td><td style=\"text-align: right;\"> -2.6918</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            269.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-24-45\n",
      "  done: false\n",
      "  episode_len_mean: 271.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.099999999999999\n",
      "  episode_reward_mean: -2.7141999999999857\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2107\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.972890732372816e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0867418825626374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008430559412366673\n",
      "          policy_loss: 0.010051490449243122\n",
      "          total_loss: 0.015138094789452022\n",
      "          vf_explained_var: 0.03958673030138016\n",
      "          vf_loss: 0.015954023133963345\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_agent_steps_trained: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 612\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.5125\n",
      "    ram_util_percent: 39.118750000000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038648288831849224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.259520291027364\n",
      "    mean_inference_ms: 1.9279494598309812\n",
      "    mean_raw_obs_processing_ms: 2.1596686219099963\n",
      "  time_since_restore: 15408.429368495941\n",
      "  time_this_iter_s: 22.443891286849976\n",
      "  time_total_s: 15408.429368495941\n",
      "  timers:\n",
      "    learn_throughput: 1452.811\n",
      "    learn_time_ms: 688.321\n",
      "    load_throughput: 41129.781\n",
      "    load_time_ms: 24.313\n",
      "    sample_throughput: 40.218\n",
      "    sample_time_ms: 24864.496\n",
      "    update_time_ms: 3.354\n",
      "  timestamp: 1635078285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 612\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   612</td><td style=\"text-align: right;\">         15408.4</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\"> -2.7142</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            271.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 613000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-25-10\n",
      "  done: false\n",
      "  episode_len_mean: 272.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.7250999999999856\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2110\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.972890732372816e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9981858902507358\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011233419788723253\n",
      "          policy_loss: -0.02997674619158109\n",
      "          total_loss: -0.02877767003244824\n",
      "          vf_explained_var: 0.13845546543598175\n",
      "          vf_loss: 0.011180933595945438\n",
      "    num_agent_steps_sampled: 613000\n",
      "    num_agent_steps_trained: 613000\n",
      "    num_steps_sampled: 613000\n",
      "    num_steps_trained: 613000\n",
      "  iterations_since_restore: 613\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.90277777777778\n",
      "    ram_util_percent: 39.13611111111111\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864807997762972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.261038001954446\n",
      "    mean_inference_ms: 1.927946997782901\n",
      "    mean_raw_obs_processing_ms: 2.1595940532439935\n",
      "  time_since_restore: 15433.51707315445\n",
      "  time_this_iter_s: 25.0877046585083\n",
      "  time_total_s: 15433.51707315445\n",
      "  timers:\n",
      "    learn_throughput: 1450.472\n",
      "    learn_time_ms: 689.431\n",
      "    load_throughput: 41341.113\n",
      "    load_time_ms: 24.189\n",
      "    sample_throughput: 40.093\n",
      "    sample_time_ms: 24941.846\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1635078310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 613000\n",
      "  training_iteration: 613\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   613</td><td style=\"text-align: right;\">         15433.5</td><td style=\"text-align: right;\">613000</td><td style=\"text-align: right;\"> -2.7251</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            272.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 614000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-25-35\n",
      "  done: false\n",
      "  episode_len_mean: 272.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.7287999999999863\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2114\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.972890732372816e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0082874609364403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005268330767110759\n",
      "          policy_loss: 0.013308463825119866\n",
      "          total_loss: 0.018110796643628014\n",
      "          vf_explained_var: 0.12658607959747314\n",
      "          vf_loss: 0.014885205837587515\n",
      "    num_agent_steps_sampled: 614000\n",
      "    num_agent_steps_trained: 614000\n",
      "    num_steps_sampled: 614000\n",
      "    num_steps_trained: 614000\n",
      "  iterations_since_restore: 614\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.83714285714285\n",
      "    ram_util_percent: 39.177142857142854\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864777914388228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.2630079390054\n",
      "    mean_inference_ms: 1.9279434387633865\n",
      "    mean_raw_obs_processing_ms: 2.1594684853724697\n",
      "  time_since_restore: 15457.967506170273\n",
      "  time_this_iter_s: 24.450433015823364\n",
      "  time_total_s: 15457.967506170273\n",
      "  timers:\n",
      "    learn_throughput: 1453.325\n",
      "    learn_time_ms: 688.078\n",
      "    load_throughput: 40903.837\n",
      "    load_time_ms: 24.448\n",
      "    sample_throughput: 40.056\n",
      "    sample_time_ms: 24965.024\n",
      "    update_time_ms: 2.382\n",
      "  timestamp: 1635078335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 614000\n",
      "  training_iteration: 614\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   614</td><td style=\"text-align: right;\">           15458</td><td style=\"text-align: right;\">614000</td><td style=\"text-align: right;\"> -2.7288</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            272.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 615000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-25-59\n",
      "  done: false\n",
      "  episode_len_mean: 273.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.739599999999985\n",
      "  episode_reward_min: -3.979999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2118\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.972890732372816e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0598688589202032\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03643278399268814\n",
      "          policy_loss: 0.03071789683567153\n",
      "          total_loss: 0.03455770305461354\n",
      "          vf_explained_var: 0.18257728219032288\n",
      "          vf_loss: 0.014438495816042026\n",
      "    num_agent_steps_sampled: 615000\n",
      "    num_agent_steps_trained: 615000\n",
      "    num_steps_sampled: 615000\n",
      "    num_steps_trained: 615000\n",
      "  iterations_since_restore: 615\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.21176470588235\n",
      "    ram_util_percent: 39.19411764705882\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038647469505116466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.26481442255494\n",
      "    mean_inference_ms: 1.927939473403439\n",
      "    mean_raw_obs_processing_ms: 2.1593474164909034\n",
      "  time_since_restore: 15482.310685873032\n",
      "  time_this_iter_s: 24.34317970275879\n",
      "  time_total_s: 15482.310685873032\n",
      "  timers:\n",
      "    learn_throughput: 1451.867\n",
      "    learn_time_ms: 688.768\n",
      "    load_throughput: 41302.439\n",
      "    load_time_ms: 24.212\n",
      "    sample_throughput: 40.019\n",
      "    sample_time_ms: 24988.287\n",
      "    update_time_ms: 2.419\n",
      "  timestamp: 1635078359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615000\n",
      "  training_iteration: 615\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   615</td><td style=\"text-align: right;\">         15482.3</td><td style=\"text-align: right;\">615000</td><td style=\"text-align: right;\"> -2.7396</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -3.98</td><td style=\"text-align: right;\">            273.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-26-18\n",
      "  done: false\n",
      "  episode_len_mean: 277.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.7763999999999847\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2121\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1959336098559223e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9160794284608629\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01199255651436566\n",
      "          policy_loss: 0.03981558221081893\n",
      "          total_loss: 0.0424196705636051\n",
      "          vf_explained_var: 0.17983953654766083\n",
      "          vf_loss: 0.011764882735830422\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 616\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.82857142857143\n",
      "    ram_util_percent: 39.21785714285715\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864723783656024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.265857737251487\n",
      "    mean_inference_ms: 1.9279364624561228\n",
      "    mean_raw_obs_processing_ms: 2.1592199875614324\n",
      "  time_since_restore: 15501.450842380524\n",
      "  time_this_iter_s: 19.140156507492065\n",
      "  time_total_s: 15501.450842380524\n",
      "  timers:\n",
      "    learn_throughput: 1450.524\n",
      "    learn_time_ms: 689.406\n",
      "    load_throughput: 40945.525\n",
      "    load_time_ms: 24.423\n",
      "    sample_throughput: 40.442\n",
      "    sample_time_ms: 24727.024\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1635078378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 616\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   616</td><td style=\"text-align: right;\">         15501.5</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\"> -2.7764</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            277.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 617000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-26-39\n",
      "  done: false\n",
      "  episode_len_mean: 280.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.805899999999984\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2124\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1959336098559223e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9856385787328085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013462601659349913\n",
      "          policy_loss: 0.009396098885271284\n",
      "          total_loss: 0.011380418886741002\n",
      "          vf_explained_var: 0.24659638106822968\n",
      "          vf_loss: 0.011840708242056684\n",
      "    num_agent_steps_sampled: 617000\n",
      "    num_agent_steps_trained: 617000\n",
      "    num_steps_sampled: 617000\n",
      "    num_steps_trained: 617000\n",
      "  iterations_since_restore: 617\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.486206896551735\n",
      "    ram_util_percent: 39.18275862068967\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864700208716789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.26664279951407\n",
      "    mean_inference_ms: 1.927933046957159\n",
      "    mean_raw_obs_processing_ms: 2.1590628553503404\n",
      "  time_since_restore: 15521.93926525116\n",
      "  time_this_iter_s: 20.488422870635986\n",
      "  time_total_s: 15521.93926525116\n",
      "  timers:\n",
      "    learn_throughput: 1449.184\n",
      "    learn_time_ms: 690.044\n",
      "    load_throughput: 40936.573\n",
      "    load_time_ms: 24.428\n",
      "    sample_throughput: 41.015\n",
      "    sample_time_ms: 24381.135\n",
      "    update_time_ms: 2.41\n",
      "  timestamp: 1635078399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 617000\n",
      "  training_iteration: 617\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   617</td><td style=\"text-align: right;\">         15521.9</td><td style=\"text-align: right;\">617000</td><td style=\"text-align: right;\"> -2.8059</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            280.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 618000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 280.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.8085999999999838\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2126\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1959336098559223e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0285078697734409\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011309279656664097\n",
      "          policy_loss: -0.0914161612590154\n",
      "          total_loss: -0.08993727829721239\n",
      "          vf_explained_var: 0.004627023823559284\n",
      "          vf_loss: 0.011763963524976538\n",
      "    num_agent_steps_sampled: 618000\n",
      "    num_agent_steps_trained: 618000\n",
      "    num_steps_sampled: 618000\n",
      "    num_steps_trained: 618000\n",
      "  iterations_since_restore: 618\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.614285714285714\n",
      "    ram_util_percent: 39.15714285714286\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038646843744206155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.267076590215552\n",
      "    mean_inference_ms: 1.92793083666845\n",
      "    mean_raw_obs_processing_ms: 2.158960472330799\n",
      "  time_since_restore: 15541.204742193222\n",
      "  time_this_iter_s: 19.265476942062378\n",
      "  time_total_s: 15541.204742193222\n",
      "  timers:\n",
      "    learn_throughput: 1452.755\n",
      "    learn_time_ms: 688.347\n",
      "    load_throughput: 39874.169\n",
      "    load_time_ms: 25.079\n",
      "    sample_throughput: 42.141\n",
      "    sample_time_ms: 23729.826\n",
      "    update_time_ms: 2.412\n",
      "  timestamp: 1635078418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 618000\n",
      "  training_iteration: 618\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   618</td><td style=\"text-align: right;\">         15541.2</td><td style=\"text-align: right;\">618000</td><td style=\"text-align: right;\"> -2.8086</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            280.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 619000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-27-36\n",
      "  done: false\n",
      "  episode_len_mean: 283.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.837899999999983\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2130\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1959336098559223e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1491263773706224\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02390836518476319\n",
      "          policy_loss: -0.002963510693775283\n",
      "          total_loss: 0.0020162138673994277\n",
      "          vf_explained_var: 0.12869036197662354\n",
      "          vf_loss: 0.016470985487103462\n",
      "    num_agent_steps_sampled: 619000\n",
      "    num_agent_steps_trained: 619000\n",
      "    num_steps_sampled: 619000\n",
      "    num_steps_trained: 619000\n",
      "  iterations_since_restore: 619\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.972222222222214\n",
      "    ram_util_percent: 39.111111111111114\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864654856646524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.267646354238565\n",
      "    mean_inference_ms: 1.9279262044019327\n",
      "    mean_raw_obs_processing_ms: 2.1598161720925417\n",
      "  time_since_restore: 15579.113579034805\n",
      "  time_this_iter_s: 37.90883684158325\n",
      "  time_total_s: 15579.113579034805\n",
      "  timers:\n",
      "    learn_throughput: 1440.312\n",
      "    learn_time_ms: 694.294\n",
      "    load_throughput: 40141.375\n",
      "    load_time_ms: 24.912\n",
      "    sample_throughput: 39.891\n",
      "    sample_time_ms: 25068.082\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1635078456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 619000\n",
      "  training_iteration: 619\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   619</td><td style=\"text-align: right;\">         15579.1</td><td style=\"text-align: right;\">619000</td><td style=\"text-align: right;\"> -2.8379</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            283.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-28-01\n",
      "  done: false\n",
      "  episode_len_mean: 284.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.847099999999983\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2133\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7939004147838834e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1267242550849914\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0190750429888668\n",
      "          policy_loss: 0.04881604106889831\n",
      "          total_loss: 0.04886771995160315\n",
      "          vf_explained_var: 0.0921858474612236\n",
      "          vf_loss: 0.011318921820364064\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_agent_steps_trained: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 620\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.291428571428575\n",
      "    ram_util_percent: 38.90571428571429\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038646331852620175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.268057115705467\n",
      "    mean_inference_ms: 1.9279226687682895\n",
      "    mean_raw_obs_processing_ms: 2.160451162515858\n",
      "  time_since_restore: 15603.878191709518\n",
      "  time_this_iter_s: 24.764612674713135\n",
      "  time_total_s: 15603.878191709518\n",
      "  timers:\n",
      "    learn_throughput: 1439.08\n",
      "    learn_time_ms: 694.888\n",
      "    load_throughput: 39747.243\n",
      "    load_time_ms: 25.159\n",
      "    sample_throughput: 42.398\n",
      "    sample_time_ms: 23585.86\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1635078481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 620\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   620</td><td style=\"text-align: right;\">         15603.9</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\"> -2.8471</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            284.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 621000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 284.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.8478999999999832\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2137\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7939004147838834e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1360572947396173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006979998270812062\n",
      "          policy_loss: 0.01476885403196017\n",
      "          total_loss: 0.018873012479808597\n",
      "          vf_explained_var: 0.1427784264087677\n",
      "          vf_loss: 0.015464732454468806\n",
      "    num_agent_steps_sampled: 621000\n",
      "    num_agent_steps_trained: 621000\n",
      "    num_steps_sampled: 621000\n",
      "    num_steps_trained: 621000\n",
      "  iterations_since_restore: 621\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.02941176470589\n",
      "    ram_util_percent: 39.067647058823525\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864604460741263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.268579357458574\n",
      "    mean_inference_ms: 1.927917713730758\n",
      "    mean_raw_obs_processing_ms: 2.1612814823715354\n",
      "  time_since_restore: 15627.956366062164\n",
      "  time_this_iter_s: 24.078174352645874\n",
      "  time_total_s: 15627.956366062164\n",
      "  timers:\n",
      "    learn_throughput: 1438.607\n",
      "    learn_time_ms: 695.117\n",
      "    load_throughput: 39811.682\n",
      "    load_time_ms: 25.118\n",
      "    sample_throughput: 42.606\n",
      "    sample_time_ms: 23471.031\n",
      "    update_time_ms: 2.392\n",
      "  timestamp: 1635078505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 621000\n",
      "  training_iteration: 621\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   621</td><td style=\"text-align: right;\">           15628</td><td style=\"text-align: right;\">621000</td><td style=\"text-align: right;\"> -2.8479</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            284.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 622000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-28-50\n",
      "  done: false\n",
      "  episode_len_mean: 285.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4199999999999924\n",
      "  episode_reward_mean: -2.8507999999999822\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2140\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7939004147838834e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.062770081890954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020203726739024732\n",
      "          policy_loss: -0.10674496011601554\n",
      "          total_loss: -0.10266801921857728\n",
      "          vf_explained_var: 0.2042941004037857\n",
      "          vf_loss: 0.01470463913347986\n",
      "    num_agent_steps_sampled: 622000\n",
      "    num_agent_steps_trained: 622000\n",
      "    num_steps_sampled: 622000\n",
      "    num_steps_trained: 622000\n",
      "  iterations_since_restore: 622\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.394594594594594\n",
      "    ram_util_percent: 39.09459459459459\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864584570048787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.26908963056521\n",
      "    mean_inference_ms: 1.9279142410319463\n",
      "    mean_raw_obs_processing_ms: 2.1613583892638695\n",
      "  time_since_restore: 15653.70883846283\n",
      "  time_this_iter_s: 25.752472400665283\n",
      "  time_total_s: 15653.70883846283\n",
      "  timers:\n",
      "    learn_throughput: 1435.811\n",
      "    learn_time_ms: 696.471\n",
      "    load_throughput: 40370.372\n",
      "    load_time_ms: 24.771\n",
      "    sample_throughput: 42.015\n",
      "    sample_time_ms: 23800.869\n",
      "    update_time_ms: 2.422\n",
      "  timestamp: 1635078530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 622000\n",
      "  training_iteration: 622\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   622</td><td style=\"text-align: right;\">         15653.7</td><td style=\"text-align: right;\">622000</td><td style=\"text-align: right;\"> -2.8508</td><td style=\"text-align: right;\">               -2.42</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            285.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 623000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-29-16\n",
      "  done: false\n",
      "  episode_len_mean: 282.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4199999999999924\n",
      "  episode_reward_mean: -2.8284999999999836\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2144\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.690850622175825e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0250228881835937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019925035241533665\n",
      "          policy_loss: -0.03358717846373717\n",
      "          total_loss: -0.02889613943795363\n",
      "          vf_explained_var: 0.2050657719373703\n",
      "          vf_loss: 0.014941270659781165\n",
      "    num_agent_steps_sampled: 623000\n",
      "    num_agent_steps_trained: 623000\n",
      "    num_steps_sampled: 623000\n",
      "    num_steps_trained: 623000\n",
      "  iterations_since_restore: 623\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.00810810810811\n",
      "    ram_util_percent: 39.17297297297298\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038645575466941695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.26983626247665\n",
      "    mean_inference_ms: 1.9279092764919585\n",
      "    mean_raw_obs_processing_ms: 2.1610695720230275\n",
      "  time_since_restore: 15679.167411088943\n",
      "  time_this_iter_s: 25.45857262611389\n",
      "  time_total_s: 15679.167411088943\n",
      "  timers:\n",
      "    learn_throughput: 1436.024\n",
      "    learn_time_ms: 696.367\n",
      "    load_throughput: 40693.347\n",
      "    load_time_ms: 24.574\n",
      "    sample_throughput: 41.949\n",
      "    sample_time_ms: 23838.273\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1635078556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 623000\n",
      "  training_iteration: 623\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   623</td><td style=\"text-align: right;\">         15679.2</td><td style=\"text-align: right;\">623000</td><td style=\"text-align: right;\"> -2.8285</td><td style=\"text-align: right;\">               -2.42</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            282.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-29-41\n",
      "  done: false\n",
      "  episode_len_mean: 282.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4199999999999924\n",
      "  episode_reward_mean: -2.8297999999999837\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2148\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.690850622175825e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9300015409787495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008041499953964306\n",
      "          policy_loss: -0.023383255137337577\n",
      "          total_loss: -0.01915106905831231\n",
      "          vf_explained_var: 0.23357799649238586\n",
      "          vf_loss: 0.013532200341837273\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 624\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.12222222222221\n",
      "    ram_util_percent: 39.18333333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386453102691961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.270525380969403\n",
      "    mean_inference_ms: 1.9279039215586016\n",
      "    mean_raw_obs_processing_ms: 2.16082007433672\n",
      "  time_since_restore: 15704.588825941086\n",
      "  time_this_iter_s: 25.421414852142334\n",
      "  time_total_s: 15704.588825941086\n",
      "  timers:\n",
      "    learn_throughput: 1435.937\n",
      "    learn_time_ms: 696.409\n",
      "    load_throughput: 40968.482\n",
      "    load_time_ms: 24.409\n",
      "    sample_throughput: 41.779\n",
      "    sample_time_ms: 23935.465\n",
      "    update_time_ms: 2.438\n",
      "  timestamp: 1635078581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 624\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   624</td><td style=\"text-align: right;\">         15704.6</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\"> -2.8298</td><td style=\"text-align: right;\">               -2.42</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            282.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 625000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 282.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4199999999999924\n",
      "  episode_reward_mean: -2.8294999999999835\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2152\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.690850622175825e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0422303716341654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014835828850882157\n",
      "          policy_loss: 0.009728989377617837\n",
      "          total_loss: 0.01195501432650619\n",
      "          vf_explained_var: 0.36112475395202637\n",
      "          vf_loss: 0.012648327431331078\n",
      "    num_agent_steps_sampled: 625000\n",
      "    num_agent_steps_trained: 625000\n",
      "    num_steps_sampled: 625000\n",
      "    num_steps_trained: 625000\n",
      "  iterations_since_restore: 625\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.18888888888888\n",
      "    ram_util_percent: 39.172222222222224\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864506679082246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.271228362268715\n",
      "    mean_inference_ms: 1.9278988696393742\n",
      "    mean_raw_obs_processing_ms: 2.1605767510195864\n",
      "  time_since_restore: 15730.182884454727\n",
      "  time_this_iter_s: 25.594058513641357\n",
      "  time_total_s: 15730.182884454727\n",
      "  timers:\n",
      "    learn_throughput: 1434.957\n",
      "    learn_time_ms: 696.885\n",
      "    load_throughput: 40863.667\n",
      "    load_time_ms: 24.472\n",
      "    sample_throughput: 41.563\n",
      "    sample_time_ms: 24060.052\n",
      "    update_time_ms: 2.404\n",
      "  timestamp: 1635078607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 625000\n",
      "  training_iteration: 625\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   625</td><td style=\"text-align: right;\">         15730.2</td><td style=\"text-align: right;\">625000</td><td style=\"text-align: right;\"> -2.8295</td><td style=\"text-align: right;\">               -2.42</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            282.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 626000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-30-32\n",
      "  done: false\n",
      "  episode_len_mean: 283.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -2.8347999999999827\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2156\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.690850622175825e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0375155601236554\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0064557195763906665\n",
      "          policy_loss: -0.001992031517955992\n",
      "          total_loss: -0.0003675965799225701\n",
      "          vf_explained_var: 0.4600020945072174\n",
      "          vf_loss: 0.011999591605530845\n",
      "    num_agent_steps_sampled: 626000\n",
      "    num_agent_steps_trained: 626000\n",
      "    num_steps_sampled: 626000\n",
      "    num_steps_trained: 626000\n",
      "  iterations_since_restore: 626\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.775000000000006\n",
      "    ram_util_percent: 39.125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864483111606678\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.271826492954627\n",
      "    mean_inference_ms: 1.9278943695114419\n",
      "    mean_raw_obs_processing_ms: 2.1603384305444133\n",
      "  time_since_restore: 15754.917481422424\n",
      "  time_this_iter_s: 24.734596967697144\n",
      "  time_total_s: 15754.917481422424\n",
      "  timers:\n",
      "    learn_throughput: 1437.946\n",
      "    learn_time_ms: 695.436\n",
      "    load_throughput: 40520.996\n",
      "    load_time_ms: 24.679\n",
      "    sample_throughput: 40.616\n",
      "    sample_time_ms: 24620.723\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1635078632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 626000\n",
      "  training_iteration: 626\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   626</td><td style=\"text-align: right;\">         15754.9</td><td style=\"text-align: right;\">626000</td><td style=\"text-align: right;\"> -2.8348</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            283.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 627000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-31-13\n",
      "  done: false\n",
      "  episode_len_mean: 283.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.8344999999999834\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2160\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.690850622175825e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.028136150042216\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007741720506999646\n",
      "          policy_loss: 0.021825455791420405\n",
      "          total_loss: 0.023237848447428808\n",
      "          vf_explained_var: 0.4859423339366913\n",
      "          vf_loss: 0.011693753643582265\n",
      "    num_agent_steps_sampled: 627000\n",
      "    num_agent_steps_trained: 627000\n",
      "    num_steps_sampled: 627000\n",
      "    num_steps_trained: 627000\n",
      "  iterations_since_restore: 627\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.313793103448276\n",
      "    ram_util_percent: 39.115517241379315\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864459566442331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.272504148609098\n",
      "    mean_inference_ms: 1.9278903245223855\n",
      "    mean_raw_obs_processing_ms: 2.1611465050607275\n",
      "  time_since_restore: 15795.957910776138\n",
      "  time_this_iter_s: 41.04042935371399\n",
      "  time_total_s: 15795.957910776138\n",
      "  timers:\n",
      "    learn_throughput: 1439.964\n",
      "    learn_time_ms: 694.462\n",
      "    load_throughput: 40659.028\n",
      "    load_time_ms: 24.595\n",
      "    sample_throughput: 37.485\n",
      "    sample_time_ms: 26677.017\n",
      "    update_time_ms: 2.393\n",
      "  timestamp: 1635078673\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 627000\n",
      "  training_iteration: 627\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   627</td><td style=\"text-align: right;\">           15796</td><td style=\"text-align: right;\">627000</td><td style=\"text-align: right;\"> -2.8345</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            283.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-31-40\n",
      "  done: false\n",
      "  episode_len_mean: 281.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.8151999999999835\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2163\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.690850622175825e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9619296928246815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008508461959485677\n",
      "          policy_loss: -0.10266881883144378\n",
      "          total_loss: -0.10072447376118766\n",
      "          vf_explained_var: 0.4339890480041504\n",
      "          vf_loss: 0.011563638339026107\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_agent_steps_trained: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 628\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06666666666667\n",
      "    ram_util_percent: 38.8948717948718\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038644423299537614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.273259782892477\n",
      "    mean_inference_ms: 1.927887674374825\n",
      "    mean_raw_obs_processing_ms: 2.161748187772764\n",
      "  time_since_restore: 15822.857021808624\n",
      "  time_this_iter_s: 26.899111032485962\n",
      "  time_total_s: 15822.857021808624\n",
      "  timers:\n",
      "    learn_throughput: 1441.302\n",
      "    learn_time_ms: 693.817\n",
      "    load_throughput: 41551.5\n",
      "    load_time_ms: 24.067\n",
      "    sample_throughput: 36.441\n",
      "    sample_time_ms: 27441.559\n",
      "    update_time_ms: 2.384\n",
      "  timestamp: 1635078700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 628\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   628</td><td style=\"text-align: right;\">         15822.9</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\"> -2.8152</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            281.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 629000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-32-05\n",
      "  done: false\n",
      "  episode_len_mean: 279.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.7948999999999837\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2167\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.690850622175825e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9854300982422299\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03070434079274528\n",
      "          policy_loss: -0.013543696370389727\n",
      "          total_loss: -0.0127606939110491\n",
      "          vf_explained_var: 0.5293312668800354\n",
      "          vf_loss: 0.010637304300649299\n",
      "    num_agent_steps_sampled: 629000\n",
      "    num_agent_steps_trained: 629000\n",
      "    num_steps_sampled: 629000\n",
      "    num_steps_trained: 629000\n",
      "  iterations_since_restore: 629\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.99166666666667\n",
      "    ram_util_percent: 39.08055555555555\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864421908810147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.274518275229017\n",
      "    mean_inference_ms: 1.9278848952727292\n",
      "    mean_raw_obs_processing_ms: 2.1625677279380415\n",
      "  time_since_restore: 15848.086970567703\n",
      "  time_this_iter_s: 25.22994875907898\n",
      "  time_total_s: 15848.086970567703\n",
      "  timers:\n",
      "    learn_throughput: 1453.217\n",
      "    learn_time_ms: 688.128\n",
      "    load_throughput: 41300.528\n",
      "    load_time_ms: 24.213\n",
      "    sample_throughput: 38.198\n",
      "    sample_time_ms: 26179.232\n",
      "    update_time_ms: 2.377\n",
      "  timestamp: 1635078725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 629000\n",
      "  training_iteration: 629\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   629</td><td style=\"text-align: right;\">         15848.1</td><td style=\"text-align: right;\">629000</td><td style=\"text-align: right;\"> -2.7949</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            279.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 630000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-32-29\n",
      "  done: false\n",
      "  episode_len_mean: 279.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.794699999999984\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2171\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.036275933263736e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8303634021017287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011769984595439237\n",
      "          policy_loss: 0.012271019361085362\n",
      "          total_loss: 0.015900906547904013\n",
      "          vf_explained_var: 0.4616582691669464\n",
      "          vf_loss: 0.011933523996008766\n",
      "    num_agent_steps_sampled: 630000\n",
      "    num_agent_steps_trained: 630000\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "  iterations_since_restore: 630\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.94411764705882\n",
      "    ram_util_percent: 39.16470588235295\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864404823851552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.27572602213751\n",
      "    mean_inference_ms: 1.9278827585544158\n",
      "    mean_raw_obs_processing_ms: 2.1622918500414094\n",
      "  time_since_restore: 15871.88558959961\n",
      "  time_this_iter_s: 23.798619031906128\n",
      "  time_total_s: 15871.88558959961\n",
      "  timers:\n",
      "    learn_throughput: 1453.736\n",
      "    learn_time_ms: 687.883\n",
      "    load_throughput: 41658.223\n",
      "    load_time_ms: 24.005\n",
      "    sample_throughput: 38.339\n",
      "    sample_time_ms: 26083.102\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1635078749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 630\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   630</td><td style=\"text-align: right;\">         15871.9</td><td style=\"text-align: right;\">630000</td><td style=\"text-align: right;\"> -2.7947</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            279.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 631000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-32-52\n",
      "  done: false\n",
      "  episode_len_mean: 279.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.7985999999999835\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2174\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.036275933263736e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8565540836917029\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014912935350816031\n",
      "          policy_loss: 0.028322664234373303\n",
      "          total_loss: 0.02838024331463708\n",
      "          vf_explained_var: 0.4364531934261322\n",
      "          vf_loss: 0.008623119816830796\n",
      "    num_agent_steps_sampled: 631000\n",
      "    num_agent_steps_trained: 631000\n",
      "    num_steps_sampled: 631000\n",
      "    num_steps_trained: 631000\n",
      "  iterations_since_restore: 631\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.4060606060606\n",
      "    ram_util_percent: 39.23030303030303\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864392026864869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.276567737167518\n",
      "    mean_inference_ms: 1.9278808440215613\n",
      "    mean_raw_obs_processing_ms: 2.1620900382059474\n",
      "  time_since_restore: 15894.941960334778\n",
      "  time_this_iter_s: 23.056370735168457\n",
      "  time_total_s: 15894.941960334778\n",
      "  timers:\n",
      "    learn_throughput: 1450.954\n",
      "    learn_time_ms: 689.202\n",
      "    load_throughput: 41654.499\n",
      "    load_time_ms: 24.007\n",
      "    sample_throughput: 38.492\n",
      "    sample_time_ms: 25979.556\n",
      "    update_time_ms: 2.38\n",
      "  timestamp: 1635078772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 631000\n",
      "  training_iteration: 631\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   631</td><td style=\"text-align: right;\">         15894.9</td><td style=\"text-align: right;\">631000</td><td style=\"text-align: right;\"> -2.7986</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            279.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-33-15\n",
      "  done: false\n",
      "  episode_len_mean: 280.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.8068999999999833\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2178\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.036275933263736e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9556933422883351\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023139190406923957\n",
      "          policy_loss: 0.003302439757519298\n",
      "          total_loss: 0.006770220398902893\n",
      "          vf_explained_var: 0.38852983713150024\n",
      "          vf_loss: 0.013024716679420735\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 632\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.97575757575758\n",
      "    ram_util_percent: 39.20303030303031\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038643749058904435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.277614702968336\n",
      "    mean_inference_ms: 1.9278781051004898\n",
      "    mean_raw_obs_processing_ms: 2.1618269404287362\n",
      "  time_since_restore: 15918.04957652092\n",
      "  time_this_iter_s: 23.107616186141968\n",
      "  time_total_s: 15918.04957652092\n",
      "  timers:\n",
      "    learn_throughput: 1451.887\n",
      "    learn_time_ms: 688.759\n",
      "    load_throughput: 41529.983\n",
      "    load_time_ms: 24.079\n",
      "    sample_throughput: 38.887\n",
      "    sample_time_ms: 25715.488\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1635078795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 632\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   632</td><td style=\"text-align: right;\">           15918</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\"> -2.8069</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            280.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 633000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-33-37\n",
      "  done: false\n",
      "  episode_len_mean: 281.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.8121999999999834\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2181\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.054413899895605e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0017505486806233\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006773767975716933\n",
      "          policy_loss: 0.03440891835424635\n",
      "          total_loss: 0.03476501794324981\n",
      "          vf_explained_var: 0.4284026324748993\n",
      "          vf_loss: 0.010373604844789951\n",
      "    num_agent_steps_sampled: 633000\n",
      "    num_agent_steps_trained: 633000\n",
      "    num_steps_sampled: 633000\n",
      "    num_steps_trained: 633000\n",
      "  iterations_since_restore: 633\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.11612903225806\n",
      "    ram_util_percent: 39.2\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864359374012769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.27831002741091\n",
      "    mean_inference_ms: 1.9278756907295733\n",
      "    mean_raw_obs_processing_ms: 2.161633052092738\n",
      "  time_since_restore: 15940.28930091858\n",
      "  time_this_iter_s: 22.2397243976593\n",
      "  time_total_s: 15940.28930091858\n",
      "  timers:\n",
      "    learn_throughput: 1453.856\n",
      "    learn_time_ms: 687.826\n",
      "    load_throughput: 41260.671\n",
      "    load_time_ms: 24.236\n",
      "    sample_throughput: 39.379\n",
      "    sample_time_ms: 25394.376\n",
      "    update_time_ms: 2.345\n",
      "  timestamp: 1635078817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 633000\n",
      "  training_iteration: 633\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   633</td><td style=\"text-align: right;\">         15940.3</td><td style=\"text-align: right;\">633000</td><td style=\"text-align: right;\"> -2.8122</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            281.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 634000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-34-00\n",
      "  done: false\n",
      "  episode_len_mean: 281.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.815999999999983\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2184\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.054413899895605e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9933000266551971\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007761599414736983\n",
      "          policy_loss: -0.09140748795535829\n",
      "          total_loss: -0.08534796668423547\n",
      "          vf_explained_var: 0.1471419483423233\n",
      "          vf_loss: 0.01599252089444134\n",
      "    num_agent_steps_sampled: 634000\n",
      "    num_agent_steps_trained: 634000\n",
      "    num_steps_sampled: 634000\n",
      "    num_steps_trained: 634000\n",
      "  iterations_since_restore: 634\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.90909090909091\n",
      "    ram_util_percent: 39.199999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038643439473483664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.279045507592414\n",
      "    mean_inference_ms: 1.9278732136614973\n",
      "    mean_raw_obs_processing_ms: 2.1614117186050374\n",
      "  time_since_restore: 15963.35365653038\n",
      "  time_this_iter_s: 23.064355611801147\n",
      "  time_total_s: 15963.35365653038\n",
      "  timers:\n",
      "    learn_throughput: 1451.649\n",
      "    learn_time_ms: 688.872\n",
      "    load_throughput: 40903.877\n",
      "    load_time_ms: 24.448\n",
      "    sample_throughput: 39.75\n",
      "    sample_time_ms: 25157.414\n",
      "    update_time_ms: 2.353\n",
      "  timestamp: 1635078840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 634000\n",
      "  training_iteration: 634\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   634</td><td style=\"text-align: right;\">         15963.4</td><td style=\"text-align: right;\">634000</td><td style=\"text-align: right;\">  -2.816</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">             281.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 635000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 281.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -2.8143999999999845\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2188\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.054413899895605e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.032263493537903\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013374609260145812\n",
      "          policy_loss: 0.01796160837014516\n",
      "          total_loss: 0.0209532945520348\n",
      "          vf_explained_var: 0.3696841299533844\n",
      "          vf_loss: 0.01331432152332531\n",
      "    num_agent_steps_sampled: 635000\n",
      "    num_agent_steps_trained: 635000\n",
      "    num_steps_sampled: 635000\n",
      "    num_steps_trained: 635000\n",
      "  iterations_since_restore: 635\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.80882352941177\n",
      "    ram_util_percent: 39.12941176470588\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038643223947067894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.280031593972762\n",
      "    mean_inference_ms: 1.9278701818739545\n",
      "    mean_raw_obs_processing_ms: 2.161135343752196\n",
      "  time_since_restore: 15986.784393548965\n",
      "  time_this_iter_s: 23.430737018585205\n",
      "  time_total_s: 15986.784393548965\n",
      "  timers:\n",
      "    learn_throughput: 1451.149\n",
      "    learn_time_ms: 689.109\n",
      "    load_throughput: 40923.313\n",
      "    load_time_ms: 24.436\n",
      "    sample_throughput: 40.095\n",
      "    sample_time_ms: 24940.871\n",
      "    update_time_ms: 2.343\n",
      "  timestamp: 1635078864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 635000\n",
      "  training_iteration: 635\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   635</td><td style=\"text-align: right;\">         15986.8</td><td style=\"text-align: right;\">635000</td><td style=\"text-align: right;\"> -2.8144</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            281.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 281.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.349999999999994\n",
      "  episode_reward_mean: -2.816599999999984\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2192\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.054413899895605e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8972231401337518\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00857596096822153\n",
      "          policy_loss: 0.006110280421045091\n",
      "          total_loss: 0.010469178441498014\n",
      "          vf_explained_var: 0.35501331090927124\n",
      "          vf_loss: 0.013331128294683164\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_agent_steps_trained: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 636\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.423333333333325\n",
      "    ram_util_percent: 39.001666666666665\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038642994089807974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.28098195363453\n",
      "    mean_inference_ms: 1.927866858308099\n",
      "    mean_raw_obs_processing_ms: 2.161940115260304\n",
      "  time_since_restore: 16028.913415908813\n",
      "  time_this_iter_s: 42.12902235984802\n",
      "  time_total_s: 16028.913415908813\n",
      "  timers:\n",
      "    learn_throughput: 1449.437\n",
      "    learn_time_ms: 689.923\n",
      "    load_throughput: 41298.82\n",
      "    load_time_ms: 24.214\n",
      "    sample_throughput: 37.482\n",
      "    sample_time_ms: 26679.697\n",
      "    update_time_ms: 2.359\n",
      "  timestamp: 1635078906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 636\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   636</td><td style=\"text-align: right;\">         16028.9</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\"> -2.8166</td><td style=\"text-align: right;\">               -2.35</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            281.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 637000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-35-30\n",
      "  done: false\n",
      "  episode_len_mean: 282.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.349999999999994\n",
      "  episode_reward_mean: -2.820199999999984\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2195\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.054413899895605e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9162838359673818\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.038524768429277226\n",
      "          policy_loss: 0.05064617238110966\n",
      "          total_loss: 0.04974636435508728\n",
      "          vf_explained_var: 0.28162682056427\n",
      "          vf_loss: 0.008263030011827746\n",
      "    num_agent_steps_sampled: 637000\n",
      "    num_agent_steps_trained: 637000\n",
      "    num_steps_sampled: 637000\n",
      "    num_steps_trained: 637000\n",
      "  iterations_since_restore: 637\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.04285714285715\n",
      "    ram_util_percent: 39.11142857142857\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038642813425955366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.281688264083556\n",
      "    mean_inference_ms: 1.9278639109678994\n",
      "    mean_raw_obs_processing_ms: 2.1625454629063094\n",
      "  time_since_restore: 16053.292756080627\n",
      "  time_this_iter_s: 24.379340171813965\n",
      "  time_total_s: 16053.292756080627\n",
      "  timers:\n",
      "    learn_throughput: 1446.329\n",
      "    learn_time_ms: 691.406\n",
      "    load_throughput: 40897.974\n",
      "    load_time_ms: 24.451\n",
      "    sample_throughput: 39.981\n",
      "    sample_time_ms: 25011.881\n",
      "    update_time_ms: 2.35\n",
      "  timestamp: 1635078930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 637000\n",
      "  training_iteration: 637\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   637</td><td style=\"text-align: right;\">         16053.3</td><td style=\"text-align: right;\">637000</td><td style=\"text-align: right;\"> -2.8202</td><td style=\"text-align: right;\">               -2.35</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            282.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 638000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-35-56\n",
      "  done: false\n",
      "  episode_len_mean: 281.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.349999999999994\n",
      "  episode_reward_mean: -2.8130999999999835\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2199\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.081620849843405e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0476633853382535\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015460063274895772\n",
      "          policy_loss: 0.009605930373072624\n",
      "          total_loss: 0.01181054049068027\n",
      "          vf_explained_var: 0.36595606803894043\n",
      "          vf_loss: 0.012681244303368859\n",
      "    num_agent_steps_sampled: 638000\n",
      "    num_agent_steps_trained: 638000\n",
      "    num_steps_sampled: 638000\n",
      "    num_steps_trained: 638000\n",
      "  iterations_since_restore: 638\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.08611111111111\n",
      "    ram_util_percent: 39.12777777777777\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038642591258167415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.28276862571162\n",
      "    mean_inference_ms: 1.9278602029175358\n",
      "    mean_raw_obs_processing_ms: 2.162504482384074\n",
      "  time_since_restore: 16078.499362707138\n",
      "  time_this_iter_s: 25.20660662651062\n",
      "  time_total_s: 16078.499362707138\n",
      "  timers:\n",
      "    learn_throughput: 1444.415\n",
      "    learn_time_ms: 692.322\n",
      "    load_throughput: 40963.88\n",
      "    load_time_ms: 24.412\n",
      "    sample_throughput: 40.255\n",
      "    sample_time_ms: 24841.747\n",
      "    update_time_ms: 2.362\n",
      "  timestamp: 1635078956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 638000\n",
      "  training_iteration: 638\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   638</td><td style=\"text-align: right;\">         16078.5</td><td style=\"text-align: right;\">638000</td><td style=\"text-align: right;\"> -2.8131</td><td style=\"text-align: right;\">               -2.35</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            281.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 639000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-36-19\n",
      "  done: false\n",
      "  episode_len_mean: 281.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.349999999999994\n",
      "  episode_reward_mean: -2.8111999999999835\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2203\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.081620849843405e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9952356490823958\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007548185153213538\n",
      "          policy_loss: 0.01708909629119767\n",
      "          total_loss: 0.019893782999780445\n",
      "          vf_explained_var: 0.39804166555404663\n",
      "          vf_loss: 0.012757043944050868\n",
      "    num_agent_steps_sampled: 639000\n",
      "    num_agent_steps_trained: 639000\n",
      "    num_steps_sampled: 639000\n",
      "    num_steps_trained: 639000\n",
      "  iterations_since_restore: 639\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.01176470588236\n",
      "    ram_util_percent: 39.15882352941177\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864236027715919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.28379834677087\n",
      "    mean_inference_ms: 1.9278563023663708\n",
      "    mean_raw_obs_processing_ms: 2.1621968448687117\n",
      "  time_since_restore: 16102.441595077515\n",
      "  time_this_iter_s: 23.942232370376587\n",
      "  time_total_s: 16102.441595077515\n",
      "  timers:\n",
      "    learn_throughput: 1444.807\n",
      "    learn_time_ms: 692.134\n",
      "    load_throughput: 40905.712\n",
      "    load_time_ms: 24.446\n",
      "    sample_throughput: 40.464\n",
      "    sample_time_ms: 24713.091\n",
      "    update_time_ms: 2.355\n",
      "  timestamp: 1635078979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 639000\n",
      "  training_iteration: 639\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   639</td><td style=\"text-align: right;\">         16102.4</td><td style=\"text-align: right;\">639000</td><td style=\"text-align: right;\"> -2.8112</td><td style=\"text-align: right;\">               -2.35</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            281.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-36-45\n",
      "  done: false\n",
      "  episode_len_mean: 280.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.349999999999994\n",
      "  episode_reward_mean: -2.805299999999984\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2206\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.081620849843405e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1073984702428181\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007241251209229264\n",
      "          policy_loss: -0.09200909468862746\n",
      "          total_loss: -0.09008679704533683\n",
      "          vf_explained_var: 0.36931291222572327\n",
      "          vf_loss: 0.012996282604419523\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 640\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.402777777777786\n",
      "    ram_util_percent: 39.23055555555556\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864217972143161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.28466814061892\n",
      "    mean_inference_ms: 1.9278536479622255\n",
      "    mean_raw_obs_processing_ms: 2.1619928794605765\n",
      "  time_since_restore: 16127.463794469833\n",
      "  time_this_iter_s: 25.022199392318726\n",
      "  time_total_s: 16127.463794469833\n",
      "  timers:\n",
      "    learn_throughput: 1443.423\n",
      "    learn_time_ms: 692.797\n",
      "    load_throughput: 40527.222\n",
      "    load_time_ms: 24.675\n",
      "    sample_throughput: 40.267\n",
      "    sample_time_ms: 24834.466\n",
      "    update_time_ms: 2.461\n",
      "  timestamp: 1635079005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 640\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   640</td><td style=\"text-align: right;\">         16127.5</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\"> -2.8053</td><td style=\"text-align: right;\">               -2.35</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            280.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 641000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-37-09\n",
      "  done: false\n",
      "  episode_len_mean: 280.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.349999999999994\n",
      "  episode_reward_mean: -2.8052999999999844\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2210\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.081620849843405e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0664254440201653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014389986502525194\n",
      "          policy_loss: 0.0016943875286314222\n",
      "          total_loss: 0.005026529563797844\n",
      "          vf_explained_var: 0.21238218247890472\n",
      "          vf_loss: 0.013996399980452325\n",
      "    num_agent_steps_sampled: 641000\n",
      "    num_agent_steps_trained: 641000\n",
      "    num_steps_sampled: 641000\n",
      "    num_steps_trained: 641000\n",
      "  iterations_since_restore: 641\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.98529411764706\n",
      "    ram_util_percent: 39.173529411764704\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864193286190093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.285787670012194\n",
      "    mean_inference_ms: 1.92785024913005\n",
      "    mean_raw_obs_processing_ms: 2.161696784622077\n",
      "  time_since_restore: 16151.560452699661\n",
      "  time_this_iter_s: 24.09665822982788\n",
      "  time_total_s: 16151.560452699661\n",
      "  timers:\n",
      "    learn_throughput: 1446.894\n",
      "    learn_time_ms: 691.136\n",
      "    load_throughput: 40434.977\n",
      "    load_time_ms: 24.731\n",
      "    sample_throughput: 40.096\n",
      "    sample_time_ms: 24940.139\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1635079029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 641000\n",
      "  training_iteration: 641\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   641</td><td style=\"text-align: right;\">         16151.6</td><td style=\"text-align: right;\">641000</td><td style=\"text-align: right;\"> -2.8053</td><td style=\"text-align: right;\">               -2.35</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            280.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 642000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-37-33\n",
      "  done: false\n",
      "  episode_len_mean: 281.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.349999999999994\n",
      "  episode_reward_mean: -2.8102999999999843\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2214\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.081620849843405e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2214301890797086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028837797712470307\n",
      "          policy_loss: -0.006596516817808151\n",
      "          total_loss: -0.006221826126178106\n",
      "          vf_explained_var: 0.42518025636672974\n",
      "          vf_loss: 0.012588993594464328\n",
      "    num_agent_steps_sampled: 642000\n",
      "    num_agent_steps_trained: 642000\n",
      "    num_steps_sampled: 642000\n",
      "    num_steps_trained: 642000\n",
      "  iterations_since_restore: 642\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03235294117647\n",
      "    ram_util_percent: 39.182352941176475\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864173109120847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.28684062201597\n",
      "    mean_inference_ms: 1.927847492921427\n",
      "    mean_raw_obs_processing_ms: 2.1614378876406217\n",
      "  time_since_restore: 16175.505592107773\n",
      "  time_this_iter_s: 23.945139408111572\n",
      "  time_total_s: 16175.505592107773\n",
      "  timers:\n",
      "    learn_throughput: 1450.106\n",
      "    learn_time_ms: 689.605\n",
      "    load_throughput: 40345.596\n",
      "    load_time_ms: 24.786\n",
      "    sample_throughput: 39.959\n",
      "    sample_time_ms: 25025.352\n",
      "    update_time_ms: 2.453\n",
      "  timestamp: 1635079053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 642000\n",
      "  training_iteration: 642\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   642</td><td style=\"text-align: right;\">         16175.5</td><td style=\"text-align: right;\">642000</td><td style=\"text-align: right;\"> -2.8103</td><td style=\"text-align: right;\">               -2.35</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            281.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 643000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-37-58\n",
      "  done: false\n",
      "  episode_len_mean: 281.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.349999999999994\n",
      "  episode_reward_mean: -2.815099999999984\n",
      "  episode_reward_min: -4.009999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2217\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3622431274765115e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9579644481341044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00816995870060361\n",
      "          policy_loss: -0.06218006130721834\n",
      "          total_loss: -0.061547075543138714\n",
      "          vf_explained_var: 0.41414886713027954\n",
      "          vf_loss: 0.010212631435650918\n",
      "    num_agent_steps_sampled: 643000\n",
      "    num_agent_steps_trained: 643000\n",
      "    num_steps_sampled: 643000\n",
      "    num_steps_trained: 643000\n",
      "  iterations_since_restore: 643\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.93333333333334\n",
      "    ram_util_percent: 39.138888888888886\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038641591798005165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.287658942058926\n",
      "    mean_inference_ms: 1.9278457390469168\n",
      "    mean_raw_obs_processing_ms: 2.161246819693017\n",
      "  time_since_restore: 16200.516825199127\n",
      "  time_this_iter_s: 25.01123309135437\n",
      "  time_total_s: 16200.516825199127\n",
      "  timers:\n",
      "    learn_throughput: 1448.425\n",
      "    learn_time_ms: 690.405\n",
      "    load_throughput: 40445.817\n",
      "    load_time_ms: 24.724\n",
      "    sample_throughput: 39.523\n",
      "    sample_time_ms: 25301.764\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1635079078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 643000\n",
      "  training_iteration: 643\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   643</td><td style=\"text-align: right;\">         16200.5</td><td style=\"text-align: right;\">643000</td><td style=\"text-align: right;\"> -2.8151</td><td style=\"text-align: right;\">               -2.35</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">            281.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-38-41\n",
      "  done: false\n",
      "  episode_len_mean: 278.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.781299999999984\n",
      "  episode_reward_min: -3.93999999999996\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2221\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3622431274765115e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9685194777117835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003655920860931457\n",
      "          policy_loss: -0.06752194753951496\n",
      "          total_loss: -0.06560951620340347\n",
      "          vf_explained_var: 0.4280330538749695\n",
      "          vf_loss: 0.01159762976070245\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_agent_steps_trained: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 644\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.61451612903226\n",
      "    ram_util_percent: 39.03387096774194\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864139317319594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.289145389695012\n",
      "    mean_inference_ms: 1.9278434219088572\n",
      "    mean_raw_obs_processing_ms: 2.162002009980782\n",
      "  time_since_restore: 16243.597041606903\n",
      "  time_this_iter_s: 43.08021640777588\n",
      "  time_total_s: 16243.597041606903\n",
      "  timers:\n",
      "    learn_throughput: 1448.926\n",
      "    learn_time_ms: 690.166\n",
      "    load_throughput: 40885.177\n",
      "    load_time_ms: 24.459\n",
      "    sample_throughput: 36.625\n",
      "    sample_time_ms: 27303.78\n",
      "    update_time_ms: 2.523\n",
      "  timestamp: 1635079121\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 644\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   644</td><td style=\"text-align: right;\">         16243.6</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\"> -2.7813</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.94</td><td style=\"text-align: right;\">            278.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 645000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-39-06\n",
      "  done: false\n",
      "  episode_len_mean: 274.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7463999999999853\n",
      "  episode_reward_min: -3.8299999999999623\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2225\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.811215637382557e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9606723142994775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009629574849261581\n",
      "          policy_loss: 0.013559089663128058\n",
      "          total_loss: 0.01632769935660892\n",
      "          vf_explained_var: 0.35979360342025757\n",
      "          vf_loss: 0.012375330045405362\n",
      "    num_agent_steps_sampled: 645000\n",
      "    num_agent_steps_trained: 645000\n",
      "    num_steps_sampled: 645000\n",
      "    num_steps_trained: 645000\n",
      "  iterations_since_restore: 645\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.34444444444444\n",
      "    ram_util_percent: 39.10833333333332\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864121230188503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.291055760877448\n",
      "    mean_inference_ms: 1.9278417666776664\n",
      "    mean_raw_obs_processing_ms: 2.162826661852248\n",
      "  time_since_restore: 16268.786040306091\n",
      "  time_this_iter_s: 25.188998699188232\n",
      "  time_total_s: 16268.786040306091\n",
      "  timers:\n",
      "    learn_throughput: 1450.009\n",
      "    learn_time_ms: 689.651\n",
      "    load_throughput: 41171.931\n",
      "    load_time_ms: 24.288\n",
      "    sample_throughput: 36.39\n",
      "    sample_time_ms: 27480.294\n",
      "    update_time_ms: 2.526\n",
      "  timestamp: 1635079146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 645000\n",
      "  training_iteration: 645\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   645</td><td style=\"text-align: right;\">         16268.8</td><td style=\"text-align: right;\">645000</td><td style=\"text-align: right;\"> -2.7464</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.83</td><td style=\"text-align: right;\">            274.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 646000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-39-31\n",
      "  done: false\n",
      "  episode_len_mean: 272.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7227999999999857\n",
      "  episode_reward_min: -3.3599999999999723\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2229\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.811215637382557e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.954953075117535\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009701593846356991\n",
      "          policy_loss: -0.004271667119529512\n",
      "          total_loss: -0.0017533199654685126\n",
      "          vf_explained_var: 0.39432293176651\n",
      "          vf_loss: 0.012067878887885146\n",
      "    num_agent_steps_sampled: 646000\n",
      "    num_agent_steps_trained: 646000\n",
      "    num_steps_sampled: 646000\n",
      "    num_steps_trained: 646000\n",
      "  iterations_since_restore: 646\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.24857142857143\n",
      "    ram_util_percent: 39.122857142857136\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038641023046043985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.293367820665544\n",
      "    mean_inference_ms: 1.927840329325279\n",
      "    mean_raw_obs_processing_ms: 2.162904708710351\n",
      "  time_since_restore: 16293.768742799759\n",
      "  time_this_iter_s: 24.982702493667603\n",
      "  time_total_s: 16293.768742799759\n",
      "  timers:\n",
      "    learn_throughput: 1452.39\n",
      "    learn_time_ms: 688.52\n",
      "    load_throughput: 40876.889\n",
      "    load_time_ms: 24.464\n",
      "    sample_throughput: 38.81\n",
      "    sample_time_ms: 25766.588\n",
      "    update_time_ms: 2.535\n",
      "  timestamp: 1635079171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 646000\n",
      "  training_iteration: 646\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   646</td><td style=\"text-align: right;\">         16293.8</td><td style=\"text-align: right;\">646000</td><td style=\"text-align: right;\"> -2.7228</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.36</td><td style=\"text-align: right;\">            272.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 647000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-39-56\n",
      "  done: false\n",
      "  episode_len_mean: 271.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7119999999999864\n",
      "  episode_reward_min: -3.3599999999999723\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2233\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.811215637382557e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8761673914061652\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006582295008293215\n",
      "          policy_loss: 0.005533182289865282\n",
      "          total_loss: 0.009662089662419425\n",
      "          vf_explained_var: 0.34723788499832153\n",
      "          vf_loss: 0.012890580813917848\n",
      "    num_agent_steps_sampled: 647000\n",
      "    num_agent_steps_trained: 647000\n",
      "    num_steps_sampled: 647000\n",
      "    num_steps_trained: 647000\n",
      "  iterations_since_restore: 647\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.777777777777786\n",
      "    ram_util_percent: 39.15833333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386408138296183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.29573691810736\n",
      "    mean_inference_ms: 1.9278387260854013\n",
      "    mean_raw_obs_processing_ms: 2.162717745132913\n",
      "  time_since_restore: 16318.618675470352\n",
      "  time_this_iter_s: 24.84993267059326\n",
      "  time_total_s: 16318.618675470352\n",
      "  timers:\n",
      "    learn_throughput: 1455.06\n",
      "    learn_time_ms: 687.257\n",
      "    load_throughput: 40838.561\n",
      "    load_time_ms: 24.487\n",
      "    sample_throughput: 38.737\n",
      "    sample_time_ms: 25814.869\n",
      "    update_time_ms: 2.547\n",
      "  timestamp: 1635079196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647000\n",
      "  training_iteration: 647\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   647</td><td style=\"text-align: right;\">         16318.6</td><td style=\"text-align: right;\">647000</td><td style=\"text-align: right;\">  -2.712</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.36</td><td style=\"text-align: right;\">             271.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-40-22\n",
      "  done: false\n",
      "  episode_len_mean: 270.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7062999999999864\n",
      "  episode_reward_min: -3.3599999999999723\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2236\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.811215637382557e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9167587843206194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012303884643828885\n",
      "          policy_loss: -0.09995756447315216\n",
      "          total_loss: -0.09751396460665597\n",
      "          vf_explained_var: 0.4477604031562805\n",
      "          vf_loss: 0.011611183836228318\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 648\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.88918918918919\n",
      "    ram_util_percent: 39.21081081081081\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864064720949539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.297577592217618\n",
      "    mean_inference_ms: 1.9278376949078981\n",
      "    mean_raw_obs_processing_ms: 2.1626039857897617\n",
      "  time_since_restore: 16344.561136484146\n",
      "  time_this_iter_s: 25.942461013793945\n",
      "  time_total_s: 16344.561136484146\n",
      "  timers:\n",
      "    learn_throughput: 1452.883\n",
      "    learn_time_ms: 688.286\n",
      "    load_throughput: 40850.295\n",
      "    load_time_ms: 24.48\n",
      "    sample_throughput: 38.629\n",
      "    sample_time_ms: 25887.45\n",
      "    update_time_ms: 2.538\n",
      "  timestamp: 1635079222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 648\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   648</td><td style=\"text-align: right;\">         16344.6</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\"> -2.7063</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.36</td><td style=\"text-align: right;\">            270.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 649000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-40-47\n",
      "  done: false\n",
      "  episode_len_mean: 270.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7035999999999856\n",
      "  episode_reward_min: -3.3599999999999723\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2240\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.811215637382557e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1778876854313745\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.13608666742816966\n",
      "          policy_loss: -0.0030441227886411877\n",
      "          total_loss: -0.004459986339012781\n",
      "          vf_explained_var: 0.5281115174293518\n",
      "          vf_loss: 0.01036301297135651\n",
      "    num_agent_steps_sampled: 649000\n",
      "    num_agent_steps_trained: 649000\n",
      "    num_steps_sampled: 649000\n",
      "    num_steps_trained: 649000\n",
      "  iterations_since_restore: 649\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.54857142857143\n",
      "    ram_util_percent: 39.18571428571429\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864040678955724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.300007592705782\n",
      "    mean_inference_ms: 1.9278359167682473\n",
      "    mean_raw_obs_processing_ms: 2.162428192159196\n",
      "  time_since_restore: 16369.455069303513\n",
      "  time_this_iter_s: 24.893932819366455\n",
      "  time_total_s: 16369.455069303513\n",
      "  timers:\n",
      "    learn_throughput: 1451.226\n",
      "    learn_time_ms: 689.073\n",
      "    load_throughput: 41131.959\n",
      "    load_time_ms: 24.312\n",
      "    sample_throughput: 38.488\n",
      "    sample_time_ms: 25982.033\n",
      "    update_time_ms: 2.545\n",
      "  timestamp: 1635079247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 649000\n",
      "  training_iteration: 649\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   649</td><td style=\"text-align: right;\">         16369.5</td><td style=\"text-align: right;\">649000</td><td style=\"text-align: right;\"> -2.7036</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.36</td><td style=\"text-align: right;\">            270.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 650000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-41-09\n",
      "  done: false\n",
      "  episode_len_mean: 271.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.716999999999985\n",
      "  episode_reward_min: -3.3599999999999723\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2244\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0216823456073836e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9512102584044139\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02297832407394216\n",
      "          policy_loss: 0.01901499844259686\n",
      "          total_loss: 0.023511399411492877\n",
      "          vf_explained_var: 0.23997873067855835\n",
      "          vf_loss: 0.014008501999908024\n",
      "    num_agent_steps_sampled: 650000\n",
      "    num_agent_steps_trained: 650000\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "  iterations_since_restore: 650\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.61515151515152\n",
      "    ram_util_percent: 39.190909090909095\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864014518238979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.302211257460964\n",
      "    mean_inference_ms: 1.927833174705427\n",
      "    mean_raw_obs_processing_ms: 2.16228903008998\n",
      "  time_since_restore: 16392.032176733017\n",
      "  time_this_iter_s: 22.577107429504395\n",
      "  time_total_s: 16392.032176733017\n",
      "  timers:\n",
      "    learn_throughput: 1453.25\n",
      "    learn_time_ms: 688.113\n",
      "    load_throughput: 41173.83\n",
      "    load_time_ms: 24.287\n",
      "    sample_throughput: 38.852\n",
      "    sample_time_ms: 25738.584\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1635079269\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 650\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   650</td><td style=\"text-align: right;\">           16392</td><td style=\"text-align: right;\">650000</td><td style=\"text-align: right;\">  -2.717</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.36</td><td style=\"text-align: right;\">             271.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 651000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-41-30\n",
      "  done: false\n",
      "  episode_len_mean: 273.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7386999999999855\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2247\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5325235184110753e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0985966245333354\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0343436978574978\n",
      "          policy_loss: 0.08088838557402293\n",
      "          total_loss: 0.07991874338024192\n",
      "          vf_explained_var: 0.1803930550813675\n",
      "          vf_loss: 0.010016323432015877\n",
      "    num_agent_steps_sampled: 651000\n",
      "    num_agent_steps_trained: 651000\n",
      "    num_steps_sampled: 651000\n",
      "    num_steps_trained: 651000\n",
      "  iterations_since_restore: 651\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.85666666666667\n",
      "    ram_util_percent: 39.15\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038639939094296284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.303654291227154\n",
      "    mean_inference_ms: 1.9278310284653615\n",
      "    mean_raw_obs_processing_ms: 2.16218745659531\n",
      "  time_since_restore: 16413.033350229263\n",
      "  time_this_iter_s: 21.001173496246338\n",
      "  time_total_s: 16413.033350229263\n",
      "  timers:\n",
      "    learn_throughput: 1452.271\n",
      "    learn_time_ms: 688.577\n",
      "    load_throughput: 41117.685\n",
      "    load_time_ms: 24.32\n",
      "    sample_throughput: 39.326\n",
      "    sample_time_ms: 25428.548\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1635079290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 651000\n",
      "  training_iteration: 651\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   651</td><td style=\"text-align: right;\">           16413</td><td style=\"text-align: right;\">651000</td><td style=\"text-align: right;\"> -2.7387</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            273.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 652000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-42-09\n",
      "  done: false\n",
      "  episode_len_mean: 275.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.751899999999986\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2250\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2987852776166124e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9663526084687974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0065616802463419315\n",
      "          policy_loss: 0.04898728024628427\n",
      "          total_loss: 0.04975039677487479\n",
      "          vf_explained_var: 0.018184663727879524\n",
      "          vf_loss: 0.010426641545361943\n",
      "    num_agent_steps_sampled: 652000\n",
      "    num_agent_steps_trained: 652000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 652\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.540000000000006\n",
      "    ram_util_percent: 39.11636363636364\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863974779208505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.304944033498177\n",
      "    mean_inference_ms: 1.927828587795314\n",
      "    mean_raw_obs_processing_ms: 2.1628445642057135\n",
      "  time_since_restore: 16451.686087608337\n",
      "  time_this_iter_s: 38.6527373790741\n",
      "  time_total_s: 16451.686087608337\n",
      "  timers:\n",
      "    learn_throughput: 1449.129\n",
      "    learn_time_ms: 690.07\n",
      "    load_throughput: 40962.92\n",
      "    load_time_ms: 24.412\n",
      "    sample_throughput: 37.178\n",
      "    sample_time_ms: 26897.723\n",
      "    update_time_ms: 2.445\n",
      "  timestamp: 1635079329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 652\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   652</td><td style=\"text-align: right;\">         16451.7</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\"> -2.7519</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            275.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 653000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-42-31\n",
      "  done: false\n",
      "  episode_len_mean: 276.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.768699999999985\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2253\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2987852776166124e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.058207282092836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01806377220075348\n",
      "          policy_loss: 0.01698566691743003\n",
      "          total_loss: 0.017140176561143663\n",
      "          vf_explained_var: 0.26520973443984985\n",
      "          vf_loss: 0.010736581335206413\n",
      "    num_agent_steps_sampled: 653000\n",
      "    num_agent_steps_trained: 653000\n",
      "    num_steps_sampled: 653000\n",
      "    num_steps_trained: 653000\n",
      "  iterations_since_restore: 653\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.70000000000001\n",
      "    ram_util_percent: 39.026666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863955982973049\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.30609285859206\n",
      "    mean_inference_ms: 1.92782611563212\n",
      "    mean_raw_obs_processing_ms: 2.163472600096039\n",
      "  time_since_restore: 16473.209465265274\n",
      "  time_this_iter_s: 21.523377656936646\n",
      "  time_total_s: 16473.209465265274\n",
      "  timers:\n",
      "    learn_throughput: 1450.218\n",
      "    learn_time_ms: 689.551\n",
      "    load_throughput: 40575.798\n",
      "    load_time_ms: 24.645\n",
      "    sample_throughput: 37.666\n",
      "    sample_time_ms: 26549.226\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1635079351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 653000\n",
      "  training_iteration: 653\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   653</td><td style=\"text-align: right;\">         16473.2</td><td style=\"text-align: right;\">653000</td><td style=\"text-align: right;\"> -2.7687</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            276.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 654000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-42-53\n",
      "  done: false\n",
      "  episode_len_mean: 278.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.781999999999984\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2256\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2987852776166124e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.16801930003696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01926603297830872\n",
      "          policy_loss: -0.09495002726713817\n",
      "          total_loss: -0.0917413239263826\n",
      "          vf_explained_var: 0.2323310822248459\n",
      "          vf_loss: 0.014888896306769716\n",
      "    num_agent_steps_sampled: 654000\n",
      "    num_agent_steps_trained: 654000\n",
      "    num_steps_sampled: 654000\n",
      "    num_steps_trained: 654000\n",
      "  iterations_since_restore: 654\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.396875\n",
      "    ram_util_percent: 39.08125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863937098569443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.307150836359853\n",
      "    mean_inference_ms: 1.9278233953534727\n",
      "    mean_raw_obs_processing_ms: 2.1640721205475204\n",
      "  time_since_restore: 16495.534283161163\n",
      "  time_this_iter_s: 22.324817895889282\n",
      "  time_total_s: 16495.534283161163\n",
      "  timers:\n",
      "    learn_throughput: 1448.37\n",
      "    learn_time_ms: 690.432\n",
      "    load_throughput: 40428.001\n",
      "    load_time_ms: 24.735\n",
      "    sample_throughput: 40.862\n",
      "    sample_time_ms: 24472.81\n",
      "    update_time_ms: 2.36\n",
      "  timestamp: 1635079373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 654000\n",
      "  training_iteration: 654\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   654</td><td style=\"text-align: right;\">         16495.5</td><td style=\"text-align: right;\">654000</td><td style=\"text-align: right;\">  -2.782</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">             278.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 655000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-43-16\n",
      "  done: false\n",
      "  episode_len_mean: 279.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7909999999999844\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2260\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2987852776166124e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.060694999827279\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02177798407836396\n",
      "          policy_loss: 0.019381410462988746\n",
      "          total_loss: 0.024007791529099147\n",
      "          vf_explained_var: 0.10138574242591858\n",
      "          vf_loss: 0.015233330676952999\n",
      "    num_agent_steps_sampled: 655000\n",
      "    num_agent_steps_trained: 655000\n",
      "    num_steps_sampled: 655000\n",
      "    num_steps_trained: 655000\n",
      "  iterations_since_restore: 655\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.91515151515151\n",
      "    ram_util_percent: 39.13030303030303\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863910827745208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.308451164573555\n",
      "    mean_inference_ms: 1.9278195185582963\n",
      "    mean_raw_obs_processing_ms: 2.1638352297541394\n",
      "  time_since_restore: 16518.580418109894\n",
      "  time_this_iter_s: 23.04613494873047\n",
      "  time_total_s: 16518.580418109894\n",
      "  timers:\n",
      "    learn_throughput: 1447.032\n",
      "    learn_time_ms: 691.07\n",
      "    load_throughput: 40179.906\n",
      "    load_time_ms: 24.888\n",
      "    sample_throughput: 41.224\n",
      "    sample_time_ms: 24257.717\n",
      "    update_time_ms: 2.372\n",
      "  timestamp: 1635079396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 655000\n",
      "  training_iteration: 655\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   655</td><td style=\"text-align: right;\">         16518.6</td><td style=\"text-align: right;\">655000</td><td style=\"text-align: right;\">  -2.791</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">             279.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-43-41\n",
      "  done: false\n",
      "  episode_len_mean: 279.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.795799999999984\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2264\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4481779164249194e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1160029040442572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01524342156125079\n",
      "          policy_loss: -0.0026253409683704375\n",
      "          total_loss: 0.0008298364778359731\n",
      "          vf_explained_var: 0.17300979793071747\n",
      "          vf_loss: 0.014615205033785767\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 656\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.694444444444436\n",
      "    ram_util_percent: 39.20833333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386388293266961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.30958418559122\n",
      "    mean_inference_ms: 1.9278155569428044\n",
      "    mean_raw_obs_processing_ms: 2.1636368556693504\n",
      "  time_since_restore: 16543.494270324707\n",
      "  time_this_iter_s: 24.913852214813232\n",
      "  time_total_s: 16543.494270324707\n",
      "  timers:\n",
      "    learn_throughput: 1444.934\n",
      "    learn_time_ms: 692.073\n",
      "    load_throughput: 40075.023\n",
      "    load_time_ms: 24.953\n",
      "    sample_throughput: 41.237\n",
      "    sample_time_ms: 24249.818\n",
      "    update_time_ms: 2.35\n",
      "  timestamp: 1635079421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 656\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   656</td><td style=\"text-align: right;\">         16543.5</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\"> -2.7958</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 657000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-44-05\n",
      "  done: false\n",
      "  episode_len_mean: 279.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.798299999999984\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2267\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4481779164249194e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0325592597325643\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005342935678040759\n",
      "          policy_loss: -0.0466081403195858\n",
      "          total_loss: -0.04703481838934952\n",
      "          vf_explained_var: 0.24793872237205505\n",
      "          vf_loss: 0.009898914945208365\n",
      "    num_agent_steps_sampled: 657000\n",
      "    num_agent_steps_trained: 657000\n",
      "    num_steps_sampled: 657000\n",
      "    num_steps_trained: 657000\n",
      "  iterations_since_restore: 657\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.88857142857143\n",
      "    ram_util_percent: 39.21714285714287\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863858047953209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.310414628737398\n",
      "    mean_inference_ms: 1.9278124731983484\n",
      "    mean_raw_obs_processing_ms: 2.1634680554008296\n",
      "  time_since_restore: 16567.879882335663\n",
      "  time_this_iter_s: 24.38561201095581\n",
      "  time_total_s: 16567.879882335663\n",
      "  timers:\n",
      "    learn_throughput: 1441.722\n",
      "    learn_time_ms: 693.615\n",
      "    load_throughput: 40564.143\n",
      "    load_time_ms: 24.652\n",
      "    sample_throughput: 41.319\n",
      "    sample_time_ms: 24202.094\n",
      "    update_time_ms: 2.346\n",
      "  timestamp: 1635079445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 657000\n",
      "  training_iteration: 657\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   657</td><td style=\"text-align: right;\">         16567.9</td><td style=\"text-align: right;\">657000</td><td style=\"text-align: right;\"> -2.7983</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 658000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-44-27\n",
      "  done: false\n",
      "  episode_len_mean: 280.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.8098999999999843\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2270\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4481779164249194e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9281504028373294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04532824479537325\n",
      "          policy_loss: -0.10082471784618166\n",
      "          total_loss: -0.09508182464374436\n",
      "          vf_explained_var: 0.16998352110385895\n",
      "          vf_loss: 0.015024395080076323\n",
      "    num_agent_steps_sampled: 658000\n",
      "    num_agent_steps_trained: 658000\n",
      "    num_steps_sampled: 658000\n",
      "    num_steps_trained: 658000\n",
      "  iterations_since_restore: 658\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.980645161290326\n",
      "    ram_util_percent: 39.23548387096775\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863831007659632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.311162061497228\n",
      "    mean_inference_ms: 1.9278089972344652\n",
      "    mean_raw_obs_processing_ms: 2.1633022055527356\n",
      "  time_since_restore: 16589.867600917816\n",
      "  time_this_iter_s: 21.98771858215332\n",
      "  time_total_s: 16589.867600917816\n",
      "  timers:\n",
      "    learn_throughput: 1445.281\n",
      "    learn_time_ms: 691.907\n",
      "    load_throughput: 40258.542\n",
      "    load_time_ms: 24.839\n",
      "    sample_throughput: 42.002\n",
      "    sample_time_ms: 23808.15\n",
      "    update_time_ms: 2.343\n",
      "  timestamp: 1635079467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 658000\n",
      "  training_iteration: 658\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   658</td><td style=\"text-align: right;\">         16589.9</td><td style=\"text-align: right;\">658000</td><td style=\"text-align: right;\"> -2.8099</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            280.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 659000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-44-52\n",
      "  done: false\n",
      "  episode_len_mean: 280.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.8021999999999845\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2274\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.1722668746373794e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1603433854050107\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015710744451283176\n",
      "          policy_loss: -0.02195229422714975\n",
      "          total_loss: -0.018493029806349013\n",
      "          vf_explained_var: 0.1937660574913025\n",
      "          vf_loss: 0.01506269940485557\n",
      "    num_agent_steps_sampled: 659000\n",
      "    num_agent_steps_trained: 659000\n",
      "    num_steps_sampled: 659000\n",
      "    num_steps_trained: 659000\n",
      "  iterations_since_restore: 659\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.582857142857144\n",
      "    ram_util_percent: 39.17428571428571\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863796699644995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.31225456500647\n",
      "    mean_inference_ms: 1.9278046378787836\n",
      "    mean_raw_obs_processing_ms: 2.1630580652702416\n",
      "  time_since_restore: 16614.606491088867\n",
      "  time_this_iter_s: 24.738890171051025\n",
      "  time_total_s: 16614.606491088867\n",
      "  timers:\n",
      "    learn_throughput: 1444.99\n",
      "    learn_time_ms: 692.046\n",
      "    load_throughput: 40531.059\n",
      "    load_time_ms: 24.672\n",
      "    sample_throughput: 42.03\n",
      "    sample_time_ms: 23792.59\n",
      "    update_time_ms: 2.366\n",
      "  timestamp: 1635079492\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 659000\n",
      "  training_iteration: 659\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   659</td><td style=\"text-align: right;\">         16614.6</td><td style=\"text-align: right;\">659000</td><td style=\"text-align: right;\"> -2.8022</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            280.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-45-16\n",
      "  done: false\n",
      "  episode_len_mean: 279.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7988999999999846\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2278\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.1722668746373794e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4080821010801527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011265423918488043\n",
      "          policy_loss: 0.021253524555100337\n",
      "          total_loss: 0.022052632768948872\n",
      "          vf_explained_var: 0.25062355399131775\n",
      "          vf_loss: 0.014879928808659315\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_agent_steps_trained: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 660\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.90882352941176\n",
      "    ram_util_percent: 39.182352941176475\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038637639187089005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.313373072629798\n",
      "    mean_inference_ms: 1.9278002576002153\n",
      "    mean_raw_obs_processing_ms: 2.162850420418805\n",
      "  time_since_restore: 16638.11530303955\n",
      "  time_this_iter_s: 23.508811950683594\n",
      "  time_total_s: 16638.11530303955\n",
      "  timers:\n",
      "    learn_throughput: 1444.102\n",
      "    learn_time_ms: 692.472\n",
      "    load_throughput: 40677.679\n",
      "    load_time_ms: 24.584\n",
      "    sample_throughput: 41.867\n",
      "    sample_time_ms: 23885.407\n",
      "    update_time_ms: 2.373\n",
      "  timestamp: 1635079516\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 660\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   660</td><td style=\"text-align: right;\">         16638.1</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\"> -2.7989</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 661000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-45-56\n",
      "  done: false\n",
      "  episode_len_mean: 279.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7934999999999843\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2281\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.1722668746373794e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3265843636459775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013205567373849666\n",
      "          policy_loss: -0.09626515044106378\n",
      "          total_loss: -0.09551227821244133\n",
      "          vf_explained_var: 0.37838804721832275\n",
      "          vf_loss: 0.014018715659363402\n",
      "    num_agent_steps_sampled: 661000\n",
      "    num_agent_steps_trained: 661000\n",
      "    num_steps_sampled: 661000\n",
      "    num_steps_trained: 661000\n",
      "  iterations_since_restore: 661\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.234482758620686\n",
      "    ram_util_percent: 39.050000000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038637407906837125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.31425247831543\n",
      "    mean_inference_ms: 1.9277971908326195\n",
      "    mean_raw_obs_processing_ms: 2.163489243845169\n",
      "  time_since_restore: 16678.660531520844\n",
      "  time_this_iter_s: 40.545228481292725\n",
      "  time_total_s: 16678.660531520844\n",
      "  timers:\n",
      "    learn_throughput: 1443.736\n",
      "    learn_time_ms: 692.647\n",
      "    load_throughput: 40853.0\n",
      "    load_time_ms: 24.478\n",
      "    sample_throughput: 38.7\n",
      "    sample_time_ms: 25839.64\n",
      "    update_time_ms: 2.477\n",
      "  timestamp: 1635079556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 661000\n",
      "  training_iteration: 661\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   661</td><td style=\"text-align: right;\">         16678.7</td><td style=\"text-align: right;\">661000</td><td style=\"text-align: right;\"> -2.7935</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 662000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 279.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.793499999999984\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2284\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.1722668746373794e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4320638683107165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017614115931357963\n",
      "          policy_loss: -0.09246507651276059\n",
      "          total_loss: -0.09422650420003467\n",
      "          vf_explained_var: 0.5254421234130859\n",
      "          vf_loss: 0.012559211005767186\n",
      "    num_agent_steps_sampled: 662000\n",
      "    num_agent_steps_trained: 662000\n",
      "    num_steps_sampled: 662000\n",
      "    num_steps_trained: 662000\n",
      "  iterations_since_restore: 662\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03548387096774\n",
      "    ram_util_percent: 39.09354838709677\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863716651975841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.315076326349352\n",
      "    mean_inference_ms: 1.9277941108326149\n",
      "    mean_raw_obs_processing_ms: 2.164130785813319\n",
      "  time_since_restore: 16700.509642124176\n",
      "  time_this_iter_s: 21.84911060333252\n",
      "  time_total_s: 16700.509642124176\n",
      "  timers:\n",
      "    learn_throughput: 1444.34\n",
      "    learn_time_ms: 692.358\n",
      "    load_throughput: 40845.76\n",
      "    load_time_ms: 24.482\n",
      "    sample_throughput: 41.391\n",
      "    sample_time_ms: 24159.565\n",
      "    update_time_ms: 2.478\n",
      "  timestamp: 1635079578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 662000\n",
      "  training_iteration: 662\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   662</td><td style=\"text-align: right;\">         16700.5</td><td style=\"text-align: right;\">662000</td><td style=\"text-align: right;\"> -2.7935</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 663000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-46-43\n",
      "  done: false\n",
      "  episode_len_mean: 279.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.796499999999984\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2288\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.1722668746373794e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3669948299725851\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026222732902481817\n",
      "          policy_loss: 0.037680919013089606\n",
      "          total_loss: 0.03403605471054713\n",
      "          vf_explained_var: 0.5756799578666687\n",
      "          vf_loss: 0.010025084743069278\n",
      "    num_agent_steps_sampled: 663000\n",
      "    num_agent_steps_trained: 663000\n",
      "    num_steps_sampled: 663000\n",
      "    num_steps_trained: 663000\n",
      "  iterations_since_restore: 663\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.22571428571429\n",
      "    ram_util_percent: 39.1\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038636842925540406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.316244959862853\n",
      "    mean_inference_ms: 1.9277899535891072\n",
      "    mean_raw_obs_processing_ms: 2.164990645204131\n",
      "  time_since_restore: 16725.14515519142\n",
      "  time_this_iter_s: 24.635513067245483\n",
      "  time_total_s: 16725.14515519142\n",
      "  timers:\n",
      "    learn_throughput: 1441.93\n",
      "    learn_time_ms: 693.515\n",
      "    load_throughput: 41450.689\n",
      "    load_time_ms: 24.125\n",
      "    sample_throughput: 40.866\n",
      "    sample_time_ms: 24469.972\n",
      "    update_time_ms: 2.472\n",
      "  timestamp: 1635079603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 663000\n",
      "  training_iteration: 663\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   663</td><td style=\"text-align: right;\">         16725.1</td><td style=\"text-align: right;\">663000</td><td style=\"text-align: right;\"> -2.7965</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-47-06\n",
      "  done: false\n",
      "  episode_len_mean: 280.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.802399999999985\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2292\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.758400311956069e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.255239889356825\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010726406921342817\n",
      "          policy_loss: 0.0332606499393781\n",
      "          total_loss: 0.029626402424441443\n",
      "          vf_explained_var: 0.7195810079574585\n",
      "          vf_loss: 0.008918150276359584\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 664\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03636363636363\n",
      "    ram_util_percent: 39.166666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863651040090974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.31726438232771\n",
      "    mean_inference_ms: 1.9277856385347105\n",
      "    mean_raw_obs_processing_ms: 2.164811269604459\n",
      "  time_since_restore: 16748.342080831528\n",
      "  time_this_iter_s: 23.1969256401062\n",
      "  time_total_s: 16748.342080831528\n",
      "  timers:\n",
      "    learn_throughput: 1442.827\n",
      "    learn_time_ms: 693.084\n",
      "    load_throughput: 41315.499\n",
      "    load_time_ms: 24.204\n",
      "    sample_throughput: 40.721\n",
      "    sample_time_ms: 24557.537\n",
      "    update_time_ms: 2.473\n",
      "  timestamp: 1635079626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 664\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   664</td><td style=\"text-align: right;\">         16748.3</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\"> -2.8024</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            280.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 665000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 279.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.794699999999984\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2295\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.758400311956069e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8009226083755493\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006266993275821174\n",
      "          policy_loss: -0.08430158657332261\n",
      "          total_loss: -0.08391512595117093\n",
      "          vf_explained_var: 0.6895290613174438\n",
      "          vf_loss: 0.008395684939912624\n",
      "    num_agent_steps_sampled: 665000\n",
      "    num_agent_steps_trained: 665000\n",
      "    num_steps_sampled: 665000\n",
      "    num_steps_trained: 665000\n",
      "  iterations_since_restore: 665\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.15135135135135\n",
      "    ram_util_percent: 39.208108108108114\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038636251225582396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.318097098463387\n",
      "    mean_inference_ms: 1.9277826234099829\n",
      "    mean_raw_obs_processing_ms: 2.1646807196060234\n",
      "  time_since_restore: 16774.29622220993\n",
      "  time_this_iter_s: 25.95414137840271\n",
      "  time_total_s: 16774.29622220993\n",
      "  timers:\n",
      "    learn_throughput: 1446.396\n",
      "    learn_time_ms: 691.374\n",
      "    load_throughput: 41308.175\n",
      "    load_time_ms: 24.208\n",
      "    sample_throughput: 40.241\n",
      "    sample_time_ms: 24850.022\n",
      "    update_time_ms: 2.475\n",
      "  timestamp: 1635079652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 665000\n",
      "  training_iteration: 665\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   665</td><td style=\"text-align: right;\">         16774.3</td><td style=\"text-align: right;\">665000</td><td style=\"text-align: right;\"> -2.7947</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 666000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-47-58\n",
      "  done: false\n",
      "  episode_len_mean: 279.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7921999999999843\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2299\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.758400311956069e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8631406181388431\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009087731303622348\n",
      "          policy_loss: -0.08888369703458415\n",
      "          total_loss: -0.08585951858096652\n",
      "          vf_explained_var: 0.47793057560920715\n",
      "          vf_loss: 0.011655584836585655\n",
      "    num_agent_steps_sampled: 666000\n",
      "    num_agent_steps_trained: 666000\n",
      "    num_steps_sampled: 666000\n",
      "    num_steps_trained: 666000\n",
      "  iterations_since_restore: 666\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.91621621621621\n",
      "    ram_util_percent: 39.1864864864865\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863590940288608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.31923952888695\n",
      "    mean_inference_ms: 1.9277787931358392\n",
      "    mean_raw_obs_processing_ms: 2.1645118040107802\n",
      "  time_since_restore: 16800.15170288086\n",
      "  time_this_iter_s: 25.855480670928955\n",
      "  time_total_s: 16800.15170288086\n",
      "  timers:\n",
      "    learn_throughput: 1446.152\n",
      "    learn_time_ms: 691.49\n",
      "    load_throughput: 41609.374\n",
      "    load_time_ms: 24.033\n",
      "    sample_throughput: 40.089\n",
      "    sample_time_ms: 24944.246\n",
      "    update_time_ms: 2.474\n",
      "  timestamp: 1635079678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 666000\n",
      "  training_iteration: 666\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   666</td><td style=\"text-align: right;\">         16800.2</td><td style=\"text-align: right;\">666000</td><td style=\"text-align: right;\"> -2.7922</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 667000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-48-22\n",
      "  done: false\n",
      "  episode_len_mean: 279.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7928999999999835\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2303\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.758400311956069e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9816585255993737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007655269264999968\n",
      "          policy_loss: 0.05050495614608129\n",
      "          total_loss: 0.050614555428425474\n",
      "          vf_explained_var: 0.4857889413833618\n",
      "          vf_loss: 0.00992618492907948\n",
      "    num_agent_steps_sampled: 667000\n",
      "    num_agent_steps_trained: 667000\n",
      "    num_steps_sampled: 667000\n",
      "    num_steps_trained: 667000\n",
      "  iterations_since_restore: 667\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.09705882352941\n",
      "    ram_util_percent: 39.17941176470589\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863557500634261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.32037513734907\n",
      "    mean_inference_ms: 1.9277749192550817\n",
      "    mean_raw_obs_processing_ms: 2.164348644658258\n",
      "  time_since_restore: 16824.038736581802\n",
      "  time_this_iter_s: 23.887033700942993\n",
      "  time_total_s: 16824.038736581802\n",
      "  timers:\n",
      "    learn_throughput: 1450.307\n",
      "    learn_time_ms: 689.509\n",
      "    load_throughput: 40942.128\n",
      "    load_time_ms: 24.425\n",
      "    sample_throughput: 40.167\n",
      "    sample_time_ms: 24896.032\n",
      "    update_time_ms: 2.477\n",
      "  timestamp: 1635079702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 667000\n",
      "  training_iteration: 667\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   667</td><td style=\"text-align: right;\">           16824</td><td style=\"text-align: right;\">667000</td><td style=\"text-align: right;\"> -2.7929</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 668000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-48-44\n",
      "  done: false\n",
      "  episode_len_mean: 279.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.7960999999999836\n",
      "  episode_reward_min: -3.4099999999999713\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2306\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.758400311956069e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1305677904023064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02184580868566949\n",
      "          policy_loss: -0.08950768990649118\n",
      "          total_loss: -0.08763634430037605\n",
      "          vf_explained_var: 0.30567532777786255\n",
      "          vf_loss: 0.013177023859073718\n",
      "    num_agent_steps_sampled: 668000\n",
      "    num_agent_steps_trained: 668000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "  iterations_since_restore: 668\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.318749999999994\n",
      "    ram_util_percent: 39.181250000000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038635345438936194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.32110211109772\n",
      "    mean_inference_ms: 1.9277718368685146\n",
      "    mean_raw_obs_processing_ms: 2.164231880198764\n",
      "  time_since_restore: 16846.414820432663\n",
      "  time_this_iter_s: 22.376083850860596\n",
      "  time_total_s: 16846.414820432663\n",
      "  timers:\n",
      "    learn_throughput: 1447.784\n",
      "    learn_time_ms: 690.711\n",
      "    load_throughput: 41316.313\n",
      "    load_time_ms: 24.204\n",
      "    sample_throughput: 40.106\n",
      "    sample_time_ms: 24933.865\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1635079724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 668\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   668</td><td style=\"text-align: right;\">         16846.4</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\"> -2.7961</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">            279.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 669000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-49-24\n",
      "  done: false\n",
      "  episode_len_mean: 280.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.804499999999984\n",
      "  episode_reward_min: -3.6699999999999657\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2310\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1637600467934101e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9839120772149827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013668877977607252\n",
      "          policy_loss: 0.04975605706373851\n",
      "          total_loss: 0.05025734901428223\n",
      "          vf_explained_var: 0.5253056287765503\n",
      "          vf_loss: 0.010340415365580056\n",
      "    num_agent_steps_sampled: 669000\n",
      "    num_agent_steps_trained: 669000\n",
      "    num_steps_sampled: 669000\n",
      "    num_steps_trained: 669000\n",
      "  iterations_since_restore: 669\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.33859649122807\n",
      "    ram_util_percent: 39.12631578947368\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038635041339840026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.322004438371966\n",
      "    mean_inference_ms: 1.9277672556772947\n",
      "    mean_raw_obs_processing_ms: 2.1650782263382276\n",
      "  time_since_restore: 16886.110944747925\n",
      "  time_this_iter_s: 39.69612431526184\n",
      "  time_total_s: 16886.110944747925\n",
      "  timers:\n",
      "    learn_throughput: 1449.436\n",
      "    learn_time_ms: 689.923\n",
      "    load_throughput: 41116.838\n",
      "    load_time_ms: 24.321\n",
      "    sample_throughput: 37.835\n",
      "    sample_time_ms: 26430.309\n",
      "    update_time_ms: 2.48\n",
      "  timestamp: 1635079764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 669000\n",
      "  training_iteration: 669\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   669</td><td style=\"text-align: right;\">         16886.1</td><td style=\"text-align: right;\">669000</td><td style=\"text-align: right;\"> -2.8045</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">            280.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 670000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-49-49\n",
      "  done: false\n",
      "  episode_len_mean: 281.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.811599999999984\n",
      "  episode_reward_min: -3.6699999999999657\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2313\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1637600467934101e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8341118249628279\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027727895591233714\n",
      "          policy_loss: -0.0381608200362987\n",
      "          total_loss: -0.03255808409303427\n",
      "          vf_explained_var: 0.2112812101840973\n",
      "          vf_loss: 0.01394384942120976\n",
      "    num_agent_steps_sampled: 670000\n",
      "    num_agent_steps_trained: 670000\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "  iterations_since_restore: 670\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.236111111111114\n",
      "    ram_util_percent: 39.05833333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038634805396154215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.322729153199717\n",
      "    mean_inference_ms: 1.927763683982924\n",
      "    mean_raw_obs_processing_ms: 2.1657145762345995\n",
      "  time_since_restore: 16911.169048070908\n",
      "  time_this_iter_s: 25.058103322982788\n",
      "  time_total_s: 16911.169048070908\n",
      "  timers:\n",
      "    learn_throughput: 1448.822\n",
      "    learn_time_ms: 690.216\n",
      "    load_throughput: 40980.17\n",
      "    load_time_ms: 24.402\n",
      "    sample_throughput: 37.615\n",
      "    sample_time_ms: 26584.896\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1635079789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 670\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   670</td><td style=\"text-align: right;\">         16911.2</td><td style=\"text-align: right;\">670000</td><td style=\"text-align: right;\"> -2.8116</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">            281.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 671000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-50-11\n",
      "  done: false\n",
      "  episode_len_mean: 281.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.815399999999984\n",
      "  episode_reward_min: -3.6699999999999657\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2317\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7456400701901155e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0546263794104258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02864804332013221\n",
      "          policy_loss: 0.008858098917537265\n",
      "          total_loss: 0.014790531661775377\n",
      "          vf_explained_var: -0.02894674427807331\n",
      "          vf_loss: 0.016478696589668593\n",
      "    num_agent_steps_sampled: 671000\n",
      "    num_agent_steps_trained: 671000\n",
      "    num_steps_sampled: 671000\n",
      "    num_steps_trained: 671000\n",
      "  iterations_since_restore: 671\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.824999999999996\n",
      "    ram_util_percent: 39.146875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863450427879921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.323549393486697\n",
      "    mean_inference_ms: 1.9277586349884939\n",
      "    mean_raw_obs_processing_ms: 2.166538918184827\n",
      "  time_since_restore: 16933.403057575226\n",
      "  time_this_iter_s: 22.234009504318237\n",
      "  time_total_s: 16933.403057575226\n",
      "  timers:\n",
      "    learn_throughput: 1449.837\n",
      "    learn_time_ms: 689.733\n",
      "    load_throughput: 40668.095\n",
      "    load_time_ms: 24.589\n",
      "    sample_throughput: 40.397\n",
      "    sample_time_ms: 24754.159\n",
      "    update_time_ms: 2.373\n",
      "  timestamp: 1635079811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 671000\n",
      "  training_iteration: 671\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   671</td><td style=\"text-align: right;\">         16933.4</td><td style=\"text-align: right;\">671000</td><td style=\"text-align: right;\"> -2.8154</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">            281.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 282.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.826899999999983\n",
      "  episode_reward_min: -3.6699999999999657\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2320\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.618460105285173e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1247735699017842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008628356765755048\n",
      "          policy_loss: -0.03574186116456986\n",
      "          total_loss: -0.035964604715506235\n",
      "          vf_explained_var: -0.308349072933197\n",
      "          vf_loss: 0.01102499150308884\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 672\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.57272727272728\n",
      "    ram_util_percent: 39.13333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863430299266802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.323998061353176\n",
      "    mean_inference_ms: 1.927754756906383\n",
      "    mean_raw_obs_processing_ms: 2.1664047721146202\n",
      "  time_since_restore: 16956.669971466064\n",
      "  time_this_iter_s: 23.266913890838623\n",
      "  time_total_s: 16956.669971466064\n",
      "  timers:\n",
      "    learn_throughput: 1449.734\n",
      "    learn_time_ms: 689.782\n",
      "    load_throughput: 40813.089\n",
      "    load_time_ms: 24.502\n",
      "    sample_throughput: 40.167\n",
      "    sample_time_ms: 24895.974\n",
      "    update_time_ms: 2.376\n",
      "  timestamp: 1635079834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 672\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   672</td><td style=\"text-align: right;\">         16956.7</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\"> -2.8269</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">            282.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 673000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 283.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.8318999999999837\n",
      "  episode_reward_min: -3.6699999999999657\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2324\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.618460105285173e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9356706665621863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008078624107482252\n",
      "          policy_loss: 0.020416374338997734\n",
      "          total_loss: 0.02533302737606896\n",
      "          vf_explained_var: 0.08448073267936707\n",
      "          vf_loss: 0.0142733591194782\n",
      "    num_agent_steps_sampled: 673000\n",
      "    num_agent_steps_trained: 673000\n",
      "    num_steps_sampled: 673000\n",
      "    num_steps_trained: 673000\n",
      "  iterations_since_restore: 673\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.12058823529412\n",
      "    ram_util_percent: 39.182352941176475\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863403086723425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.324544055482633\n",
      "    mean_inference_ms: 1.9277495909306908\n",
      "    mean_raw_obs_processing_ms: 2.166202391607622\n",
      "  time_since_restore: 16980.609996557236\n",
      "  time_this_iter_s: 23.940025091171265\n",
      "  time_total_s: 16980.609996557236\n",
      "  timers:\n",
      "    learn_throughput: 1452.791\n",
      "    learn_time_ms: 688.33\n",
      "    load_throughput: 40380.671\n",
      "    load_time_ms: 24.764\n",
      "    sample_throughput: 40.278\n",
      "    sample_time_ms: 24827.565\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1635079858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 673000\n",
      "  training_iteration: 673\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   673</td><td style=\"text-align: right;\">         16980.6</td><td style=\"text-align: right;\">673000</td><td style=\"text-align: right;\"> -2.8319</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">            283.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 674000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-51-18\n",
      "  done: false\n",
      "  episode_len_mean: 286.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.861799999999983\n",
      "  episode_reward_min: -3.799999999999963\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2327\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.618460105285173e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1865157670444912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012958969302216398\n",
      "          policy_loss: 0.06297075127561887\n",
      "          total_loss: 0.06116650957200262\n",
      "          vf_explained_var: -0.10094909369945526\n",
      "          vf_loss: 0.01006091253625022\n",
      "    num_agent_steps_sampled: 674000\n",
      "    num_agent_steps_trained: 674000\n",
      "    num_steps_sampled: 674000\n",
      "    num_steps_trained: 674000\n",
      "  iterations_since_restore: 674\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.707142857142856\n",
      "    ram_util_percent: 39.20714285714287\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863382710445605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.324698203785612\n",
      "    mean_inference_ms: 1.9277451979123035\n",
      "    mean_raw_obs_processing_ms: 2.166046003022048\n",
      "  time_since_restore: 16999.817949533463\n",
      "  time_this_iter_s: 19.207952976226807\n",
      "  time_total_s: 16999.817949533463\n",
      "  timers:\n",
      "    learn_throughput: 1453.058\n",
      "    learn_time_ms: 688.204\n",
      "    load_throughput: 40594.491\n",
      "    load_time_ms: 24.634\n",
      "    sample_throughput: 40.935\n",
      "    sample_time_ms: 24428.814\n",
      "    update_time_ms: 2.51\n",
      "  timestamp: 1635079878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 674000\n",
      "  training_iteration: 674\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   674</td><td style=\"text-align: right;\">         16999.8</td><td style=\"text-align: right;\">674000</td><td style=\"text-align: right;\"> -2.8618</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">                -3.8</td><td style=\"text-align: right;\">            286.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 675000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-51-38\n",
      "  done: false\n",
      "  episode_len_mean: 288.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.882199999999982\n",
      "  episode_reward_min: -3.799999999999963\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2329\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.618460105285173e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2532319903373719\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021603419460274824\n",
      "          policy_loss: -0.08785949415630764\n",
      "          total_loss: -0.08868541320164998\n",
      "          vf_explained_var: -0.29877349734306335\n",
      "          vf_loss: 0.01170640018535778\n",
      "    num_agent_steps_sampled: 675000\n",
      "    num_agent_steps_trained: 675000\n",
      "    num_steps_sampled: 675000\n",
      "    num_steps_trained: 675000\n",
      "  iterations_since_restore: 675\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.72758620689655\n",
      "    ram_util_percent: 39.21724137931035\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863368888250747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.3246968852454\n",
      "    mean_inference_ms: 1.9277420615554446\n",
      "    mean_raw_obs_processing_ms: 2.165903033220161\n",
      "  time_since_restore: 17020.12952852249\n",
      "  time_this_iter_s: 20.31157898902893\n",
      "  time_total_s: 17020.12952852249\n",
      "  timers:\n",
      "    learn_throughput: 1451.602\n",
      "    learn_time_ms: 688.894\n",
      "    load_throughput: 40400.469\n",
      "    load_time_ms: 24.752\n",
      "    sample_throughput: 41.905\n",
      "    sample_time_ms: 23863.664\n",
      "    update_time_ms: 2.594\n",
      "  timestamp: 1635079898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 675000\n",
      "  training_iteration: 675\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   675</td><td style=\"text-align: right;\">         17020.1</td><td style=\"text-align: right;\">675000</td><td style=\"text-align: right;\"> -2.8822</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">                -3.8</td><td style=\"text-align: right;\">            288.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 676000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 289.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.894899999999982\n",
      "  episode_reward_min: -3.799999999999963\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2333\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.92769015792776e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1867190851105585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015267517383276344\n",
      "          policy_loss: 0.041991300880908966\n",
      "          total_loss: 0.04344789998398887\n",
      "          vf_explained_var: 0.05123494938015938\n",
      "          vf_loss: 0.01332378718070686\n",
      "    num_agent_steps_sampled: 676000\n",
      "    num_agent_steps_trained: 676000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "  iterations_since_restore: 676\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.81666666666668\n",
      "    ram_util_percent: 39.17\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386334205474663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.3244784798296\n",
      "    mean_inference_ms: 1.9277353707666105\n",
      "    mean_raw_obs_processing_ms: 2.16562320561934\n",
      "  time_since_restore: 17041.41565012932\n",
      "  time_this_iter_s: 21.286121606826782\n",
      "  time_total_s: 17041.41565012932\n",
      "  timers:\n",
      "    learn_throughput: 1451.704\n",
      "    learn_time_ms: 688.846\n",
      "    load_throughput: 40422.507\n",
      "    load_time_ms: 24.739\n",
      "    sample_throughput: 42.723\n",
      "    sample_time_ms: 23406.797\n",
      "    update_time_ms: 2.592\n",
      "  timestamp: 1635079919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 676\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   676</td><td style=\"text-align: right;\">         17041.4</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\"> -2.8949</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">                -3.8</td><td style=\"text-align: right;\">            289.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 677000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-52-21\n",
      "  done: false\n",
      "  episode_len_mean: 290.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.908899999999982\n",
      "  episode_reward_min: -3.799999999999963\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2336\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.92769015792776e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1444707996315426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010204364475788688\n",
      "          policy_loss: 0.06628366692198648\n",
      "          total_loss: 0.06310528284973568\n",
      "          vf_explained_var: 0.280841201543808\n",
      "          vf_loss: 0.008266322415632505\n",
      "    num_agent_steps_sampled: 677000\n",
      "    num_agent_steps_trained: 677000\n",
      "    num_steps_sampled: 677000\n",
      "    num_steps_trained: 677000\n",
      "  iterations_since_restore: 677\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.89677419354839\n",
      "    ram_util_percent: 39.148387096774194\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863322136412188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.3241284307818\n",
      "    mean_inference_ms: 1.9277298818026996\n",
      "    mean_raw_obs_processing_ms: 2.165417346313954\n",
      "  time_since_restore: 17063.257577896118\n",
      "  time_this_iter_s: 21.841927766799927\n",
      "  time_total_s: 17063.257577896118\n",
      "  timers:\n",
      "    learn_throughput: 1447.505\n",
      "    learn_time_ms: 690.844\n",
      "    load_throughput: 40894.146\n",
      "    load_time_ms: 24.453\n",
      "    sample_throughput: 43.102\n",
      "    sample_time_ms: 23200.554\n",
      "    update_time_ms: 2.608\n",
      "  timestamp: 1635079941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 677000\n",
      "  training_iteration: 677\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   677</td><td style=\"text-align: right;\">         17063.3</td><td style=\"text-align: right;\">677000</td><td style=\"text-align: right;\"> -2.9089</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">                -3.8</td><td style=\"text-align: right;\">            290.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 678000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-52-40\n",
      "  done: false\n",
      "  episode_len_mean: 294.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.944999999999981\n",
      "  episode_reward_min: -3.899999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2339\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.92769015792776e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3902101437250773\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010653968802863447\n",
      "          policy_loss: 0.0476455678542455\n",
      "          total_loss: 0.042275451040930216\n",
      "          vf_explained_var: -0.02438880316913128\n",
      "          vf_loss: 0.008531983113951154\n",
      "    num_agent_steps_sampled: 678000\n",
      "    num_agent_steps_trained: 678000\n",
      "    num_steps_sampled: 678000\n",
      "    num_steps_trained: 678000\n",
      "  iterations_since_restore: 678\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.36666666666667\n",
      "    ram_util_percent: 39.13703703703705\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038633029632516955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.323500689815898\n",
      "    mean_inference_ms: 1.9277237641175784\n",
      "    mean_raw_obs_processing_ms: 2.1652150850698413\n",
      "  time_since_restore: 17081.910461187363\n",
      "  time_this_iter_s: 18.652883291244507\n",
      "  time_total_s: 17081.910461187363\n",
      "  timers:\n",
      "    learn_throughput: 1446.577\n",
      "    learn_time_ms: 691.287\n",
      "    load_throughput: 40854.512\n",
      "    load_time_ms: 24.477\n",
      "    sample_throughput: 43.806\n",
      "    sample_time_ms: 22827.704\n",
      "    update_time_ms: 2.637\n",
      "  timestamp: 1635079960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 678000\n",
      "  training_iteration: 678\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   678</td><td style=\"text-align: right;\">         17081.9</td><td style=\"text-align: right;\">678000</td><td style=\"text-align: right;\">  -2.945</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">             294.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 679000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-53-17\n",
      "  done: false\n",
      "  episode_len_mean: 295.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.955899999999981\n",
      "  episode_reward_min: -3.899999999999961\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2341\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.92769015792776e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.304899197154575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01767788587791538\n",
      "          policy_loss: -0.0378097555703587\n",
      "          total_loss: -0.04091794358359443\n",
      "          vf_explained_var: 0.5430017709732056\n",
      "          vf_loss: 0.009940796899738619\n",
      "    num_agent_steps_sampled: 679000\n",
      "    num_agent_steps_trained: 679000\n",
      "    num_steps_sampled: 679000\n",
      "    num_steps_trained: 679000\n",
      "  iterations_since_restore: 679\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.384905660377356\n",
      "    ram_util_percent: 38.99622641509434\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863290864211381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.32301248314207\n",
      "    mean_inference_ms: 1.9277197864010076\n",
      "    mean_raw_obs_processing_ms: 2.165539339875004\n",
      "  time_since_restore: 17118.944911003113\n",
      "  time_this_iter_s: 37.03444981575012\n",
      "  time_total_s: 17118.944911003113\n",
      "  timers:\n",
      "    learn_throughput: 1446.316\n",
      "    learn_time_ms: 691.412\n",
      "    load_throughput: 40784.515\n",
      "    load_time_ms: 24.519\n",
      "    sample_throughput: 44.324\n",
      "    sample_time_ms: 22561.39\n",
      "    update_time_ms: 2.631\n",
      "  timestamp: 1635079997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 679000\n",
      "  training_iteration: 679\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   679</td><td style=\"text-align: right;\">         17118.9</td><td style=\"text-align: right;\">679000</td><td style=\"text-align: right;\"> -2.9559</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">            295.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-53-39\n",
      "  done: false\n",
      "  episode_len_mean: 296.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9675999999999814\n",
      "  episode_reward_min: -4.039999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2345\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.92769015792776e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0814211222860548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020853242150828975\n",
      "          policy_loss: -0.024620591269599066\n",
      "          total_loss: -0.02398169818851683\n",
      "          vf_explained_var: 0.5199992060661316\n",
      "          vf_loss: 0.011453098472621706\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 680\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.316129032258075\n",
      "    ram_util_percent: 39.02903225806451\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038632654008688865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.322012141064736\n",
      "    mean_inference_ms: 1.927712182809614\n",
      "    mean_raw_obs_processing_ms: 2.166163596003065\n",
      "  time_since_restore: 17140.8641705513\n",
      "  time_this_iter_s: 21.919259548187256\n",
      "  time_total_s: 17140.8641705513\n",
      "  timers:\n",
      "    learn_throughput: 1446.215\n",
      "    learn_time_ms: 691.46\n",
      "    load_throughput: 40917.125\n",
      "    load_time_ms: 24.44\n",
      "    sample_throughput: 44.949\n",
      "    sample_time_ms: 22247.458\n",
      "    update_time_ms: 2.692\n",
      "  timestamp: 1635080019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 680\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   680</td><td style=\"text-align: right;\">         17140.9</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\"> -2.9676</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.04</td><td style=\"text-align: right;\">            296.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 681000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-54-00\n",
      "  done: false\n",
      "  episode_len_mean: 296.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9676999999999807\n",
      "  episode_reward_min: -4.039999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2348\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.89153523689164e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1644933144251506\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022163974920401428\n",
      "          policy_loss: 0.079897022081746\n",
      "          total_loss: 0.07677146262592739\n",
      "          vf_explained_var: 0.6239314079284668\n",
      "          vf_loss: 0.00851936377132208\n",
      "    num_agent_steps_sampled: 681000\n",
      "    num_agent_steps_trained: 681000\n",
      "    num_steps_sampled: 681000\n",
      "    num_steps_trained: 681000\n",
      "  iterations_since_restore: 681\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.85333333333334\n",
      "    ram_util_percent: 39.14\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863245941513209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.321268317208855\n",
      "    mean_inference_ms: 1.927706372396922\n",
      "    mean_raw_obs_processing_ms: 2.1663808965125204\n",
      "  time_since_restore: 17161.903216838837\n",
      "  time_this_iter_s: 21.03904628753662\n",
      "  time_total_s: 17161.903216838837\n",
      "  timers:\n",
      "    learn_throughput: 1443.619\n",
      "    learn_time_ms: 692.703\n",
      "    load_throughput: 40822.304\n",
      "    load_time_ms: 24.496\n",
      "    sample_throughput: 45.194\n",
      "    sample_time_ms: 22126.671\n",
      "    update_time_ms: 2.681\n",
      "  timestamp: 1635080040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 681000\n",
      "  training_iteration: 681\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   681</td><td style=\"text-align: right;\">         17161.9</td><td style=\"text-align: right;\">681000</td><td style=\"text-align: right;\"> -2.9677</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.04</td><td style=\"text-align: right;\">            296.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 682000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-54-19\n",
      "  done: false\n",
      "  episode_len_mean: 297.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.979799999999981\n",
      "  episode_reward_min: -4.039999999999958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2350\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.837302855337459e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5138048079278734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01480240889317612\n",
      "          policy_loss: -0.04426650587055418\n",
      "          total_loss: -0.049272267189290786\n",
      "          vf_explained_var: 0.585063099861145\n",
      "          vf_loss: 0.01013227536265428\n",
      "    num_agent_steps_sampled: 682000\n",
      "    num_agent_steps_trained: 682000\n",
      "    num_steps_sampled: 682000\n",
      "    num_steps_trained: 682000\n",
      "  iterations_since_restore: 682\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.75925925925927\n",
      "    ram_util_percent: 39.229629629629635\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863230937697788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.320687422329737\n",
      "    mean_inference_ms: 1.927702346464128\n",
      "    mean_raw_obs_processing_ms: 2.166157950205856\n",
      "  time_since_restore: 17181.032563209534\n",
      "  time_this_iter_s: 19.12934637069702\n",
      "  time_total_s: 17181.032563209534\n",
      "  timers:\n",
      "    learn_throughput: 1442.007\n",
      "    learn_time_ms: 693.478\n",
      "    load_throughput: 40849.499\n",
      "    load_time_ms: 24.48\n",
      "    sample_throughput: 46.057\n",
      "    sample_time_ms: 21712.167\n",
      "    update_time_ms: 2.679\n",
      "  timestamp: 1635080059\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 682000\n",
      "  training_iteration: 682\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   682</td><td style=\"text-align: right;\">           17181</td><td style=\"text-align: right;\">682000</td><td style=\"text-align: right;\"> -2.9798</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.04</td><td style=\"text-align: right;\">            297.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 683000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-54-39\n",
      "  done: false\n",
      "  episode_len_mean: 298.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9861999999999806\n",
      "  episode_reward_min: -4.039999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2353\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.837302855337459e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.077889002694024\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.029407675178640288\n",
      "          policy_loss: -0.0896350510004494\n",
      "          total_loss: -0.09080894655651517\n",
      "          vf_explained_var: 0.7179403305053711\n",
      "          vf_loss: 0.009604973977224695\n",
      "    num_agent_steps_sampled: 683000\n",
      "    num_agent_steps_trained: 683000\n",
      "    num_steps_sampled: 683000\n",
      "    num_steps_trained: 683000\n",
      "  iterations_since_restore: 683\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.055172413793116\n",
      "    ram_util_percent: 39.241379310344826\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863209734804701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.319766609904125\n",
      "    mean_inference_ms: 1.9276959954856867\n",
      "    mean_raw_obs_processing_ms: 2.165828159310439\n",
      "  time_since_restore: 17201.322010040283\n",
      "  time_this_iter_s: 20.28944683074951\n",
      "  time_total_s: 17201.322010040283\n",
      "  timers:\n",
      "    learn_throughput: 1440.686\n",
      "    learn_time_ms: 694.114\n",
      "    load_throughput: 40952.801\n",
      "    load_time_ms: 24.418\n",
      "    sample_throughput: 46.846\n",
      "    sample_time_ms: 21346.428\n",
      "    update_time_ms: 2.71\n",
      "  timestamp: 1635080079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 683000\n",
      "  training_iteration: 683\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   683</td><td style=\"text-align: right;\">         17201.3</td><td style=\"text-align: right;\">683000</td><td style=\"text-align: right;\"> -2.9862</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.04</td><td style=\"text-align: right;\">            298.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 684000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-55-02\n",
      "  done: false\n",
      "  episode_len_mean: 299.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9930999999999788\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2357\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3255954283006186e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8157607542143928\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0046525778858813914\n",
      "          policy_loss: 0.0668417404923174\n",
      "          total_loss: 0.0687910166879495\n",
      "          vf_explained_var: 0.591895580291748\n",
      "          vf_loss: 0.010106878148184882\n",
      "    num_agent_steps_sampled: 684000\n",
      "    num_agent_steps_trained: 684000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "  iterations_since_restore: 684\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.58787878787879\n",
      "    ram_util_percent: 39.24848484848486\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038631826570739936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.318571941949852\n",
      "    mean_inference_ms: 1.9276874636578998\n",
      "    mean_raw_obs_processing_ms: 2.165426336284623\n",
      "  time_since_restore: 17224.106759548187\n",
      "  time_this_iter_s: 22.784749507904053\n",
      "  time_total_s: 17224.106759548187\n",
      "  timers:\n",
      "    learn_throughput: 1441.082\n",
      "    learn_time_ms: 693.923\n",
      "    load_throughput: 40947.004\n",
      "    load_time_ms: 24.422\n",
      "    sample_throughput: 46.074\n",
      "    sample_time_ms: 21704.4\n",
      "    update_time_ms: 2.61\n",
      "  timestamp: 1635080102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 684\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   684</td><td style=\"text-align: right;\">         17224.1</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\"> -2.9931</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            299.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 685000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-55-25\n",
      "  done: false\n",
      "  episode_len_mean: 298.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9893999999999794\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2360\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8548245661788516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006906738919604136\n",
      "          policy_loss: 0.04663644010821978\n",
      "          total_loss: 0.04639876791172558\n",
      "          vf_explained_var: 0.5640754699707031\n",
      "          vf_loss: 0.00831057280043347\n",
      "    num_agent_steps_sampled: 685000\n",
      "    num_agent_steps_trained: 685000\n",
      "    num_steps_sampled: 685000\n",
      "    num_steps_trained: 685000\n",
      "  iterations_since_restore: 685\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.86666666666667\n",
      "    ram_util_percent: 39.190909090909095\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863163100673221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.31767725396799\n",
      "    mean_inference_ms: 1.9276810947197183\n",
      "    mean_raw_obs_processing_ms: 2.1651056575566954\n",
      "  time_since_restore: 17247.171299934387\n",
      "  time_this_iter_s: 23.06454038619995\n",
      "  time_total_s: 17247.171299934387\n",
      "  timers:\n",
      "    learn_throughput: 1439.617\n",
      "    learn_time_ms: 694.629\n",
      "    load_throughput: 40880.953\n",
      "    load_time_ms: 24.461\n",
      "    sample_throughput: 45.498\n",
      "    sample_time_ms: 21979.055\n",
      "    update_time_ms: 2.524\n",
      "  timestamp: 1635080125\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 685000\n",
      "  training_iteration: 685\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   685</td><td style=\"text-align: right;\">         17247.2</td><td style=\"text-align: right;\">685000</td><td style=\"text-align: right;\"> -2.9894</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            298.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 686000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-55-47\n",
      "  done: false\n",
      "  episode_len_mean: 300.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.006699999999979\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2363\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1854996350076463\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016810023952988314\n",
      "          policy_loss: -0.03082374913824929\n",
      "          total_loss: -0.030562077545457415\n",
      "          vf_explained_var: 0.38807761669158936\n",
      "          vf_loss: 0.012116660552823708\n",
      "    num_agent_steps_sampled: 686000\n",
      "    num_agent_steps_trained: 686000\n",
      "    num_steps_sampled: 686000\n",
      "    num_steps_trained: 686000\n",
      "  iterations_since_restore: 686\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.99032258064517\n",
      "    ram_util_percent: 39.13548387096775\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863144713663344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.316644517114536\n",
      "    mean_inference_ms: 1.9276745726274236\n",
      "    mean_raw_obs_processing_ms: 2.164787093113292\n",
      "  time_since_restore: 17268.915857315063\n",
      "  time_this_iter_s: 21.74455738067627\n",
      "  time_total_s: 17268.915857315063\n",
      "  timers:\n",
      "    learn_throughput: 1439.936\n",
      "    learn_time_ms: 694.475\n",
      "    load_throughput: 40784.198\n",
      "    load_time_ms: 24.519\n",
      "    sample_throughput: 45.403\n",
      "    sample_time_ms: 22024.991\n",
      "    update_time_ms: 2.529\n",
      "  timestamp: 1635080147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 686000\n",
      "  training_iteration: 686\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   686</td><td style=\"text-align: right;\">         17268.9</td><td style=\"text-align: right;\">686000</td><td style=\"text-align: right;\"> -3.0067</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            300.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 687000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-56-08\n",
      "  done: false\n",
      "  episode_len_mean: 302.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.028399999999979\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2366\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1954511271582708\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013431696063073812\n",
      "          policy_loss: -0.036797168850898745\n",
      "          total_loss: -0.03720296306742562\n",
      "          vf_explained_var: 0.38986408710479736\n",
      "          vf_loss: 0.01154871255469819\n",
      "    num_agent_steps_sampled: 687000\n",
      "    num_agent_steps_trained: 687000\n",
      "    num_steps_sampled: 687000\n",
      "    num_steps_trained: 687000\n",
      "  iterations_since_restore: 687\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.07333333333334\n",
      "    ram_util_percent: 39.096666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863126608985102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.315486224192327\n",
      "    mean_inference_ms: 1.9276676230774978\n",
      "    mean_raw_obs_processing_ms: 2.16444310303322\n",
      "  time_since_restore: 17290.042816877365\n",
      "  time_this_iter_s: 21.126959562301636\n",
      "  time_total_s: 17290.042816877365\n",
      "  timers:\n",
      "    learn_throughput: 1440.346\n",
      "    learn_time_ms: 694.278\n",
      "    load_throughput: 40485.56\n",
      "    load_time_ms: 24.7\n",
      "    sample_throughput: 45.551\n",
      "    sample_time_ms: 21953.504\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1635080168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 687000\n",
      "  training_iteration: 687\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   687</td><td style=\"text-align: right;\">           17290</td><td style=\"text-align: right;\">687000</td><td style=\"text-align: right;\"> -3.0284</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            302.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-56-28\n",
      "  done: false\n",
      "  episode_len_mean: 303.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.037299999999979\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2369\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.209587154785792\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014150531809036377\n",
      "          policy_loss: -0.10796659423245324\n",
      "          total_loss: -0.10417154129180643\n",
      "          vf_explained_var: 0.10987730324268341\n",
      "          vf_loss: 0.01589092193171382\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "  iterations_since_restore: 688\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.02758620689656\n",
      "    ram_util_percent: 39.07931034482758\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863109999713999\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.31422556012167\n",
      "    mean_inference_ms: 1.9276605902556683\n",
      "    mean_raw_obs_processing_ms: 2.1641034345367287\n",
      "  time_since_restore: 17310.415613651276\n",
      "  time_this_iter_s: 20.372796773910522\n",
      "  time_total_s: 17310.415613651276\n",
      "  timers:\n",
      "    learn_throughput: 1442.046\n",
      "    learn_time_ms: 693.459\n",
      "    load_throughput: 40924.551\n",
      "    load_time_ms: 24.435\n",
      "    sample_throughput: 45.194\n",
      "    sample_time_ms: 22126.632\n",
      "    update_time_ms: 2.499\n",
      "  timestamp: 1635080188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 688\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   688</td><td style=\"text-align: right;\">         17310.4</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\"> -3.0373</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            303.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 689000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-57-08\n",
      "  done: false\n",
      "  episode_len_mean: 304.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0433999999999792\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2372\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1049692571163177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010206568710232962\n",
      "          policy_loss: -0.12658136354552374\n",
      "          total_loss: -0.12370792975028357\n",
      "          vf_explained_var: 0.2797267735004425\n",
      "          vf_loss: 0.013923117953042189\n",
      "    num_agent_steps_sampled: 689000\n",
      "    num_agent_steps_trained: 689000\n",
      "    num_steps_sampled: 689000\n",
      "    num_steps_trained: 689000\n",
      "  iterations_since_restore: 689\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.8859649122807\n",
      "    ram_util_percent: 39.0\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038630927769283556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.312924506395902\n",
      "    mean_inference_ms: 1.9276534427257759\n",
      "    mean_raw_obs_processing_ms: 2.1645000468847395\n",
      "  time_since_restore: 17350.093935728073\n",
      "  time_this_iter_s: 39.678322076797485\n",
      "  time_total_s: 17350.093935728073\n",
      "  timers:\n",
      "    learn_throughput: 1442.772\n",
      "    learn_time_ms: 693.11\n",
      "    load_throughput: 40740.977\n",
      "    load_time_ms: 24.545\n",
      "    sample_throughput: 44.66\n",
      "    sample_time_ms: 22391.259\n",
      "    update_time_ms: 2.491\n",
      "  timestamp: 1635080228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 689000\n",
      "  training_iteration: 689\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   689</td><td style=\"text-align: right;\">         17350.1</td><td style=\"text-align: right;\">689000</td><td style=\"text-align: right;\"> -3.0434</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            304.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 690000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-57-30\n",
      "  done: false\n",
      "  episode_len_mean: 306.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.064699999999978\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2376\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.107328685786989\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010279547122175358\n",
      "          policy_loss: -0.022166218525833555\n",
      "          total_loss: -0.02066626755727662\n",
      "          vf_explained_var: 0.3912896513938904\n",
      "          vf_loss: 0.012573231001281077\n",
      "    num_agent_steps_sampled: 690000\n",
      "    num_agent_steps_trained: 690000\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "  iterations_since_restore: 690\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06666666666667\n",
      "    ram_util_percent: 39.11333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863068187043474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.311049661319753\n",
      "    mean_inference_ms: 1.9276435567481842\n",
      "    mean_raw_obs_processing_ms: 2.165013772212266\n",
      "  time_since_restore: 17371.535024404526\n",
      "  time_this_iter_s: 21.441088676452637\n",
      "  time_total_s: 17371.535024404526\n",
      "  timers:\n",
      "    learn_throughput: 1443.792\n",
      "    learn_time_ms: 692.62\n",
      "    load_throughput: 40602.94\n",
      "    load_time_ms: 24.629\n",
      "    sample_throughput: 44.755\n",
      "    sample_time_ms: 22343.914\n",
      "    update_time_ms: 2.441\n",
      "  timestamp: 1635080250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 690\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   690</td><td style=\"text-align: right;\">         17371.5</td><td style=\"text-align: right;\">690000</td><td style=\"text-align: right;\"> -3.0647</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            306.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 691000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-57-53\n",
      "  done: false\n",
      "  episode_len_mean: 306.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0611999999999777\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2379\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6969117638137605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005544989006634498\n",
      "          policy_loss: 0.04020382389426232\n",
      "          total_loss: 0.04101318990190824\n",
      "          vf_explained_var: 0.5732313990592957\n",
      "          vf_loss: 0.00777847981468464\n",
      "    num_agent_steps_sampled: 691000\n",
      "    num_agent_steps_trained: 691000\n",
      "    num_steps_sampled: 691000\n",
      "    num_steps_trained: 691000\n",
      "  iterations_since_restore: 691\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.79705882352941\n",
      "    ram_util_percent: 39.120588235294115\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863050321715288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.30964603655026\n",
      "    mean_inference_ms: 1.927636111117666\n",
      "    mean_raw_obs_processing_ms: 2.165123521861274\n",
      "  time_since_restore: 17394.878557682037\n",
      "  time_this_iter_s: 23.343533277511597\n",
      "  time_total_s: 17394.878557682037\n",
      "  timers:\n",
      "    learn_throughput: 1444.429\n",
      "    learn_time_ms: 692.315\n",
      "    load_throughput: 40891.156\n",
      "    load_time_ms: 24.455\n",
      "    sample_throughput: 44.297\n",
      "    sample_time_ms: 22574.821\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1635080273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 691000\n",
      "  training_iteration: 691\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   691</td><td style=\"text-align: right;\">         17394.9</td><td style=\"text-align: right;\">691000</td><td style=\"text-align: right;\"> -3.0612</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            306.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 692000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 305.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.056999999999978\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2383\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7543751259644826\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01320955316315633\n",
      "          policy_loss: -0.001638372325234943\n",
      "          total_loss: 0.002480197532309426\n",
      "          vf_explained_var: 0.3837321102619171\n",
      "          vf_loss: 0.011662316073973973\n",
      "    num_agent_steps_sampled: 692000\n",
      "    num_agent_steps_trained: 692000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "  iterations_since_restore: 692\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.82058823529411\n",
      "    ram_util_percent: 39.16470588235295\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038630258555487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.307848603376698\n",
      "    mean_inference_ms: 1.9276262133409818\n",
      "    mean_raw_obs_processing_ms: 2.1645937497982253\n",
      "  time_since_restore: 17418.573345422745\n",
      "  time_this_iter_s: 23.694787740707397\n",
      "  time_total_s: 17418.573345422745\n",
      "  timers:\n",
      "    learn_throughput: 1445.774\n",
      "    learn_time_ms: 691.671\n",
      "    load_throughput: 40876.491\n",
      "    load_time_ms: 24.464\n",
      "    sample_throughput: 43.418\n",
      "    sample_time_ms: 23031.984\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1635080297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 692\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   692</td><td style=\"text-align: right;\">         17418.6</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\">  -3.057</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">             305.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 693000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 304.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.043499999999978\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2387\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6965287817849053\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013129325121658693\n",
      "          policy_loss: 0.00868890459338824\n",
      "          total_loss: 0.015020179914103614\n",
      "          vf_explained_var: 0.23249395191669464\n",
      "          vf_loss: 0.013296556576258606\n",
      "    num_agent_steps_sampled: 693000\n",
      "    num_agent_steps_trained: 693000\n",
      "    num_steps_sampled: 693000\n",
      "    num_steps_trained: 693000\n",
      "  iterations_since_restore: 693\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.37027027027027\n",
      "    ram_util_percent: 39.25945945945946\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863003833895531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.30619065510749\n",
      "    mean_inference_ms: 1.927616885993681\n",
      "    mean_raw_obs_processing_ms: 2.164098951046185\n",
      "  time_since_restore: 17444.52857041359\n",
      "  time_this_iter_s: 25.955224990844727\n",
      "  time_total_s: 17444.52857041359\n",
      "  timers:\n",
      "    learn_throughput: 1447.654\n",
      "    learn_time_ms: 690.773\n",
      "    load_throughput: 41290.892\n",
      "    load_time_ms: 24.218\n",
      "    sample_throughput: 42.373\n",
      "    sample_time_ms: 23599.866\n",
      "    update_time_ms: 2.364\n",
      "  timestamp: 1635080323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 693000\n",
      "  training_iteration: 693\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   693</td><td style=\"text-align: right;\">         17444.5</td><td style=\"text-align: right;\">693000</td><td style=\"text-align: right;\"> -3.0435</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            304.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 694000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-59-09\n",
      "  done: false\n",
      "  episode_len_mean: 303.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.0314999999999794\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2391\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.627977141503093e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5147546976804733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004773034650999285\n",
      "          policy_loss: -0.0029113153202666177\n",
      "          total_loss: 0.006053624384933048\n",
      "          vf_explained_var: 0.24546708166599274\n",
      "          vf_loss: 0.014112484134319756\n",
      "    num_agent_steps_sampled: 694000\n",
      "    num_agent_steps_trained: 694000\n",
      "    num_steps_sampled: 694000\n",
      "    num_steps_trained: 694000\n",
      "  iterations_since_restore: 694\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06578947368421\n",
      "    ram_util_percent: 39.21842105263159\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862984332098268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.304706906639012\n",
      "    mean_inference_ms: 1.927608534272525\n",
      "    mean_raw_obs_processing_ms: 2.163609791548207\n",
      "  time_since_restore: 17471.094976902008\n",
      "  time_this_iter_s: 26.56640648841858\n",
      "  time_total_s: 17471.094976902008\n",
      "  timers:\n",
      "    learn_throughput: 1446.427\n",
      "    learn_time_ms: 691.359\n",
      "    load_throughput: 41413.2\n",
      "    load_time_ms: 24.147\n",
      "    sample_throughput: 41.706\n",
      "    sample_time_ms: 23977.292\n",
      "    update_time_ms: 2.59\n",
      "  timestamp: 1635080349\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 694000\n",
      "  training_iteration: 694\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   694</td><td style=\"text-align: right;\">         17471.1</td><td style=\"text-align: right;\">694000</td><td style=\"text-align: right;\"> -3.0315</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            303.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 695000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_12-59-34\n",
      "  done: false\n",
      "  episode_len_mean: 302.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.027199999999979\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2394\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3139885707515464e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7030450287792418\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008071935710014107\n",
      "          policy_loss: -0.09495823267433379\n",
      "          total_loss: -0.08701351053184933\n",
      "          vf_explained_var: 0.20368225872516632\n",
      "          vf_loss: 0.014975172881450918\n",
      "    num_agent_steps_sampled: 695000\n",
      "    num_agent_steps_trained: 695000\n",
      "    num_steps_sampled: 695000\n",
      "    num_steps_trained: 695000\n",
      "  iterations_since_restore: 695\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.127777777777766\n",
      "    ram_util_percent: 39.211111111111116\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386297060658223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.303601306279713\n",
      "    mean_inference_ms: 1.927602239428629\n",
      "    mean_raw_obs_processing_ms: 2.1632394659643097\n",
      "  time_since_restore: 17496.347264528275\n",
      "  time_this_iter_s: 25.25228762626648\n",
      "  time_total_s: 17496.347264528275\n",
      "  timers:\n",
      "    learn_throughput: 1446.255\n",
      "    learn_time_ms: 691.441\n",
      "    load_throughput: 41499.493\n",
      "    load_time_ms: 24.097\n",
      "    sample_throughput: 41.329\n",
      "    sample_time_ms: 24196.042\n",
      "    update_time_ms: 2.581\n",
      "  timestamp: 1635080374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 695000\n",
      "  training_iteration: 695\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   695</td><td style=\"text-align: right;\">         17496.3</td><td style=\"text-align: right;\">695000</td><td style=\"text-align: right;\"> -3.0272</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            302.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-00-00\n",
      "  done: false\n",
      "  episode_len_mean: 302.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.0280999999999785\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2398\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3139885707515464e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.704253496726354\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005060121901322153\n",
      "          policy_loss: -0.01491486132144928\n",
      "          total_loss: -0.008544496612416373\n",
      "          vf_explained_var: 0.2807612121105194\n",
      "          vf_loss: 0.013412899503277408\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "  iterations_since_restore: 696\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.19444444444444\n",
      "    ram_util_percent: 39.172222222222224\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862952731566211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.30209069182623\n",
      "    mean_inference_ms: 1.9275937567144896\n",
      "    mean_raw_obs_processing_ms: 2.1627626077987387\n",
      "  time_since_restore: 17521.946415901184\n",
      "  time_this_iter_s: 25.599151372909546\n",
      "  time_total_s: 17521.946415901184\n",
      "  timers:\n",
      "    learn_throughput: 1444.091\n",
      "    learn_time_ms: 692.477\n",
      "    load_throughput: 41354.809\n",
      "    load_time_ms: 24.181\n",
      "    sample_throughput: 40.683\n",
      "    sample_time_ms: 24580.374\n",
      "    update_time_ms: 2.581\n",
      "  timestamp: 1635080400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 696\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   696</td><td style=\"text-align: right;\">         17521.9</td><td style=\"text-align: right;\">696000</td><td style=\"text-align: right;\"> -3.0281</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            302.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 697000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-00-45\n",
      "  done: false\n",
      "  episode_len_mean: 302.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -3.025799999999979\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2402\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3139885707515464e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6015043516953786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003046991804488286\n",
      "          policy_loss: -0.02757163950138622\n",
      "          total_loss: -0.02110407559408082\n",
      "          vf_explained_var: 0.37214502692222595\n",
      "          vf_loss: 0.01248260640228788\n",
      "    num_agent_steps_sampled: 697000\n",
      "    num_agent_steps_trained: 697000\n",
      "    num_steps_sampled: 697000\n",
      "    num_steps_trained: 697000\n",
      "  iterations_since_restore: 697\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.9734375\n",
      "    ram_util_percent: 39.048437500000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862938163448763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.300787251437104\n",
      "    mean_inference_ms: 1.9275858735517841\n",
      "    mean_raw_obs_processing_ms: 2.163250184064476\n",
      "  time_since_restore: 17566.677623987198\n",
      "  time_this_iter_s: 44.731208086013794\n",
      "  time_total_s: 17566.677623987198\n",
      "  timers:\n",
      "    learn_throughput: 1444.071\n",
      "    learn_time_ms: 692.487\n",
      "    load_throughput: 41740.64\n",
      "    load_time_ms: 23.957\n",
      "    sample_throughput: 37.118\n",
      "    sample_time_ms: 26941.037\n",
      "    update_time_ms: 2.562\n",
      "  timestamp: 1635080445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 697000\n",
      "  training_iteration: 697\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   697</td><td style=\"text-align: right;\">         17566.7</td><td style=\"text-align: right;\">697000</td><td style=\"text-align: right;\"> -3.0258</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            302.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 698000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-01-12\n",
      "  done: false\n",
      "  episode_len_mean: 300.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -3.00479999999998\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2406\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6569942853757732e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6959113912449942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0067895412525156\n",
      "          policy_loss: -0.08839452274971538\n",
      "          total_loss: -0.0774703679813279\n",
      "          vf_explained_var: 0.17896434664726257\n",
      "          vf_loss: 0.017883265908393595\n",
      "    num_agent_steps_sampled: 698000\n",
      "    num_agent_steps_trained: 698000\n",
      "    num_steps_sampled: 698000\n",
      "    num_steps_trained: 698000\n",
      "  iterations_since_restore: 698\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.04736842105263\n",
      "    ram_util_percent: 39.089473684210525\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038629248056221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.299713962832076\n",
      "    mean_inference_ms: 1.9275784021423283\n",
      "    mean_raw_obs_processing_ms: 2.163742722749554\n",
      "  time_since_restore: 17593.40560889244\n",
      "  time_this_iter_s: 26.72798490524292\n",
      "  time_total_s: 17593.40560889244\n",
      "  timers:\n",
      "    learn_throughput: 1445.095\n",
      "    learn_time_ms: 691.996\n",
      "    load_throughput: 41014.232\n",
      "    load_time_ms: 24.382\n",
      "    sample_throughput: 36.263\n",
      "    sample_time_ms: 27576.611\n",
      "    update_time_ms: 2.566\n",
      "  timestamp: 1635080472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 698000\n",
      "  training_iteration: 698\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   698</td><td style=\"text-align: right;\">         17593.4</td><td style=\"text-align: right;\">698000</td><td style=\"text-align: right;\"> -3.0048</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            300.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 699000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 299.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.9971999999999794\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2410\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6569942853757732e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.901026044289271\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010278168573454613\n",
      "          policy_loss: 0.02936441285742654\n",
      "          total_loss: 0.03318015866809421\n",
      "          vf_explained_var: 0.23886436223983765\n",
      "          vf_loss: 0.012826005400468906\n",
      "    num_agent_steps_sampled: 699000\n",
      "    num_agent_steps_trained: 699000\n",
      "    num_steps_sampled: 699000\n",
      "    num_steps_trained: 699000\n",
      "  iterations_since_restore: 699\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.95000000000001\n",
      "    ram_util_percent: 39.172222222222224\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386291116408791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.298771832306034\n",
      "    mean_inference_ms: 1.9275718666458783\n",
      "    mean_raw_obs_processing_ms: 2.163271896093317\n",
      "  time_since_restore: 17618.46417617798\n",
      "  time_this_iter_s: 25.05856728553772\n",
      "  time_total_s: 17618.46417617798\n",
      "  timers:\n",
      "    learn_throughput: 1445.416\n",
      "    learn_time_ms: 691.843\n",
      "    load_throughput: 41144.507\n",
      "    load_time_ms: 24.305\n",
      "    sample_throughput: 38.292\n",
      "    sample_time_ms: 26114.882\n",
      "    update_time_ms: 2.558\n",
      "  timestamp: 1635080497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 699000\n",
      "  training_iteration: 699\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   699</td><td style=\"text-align: right;\">         17618.5</td><td style=\"text-align: right;\">699000</td><td style=\"text-align: right;\"> -2.9972</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            299.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 298.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.98679999999998\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2414\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6569942853757732e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.795767472518815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013556857693162092\n",
      "          policy_loss: 0.055104067342148885\n",
      "          total_loss: 0.057813585963514116\n",
      "          vf_explained_var: 0.3915731608867645\n",
      "          vf_loss: 0.010667190897381967\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_agent_steps_trained: 700000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 700\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.825714285714284\n",
      "    ram_util_percent: 39.22571428571429\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862897006533597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.297807745311616\n",
      "    mean_inference_ms: 1.9275655022422635\n",
      "    mean_raw_obs_processing_ms: 2.162838396202339\n",
      "  time_since_restore: 17642.960032463074\n",
      "  time_this_iter_s: 24.495856285095215\n",
      "  time_total_s: 17642.960032463074\n",
      "  timers:\n",
      "    learn_throughput: 1446.208\n",
      "    learn_time_ms: 691.464\n",
      "    load_throughput: 41245.009\n",
      "    load_time_ms: 24.245\n",
      "    sample_throughput: 37.849\n",
      "    sample_time_ms: 26420.811\n",
      "    update_time_ms: 2.551\n",
      "  timestamp: 1635080521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 700\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   700</td><td style=\"text-align: right;\">           17643</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\"> -2.9868</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            298.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 701000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 297.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.9701999999999806\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2418\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6569942853757732e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.620966363284323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01643206703052158\n",
      "          policy_loss: 0.04568207272224956\n",
      "          total_loss: 0.051875877711508006\n",
      "          vf_explained_var: 0.3136634826660156\n",
      "          vf_loss: 0.012403462651289172\n",
      "    num_agent_steps_sampled: 701000\n",
      "    num_agent_steps_trained: 701000\n",
      "    num_steps_sampled: 701000\n",
      "    num_steps_trained: 701000\n",
      "  iterations_since_restore: 701\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.068421052631585\n",
      "    ram_util_percent: 39.22105263157896\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038628830633847644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.297084610258494\n",
      "    mean_inference_ms: 1.9275595363746418\n",
      "    mean_raw_obs_processing_ms: 2.1624116318989963\n",
      "  time_since_restore: 17669.640095949173\n",
      "  time_this_iter_s: 26.680063486099243\n",
      "  time_total_s: 17669.640095949173\n",
      "  timers:\n",
      "    learn_throughput: 1448.58\n",
      "    learn_time_ms: 690.331\n",
      "    load_throughput: 41299.186\n",
      "    load_time_ms: 24.214\n",
      "    sample_throughput: 37.375\n",
      "    sample_time_ms: 26755.647\n",
      "    update_time_ms: 2.566\n",
      "  timestamp: 1635080548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 701000\n",
      "  training_iteration: 701\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   701</td><td style=\"text-align: right;\">         17669.6</td><td style=\"text-align: right;\">701000</td><td style=\"text-align: right;\"> -2.9702</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            297.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 702000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-02-54\n",
      "  done: false\n",
      "  episode_len_mean: 296.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.963599999999981\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2422\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6569942853757732e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5537225643793742\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012698087462744449\n",
      "          policy_loss: 0.006350513382090463\n",
      "          total_loss: 0.012638420363267263\n",
      "          vf_explained_var: 0.40571945905685425\n",
      "          vf_loss: 0.01182512915175822\n",
      "    num_agent_steps_sampled: 702000\n",
      "    num_agent_steps_trained: 702000\n",
      "    num_steps_sampled: 702000\n",
      "    num_steps_trained: 702000\n",
      "  iterations_since_restore: 702\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.48918918918919\n",
      "    ram_util_percent: 39.213513513513526\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038628677875426426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.29650672737316\n",
      "    mean_inference_ms: 1.9275540889235352\n",
      "    mean_raw_obs_processing_ms: 2.1620204233076574\n",
      "  time_since_restore: 17695.804518699646\n",
      "  time_this_iter_s: 26.164422750473022\n",
      "  time_total_s: 17695.804518699646\n",
      "  timers:\n",
      "    learn_throughput: 1449.106\n",
      "    learn_time_ms: 690.081\n",
      "    load_throughput: 41136.437\n",
      "    load_time_ms: 24.309\n",
      "    sample_throughput: 37.033\n",
      "    sample_time_ms: 27002.785\n",
      "    update_time_ms: 2.553\n",
      "  timestamp: 1635080574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 702000\n",
      "  training_iteration: 702\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   702</td><td style=\"text-align: right;\">         17695.8</td><td style=\"text-align: right;\">702000</td><td style=\"text-align: right;\"> -2.9636</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            296.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 703000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 296.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.962199999999981\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2425\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6569942853757732e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8536308252149158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04945867115530389\n",
      "          policy_loss: 0.03472631739245521\n",
      "          total_loss: 0.035186825692653655\n",
      "          vf_explained_var: 0.5514708161354065\n",
      "          vf_loss: 0.008996808169952904\n",
      "    num_agent_steps_sampled: 703000\n",
      "    num_agent_steps_trained: 703000\n",
      "    num_steps_sampled: 703000\n",
      "    num_steps_trained: 703000\n",
      "  iterations_since_restore: 703\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.76176470588236\n",
      "    ram_util_percent: 39.194117647058825\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862857392903604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.296118158340292\n",
      "    mean_inference_ms: 1.9275504019405147\n",
      "    mean_raw_obs_processing_ms: 2.16171588902037\n",
      "  time_since_restore: 17719.041830062866\n",
      "  time_this_iter_s: 23.237311363220215\n",
      "  time_total_s: 17719.041830062866\n",
      "  timers:\n",
      "    learn_throughput: 1445.408\n",
      "    learn_time_ms: 691.846\n",
      "    load_throughput: 41280.894\n",
      "    load_time_ms: 24.224\n",
      "    sample_throughput: 37.412\n",
      "    sample_time_ms: 26729.292\n",
      "    update_time_ms: 2.584\n",
      "  timestamp: 1635080597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 703000\n",
      "  training_iteration: 703\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   703</td><td style=\"text-align: right;\">           17719</td><td style=\"text-align: right;\">703000</td><td style=\"text-align: right;\"> -2.9622</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            296.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 293.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.9359999999999813\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2428\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4854914280636605e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8515385760201348\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03253050042402587\n",
      "          policy_loss: -0.10129815580116378\n",
      "          total_loss: -0.09793127311600579\n",
      "          vf_explained_var: 0.4969036877155304\n",
      "          vf_loss: 0.011882261735283665\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "  iterations_since_restore: 704\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.614285714285714\n",
      "    ram_util_percent: 39.182857142857145\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862849064722215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.29594994527005\n",
      "    mean_inference_ms: 1.9275474120641058\n",
      "    mean_raw_obs_processing_ms: 2.161416223844828\n",
      "  time_since_restore: 17743.634177207947\n",
      "  time_this_iter_s: 24.592347145080566\n",
      "  time_total_s: 17743.634177207947\n",
      "  timers:\n",
      "    learn_throughput: 1448.254\n",
      "    learn_time_ms: 690.487\n",
      "    load_throughput: 41408.907\n",
      "    load_time_ms: 24.149\n",
      "    sample_throughput: 37.688\n",
      "    sample_time_ms: 26533.463\n",
      "    update_time_ms: 2.441\n",
      "  timestamp: 1635080622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 704\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   704</td><td style=\"text-align: right;\">         17743.6</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\">  -2.936</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">             293.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 705000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-04-21\n",
      "  done: false\n",
      "  episode_len_mean: 294.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.9432999999999807\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2431\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.728237142095491e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3508150378863018\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03564669454781987\n",
      "          policy_loss: -0.10443119046588739\n",
      "          total_loss: -0.10566044273889727\n",
      "          vf_explained_var: 0.547081470489502\n",
      "          vf_loss: 0.01227888990090125\n",
      "    num_agent_steps_sampled: 705000\n",
      "    num_agent_steps_trained: 705000\n",
      "    num_steps_sampled: 705000\n",
      "    num_steps_trained: 705000\n",
      "  iterations_since_restore: 705\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.56964285714286\n",
      "    ram_util_percent: 39.001785714285724\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862839954799608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.29589204136365\n",
      "    mean_inference_ms: 1.9275456102779287\n",
      "    mean_raw_obs_processing_ms: 2.1618701595350864\n",
      "  time_since_restore: 17783.026244163513\n",
      "  time_this_iter_s: 39.392066955566406\n",
      "  time_total_s: 17783.026244163513\n",
      "  timers:\n",
      "    learn_throughput: 1448.31\n",
      "    learn_time_ms: 690.46\n",
      "    load_throughput: 41224.335\n",
      "    load_time_ms: 24.258\n",
      "    sample_throughput: 35.782\n",
      "    sample_time_ms: 27947.353\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1635080661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 705000\n",
      "  training_iteration: 705\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   705</td><td style=\"text-align: right;\">           17783</td><td style=\"text-align: right;\">705000</td><td style=\"text-align: right;\"> -2.9433</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            294.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 706000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-04-42\n",
      "  done: false\n",
      "  episode_len_mean: 297.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.9729999999999808\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2434\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.592355713143238e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6185429679022896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.048299672645290455\n",
      "          policy_loss: -0.010106225146187676\n",
      "          total_loss: -0.015906264384587605\n",
      "          vf_explained_var: 0.58631831407547\n",
      "          vf_loss: 0.010385363101441827\n",
      "    num_agent_steps_sampled: 706000\n",
      "    num_agent_steps_trained: 706000\n",
      "    num_steps_sampled: 706000\n",
      "    num_steps_trained: 706000\n",
      "  iterations_since_restore: 706\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.13103448275861\n",
      "    ram_util_percent: 39.16206896551724\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862830591283521\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.29575194405757\n",
      "    mean_inference_ms: 1.9275442777173242\n",
      "    mean_raw_obs_processing_ms: 2.162297710632739\n",
      "  time_since_restore: 17803.160850048065\n",
      "  time_this_iter_s: 20.134605884552002\n",
      "  time_total_s: 17803.160850048065\n",
      "  timers:\n",
      "    learn_throughput: 1451.24\n",
      "    learn_time_ms: 689.066\n",
      "    load_throughput: 41427.966\n",
      "    load_time_ms: 24.138\n",
      "    sample_throughput: 36.493\n",
      "    sample_time_ms: 27402.389\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1635080682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 706000\n",
      "  training_iteration: 706\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   706</td><td style=\"text-align: right;\">         17803.2</td><td style=\"text-align: right;\">706000</td><td style=\"text-align: right;\">  -2.973</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">             297.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 707000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-05-05\n",
      "  done: false\n",
      "  episode_len_mean: 296.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.9643999999999813\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2437\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.388533569714854e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1129027260674371\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.054620100640506306\n",
      "          policy_loss: 0.03148876906683048\n",
      "          total_loss: 0.02819307204335928\n",
      "          vf_explained_var: 0.7123189568519592\n",
      "          vf_loss: 0.007833289297478688\n",
      "    num_agent_steps_sampled: 707000\n",
      "    num_agent_steps_trained: 707000\n",
      "    num_steps_sampled: 707000\n",
      "    num_steps_trained: 707000\n",
      "  iterations_since_restore: 707\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.57272727272728\n",
      "    ram_util_percent: 39.1060606060606\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862823162211804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.29572374153182\n",
      "    mean_inference_ms: 1.9275437478837563\n",
      "    mean_raw_obs_processing_ms: 2.162728057230768\n",
      "  time_since_restore: 17826.479234218597\n",
      "  time_this_iter_s: 23.318384170532227\n",
      "  time_total_s: 17826.479234218597\n",
      "  timers:\n",
      "    learn_throughput: 1450.754\n",
      "    learn_time_ms: 689.297\n",
      "    load_throughput: 41334.513\n",
      "    load_time_ms: 24.193\n",
      "    sample_throughput: 39.587\n",
      "    sample_time_ms: 25260.758\n",
      "    update_time_ms: 2.525\n",
      "  timestamp: 1635080705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 707000\n",
      "  training_iteration: 707\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   707</td><td style=\"text-align: right;\">         17826.5</td><td style=\"text-align: right;\">707000</td><td style=\"text-align: right;\"> -2.9644</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            296.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 708000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-05-30\n",
      "  done: false\n",
      "  episode_len_mean: 293.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.930899999999981\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2441\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2582800354572282e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8543703059355418\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016300940532316035\n",
      "          policy_loss: -0.004616777764426337\n",
      "          total_loss: -0.0012238702840275235\n",
      "          vf_explained_var: 0.4531201720237732\n",
      "          vf_loss: 0.011936595099460748\n",
      "    num_agent_steps_sampled: 708000\n",
      "    num_agent_steps_trained: 708000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "  iterations_since_restore: 708\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.85555555555556\n",
      "    ram_util_percent: 39.12222222222221\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862812442239373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.296043139211744\n",
      "    mean_inference_ms: 1.927543935667167\n",
      "    mean_raw_obs_processing_ms: 2.1628389378302693\n",
      "  time_since_restore: 17851.44430208206\n",
      "  time_this_iter_s: 24.965067863464355\n",
      "  time_total_s: 17851.44430208206\n",
      "  timers:\n",
      "    learn_throughput: 1449.236\n",
      "    learn_time_ms: 690.019\n",
      "    load_throughput: 41488.738\n",
      "    load_time_ms: 24.103\n",
      "    sample_throughput: 39.866\n",
      "    sample_time_ms: 25083.853\n",
      "    update_time_ms: 2.517\n",
      "  timestamp: 1635080730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 708\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   708</td><td style=\"text-align: right;\">         17851.4</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\"> -2.9309</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            293.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 709000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-05-55\n",
      "  done: false\n",
      "  episode_len_mean: 290.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.907299999999982\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2444\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2582800354572282e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.781556557946735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023089114844715177\n",
      "          policy_loss: -0.10047683094938596\n",
      "          total_loss: -0.09604982940687073\n",
      "          vf_explained_var: 0.30362287163734436\n",
      "          vf_loss: 0.012242542621162202\n",
      "    num_agent_steps_sampled: 709000\n",
      "    num_agent_steps_trained: 709000\n",
      "    num_steps_sampled: 709000\n",
      "    num_steps_trained: 709000\n",
      "  iterations_since_restore: 709\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.89166666666668\n",
      "    ram_util_percent: 39.18055555555556\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862806147494353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.296487274426728\n",
      "    mean_inference_ms: 1.9275442914293572\n",
      "    mean_raw_obs_processing_ms: 2.162601919209118\n",
      "  time_since_restore: 17876.682277679443\n",
      "  time_this_iter_s: 25.237975597381592\n",
      "  time_total_s: 17876.682277679443\n",
      "  timers:\n",
      "    learn_throughput: 1447.201\n",
      "    learn_time_ms: 690.989\n",
      "    load_throughput: 41310.656\n",
      "    load_time_ms: 24.207\n",
      "    sample_throughput: 39.84\n",
      "    sample_time_ms: 25100.709\n",
      "    update_time_ms: 2.525\n",
      "  timestamp: 1635080755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 709000\n",
      "  training_iteration: 709\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   709</td><td style=\"text-align: right;\">         17876.7</td><td style=\"text-align: right;\">709000</td><td style=\"text-align: right;\"> -2.9073</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            290.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 710000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-06-21\n",
      "  done: false\n",
      "  episode_len_mean: 288.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.8840999999999815\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2448\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8874200531858421e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6374001834127638\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005078672628736077\n",
      "          policy_loss: -0.06152964954574903\n",
      "          total_loss: -0.056399935401148266\n",
      "          vf_explained_var: 0.4003075361251831\n",
      "          vf_loss: 0.01150371019418041\n",
      "    num_agent_steps_sampled: 710000\n",
      "    num_agent_steps_trained: 710000\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "  iterations_since_restore: 710\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.07567567567568\n",
      "    ram_util_percent: 39.18108108108109\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862795301199341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.29734590477237\n",
      "    mean_inference_ms: 1.9275455272720523\n",
      "    mean_raw_obs_processing_ms: 2.1622635434194386\n",
      "  time_since_restore: 17902.964955806732\n",
      "  time_this_iter_s: 26.28267812728882\n",
      "  time_total_s: 17902.964955806732\n",
      "  timers:\n",
      "    learn_throughput: 1448.825\n",
      "    learn_time_ms: 690.215\n",
      "    load_throughput: 41080.795\n",
      "    load_time_ms: 24.342\n",
      "    sample_throughput: 39.557\n",
      "    sample_time_ms: 25280.015\n",
      "    update_time_ms: 2.534\n",
      "  timestamp: 1635080781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 710\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   710</td><td style=\"text-align: right;\">           17903</td><td style=\"text-align: right;\">710000</td><td style=\"text-align: right;\"> -2.8841</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            288.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 711000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-06-48\n",
      "  done: false\n",
      "  episode_len_mean: 283.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.8395999999999835\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2452\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8874200531858421e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7278489132722219\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0074352294731221645\n",
      "          policy_loss: -0.03662136714491579\n",
      "          total_loss: -0.031206702234016524\n",
      "          vf_explained_var: 0.187672421336174\n",
      "          vf_loss: 0.012693140769584312\n",
      "    num_agent_steps_sampled: 711000\n",
      "    num_agent_steps_trained: 711000\n",
      "    num_steps_sampled: 711000\n",
      "    num_steps_trained: 711000\n",
      "  iterations_since_restore: 711\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.23589743589743\n",
      "    ram_util_percent: 39.18974358974361\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038627854072021794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.298756441031333\n",
      "    mean_inference_ms: 1.927548281267779\n",
      "    mean_raw_obs_processing_ms: 2.162019117149614\n",
      "  time_since_restore: 17929.72139120102\n",
      "  time_this_iter_s: 26.75643539428711\n",
      "  time_total_s: 17929.72139120102\n",
      "  timers:\n",
      "    learn_throughput: 1446.524\n",
      "    learn_time_ms: 691.312\n",
      "    load_throughput: 41344.496\n",
      "    load_time_ms: 24.187\n",
      "    sample_throughput: 39.546\n",
      "    sample_time_ms: 25286.702\n",
      "    update_time_ms: 2.533\n",
      "  timestamp: 1635080808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 711000\n",
      "  training_iteration: 711\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   711</td><td style=\"text-align: right;\">         17929.7</td><td style=\"text-align: right;\">711000</td><td style=\"text-align: right;\"> -2.8396</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            283.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-07-15\n",
      "  done: false\n",
      "  episode_len_mean: 280.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.8027999999999844\n",
      "  episode_reward_min: -4.469999999999949\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2457\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8874200531858421e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.620860552125507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0068663210978544085\n",
      "          policy_loss: -0.04031473985976643\n",
      "          total_loss: -0.03227109354403284\n",
      "          vf_explained_var: 0.2390076369047165\n",
      "          vf_loss: 0.014252242130330868\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "  iterations_since_restore: 712\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.35\n",
      "    ram_util_percent: 39.123684210526314\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038627674024583204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.300927272626026\n",
      "    mean_inference_ms: 1.9275522057218768\n",
      "    mean_raw_obs_processing_ms: 2.1617632828796576\n",
      "  time_since_restore: 17956.903207063675\n",
      "  time_this_iter_s: 27.18181586265564\n",
      "  time_total_s: 17956.903207063675\n",
      "  timers:\n",
      "    learn_throughput: 1446.88\n",
      "    learn_time_ms: 691.142\n",
      "    load_throughput: 41254.421\n",
      "    load_time_ms: 24.24\n",
      "    sample_throughput: 39.388\n",
      "    sample_time_ms: 25388.564\n",
      "    update_time_ms: 2.535\n",
      "  timestamp: 1635080835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 712\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   712</td><td style=\"text-align: right;\">         17956.9</td><td style=\"text-align: right;\">712000</td><td style=\"text-align: right;\"> -2.8028</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -4.47</td><td style=\"text-align: right;\">            280.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 713000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 277.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.770299999999984\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2461\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8874200531858421e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5614939981036716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003784464807220742\n",
      "          policy_loss: -0.024837554080618753\n",
      "          total_loss: -0.019628986881838905\n",
      "          vf_explained_var: 0.3001583218574524\n",
      "          vf_loss: 0.010823501977655622\n",
      "    num_agent_steps_sampled: 713000\n",
      "    num_agent_steps_trained: 713000\n",
      "    num_steps_sampled: 713000\n",
      "    num_steps_trained: 713000\n",
      "  iterations_since_restore: 713\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.6921875\n",
      "    ram_util_percent: 39.04843749999999\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862752087957819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.302969824350843\n",
      "    mean_inference_ms: 1.9275556627838486\n",
      "    mean_raw_obs_processing_ms: 2.1625392883586976\n",
      "  time_since_restore: 18001.475029706955\n",
      "  time_this_iter_s: 44.57182264328003\n",
      "  time_total_s: 18001.475029706955\n",
      "  timers:\n",
      "    learn_throughput: 1445.414\n",
      "    learn_time_ms: 691.843\n",
      "    load_throughput: 40865.379\n",
      "    load_time_ms: 24.471\n",
      "    sample_throughput: 36.336\n",
      "    sample_time_ms: 27520.981\n",
      "    update_time_ms: 2.608\n",
      "  timestamp: 1635080880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 713000\n",
      "  training_iteration: 713\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   713</td><td style=\"text-align: right;\">         18001.5</td><td style=\"text-align: right;\">713000</td><td style=\"text-align: right;\"> -2.7703</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            277.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 714000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-08-27\n",
      "  done: false\n",
      "  episode_len_mean: 274.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.743299999999985\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2465\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.437100265929211e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5153956224521001\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03831116796654628\n",
      "          policy_loss: 0.009276993324359257\n",
      "          total_loss: 0.015418316216932402\n",
      "          vf_explained_var: 0.22047074139118195\n",
      "          vf_loss: 0.011295245743046205\n",
      "    num_agent_steps_sampled: 714000\n",
      "    num_agent_steps_trained: 714000\n",
      "    num_steps_sampled: 714000\n",
      "    num_steps_trained: 714000\n",
      "  iterations_since_restore: 714\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.717948717948715\n",
      "    ram_util_percent: 39.105128205128196\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862736456975431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.305370110221975\n",
      "    mean_inference_ms: 1.9275595796097071\n",
      "    mean_raw_obs_processing_ms: 2.1633482738960113\n",
      "  time_since_restore: 18028.9253013134\n",
      "  time_this_iter_s: 27.450271606445312\n",
      "  time_total_s: 18028.9253013134\n",
      "  timers:\n",
      "    learn_throughput: 1442.531\n",
      "    learn_time_ms: 693.226\n",
      "    load_throughput: 40886.014\n",
      "    load_time_ms: 24.458\n",
      "    sample_throughput: 35.964\n",
      "    sample_time_ms: 27805.479\n",
      "    update_time_ms: 2.524\n",
      "  timestamp: 1635080907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 714000\n",
      "  training_iteration: 714\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   714</td><td style=\"text-align: right;\">         18028.9</td><td style=\"text-align: right;\">714000</td><td style=\"text-align: right;\"> -2.7433</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            274.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 715000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 271.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.710199999999986\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2469\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4155650398893818e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.37339746952056885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010865104747526604\n",
      "          policy_loss: 0.009880199159185091\n",
      "          total_loss: 0.017667896176377932\n",
      "          vf_explained_var: 0.11281871050596237\n",
      "          vf_loss: 0.011521660526179605\n",
      "    num_agent_steps_sampled: 715000\n",
      "    num_agent_steps_trained: 715000\n",
      "    num_steps_sampled: 715000\n",
      "    num_steps_trained: 715000\n",
      "  iterations_since_restore: 715\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.55641025641025\n",
      "    ram_util_percent: 39.18461538461538\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862720759459856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.30818126290267\n",
      "    mean_inference_ms: 1.9275643887145832\n",
      "    mean_raw_obs_processing_ms: 2.1641902489377323\n",
      "  time_since_restore: 18056.280999183655\n",
      "  time_this_iter_s: 27.355697870254517\n",
      "  time_total_s: 18056.280999183655\n",
      "  timers:\n",
      "    learn_throughput: 1444.454\n",
      "    learn_time_ms: 692.303\n",
      "    load_throughput: 41714.611\n",
      "    load_time_ms: 23.972\n",
      "    sample_throughput: 37.589\n",
      "    sample_time_ms: 26603.176\n",
      "    update_time_ms: 2.598\n",
      "  timestamp: 1635080935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 715000\n",
      "  training_iteration: 715\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   715</td><td style=\"text-align: right;\">         18056.3</td><td style=\"text-align: right;\">715000</td><td style=\"text-align: right;\"> -2.7102</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            271.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 716000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-09-20\n",
      "  done: false\n",
      "  episode_len_mean: 268.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.687299999999987\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2473\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4155650398893818e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.43092196153269874\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007834448693397198\n",
      "          policy_loss: 0.030897550367646746\n",
      "          total_loss: 0.03823306866818004\n",
      "          vf_explained_var: 0.26243579387664795\n",
      "          vf_loss: 0.011644729050911136\n",
      "    num_agent_steps_sampled: 716000\n",
      "    num_agent_steps_trained: 716000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "  iterations_since_restore: 716\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.648648648648646\n",
      "    ram_util_percent: 39.18378378378379\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862705499647229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.311199800832426\n",
      "    mean_inference_ms: 1.9275696899549166\n",
      "    mean_raw_obs_processing_ms: 2.1641149994210913\n",
      "  time_since_restore: 18081.712566375732\n",
      "  time_this_iter_s: 25.431567192077637\n",
      "  time_total_s: 18081.712566375732\n",
      "  timers:\n",
      "    learn_throughput: 1444.099\n",
      "    learn_time_ms: 692.473\n",
      "    load_throughput: 41699.39\n",
      "    load_time_ms: 23.981\n",
      "    sample_throughput: 36.856\n",
      "    sample_time_ms: 27132.703\n",
      "    update_time_ms: 2.598\n",
      "  timestamp: 1635080960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 716\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   716</td><td style=\"text-align: right;\">         18081.7</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\"> -2.6873</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            268.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 717000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 265.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.658899999999987\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2477\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4155650398893818e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.37498671578036413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0068055665615753425\n",
      "          policy_loss: 0.040349290354384316\n",
      "          total_loss: 0.04640930742025375\n",
      "          vf_explained_var: 0.20643024146556854\n",
      "          vf_loss: 0.00980987705083357\n",
      "    num_agent_steps_sampled: 717000\n",
      "    num_agent_steps_trained: 717000\n",
      "    num_steps_sampled: 717000\n",
      "    num_steps_trained: 717000\n",
      "  iterations_since_restore: 717\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.02820512820513\n",
      "    ram_util_percent: 39.21538461538463\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862691095942866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.31452093456598\n",
      "    mean_inference_ms: 1.9275754752046834\n",
      "    mean_raw_obs_processing_ms: 2.164043994993419\n",
      "  time_since_restore: 18109.092938661575\n",
      "  time_this_iter_s: 27.380372285842896\n",
      "  time_total_s: 18109.092938661575\n",
      "  timers:\n",
      "    learn_throughput: 1443.973\n",
      "    learn_time_ms: 692.534\n",
      "    load_throughput: 41658.057\n",
      "    load_time_ms: 24.005\n",
      "    sample_throughput: 36.312\n",
      "    sample_time_ms: 27538.786\n",
      "    update_time_ms: 2.623\n",
      "  timestamp: 1635080988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 717000\n",
      "  training_iteration: 717\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   717</td><td style=\"text-align: right;\">         18109.1</td><td style=\"text-align: right;\">717000</td><td style=\"text-align: right;\"> -2.6589</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            265.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 718000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-10-14\n",
      "  done: false\n",
      "  episode_len_mean: 264.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.6420999999999872\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2481\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4155650398893818e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.46363496018780603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007347821646433298\n",
      "          policy_loss: 0.007460074126720428\n",
      "          total_loss: 0.016665391623973846\n",
      "          vf_explained_var: 0.08292820304632187\n",
      "          vf_loss: 0.013841661852267054\n",
      "    num_agent_steps_sampled: 718000\n",
      "    num_agent_steps_trained: 718000\n",
      "    num_steps_sampled: 718000\n",
      "    num_steps_trained: 718000\n",
      "  iterations_since_restore: 718\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.31315789473685\n",
      "    ram_util_percent: 39.21578947368421\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862677803946415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.318013063712485\n",
      "    mean_inference_ms: 1.92758155294299\n",
      "    mean_raw_obs_processing_ms: 2.164006509119702\n",
      "  time_since_restore: 18135.835740566254\n",
      "  time_this_iter_s: 26.742801904678345\n",
      "  time_total_s: 18135.835740566254\n",
      "  timers:\n",
      "    learn_throughput: 1442.66\n",
      "    learn_time_ms: 693.164\n",
      "    load_throughput: 41580.871\n",
      "    load_time_ms: 24.05\n",
      "    sample_throughput: 36.08\n",
      "    sample_time_ms: 27715.873\n",
      "    update_time_ms: 2.63\n",
      "  timestamp: 1635081014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 718000\n",
      "  training_iteration: 718\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   718</td><td style=\"text-align: right;\">         18135.8</td><td style=\"text-align: right;\">718000</td><td style=\"text-align: right;\"> -2.6421</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            264.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 719000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-10-43\n",
      "  done: false\n",
      "  episode_len_mean: 262.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.159999999999998\n",
      "  episode_reward_mean: -2.628999999999988\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2485\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4155650398893818e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3484570168786579\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0018087690335519824\n",
      "          policy_loss: -0.1055647258957227\n",
      "          total_loss: -0.09374508915676011\n",
      "          vf_explained_var: 0.11918091028928757\n",
      "          vf_loss: 0.015304205618384812\n",
      "    num_agent_steps_sampled: 719000\n",
      "    num_agent_steps_trained: 719000\n",
      "    num_steps_sampled: 719000\n",
      "    num_steps_trained: 719000\n",
      "  iterations_since_restore: 719\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.599999999999994\n",
      "    ram_util_percent: 39.2219512195122\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862665923049919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.321705122735803\n",
      "    mean_inference_ms: 1.9275878786806364\n",
      "    mean_raw_obs_processing_ms: 2.1639742890492473\n",
      "  time_since_restore: 18164.441400051117\n",
      "  time_this_iter_s: 28.60565948486328\n",
      "  time_total_s: 18164.441400051117\n",
      "  timers:\n",
      "    learn_throughput: 1441.823\n",
      "    learn_time_ms: 693.567\n",
      "    load_throughput: 41632.462\n",
      "    load_time_ms: 24.02\n",
      "    sample_throughput: 35.648\n",
      "    sample_time_ms: 28052.284\n",
      "    update_time_ms: 2.622\n",
      "  timestamp: 1635081043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 719000\n",
      "  training_iteration: 719\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   719</td><td style=\"text-align: right;\">         18164.4</td><td style=\"text-align: right;\">719000</td><td style=\"text-align: right;\">  -2.629</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">             262.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-11-26\n",
      "  done: false\n",
      "  episode_len_mean: 262.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9900000000000015\n",
      "  episode_reward_mean: -2.6208999999999882\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2490\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.077825199446909e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3415470924642351\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001668818060844362\n",
      "          policy_loss: 0.00584245953294966\n",
      "          total_loss: 0.01532142899102635\n",
      "          vf_explained_var: 0.20323583483695984\n",
      "          vf_loss: 0.012894441860003604\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 720\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.55245901639344\n",
      "    ram_util_percent: 39.13278688524589\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862653306151669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.32631403342883\n",
      "    mean_inference_ms: 1.9275950219506566\n",
      "    mean_raw_obs_processing_ms: 2.1650957330613863\n",
      "  time_since_restore: 18207.608920812607\n",
      "  time_this_iter_s: 43.16752076148987\n",
      "  time_total_s: 18207.608920812607\n",
      "  timers:\n",
      "    learn_throughput: 1440.431\n",
      "    learn_time_ms: 694.237\n",
      "    load_throughput: 41560.27\n",
      "    load_time_ms: 24.061\n",
      "    sample_throughput: 33.625\n",
      "    sample_time_ms: 29740.052\n",
      "    update_time_ms: 2.619\n",
      "  timestamp: 1635081086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 720\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   720</td><td style=\"text-align: right;\">         18207.6</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\"> -2.6209</td><td style=\"text-align: right;\">               -1.99</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            262.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 721000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-11-56\n",
      "  done: false\n",
      "  episode_len_mean: 261.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9900000000000015\n",
      "  episode_reward_mean: -2.610099999999987\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2494\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5389125997234545e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4604280768169297\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03751031074204418\n",
      "          policy_loss: 0.052360063874059254\n",
      "          total_loss: 0.06068906966182921\n",
      "          vf_explained_var: 0.18021507561206818\n",
      "          vf_loss: 0.012933274420599142\n",
      "    num_agent_steps_sampled: 721000\n",
      "    num_agent_steps_trained: 721000\n",
      "    num_steps_sampled: 721000\n",
      "    num_steps_trained: 721000\n",
      "  iterations_since_restore: 721\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.181395348837206\n",
      "    ram_util_percent: 39.04651162790697\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862640782606256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.330192551749185\n",
      "    mean_inference_ms: 1.9276003860605369\n",
      "    mean_raw_obs_processing_ms: 2.1660135192899195\n",
      "  time_since_restore: 18237.336571455002\n",
      "  time_this_iter_s: 29.72765064239502\n",
      "  time_total_s: 18237.336571455002\n",
      "  timers:\n",
      "    learn_throughput: 1443.034\n",
      "    learn_time_ms: 692.984\n",
      "    load_throughput: 41003.246\n",
      "    load_time_ms: 24.388\n",
      "    sample_throughput: 33.291\n",
      "    sample_time_ms: 30038.111\n",
      "    update_time_ms: 2.609\n",
      "  timestamp: 1635081116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 721000\n",
      "  training_iteration: 721\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   721</td><td style=\"text-align: right;\">         18237.3</td><td style=\"text-align: right;\">721000</td><td style=\"text-align: right;\"> -2.6101</td><td style=\"text-align: right;\">               -1.99</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            261.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 722000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-12-23\n",
      "  done: false\n",
      "  episode_len_mean: 260.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9900000000000015\n",
      "  episode_reward_mean: -2.604199999999988\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2498\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.308368899585182e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4335237021247546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005912906228310375\n",
      "          policy_loss: -0.0007727026111549801\n",
      "          total_loss: 0.007414617389440536\n",
      "          vf_explained_var: 0.23549114167690277\n",
      "          vf_loss: 0.012522558153917392\n",
      "    num_agent_steps_sampled: 722000\n",
      "    num_agent_steps_trained: 722000\n",
      "    num_steps_sampled: 722000\n",
      "    num_steps_trained: 722000\n",
      "  iterations_since_restore: 722\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.07368421052632\n",
      "    ram_util_percent: 39.21842105263157\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862628479331105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.334102815080055\n",
      "    mean_inference_ms: 1.927606044181345\n",
      "    mean_raw_obs_processing_ms: 2.1669603666532833\n",
      "  time_since_restore: 18264.165282726288\n",
      "  time_this_iter_s: 26.82871127128601\n",
      "  time_total_s: 18264.165282726288\n",
      "  timers:\n",
      "    learn_throughput: 1444.309\n",
      "    learn_time_ms: 692.373\n",
      "    load_throughput: 41249.958\n",
      "    load_time_ms: 24.242\n",
      "    sample_throughput: 33.329\n",
      "    sample_time_ms: 30003.539\n",
      "    update_time_ms: 2.64\n",
      "  timestamp: 1635081143\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 722000\n",
      "  training_iteration: 722\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   722</td><td style=\"text-align: right;\">         18264.2</td><td style=\"text-align: right;\">722000</td><td style=\"text-align: right;\"> -2.6042</td><td style=\"text-align: right;\">               -1.99</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            260.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 723000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-12-51\n",
      "  done: false\n",
      "  episode_len_mean: 259.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9900000000000015\n",
      "  episode_reward_mean: -2.597399999999988\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2502\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.308368899585182e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2690825356377496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01381334788237029\n",
      "          policy_loss: -0.10509798460536533\n",
      "          total_loss: -0.0929246999323368\n",
      "          vf_explained_var: 0.11378144472837448\n",
      "          vf_loss: 0.014864101871434185\n",
      "    num_agent_steps_sampled: 723000\n",
      "    num_agent_steps_trained: 723000\n",
      "    num_steps_sampled: 723000\n",
      "    num_steps_trained: 723000\n",
      "  iterations_since_restore: 723\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.62\n",
      "    ram_util_percent: 39.2375\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862614102957187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.337995468288916\n",
      "    mean_inference_ms: 1.927611525572275\n",
      "    mean_raw_obs_processing_ms: 2.166950833969396\n",
      "  time_since_restore: 18292.146281957626\n",
      "  time_this_iter_s: 27.9809992313385\n",
      "  time_total_s: 18292.146281957626\n",
      "  timers:\n",
      "    learn_throughput: 1444.284\n",
      "    learn_time_ms: 692.384\n",
      "    load_throughput: 41182.725\n",
      "    load_time_ms: 24.282\n",
      "    sample_throughput: 35.28\n",
      "    sample_time_ms: 28344.531\n",
      "    update_time_ms: 2.537\n",
      "  timestamp: 1635081171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 723000\n",
      "  training_iteration: 723\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   723</td><td style=\"text-align: right;\">         18292.1</td><td style=\"text-align: right;\">723000</td><td style=\"text-align: right;\"> -2.5974</td><td style=\"text-align: right;\">               -1.99</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            259.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 724000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-13-18\n",
      "  done: false\n",
      "  episode_len_mean: 259.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9900000000000015\n",
      "  episode_reward_mean: -2.590299999999989\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2507\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.308368899585182e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3131838858127594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004519102367548713\n",
      "          policy_loss: 0.027957239747047426\n",
      "          total_loss: 0.03880959037277434\n",
      "          vf_explained_var: 0.29131975769996643\n",
      "          vf_loss: 0.013984187785536051\n",
      "    num_agent_steps_sampled: 724000\n",
      "    num_agent_steps_trained: 724000\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "  iterations_since_restore: 724\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.01538461538462\n",
      "    ram_util_percent: 39.258974358974356\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038625963535345725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.342857056383966\n",
      "    mean_inference_ms: 1.9276182699650395\n",
      "    mean_raw_obs_processing_ms: 2.166972982183271\n",
      "  time_since_restore: 18319.444810152054\n",
      "  time_this_iter_s: 27.29852819442749\n",
      "  time_total_s: 18319.444810152054\n",
      "  timers:\n",
      "    learn_throughput: 1444.251\n",
      "    learn_time_ms: 692.401\n",
      "    load_throughput: 40883.941\n",
      "    load_time_ms: 24.459\n",
      "    sample_throughput: 35.299\n",
      "    sample_time_ms: 28329.164\n",
      "    update_time_ms: 2.532\n",
      "  timestamp: 1635081198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 724\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   724</td><td style=\"text-align: right;\">         18319.4</td><td style=\"text-align: right;\">724000</td><td style=\"text-align: right;\"> -2.5903</td><td style=\"text-align: right;\">               -1.99</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            259.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 725000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-13-46\n",
      "  done: false\n",
      "  episode_len_mean: 257.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9900000000000015\n",
      "  episode_reward_mean: -2.574899999999989\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2511\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.654184449792591e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3636763580971294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.041348430501617434\n",
      "          policy_loss: 0.040273305442598134\n",
      "          total_loss: 0.049459459549850884\n",
      "          vf_explained_var: 0.12170927226543427\n",
      "          vf_loss: 0.012822910108500056\n",
      "    num_agent_steps_sampled: 725000\n",
      "    num_agent_steps_trained: 725000\n",
      "    num_steps_sampled: 725000\n",
      "    num_steps_trained: 725000\n",
      "  iterations_since_restore: 725\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.13249999999999\n",
      "    ram_util_percent: 39.26\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862581027569895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.34689521472425\n",
      "    mean_inference_ms: 1.9276232599498682\n",
      "    mean_raw_obs_processing_ms: 2.1669990899080425\n",
      "  time_since_restore: 18347.300659894943\n",
      "  time_this_iter_s: 27.855849742889404\n",
      "  time_total_s: 18347.300659894943\n",
      "  timers:\n",
      "    learn_throughput: 1441.884\n",
      "    learn_time_ms: 693.537\n",
      "    load_throughput: 40665.532\n",
      "    load_time_ms: 24.591\n",
      "    sample_throughput: 35.239\n",
      "    sample_time_ms: 28377.961\n",
      "    update_time_ms: 2.478\n",
      "  timestamp: 1635081226\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 725000\n",
      "  training_iteration: 725\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   725</td><td style=\"text-align: right;\">         18347.3</td><td style=\"text-align: right;\">725000</td><td style=\"text-align: right;\"> -2.5749</td><td style=\"text-align: right;\">               -1.99</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            257.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 726000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-14-15\n",
      "  done: false\n",
      "  episode_len_mean: 256.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9900000000000015\n",
      "  episode_reward_mean: -2.5608999999999895\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2515\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.981276674688887e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.28238687101337645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0018241385211248307\n",
      "          policy_loss: 0.00911423158314493\n",
      "          total_loss: 0.018973498377535078\n",
      "          vf_explained_var: 0.09580014646053314\n",
      "          vf_loss: 0.012683135653949447\n",
      "    num_agent_steps_sampled: 726000\n",
      "    num_agent_steps_trained: 726000\n",
      "    num_steps_sampled: 726000\n",
      "    num_steps_trained: 726000\n",
      "  iterations_since_restore: 726\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.16097560975609\n",
      "    ram_util_percent: 39.22439024390244\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862565170138189\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.351131928412933\n",
      "    mean_inference_ms: 1.9276285672927183\n",
      "    mean_raw_obs_processing_ms: 2.1670275481343193\n",
      "  time_since_restore: 18376.105514764786\n",
      "  time_this_iter_s: 28.80485486984253\n",
      "  time_total_s: 18376.105514764786\n",
      "  timers:\n",
      "    learn_throughput: 1439.671\n",
      "    learn_time_ms: 694.603\n",
      "    load_throughput: 40892.272\n",
      "    load_time_ms: 24.454\n",
      "    sample_throughput: 34.826\n",
      "    sample_time_ms: 28714.333\n",
      "    update_time_ms: 2.493\n",
      "  timestamp: 1635081255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 726000\n",
      "  training_iteration: 726\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   726</td><td style=\"text-align: right;\">         18376.1</td><td style=\"text-align: right;\">726000</td><td style=\"text-align: right;\"> -2.5609</td><td style=\"text-align: right;\">               -1.99</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            256.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 727000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 254.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.5472999999999892\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2520\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9906383373444434e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.29772700783279205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0016425843385655349\n",
      "          policy_loss: -0.016293339596854316\n",
      "          total_loss: -0.0031236186623573303\n",
      "          vf_explained_var: 0.08032775670289993\n",
      "          vf_loss: 0.01614698690051834\n",
      "    num_agent_steps_sampled: 727000\n",
      "    num_agent_steps_trained: 727000\n",
      "    num_steps_sampled: 727000\n",
      "    num_steps_trained: 727000\n",
      "  iterations_since_restore: 727\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.125\n",
      "    ram_util_percent: 39.1640625\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038625480490036085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.35649219190644\n",
      "    mean_inference_ms: 1.927635389304957\n",
      "    mean_raw_obs_processing_ms: 2.1682268826829727\n",
      "  time_since_restore: 18420.838790655136\n",
      "  time_this_iter_s: 44.73327589035034\n",
      "  time_total_s: 18420.838790655136\n",
      "  timers:\n",
      "    learn_throughput: 1440.198\n",
      "    learn_time_ms: 694.349\n",
      "    load_throughput: 40511.642\n",
      "    load_time_ms: 24.684\n",
      "    sample_throughput: 32.841\n",
      "    sample_time_ms: 30449.733\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1635081300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 727000\n",
      "  training_iteration: 727\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   727</td><td style=\"text-align: right;\">         18420.8</td><td style=\"text-align: right;\">727000</td><td style=\"text-align: right;\"> -2.5473</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            254.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-15-30\n",
      "  done: false\n",
      "  episode_len_mean: 252.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.52799999999999\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2524\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.953191686722217e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.27092987100283306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001157054161027165\n",
      "          policy_loss: -0.0022957112226221297\n",
      "          total_loss: 0.007308304889334573\n",
      "          vf_explained_var: 0.12233904004096985\n",
      "          vf_loss: 0.012313313088897201\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "  iterations_since_restore: 728\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.35348837209303\n",
      "    ram_util_percent: 39.11162790697675\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038625349398312074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.36106003389026\n",
      "    mean_inference_ms: 1.9276409835610167\n",
      "    mean_raw_obs_processing_ms: 2.169199590357784\n",
      "  time_since_restore: 18451.08849644661\n",
      "  time_this_iter_s: 30.24970579147339\n",
      "  time_total_s: 18451.08849644661\n",
      "  timers:\n",
      "    learn_throughput: 1441.067\n",
      "    learn_time_ms: 693.93\n",
      "    load_throughput: 40464.585\n",
      "    load_time_ms: 24.713\n",
      "    sample_throughput: 32.467\n",
      "    sample_time_ms: 30800.829\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1635081330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 728\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   728</td><td style=\"text-align: right;\">         18451.1</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\">  -2.528</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">             252.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 729000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-15-58\n",
      "  done: false\n",
      "  episode_len_mean: 249.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.4932999999999907\n",
      "  episode_reward_min: -4.40999999999995\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2529\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.9765958433611085e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.30689094629552627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00356598880113476\n",
      "          policy_loss: -0.01415611199206776\n",
      "          total_loss: -0.0027699299156665803\n",
      "          vf_explained_var: 0.34220683574676514\n",
      "          vf_loss: 0.014455090618381898\n",
      "    num_agent_steps_sampled: 729000\n",
      "    num_agent_steps_trained: 729000\n",
      "    num_steps_sampled: 729000\n",
      "    num_steps_trained: 729000\n",
      "  iterations_since_restore: 729\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.4\n",
      "    ram_util_percent: 39.214999999999996\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862517524249729\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.367016312285322\n",
      "    mean_inference_ms: 1.9276477678024468\n",
      "    mean_raw_obs_processing_ms: 2.170249808630282\n",
      "  time_since_restore: 18479.01981639862\n",
      "  time_this_iter_s: 27.93131995201111\n",
      "  time_total_s: 18479.01981639862\n",
      "  timers:\n",
      "    learn_throughput: 1442.637\n",
      "    learn_time_ms: 693.175\n",
      "    load_throughput: 40439.032\n",
      "    load_time_ms: 24.729\n",
      "    sample_throughput: 32.537\n",
      "    sample_time_ms: 30734.1\n",
      "    update_time_ms: 2.43\n",
      "  timestamp: 1635081358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 729000\n",
      "  training_iteration: 729\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   729</td><td style=\"text-align: right;\">           18479</td><td style=\"text-align: right;\">729000</td><td style=\"text-align: right;\"> -2.4933</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -4.41</td><td style=\"text-align: right;\">            249.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 730000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-16-27\n",
      "  done: false\n",
      "  episode_len_mean: 243.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.435199999999992\n",
      "  episode_reward_min: -4.059999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2533\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4882979216805542e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.25263450824552114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002021329348860787\n",
      "          policy_loss: 0.051182399110661615\n",
      "          total_loss: 0.05912205937008063\n",
      "          vf_explained_var: 0.19455689191818237\n",
      "          vf_loss: 0.010466004194070896\n",
      "    num_agent_steps_sampled: 730000\n",
      "    num_agent_steps_trained: 730000\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "  iterations_since_restore: 730\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.41219512195122\n",
      "    ram_util_percent: 39.21707317073171\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038625057986657906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.372186160353184\n",
      "    mean_inference_ms: 1.927652703783582\n",
      "    mean_raw_obs_processing_ms: 2.1703800976104937\n",
      "  time_since_restore: 18507.823570728302\n",
      "  time_this_iter_s: 28.803754329681396\n",
      "  time_total_s: 18507.823570728302\n",
      "  timers:\n",
      "    learn_throughput: 1434.742\n",
      "    learn_time_ms: 696.99\n",
      "    load_throughput: 40271.607\n",
      "    load_time_ms: 24.831\n",
      "    sample_throughput: 34.137\n",
      "    sample_time_ms: 29293.699\n",
      "    update_time_ms: 2.482\n",
      "  timestamp: 1635081387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 730\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   730</td><td style=\"text-align: right;\">         18507.8</td><td style=\"text-align: right;\">730000</td><td style=\"text-align: right;\"> -2.4352</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -4.06</td><td style=\"text-align: right;\">            243.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 731000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-16-57\n",
      "  done: false\n",
      "  episode_len_mean: 239.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.3917999999999924\n",
      "  episode_reward_min: -2.9899999999999802\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2537\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2441489608402771e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2830946574608485\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004109517346051443\n",
      "          policy_loss: -0.025360843042532603\n",
      "          total_loss: -0.013862473848793242\n",
      "          vf_explained_var: 0.11927098035812378\n",
      "          vf_loss: 0.014329318422824144\n",
      "    num_agent_steps_sampled: 731000\n",
      "    num_agent_steps_trained: 731000\n",
      "    num_steps_sampled: 731000\n",
      "    num_steps_trained: 731000\n",
      "  iterations_since_restore: 731\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.972727272727276\n",
      "    ram_util_percent: 38.72272727272727\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862501896382376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.37782751999059\n",
      "    mean_inference_ms: 1.927659726969667\n",
      "    mean_raw_obs_processing_ms: 2.1705437885271133\n",
      "  time_since_restore: 18538.38720011711\n",
      "  time_this_iter_s: 30.563629388809204\n",
      "  time_total_s: 18538.38720011711\n",
      "  timers:\n",
      "    learn_throughput: 1422.98\n",
      "    learn_time_ms: 702.751\n",
      "    load_throughput: 39802.35\n",
      "    load_time_ms: 25.124\n",
      "    sample_throughput: 34.047\n",
      "    sample_time_ms: 29371.231\n",
      "    update_time_ms: 2.498\n",
      "  timestamp: 1635081417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 731000\n",
      "  training_iteration: 731\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   731</td><td style=\"text-align: right;\">         18538.4</td><td style=\"text-align: right;\">731000</td><td style=\"text-align: right;\"> -2.3918</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.99</td><td style=\"text-align: right;\">            239.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 732000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-17-27\n",
      "  done: false\n",
      "  episode_len_mean: 236.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.369699999999993\n",
      "  episode_reward_min: -2.7999999999999843\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2542\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.220744804201386e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.27968994196918273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003267406585911519\n",
      "          policy_loss: 0.017877554148435594\n",
      "          total_loss: 0.03161599876152144\n",
      "          vf_explained_var: 0.1629941314458847\n",
      "          vf_loss: 0.016535345133807923\n",
      "    num_agent_steps_sampled: 732000\n",
      "    num_agent_steps_trained: 732000\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "  iterations_since_restore: 732\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.09523809523811\n",
      "    ram_util_percent: 38.54523809523809\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862504068885632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.38514995540525\n",
      "    mean_inference_ms: 1.9276715960939288\n",
      "    mean_raw_obs_processing_ms: 2.170816874655709\n",
      "  time_since_restore: 18568.05797100067\n",
      "  time_this_iter_s: 29.67077088356018\n",
      "  time_total_s: 18568.05797100067\n",
      "  timers:\n",
      "    learn_throughput: 1412.335\n",
      "    learn_time_ms: 708.048\n",
      "    load_throughput: 39808.659\n",
      "    load_time_ms: 25.12\n",
      "    sample_throughput: 33.727\n",
      "    sample_time_ms: 29650.066\n",
      "    update_time_ms: 2.561\n",
      "  timestamp: 1635081447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 732\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   732</td><td style=\"text-align: right;\">         18568.1</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\"> -2.3697</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -2.8</td><td style=\"text-align: right;\">            236.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 733000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-17-57\n",
      "  done: false\n",
      "  episode_len_mean: 235.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.3574999999999933\n",
      "  episode_reward_min: -2.7999999999999843\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2546\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.110372402100693e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3457718524667952\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0034585312306168545\n",
      "          policy_loss: 0.0036142420851522023\n",
      "          total_loss: 0.013562400887409846\n",
      "          vf_explained_var: 0.16145938634872437\n",
      "          vf_loss: 0.013405876772271261\n",
      "    num_agent_steps_sampled: 733000\n",
      "    num_agent_steps_trained: 733000\n",
      "    num_steps_sampled: 733000\n",
      "    num_steps_trained: 733000\n",
      "  iterations_since_restore: 733\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.43023255813954\n",
      "    ram_util_percent: 39.01627906976744\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038625194380324454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.39120200849761\n",
      "    mean_inference_ms: 1.9276863471170926\n",
      "    mean_raw_obs_processing_ms: 2.1710726572880206\n",
      "  time_since_restore: 18598.361394882202\n",
      "  time_this_iter_s: 30.30342388153076\n",
      "  time_total_s: 18598.361394882202\n",
      "  timers:\n",
      "    learn_throughput: 1412.04\n",
      "    learn_time_ms: 708.195\n",
      "    load_throughput: 39741.858\n",
      "    load_time_ms: 25.162\n",
      "    sample_throughput: 33.465\n",
      "    sample_time_ms: 29882.12\n",
      "    update_time_ms: 2.557\n",
      "  timestamp: 1635081477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 733000\n",
      "  training_iteration: 733\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   733</td><td style=\"text-align: right;\">         18598.4</td><td style=\"text-align: right;\">733000</td><td style=\"text-align: right;\"> -2.3575</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -2.8</td><td style=\"text-align: right;\">            235.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 734000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-18-44\n",
      "  done: false\n",
      "  episode_len_mean: 234.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.3438999999999934\n",
      "  episode_reward_min: -2.7999999999999843\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2551\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5551862010503464e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3474232600794898\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004100673070052629\n",
      "          policy_loss: -0.027904454867045084\n",
      "          total_loss: -0.015520770682228936\n",
      "          vf_explained_var: 0.33496925234794617\n",
      "          vf_loss: 0.015857918239716027\n",
      "    num_agent_steps_sampled: 734000\n",
      "    num_agent_steps_trained: 734000\n",
      "    num_steps_sampled: 734000\n",
      "    num_steps_trained: 734000\n",
      "  iterations_since_restore: 734\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.702985074626866\n",
      "    ram_util_percent: 39.440298507462686\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862545132876445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.398933709788906\n",
      "    mean_inference_ms: 1.9277056991938954\n",
      "    mean_raw_obs_processing_ms: 2.1725744622780594\n",
      "  time_since_restore: 18645.070026874542\n",
      "  time_this_iter_s: 46.70863199234009\n",
      "  time_total_s: 18645.070026874542\n",
      "  timers:\n",
      "    learn_throughput: 1410.304\n",
      "    learn_time_ms: 709.067\n",
      "    load_throughput: 39890.551\n",
      "    load_time_ms: 25.069\n",
      "    sample_throughput: 31.424\n",
      "    sample_time_ms: 31822.36\n",
      "    update_time_ms: 2.553\n",
      "  timestamp: 1635081524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 734000\n",
      "  training_iteration: 734\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   734</td><td style=\"text-align: right;\">         18645.1</td><td style=\"text-align: right;\">734000</td><td style=\"text-align: right;\"> -2.3439</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -2.8</td><td style=\"text-align: right;\">            234.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 735000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-19-13\n",
      "  done: false\n",
      "  episode_len_mean: 234.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.341199999999994\n",
      "  episode_reward_min: -2.7999999999999843\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2555\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.775931005251732e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4695372412602107\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02257139463682015\n",
      "          policy_loss: -0.0590481345438295\n",
      "          total_loss: -0.052309437634216416\n",
      "          vf_explained_var: 0.31789901852607727\n",
      "          vf_loss: 0.011434067661563555\n",
      "    num_agent_steps_sampled: 735000\n",
      "    num_agent_steps_trained: 735000\n",
      "    num_steps_sampled: 735000\n",
      "    num_steps_trained: 735000\n",
      "  iterations_since_restore: 735\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.73658536585366\n",
      "    ram_util_percent: 39.390243902439025\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386257548118588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.405183939879972\n",
      "    mean_inference_ms: 1.927722809707932\n",
      "    mean_raw_obs_processing_ms: 2.1737926088279482\n",
      "  time_since_restore: 18674.179493904114\n",
      "  time_this_iter_s: 29.109467029571533\n",
      "  time_total_s: 18674.179493904114\n",
      "  timers:\n",
      "    learn_throughput: 1409.688\n",
      "    learn_time_ms: 709.377\n",
      "    load_throughput: 39678.283\n",
      "    load_time_ms: 25.203\n",
      "    sample_throughput: 31.302\n",
      "    sample_time_ms: 31947.309\n",
      "    update_time_ms: 2.525\n",
      "  timestamp: 1635081553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 735000\n",
      "  training_iteration: 735\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   735</td><td style=\"text-align: right;\">         18674.2</td><td style=\"text-align: right;\">735000</td><td style=\"text-align: right;\"> -2.3412</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -2.8</td><td style=\"text-align: right;\">            234.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-19-43\n",
      "  done: false\n",
      "  episode_len_mean: 233.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.336899999999994\n",
      "  episode_reward_min: -2.7999999999999843\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2559\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1663896507877595e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.30869272351264954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004234260845861668\n",
      "          policy_loss: 0.03280370773540603\n",
      "          total_loss: 0.03834326499038272\n",
      "          vf_explained_var: 0.4185022711753845\n",
      "          vf_loss: 0.008626484136200614\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_agent_steps_trained: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "  iterations_since_restore: 736\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.85116279069767\n",
      "    ram_util_percent: 39.37441860465116\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038626088541437606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.41157428551957\n",
      "    mean_inference_ms: 1.9277404515390975\n",
      "    mean_raw_obs_processing_ms: 2.1745236731517235\n",
      "  time_since_restore: 18703.711933851242\n",
      "  time_this_iter_s: 29.532439947128296\n",
      "  time_total_s: 18703.711933851242\n",
      "  timers:\n",
      "    learn_throughput: 1409.27\n",
      "    learn_time_ms: 709.587\n",
      "    load_throughput: 39602.643\n",
      "    load_time_ms: 25.251\n",
      "    sample_throughput: 31.231\n",
      "    sample_time_ms: 32019.761\n",
      "    update_time_ms: 2.511\n",
      "  timestamp: 1635081583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 736\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   736</td><td style=\"text-align: right;\">         18703.7</td><td style=\"text-align: right;\">736000</td><td style=\"text-align: right;\"> -2.3369</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -2.8</td><td style=\"text-align: right;\">            233.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 737000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-20-12\n",
      "  done: false\n",
      "  episode_len_mean: 233.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.334899999999994\n",
      "  episode_reward_min: -2.7999999999999843\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2563\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.831948253938797e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2783054012391302\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0031981847356380654\n",
      "          policy_loss: -0.01265493126379119\n",
      "          total_loss: -0.0031768281426694657\n",
      "          vf_explained_var: 0.21622681617736816\n",
      "          vf_loss: 0.012261158310704762\n",
      "    num_agent_steps_sampled: 737000\n",
      "    num_agent_steps_trained: 737000\n",
      "    num_steps_sampled: 737000\n",
      "    num_steps_trained: 737000\n",
      "  iterations_since_restore: 737\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.16190476190476\n",
      "    ram_util_percent: 39.400000000000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862647119514328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.41803597383717\n",
      "    mean_inference_ms: 1.9277586948931509\n",
      "    mean_raw_obs_processing_ms: 2.1747967694973123\n",
      "  time_since_restore: 18733.153421878815\n",
      "  time_this_iter_s: 29.441488027572632\n",
      "  time_total_s: 18733.153421878815\n",
      "  timers:\n",
      "    learn_throughput: 1408.078\n",
      "    learn_time_ms: 710.188\n",
      "    load_throughput: 40008.28\n",
      "    load_time_ms: 24.995\n",
      "    sample_throughput: 32.797\n",
      "    sample_time_ms: 30490.195\n",
      "    update_time_ms: 2.545\n",
      "  timestamp: 1635081612\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 737000\n",
      "  training_iteration: 737\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   737</td><td style=\"text-align: right;\">         18733.2</td><td style=\"text-align: right;\">737000</td><td style=\"text-align: right;\"> -2.3349</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -2.8</td><td style=\"text-align: right;\">            233.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 738000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-20-41\n",
      "  done: false\n",
      "  episode_len_mean: 233.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.334899999999994\n",
      "  episode_reward_min: -2.7999999999999843\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2568\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.9159741269693987e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2934283567799462\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009694084942469417\n",
      "          policy_loss: -0.016954168677330017\n",
      "          total_loss: -0.006723917689588335\n",
      "          vf_explained_var: 0.3141724765300751\n",
      "          vf_loss: 0.01316453292965889\n",
      "    num_agent_steps_sampled: 738000\n",
      "    num_agent_steps_trained: 738000\n",
      "    num_steps_sampled: 738000\n",
      "    num_steps_trained: 738000\n",
      "  iterations_since_restore: 738\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.96585365853658\n",
      "    ram_util_percent: 39.451219512195124\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862701881249335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.42613860630327\n",
      "    mean_inference_ms: 1.9277826816085115\n",
      "    mean_raw_obs_processing_ms: 2.175159643666982\n",
      "  time_since_restore: 18761.736376047134\n",
      "  time_this_iter_s: 28.582954168319702\n",
      "  time_total_s: 18761.736376047134\n",
      "  timers:\n",
      "    learn_throughput: 1406.241\n",
      "    learn_time_ms: 711.116\n",
      "    load_throughput: 40476.691\n",
      "    load_time_ms: 24.706\n",
      "    sample_throughput: 32.978\n",
      "    sample_time_ms: 30322.84\n",
      "    update_time_ms: 2.564\n",
      "  timestamp: 1635081641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 738000\n",
      "  training_iteration: 738\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   738</td><td style=\"text-align: right;\">         18761.7</td><td style=\"text-align: right;\">738000</td><td style=\"text-align: right;\"> -2.3349</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -2.8</td><td style=\"text-align: right;\">            233.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 739000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 232.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.3286999999999947\n",
      "  episode_reward_min: -2.7999999999999843\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2572\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.9159741269693987e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.28308068248960705\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06689669905240755\n",
      "          policy_loss: 0.04975532533393966\n",
      "          total_loss: 0.05774906269378132\n",
      "          vf_explained_var: 0.306782990694046\n",
      "          vf_loss: 0.010824544665714106\n",
      "    num_agent_steps_sampled: 739000\n",
      "    num_agent_steps_trained: 739000\n",
      "    num_steps_sampled: 739000\n",
      "    num_steps_trained: 739000\n",
      "  iterations_since_restore: 739\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.23\n",
      "    ram_util_percent: 39.385000000000005\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862752767417633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.432712218171865\n",
      "    mean_inference_ms: 1.927803352321692\n",
      "    mean_raw_obs_processing_ms: 2.1754694676684494\n",
      "  time_since_restore: 18790.270909309387\n",
      "  time_this_iter_s: 28.534533262252808\n",
      "  time_total_s: 18790.270909309387\n",
      "  timers:\n",
      "    learn_throughput: 1402.154\n",
      "    learn_time_ms: 713.188\n",
      "    load_throughput: 41046.904\n",
      "    load_time_ms: 24.362\n",
      "    sample_throughput: 32.915\n",
      "    sample_time_ms: 30381.432\n",
      "    update_time_ms: 2.543\n",
      "  timestamp: 1635081669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 739000\n",
      "  training_iteration: 739\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   739</td><td style=\"text-align: right;\">         18790.3</td><td style=\"text-align: right;\">739000</td><td style=\"text-align: right;\"> -2.3287</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -2.8</td><td style=\"text-align: right;\">            232.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 740000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 232.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.321699999999994\n",
      "  episode_reward_min: -2.629999999999988\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2576\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.373961190454099e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.21697337345944512\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032209427583258805\n",
      "          policy_loss: -0.012546434460414781\n",
      "          total_loss: -0.004114104186495145\n",
      "          vf_explained_var: 0.20152083039283752\n",
      "          vf_loss: 0.010602063768439822\n",
      "    num_agent_steps_sampled: 740000\n",
      "    num_agent_steps_trained: 740000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "  iterations_since_restore: 740\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.19111111111111\n",
      "    ram_util_percent: 40.911111111111104\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862811534426317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.439498478682257\n",
      "    mean_inference_ms: 1.9278263845152432\n",
      "    mean_raw_obs_processing_ms: 2.175784264917715\n",
      "  time_since_restore: 18821.505291461945\n",
      "  time_this_iter_s: 31.234382152557373\n",
      "  time_total_s: 18821.505291461945\n",
      "  timers:\n",
      "    learn_throughput: 1398.427\n",
      "    learn_time_ms: 715.089\n",
      "    load_throughput: 42010.425\n",
      "    load_time_ms: 23.804\n",
      "    sample_throughput: 32.655\n",
      "    sample_time_ms: 30623.196\n",
      "    update_time_ms: 2.493\n",
      "  timestamp: 1635081701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 740\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   740</td><td style=\"text-align: right;\">         18821.5</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\"> -2.3217</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -2.63</td><td style=\"text-align: right;\">            232.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 741000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-22-48\n",
      "  done: false\n",
      "  episode_len_mean: 231.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.3113999999999946\n",
      "  episode_reward_min: -2.629999999999988\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2581\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1869805952270494e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.34002719273169835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008028359510222356\n",
      "          policy_loss: 0.009719425191481909\n",
      "          total_loss: 0.020611274904674955\n",
      "          vf_explained_var: 0.24117238819599152\n",
      "          vf_loss: 0.014292119991862112\n",
      "    num_agent_steps_sampled: 741000\n",
      "    num_agent_steps_trained: 741000\n",
      "    num_steps_sampled: 741000\n",
      "    num_steps_trained: 741000\n",
      "  iterations_since_restore: 741\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.22421052631579\n",
      "    ram_util_percent: 49.20842105263158\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038629458905441366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.449222009272876\n",
      "    mean_inference_ms: 1.927864198256143\n",
      "    mean_raw_obs_processing_ms: 2.1776016813775585\n",
      "  time_since_restore: 18888.48544549942\n",
      "  time_this_iter_s: 66.98015403747559\n",
      "  time_total_s: 18888.48544549942\n",
      "  timers:\n",
      "    learn_throughput: 1389.138\n",
      "    learn_time_ms: 719.871\n",
      "    load_throughput: 44048.052\n",
      "    load_time_ms: 22.702\n",
      "    sample_throughput: 29.188\n",
      "    sample_time_ms: 34260.667\n",
      "    update_time_ms: 2.48\n",
      "  timestamp: 1635081768\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 741000\n",
      "  training_iteration: 741\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   741</td><td style=\"text-align: right;\">         18888.5</td><td style=\"text-align: right;\">741000</td><td style=\"text-align: right;\"> -2.3114</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -2.63</td><td style=\"text-align: right;\">            231.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 742000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-23-26\n",
      "  done: false\n",
      "  episode_len_mean: 231.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.3158999999999943\n",
      "  episode_reward_min: -2.629999999999988\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2585\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1869805952270494e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7556945863697264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04162426316165375\n",
      "          policy_loss: 0.006250951439142227\n",
      "          total_loss: 0.010988224463330375\n",
      "          vf_explained_var: 0.38572439551353455\n",
      "          vf_loss: 0.012294214508599705\n",
      "    num_agent_steps_sampled: 742000\n",
      "    num_agent_steps_trained: 742000\n",
      "    num_steps_sampled: 742000\n",
      "    num_steps_trained: 742000\n",
      "  iterations_since_restore: 742\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.73214285714286\n",
      "    ram_util_percent: 50.77500000000001\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038630812088172824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.45745731900136\n",
      "    mean_inference_ms: 1.9279071625692175\n",
      "    mean_raw_obs_processing_ms: 2.179080596852854\n",
      "  time_since_restore: 18927.05247449875\n",
      "  time_this_iter_s: 38.56702899932861\n",
      "  time_total_s: 18927.05247449875\n",
      "  timers:\n",
      "    learn_throughput: 1371.429\n",
      "    learn_time_ms: 729.166\n",
      "    load_throughput: 45368.4\n",
      "    load_time_ms: 22.042\n",
      "    sample_throughput: 28.457\n",
      "    sample_time_ms: 35141.32\n",
      "    update_time_ms: 2.726\n",
      "  timestamp: 1635081806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 742000\n",
      "  training_iteration: 742\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   742</td><td style=\"text-align: right;\">         18927.1</td><td style=\"text-align: right;\">742000</td><td style=\"text-align: right;\"> -2.3159</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -2.63</td><td style=\"text-align: right;\">            231.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 743000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-24-05\n",
      "  done: false\n",
      "  episode_len_mean: 231.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.313399999999995\n",
      "  episode_reward_min: -2.629999999999988\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2589\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.2804708928405743e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3338983575503031\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0045787871167463\n",
      "          policy_loss: 0.009919178651438819\n",
      "          total_loss: 0.016855604698260626\n",
      "          vf_explained_var: 0.30027031898498535\n",
      "          vf_loss: 0.010275412899338537\n",
      "    num_agent_steps_sampled: 743000\n",
      "    num_agent_steps_trained: 743000\n",
      "    num_steps_sampled: 743000\n",
      "    num_steps_trained: 743000\n",
      "  iterations_since_restore: 743\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.88181818181818\n",
      "    ram_util_percent: 50.48363636363637\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863236944998211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.466312123663002\n",
      "    mean_inference_ms: 1.9279594249673824\n",
      "    mean_raw_obs_processing_ms: 2.179648373549434\n",
      "  time_since_restore: 18965.899505853653\n",
      "  time_this_iter_s: 38.847031354904175\n",
      "  time_total_s: 18965.899505853653\n",
      "  timers:\n",
      "    learn_throughput: 1361.907\n",
      "    learn_time_ms: 734.265\n",
      "    load_throughput: 47239.59\n",
      "    load_time_ms: 21.169\n",
      "    sample_throughput: 27.785\n",
      "    sample_time_ms: 35990.741\n",
      "    update_time_ms: 3.145\n",
      "  timestamp: 1635081845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 743000\n",
      "  training_iteration: 743\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   743</td><td style=\"text-align: right;\">         18965.9</td><td style=\"text-align: right;\">743000</td><td style=\"text-align: right;\"> -2.3134</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -2.63</td><td style=\"text-align: right;\">            231.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-24-42\n",
      "  done: false\n",
      "  episode_len_mean: 231.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.3159999999999945\n",
      "  episode_reward_min: -2.629999999999988\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2593\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6402354464202871e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.37602347350782817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04413677422289734\n",
      "          policy_loss: -0.04083958570328024\n",
      "          total_loss: -0.0345164997710122\n",
      "          vf_explained_var: 0.25480490922927856\n",
      "          vf_loss: 0.010083321140458186\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "  iterations_since_restore: 744\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.74615384615385\n",
      "    ram_util_percent: 50.50192307692308\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386346406817913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.475595819645562\n",
      "    mean_inference_ms: 1.9280188482160736\n",
      "    mean_raw_obs_processing_ms: 2.1801931905062326\n",
      "  time_since_restore: 19002.625339984894\n",
      "  time_this_iter_s: 36.725834131240845\n",
      "  time_total_s: 19002.625339984894\n",
      "  timers:\n",
      "    learn_throughput: 1356.896\n",
      "    learn_time_ms: 736.976\n",
      "    load_throughput: 48026.015\n",
      "    load_time_ms: 20.822\n",
      "    sample_throughput: 28.58\n",
      "    sample_time_ms: 34989.923\n",
      "    update_time_ms: 3.23\n",
      "  timestamp: 1635081882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 744\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   744</td><td style=\"text-align: right;\">         19002.6</td><td style=\"text-align: right;\">744000</td><td style=\"text-align: right;\">  -2.316</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -2.63</td><td style=\"text-align: right;\">             231.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 745000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-25-17\n",
      "  done: false\n",
      "  episode_len_mean: 231.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.3135999999999943\n",
      "  episode_reward_min: -2.619999999999988\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2598\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.460353169630431e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4111360354555978\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01038859876152366\n",
      "          policy_loss: -0.013300307840108872\n",
      "          total_loss: -0.003127275655666987\n",
      "          vf_explained_var: 0.17974716424942017\n",
      "          vf_loss: 0.014284393636302815\n",
      "    num_agent_steps_sampled: 745000\n",
      "    num_agent_steps_trained: 745000\n",
      "    num_steps_sampled: 745000\n",
      "    num_steps_trained: 745000\n",
      "  iterations_since_restore: 745\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.0156862745098\n",
      "    ram_util_percent: 50.46666666666666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863770551836467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.487669712615606\n",
      "    mean_inference_ms: 1.9281017541581011\n",
      "    mean_raw_obs_processing_ms: 2.1808897203276114\n",
      "  time_since_restore: 19038.038796186447\n",
      "  time_this_iter_s: 35.413456201553345\n",
      "  time_total_s: 19038.038796186447\n",
      "  timers:\n",
      "    learn_throughput: 1345.687\n",
      "    learn_time_ms: 743.115\n",
      "    load_throughput: 50074.665\n",
      "    load_time_ms: 19.97\n",
      "    sample_throughput: 28.078\n",
      "    sample_time_ms: 35614.667\n",
      "    update_time_ms: 3.578\n",
      "  timestamp: 1635081917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 745000\n",
      "  training_iteration: 745\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   745</td><td style=\"text-align: right;\">           19038</td><td style=\"text-align: right;\">745000</td><td style=\"text-align: right;\"> -2.3136</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -2.62</td><td style=\"text-align: right;\">            231.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 746000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-25-52\n",
      "  done: false\n",
      "  episode_len_mean: 231.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.3141999999999943\n",
      "  episode_reward_min: -2.619999999999988\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2602\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.460353169630431e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5119959698783026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07623870667466762\n",
      "          policy_loss: 0.02277141511440277\n",
      "          total_loss: 0.027340818776024713\n",
      "          vf_explained_var: 0.1741880625486374\n",
      "          vf_loss: 0.00968936432359947\n",
      "    num_agent_steps_sampled: 746000\n",
      "    num_agent_steps_trained: 746000\n",
      "    num_steps_sampled: 746000\n",
      "    num_steps_trained: 746000\n",
      "  iterations_since_restore: 746\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.552\n",
      "    ram_util_percent: 50.382000000000005\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864038555830636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.49762986286372\n",
      "    mean_inference_ms: 1.9281754416578236\n",
      "    mean_raw_obs_processing_ms: 2.1814731736505117\n",
      "  time_since_restore: 19072.981053829193\n",
      "  time_this_iter_s: 34.94225764274597\n",
      "  time_total_s: 19072.981053829193\n",
      "  timers:\n",
      "    learn_throughput: 1340.554\n",
      "    learn_time_ms: 745.96\n",
      "    load_throughput: 52084.263\n",
      "    load_time_ms: 19.2\n",
      "    sample_throughput: 27.66\n",
      "    sample_time_ms: 36153.094\n",
      "    update_time_ms: 4.131\n",
      "  timestamp: 1635081952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 746000\n",
      "  training_iteration: 746\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   746</td><td style=\"text-align: right;\">           19073</td><td style=\"text-align: right;\">746000</td><td style=\"text-align: right;\"> -2.3142</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -2.62</td><td style=\"text-align: right;\">            231.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 747000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-26-23\n",
      "  done: false\n",
      "  episode_len_mean: 232.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.321299999999994\n",
      "  episode_reward_min: -2.7999999999999843\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2605\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.690529754445646e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5498520877626207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02539504599902563\n",
      "          policy_loss: -0.10039912056591775\n",
      "          total_loss: -0.09282634763254059\n",
      "          vf_explained_var: 0.07907922565937042\n",
      "          vf_loss: 0.013071295794927412\n",
      "    num_agent_steps_sampled: 747000\n",
      "    num_agent_steps_trained: 747000\n",
      "    num_steps_sampled: 747000\n",
      "    num_steps_trained: 747000\n",
      "  iterations_since_restore: 747\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.44090909090909\n",
      "    ram_util_percent: 50.37045454545455\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864251801901455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.505225499743545\n",
      "    mean_inference_ms: 1.9282351935012765\n",
      "    mean_raw_obs_processing_ms: 2.181910551691788\n",
      "  time_since_restore: 19103.93084168434\n",
      "  time_this_iter_s: 30.949787855148315\n",
      "  time_total_s: 19103.93084168434\n",
      "  timers:\n",
      "    learn_throughput: 1332.958\n",
      "    learn_time_ms: 750.211\n",
      "    load_throughput: 52671.682\n",
      "    load_time_ms: 18.986\n",
      "    sample_throughput: 27.549\n",
      "    sample_time_ms: 36299.198\n",
      "    update_time_ms: 4.77\n",
      "  timestamp: 1635081983\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 747000\n",
      "  training_iteration: 747\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   747</td><td style=\"text-align: right;\">         19103.9</td><td style=\"text-align: right;\">747000</td><td style=\"text-align: right;\"> -2.3213</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -2.8</td><td style=\"text-align: right;\">            232.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 748000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-26-49\n",
      "  done: false\n",
      "  episode_len_mean: 235.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.3565999999999936\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2609\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.53579463166847e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.848233065340254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02285917895013401\n",
      "          policy_loss: 0.03910279952817493\n",
      "          total_loss: 0.03985236701038149\n",
      "          vf_explained_var: 0.13885919749736786\n",
      "          vf_loss: 0.00923189802043554\n",
      "    num_agent_steps_sampled: 748000\n",
      "    num_agent_steps_trained: 748000\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "  iterations_since_restore: 748\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.67297297297299\n",
      "    ram_util_percent: 50.332432432432434\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864560548084663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.515349631602124\n",
      "    mean_inference_ms: 1.9283217671801216\n",
      "    mean_raw_obs_processing_ms: 2.1824461410927314\n",
      "  time_since_restore: 19130.094394207\n",
      "  time_this_iter_s: 26.1635525226593\n",
      "  time_total_s: 19130.094394207\n",
      "  timers:\n",
      "    learn_throughput: 1321.244\n",
      "    learn_time_ms: 756.862\n",
      "    load_throughput: 53896.814\n",
      "    load_time_ms: 18.554\n",
      "    sample_throughput: 27.739\n",
      "    sample_time_ms: 36050.64\n",
      "    update_time_ms: 4.9\n",
      "  timestamp: 1635082009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 748\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   748</td><td style=\"text-align: right;\">         19130.1</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\"> -2.3566</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            235.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 749000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-27-39\n",
      "  done: false\n",
      "  episode_len_mean: 236.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.3680999999999934\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2612\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.303691947502703e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8128515263398488\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07782516889532379\n",
      "          policy_loss: 0.009613001263803905\n",
      "          total_loss: 0.009843154168791241\n",
      "          vf_explained_var: 0.3616488575935364\n",
      "          vf_loss: 0.00835866611968312\n",
      "    num_agent_steps_sampled: 749000\n",
      "    num_agent_steps_trained: 749000\n",
      "    num_steps_sampled: 749000\n",
      "    num_steps_trained: 749000\n",
      "  iterations_since_restore: 749\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.11408450704226\n",
      "    ram_util_percent: 50.125352112676055\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864812098402813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.52311758751029\n",
      "    mean_inference_ms: 1.928392471436846\n",
      "    mean_raw_obs_processing_ms: 2.18352610385665\n",
      "  time_since_restore: 19179.78000998497\n",
      "  time_this_iter_s: 49.68561577796936\n",
      "  time_total_s: 19179.78000998497\n",
      "  timers:\n",
      "    learn_throughput: 1302.132\n",
      "    learn_time_ms: 767.971\n",
      "    load_throughput: 55496.215\n",
      "    load_time_ms: 18.019\n",
      "    sample_throughput: 26.209\n",
      "    sample_time_ms: 38154.815\n",
      "    update_time_ms: 4.995\n",
      "  timestamp: 1635082059\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 749000\n",
      "  training_iteration: 749\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   749</td><td style=\"text-align: right;\">         19179.8</td><td style=\"text-align: right;\">749000</td><td style=\"text-align: right;\"> -2.3681</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            236.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 750000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-28-15\n",
      "  done: false\n",
      "  episode_len_mean: 237.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.379399999999993\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2616\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2455537921254057e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7186119814713796\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.029584213771595893\n",
      "          policy_loss: -0.01771413567993376\n",
      "          total_loss: -0.014848877572351031\n",
      "          vf_explained_var: 0.3650648891925812\n",
      "          vf_loss: 0.01005137589139243\n",
      "    num_agent_steps_sampled: 750000\n",
      "    num_agent_steps_trained: 750000\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "  iterations_since_restore: 750\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.04038461538461\n",
      "    ram_util_percent: 50.93269230769231\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865175790725246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.533864223887576\n",
      "    mean_inference_ms: 1.9284949187821077\n",
      "    mean_raw_obs_processing_ms: 2.1847346093142863\n",
      "  time_since_restore: 19216.122992753983\n",
      "  time_this_iter_s: 36.34298276901245\n",
      "  time_total_s: 19216.122992753983\n",
      "  timers:\n",
      "    learn_throughput: 1280.217\n",
      "    learn_time_ms: 781.118\n",
      "    load_throughput: 57588.91\n",
      "    load_time_ms: 17.364\n",
      "    sample_throughput: 25.872\n",
      "    sample_time_ms: 38652.344\n",
      "    update_time_ms: 5.788\n",
      "  timestamp: 1635082095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 750\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   750</td><td style=\"text-align: right;\">         19216.1</td><td style=\"text-align: right;\">750000</td><td style=\"text-align: right;\"> -2.3794</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            237.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 751000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 239.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.3930999999999925\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2620\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8683306881881075e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6932403412130144\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006036594008256636\n",
      "          policy_loss: 0.011942655262019899\n",
      "          total_loss: 0.014118546081913843\n",
      "          vf_explained_var: 0.3602520823478699\n",
      "          vf_loss: 0.009108292849527464\n",
      "    num_agent_steps_sampled: 751000\n",
      "    num_agent_steps_trained: 751000\n",
      "    num_steps_sampled: 751000\n",
      "    num_steps_trained: 751000\n",
      "  iterations_since_restore: 751\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.65714285714286\n",
      "    ram_util_percent: 50.87755102040816\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386557523704691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.54495041519788\n",
      "    mean_inference_ms: 1.928604784926111\n",
      "    mean_raw_obs_processing_ms: 2.185237185743117\n",
      "  time_since_restore: 19250.135871887207\n",
      "  time_this_iter_s: 34.01287913322449\n",
      "  time_total_s: 19250.135871887207\n",
      "  timers:\n",
      "    learn_throughput: 1284.171\n",
      "    learn_time_ms: 778.713\n",
      "    load_throughput: 58165.036\n",
      "    load_time_ms: 17.192\n",
      "    sample_throughput: 28.282\n",
      "    sample_time_ms: 35357.986\n",
      "    update_time_ms: 6.318\n",
      "  timestamp: 1635082129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 751000\n",
      "  training_iteration: 751\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   751</td><td style=\"text-align: right;\">         19250.1</td><td style=\"text-align: right;\">751000</td><td style=\"text-align: right;\"> -2.3931</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            239.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-29-23\n",
      "  done: false\n",
      "  episode_len_mean: 240.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.4022999999999923\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2624\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8683306881881075e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6813027686542935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004732666509264207\n",
      "          policy_loss: 0.02511299533976449\n",
      "          total_loss: 0.027358344693978628\n",
      "          vf_explained_var: 0.3678383231163025\n",
      "          vf_loss: 0.009058377457161744\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_agent_steps_trained: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "  iterations_since_restore: 752\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.43124999999999\n",
      "    ram_util_percent: 51.025\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865996931460219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.556175818044036\n",
      "    mean_inference_ms: 1.9287227750119473\n",
      "    mean_raw_obs_processing_ms: 2.185745735031239\n",
      "  time_since_restore: 19283.77863383293\n",
      "  time_this_iter_s: 33.64276194572449\n",
      "  time_total_s: 19283.77863383293\n",
      "  timers:\n",
      "    learn_throughput: 1299.669\n",
      "    learn_time_ms: 769.426\n",
      "    load_throughput: 58072.504\n",
      "    load_time_ms: 17.22\n",
      "    sample_throughput: 28.674\n",
      "    sample_time_ms: 34874.896\n",
      "    update_time_ms: 6.236\n",
      "  timestamp: 1635082163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 752\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   752</td><td style=\"text-align: right;\">         19283.8</td><td style=\"text-align: right;\">752000</td><td style=\"text-align: right;\"> -2.4023</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            240.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 753000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-29-56\n",
      "  done: false\n",
      "  episode_len_mean: 241.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.413299999999993\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2628\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.341653440940537e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7473290999730428\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07664752614220313\n",
      "          policy_loss: 0.03341286347972022\n",
      "          total_loss: 0.03537630778219965\n",
      "          vf_explained_var: 0.3921436369419098\n",
      "          vf_loss: 0.009436733446394403\n",
      "    num_agent_steps_sampled: 753000\n",
      "    num_agent_steps_trained: 753000\n",
      "    num_steps_sampled: 753000\n",
      "    num_steps_trained: 753000\n",
      "  iterations_since_restore: 753\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.83191489361701\n",
      "    ram_util_percent: 51.12978723404255\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038664360099459776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.567644887879318\n",
      "    mean_inference_ms: 1.9288472637565468\n",
      "    mean_raw_obs_processing_ms: 2.1862581732511344\n",
      "  time_since_restore: 19316.94454240799\n",
      "  time_this_iter_s: 33.16590857505798\n",
      "  time_total_s: 19316.94454240799\n",
      "  timers:\n",
      "    learn_throughput: 1302.63\n",
      "    learn_time_ms: 767.678\n",
      "    load_throughput: 57793.241\n",
      "    load_time_ms: 17.303\n",
      "    sample_throughput: 29.147\n",
      "    sample_time_ms: 34308.435\n",
      "    update_time_ms: 6.303\n",
      "  timestamp: 1635082196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 753000\n",
      "  training_iteration: 753\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   753</td><td style=\"text-align: right;\">         19316.9</td><td style=\"text-align: right;\">753000</td><td style=\"text-align: right;\"> -2.4133</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            241.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 754000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-30-28\n",
      "  done: false\n",
      "  episode_len_mean: 243.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.432799999999992\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2632\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4012480161410814e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7180367403560215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0064937984936095615\n",
      "          policy_loss: -0.0007409773766994477\n",
      "          total_loss: 0.003154847232831849\n",
      "          vf_explained_var: 0.19901934266090393\n",
      "          vf_loss: 0.011076189898368385\n",
      "    num_agent_steps_sampled: 754000\n",
      "    num_agent_steps_trained: 754000\n",
      "    num_steps_sampled: 754000\n",
      "    num_steps_trained: 754000\n",
      "  iterations_since_restore: 754\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.97555555555556\n",
      "    ram_util_percent: 50.93333333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038668970160258706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.579294468314103\n",
      "    mean_inference_ms: 1.9289795662774776\n",
      "    mean_raw_obs_processing_ms: 2.1867491300242134\n",
      "  time_since_restore: 19348.49285030365\n",
      "  time_this_iter_s: 31.5483078956604\n",
      "  time_total_s: 19348.49285030365\n",
      "  timers:\n",
      "    learn_throughput: 1299.719\n",
      "    learn_time_ms: 769.397\n",
      "    load_throughput: 55868.483\n",
      "    load_time_ms: 17.899\n",
      "    sample_throughput: 29.596\n",
      "    sample_time_ms: 33788.094\n",
      "    update_time_ms: 6.31\n",
      "  timestamp: 1635082228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 754000\n",
      "  training_iteration: 754\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   754</td><td style=\"text-align: right;\">         19348.5</td><td style=\"text-align: right;\">754000</td><td style=\"text-align: right;\"> -2.4328</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            243.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 755000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-30-59\n",
      "  done: false\n",
      "  episode_len_mean: 244.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.4437999999999915\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2635\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4012480161410814e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.586394445432557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00816775360940838\n",
      "          policy_loss: -0.04668936381737391\n",
      "          total_loss: -0.043924236380391654\n",
      "          vf_explained_var: 0.23302900791168213\n",
      "          vf_loss: 0.008629072250591384\n",
      "    num_agent_steps_sampled: 755000\n",
      "    num_agent_steps_trained: 755000\n",
      "    num_steps_sampled: 755000\n",
      "    num_steps_trained: 755000\n",
      "  iterations_since_restore: 755\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.75333333333333\n",
      "    ram_util_percent: 50.98888888888889\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867255783731112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.58807882982301\n",
      "    mean_inference_ms: 1.9290837936436374\n",
      "    mean_raw_obs_processing_ms: 2.1871132161996694\n",
      "  time_since_restore: 19379.88140296936\n",
      "  time_this_iter_s: 31.38855266571045\n",
      "  time_total_s: 19379.88140296936\n",
      "  timers:\n",
      "    learn_throughput: 1301.235\n",
      "    learn_time_ms: 768.501\n",
      "    load_throughput: 55402.897\n",
      "    load_time_ms: 18.05\n",
      "    sample_throughput: 29.952\n",
      "    sample_time_ms: 33386.444\n",
      "    update_time_ms: 6.019\n",
      "  timestamp: 1635082259\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 755000\n",
      "  training_iteration: 755\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   755</td><td style=\"text-align: right;\">         19379.9</td><td style=\"text-align: right;\">755000</td><td style=\"text-align: right;\"> -2.4438</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            244.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 756000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-31-30\n",
      "  done: false\n",
      "  episode_len_mean: 246.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.4642999999999913\n",
      "  episode_reward_min: -3.4999999999999694\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2639\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4012480161410814e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6161439309517542\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005313964815480353\n",
      "          policy_loss: 0.025017943647172716\n",
      "          total_loss: 0.029816713763607873\n",
      "          vf_explained_var: 0.23701100051403046\n",
      "          vf_loss: 0.010960210176805655\n",
      "    num_agent_steps_sampled: 756000\n",
      "    num_agent_steps_trained: 756000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "  iterations_since_restore: 756\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.23953488372094\n",
      "    ram_util_percent: 51.06511627906976\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386774626621744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.599823662201217\n",
      "    mean_inference_ms: 1.9292277324405172\n",
      "    mean_raw_obs_processing_ms: 2.1875856105702116\n",
      "  time_since_restore: 19410.071883916855\n",
      "  time_this_iter_s: 30.190480947494507\n",
      "  time_total_s: 19410.071883916855\n",
      "  timers:\n",
      "    learn_throughput: 1299.928\n",
      "    learn_time_ms: 769.273\n",
      "    load_throughput: 55603.849\n",
      "    load_time_ms: 17.984\n",
      "    sample_throughput: 30.386\n",
      "    sample_time_ms: 32910.001\n",
      "    update_time_ms: 6.484\n",
      "  timestamp: 1635082290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 756\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   756</td><td style=\"text-align: right;\">         19410.1</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\"> -2.4643</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">            246.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 757000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-32-08\n",
      "  done: false\n",
      "  episode_len_mean: 249.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.4992999999999905\n",
      "  episode_reward_min: -4.41999999999995\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2642\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4012480161410814e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7983363058831957\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02803165247776936\n",
      "          policy_loss: 0.06563841005166372\n",
      "          total_loss: 0.06633047502901819\n",
      "          vf_explained_var: 0.23324358463287354\n",
      "          vf_loss: 0.008675425192793934\n",
      "    num_agent_steps_sampled: 757000\n",
      "    num_agent_steps_trained: 757000\n",
      "    num_steps_sampled: 757000\n",
      "    num_steps_trained: 757000\n",
      "  iterations_since_restore: 757\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.36909090909091\n",
      "    ram_util_percent: 50.99636363636364\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868117999168923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.60841970029199\n",
      "    mean_inference_ms: 1.9293369912604763\n",
      "    mean_raw_obs_processing_ms: 2.1885813736799786\n",
      "  time_since_restore: 19448.837969064713\n",
      "  time_this_iter_s: 38.766085147857666\n",
      "  time_total_s: 19448.837969064713\n",
      "  timers:\n",
      "    learn_throughput: 1306.733\n",
      "    learn_time_ms: 765.267\n",
      "    load_throughput: 54183.604\n",
      "    load_time_ms: 18.456\n",
      "    sample_throughput: 29.677\n",
      "    sample_time_ms: 33695.904\n",
      "    update_time_ms: 5.801\n",
      "  timestamp: 1635082328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 757000\n",
      "  training_iteration: 757\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   757</td><td style=\"text-align: right;\">         19448.8</td><td style=\"text-align: right;\">757000</td><td style=\"text-align: right;\"> -2.4993</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -4.42</td><td style=\"text-align: right;\">            249.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 758000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-32-25\n",
      "  done: false\n",
      "  episode_len_mean: 255.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.5582999999999894\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2644\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.101872024211622e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8755081170135074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006093214659101815\n",
      "          policy_loss: 0.11595550874869029\n",
      "          total_loss: 0.11336309661467871\n",
      "          vf_explained_var: 0.19845062494277954\n",
      "          vf_loss: 0.006162666886585713\n",
      "    num_agent_steps_sampled: 758000\n",
      "    num_agent_steps_trained: 758000\n",
      "    num_steps_sampled: 758000\n",
      "    num_steps_trained: 758000\n",
      "  iterations_since_restore: 758\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.36666666666666\n",
      "    ram_util_percent: 51.04583333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038683660942323256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.61377509095963\n",
      "    mean_inference_ms: 1.9294088571391343\n",
      "    mean_raw_obs_processing_ms: 2.1892448101562834\n",
      "  time_since_restore: 19465.41294646263\n",
      "  time_this_iter_s: 16.5749773979187\n",
      "  time_total_s: 19465.41294646263\n",
      "  timers:\n",
      "    learn_throughput: 1310.448\n",
      "    learn_time_ms: 763.098\n",
      "    load_throughput: 54721.994\n",
      "    load_time_ms: 18.274\n",
      "    sample_throughput: 30.545\n",
      "    sample_time_ms: 32739.043\n",
      "    update_time_ms: 6.307\n",
      "  timestamp: 1635082345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 758000\n",
      "  training_iteration: 758\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   758</td><td style=\"text-align: right;\">         19465.4</td><td style=\"text-align: right;\">758000</td><td style=\"text-align: right;\"> -2.5583</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            255.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 759000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-32-41\n",
      "  done: false\n",
      "  episode_len_mean: 259.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.5930999999999886\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2645\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.101872024211622e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9186794996261597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013440080633313933\n",
      "          policy_loss: -0.04931348636746406\n",
      "          total_loss: -0.05136337305108706\n",
      "          vf_explained_var: 0.08169073611497879\n",
      "          vf_loss: 0.007136907854389089\n",
      "    num_agent_steps_sampled: 759000\n",
      "    num_agent_steps_trained: 759000\n",
      "    num_steps_sampled: 759000\n",
      "    num_steps_trained: 759000\n",
      "  iterations_since_restore: 759\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.47500000000001\n",
      "    ram_util_percent: 51.041666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038684939833366815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.616355766261712\n",
      "    mean_inference_ms: 1.929446071893733\n",
      "    mean_raw_obs_processing_ms: 2.189549755687871\n",
      "  time_since_restore: 19481.862605810165\n",
      "  time_this_iter_s: 16.44965934753418\n",
      "  time_total_s: 19481.862605810165\n",
      "  timers:\n",
      "    learn_throughput: 1324.105\n",
      "    learn_time_ms: 755.227\n",
      "    load_throughput: 54863.577\n",
      "    load_time_ms: 18.227\n",
      "    sample_throughput: 33.987\n",
      "    sample_time_ms: 29423.267\n",
      "    update_time_ms: 6.629\n",
      "  timestamp: 1635082361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 759000\n",
      "  training_iteration: 759\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   759</td><td style=\"text-align: right;\">         19481.9</td><td style=\"text-align: right;\">759000</td><td style=\"text-align: right;\"> -2.5931</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            259.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-33-01\n",
      "  done: false\n",
      "  episode_len_mean: 266.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.6647999999999867\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2648\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.101872024211622e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0730706916915045\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.042433666356854007\n",
      "          policy_loss: 0.035746959182951184\n",
      "          total_loss: 0.036659413244989184\n",
      "          vf_explained_var: -0.17057937383651733\n",
      "          vf_loss: 0.011643161457808067\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "  iterations_since_restore: 760\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.37777777777777\n",
      "    ram_util_percent: 50.992592592592594\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868899307043406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.623751140647574\n",
      "    mean_inference_ms: 1.9295616546657237\n",
      "    mean_raw_obs_processing_ms: 2.18997342235344\n",
      "  time_since_restore: 19501.204756736755\n",
      "  time_this_iter_s: 19.342150926589966\n",
      "  time_total_s: 19501.204756736755\n",
      "  timers:\n",
      "    learn_throughput: 1347.158\n",
      "    learn_time_ms: 742.304\n",
      "    load_throughput: 51816.587\n",
      "    load_time_ms: 19.299\n",
      "    sample_throughput: 36.055\n",
      "    sample_time_ms: 27735.577\n",
      "    update_time_ms: 6.14\n",
      "  timestamp: 1635082381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 760\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   760</td><td style=\"text-align: right;\">         19501.2</td><td style=\"text-align: right;\">760000</td><td style=\"text-align: right;\"> -2.6648</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            266.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 761000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-33-26\n",
      "  done: false\n",
      "  episode_len_mean: 270.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.700999999999986\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2651\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1528080363174316e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1104508969518874\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007544871363589698\n",
      "          policy_loss: 0.0206705870727698\n",
      "          total_loss: 0.02207105921374427\n",
      "          vf_explained_var: 0.24178585410118103\n",
      "          vf_loss: 0.01250498055661511\n",
      "    num_agent_steps_sampled: 761000\n",
      "    num_agent_steps_trained: 761000\n",
      "    num_steps_sampled: 761000\n",
      "    num_steps_trained: 761000\n",
      "  iterations_since_restore: 761\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.6108108108108\n",
      "    ram_util_percent: 51.013513513513516\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869316464847749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.63112238383133\n",
      "    mean_inference_ms: 1.9296823566393955\n",
      "    mean_raw_obs_processing_ms: 2.1901131680174415\n",
      "  time_since_restore: 19526.72451233864\n",
      "  time_this_iter_s: 25.519755601882935\n",
      "  time_total_s: 19526.72451233864\n",
      "  timers:\n",
      "    learn_throughput: 1337.435\n",
      "    learn_time_ms: 747.7\n",
      "    load_throughput: 48998.764\n",
      "    load_time_ms: 20.409\n",
      "    sample_throughput: 37.204\n",
      "    sample_time_ms: 26878.907\n",
      "    update_time_ms: 6.123\n",
      "  timestamp: 1635082406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 761000\n",
      "  training_iteration: 761\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   761</td><td style=\"text-align: right;\">         19526.7</td><td style=\"text-align: right;\">761000</td><td style=\"text-align: right;\">  -2.701</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">             270.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 762000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-33-53\n",
      "  done: false\n",
      "  episode_len_mean: 272.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.728799999999985\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2654\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1528080363174316e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1142286115222506\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014812660928466908\n",
      "          policy_loss: 0.028404869635899863\n",
      "          total_loss: 0.028822789506779776\n",
      "          vf_explained_var: 0.3174341320991516\n",
      "          vf_loss: 0.011560204162055419\n",
      "    num_agent_steps_sampled: 762000\n",
      "    num_agent_steps_trained: 762000\n",
      "    num_steps_sampled: 762000\n",
      "    num_steps_trained: 762000\n",
      "  iterations_since_restore: 762\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.54324324324325\n",
      "    ram_util_percent: 51.24324324324324\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869808439251912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.638361957821086\n",
      "    mean_inference_ms: 1.929805883826673\n",
      "    mean_raw_obs_processing_ms: 2.190255805074762\n",
      "  time_since_restore: 19553.101927280426\n",
      "  time_this_iter_s: 26.37741494178772\n",
      "  time_total_s: 19553.101927280426\n",
      "  timers:\n",
      "    learn_throughput: 1333.514\n",
      "    learn_time_ms: 749.899\n",
      "    load_throughput: 49350.675\n",
      "    load_time_ms: 20.263\n",
      "    sample_throughput: 38.241\n",
      "    sample_time_ms: 26150.25\n",
      "    update_time_ms: 6.078\n",
      "  timestamp: 1635082433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 762000\n",
      "  training_iteration: 762\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   762</td><td style=\"text-align: right;\">         19553.1</td><td style=\"text-align: right;\">762000</td><td style=\"text-align: right;\"> -2.7288</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            272.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 763000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 275.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.754499999999985\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2657\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1528080363174316e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0711478769779206\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04563251397596954\n",
      "          policy_loss: 0.08625362631347444\n",
      "          total_loss: 0.08506436828109953\n",
      "          vf_explained_var: 0.4716328978538513\n",
      "          vf_loss: 0.009522222979770352\n",
      "    num_agent_steps_sampled: 763000\n",
      "    num_agent_steps_trained: 763000\n",
      "    num_steps_sampled: 763000\n",
      "    num_steps_trained: 763000\n",
      "  iterations_since_restore: 763\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.68\n",
      "    ram_util_percent: 51.475555555555566\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870322384903858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.645709658873326\n",
      "    mean_inference_ms: 1.9299361893781752\n",
      "    mean_raw_obs_processing_ms: 2.1903747369499498\n",
      "  time_since_restore: 19584.114134788513\n",
      "  time_this_iter_s: 31.012207508087158\n",
      "  time_total_s: 19584.114134788513\n",
      "  timers:\n",
      "    learn_throughput: 1314.87\n",
      "    learn_time_ms: 760.531\n",
      "    load_throughput: 49173.975\n",
      "    load_time_ms: 20.336\n",
      "    sample_throughput: 38.574\n",
      "    sample_time_ms: 25924.245\n",
      "    update_time_ms: 6.059\n",
      "  timestamp: 1635082464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 763000\n",
      "  training_iteration: 763\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   763</td><td style=\"text-align: right;\">         19584.1</td><td style=\"text-align: right;\">763000</td><td style=\"text-align: right;\"> -2.7545</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            275.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 764000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-34-58\n",
      "  done: false\n",
      "  episode_len_mean: 276.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.765999999999985\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2661\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.729212054476148e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8031575136714512\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03675525883397025\n",
      "          policy_loss: -0.012074589439564281\n",
      "          total_loss: -0.007762835630112224\n",
      "          vf_explained_var: 0.4425002336502075\n",
      "          vf_loss: 0.012343328446149826\n",
      "    num_agent_steps_sampled: 764000\n",
      "    num_agent_steps_trained: 764000\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "  iterations_since_restore: 764\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.88979591836735\n",
      "    ram_util_percent: 51.43469387755101\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871026426310312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.65578063728632\n",
      "    mean_inference_ms: 1.9301177269971859\n",
      "    mean_raw_obs_processing_ms: 2.1905204457396534\n",
      "  time_since_restore: 19618.500293254852\n",
      "  time_this_iter_s: 34.38615846633911\n",
      "  time_total_s: 19618.500293254852\n",
      "  timers:\n",
      "    learn_throughput: 1314.832\n",
      "    learn_time_ms: 760.553\n",
      "    load_throughput: 51878.05\n",
      "    load_time_ms: 19.276\n",
      "    sample_throughput: 38.155\n",
      "    sample_time_ms: 26208.628\n",
      "    update_time_ms: 6.781\n",
      "  timestamp: 1635082498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 764\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   764</td><td style=\"text-align: right;\">         19618.5</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\">  -2.766</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">             276.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 765000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-35-25\n",
      "  done: false\n",
      "  episode_len_mean: 279.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.797099999999984\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2664\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.093818081714222e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2726749963230557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.09396598782594912\n",
      "          policy_loss: -0.01967507708403799\n",
      "          total_loss: -0.025786948452393214\n",
      "          vf_explained_var: 0.5853976607322693\n",
      "          vf_loss: 0.006614880847822254\n",
      "    num_agent_steps_sampled: 765000\n",
      "    num_agent_steps_trained: 765000\n",
      "    num_steps_sampled: 765000\n",
      "    num_steps_trained: 765000\n",
      "  iterations_since_restore: 765\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.13947368421051\n",
      "    ram_util_percent: 51.37631578947368\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871564369345146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.663251866158294\n",
      "    mean_inference_ms: 1.9302579232577\n",
      "    mean_raw_obs_processing_ms: 2.19061828201246\n",
      "  time_since_restore: 19645.131739616394\n",
      "  time_this_iter_s: 26.631446361541748\n",
      "  time_total_s: 19645.131739616394\n",
      "  timers:\n",
      "    learn_throughput: 1317.512\n",
      "    learn_time_ms: 759.007\n",
      "    load_throughput: 49625.869\n",
      "    load_time_ms: 20.151\n",
      "    sample_throughput: 38.859\n",
      "    sample_time_ms: 25733.781\n",
      "    update_time_ms: 6.722\n",
      "  timestamp: 1635082525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 765000\n",
      "  training_iteration: 765\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   765</td><td style=\"text-align: right;\">         19645.1</td><td style=\"text-align: right;\">765000</td><td style=\"text-align: right;\"> -2.7971</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            279.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 766000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-35-55\n",
      "  done: false\n",
      "  episode_len_mean: 281.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.8135999999999837\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2666\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0640727122571332e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5843869434462654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016323170330165344\n",
      "          policy_loss: -0.08744880788856083\n",
      "          total_loss: -0.09079172313213349\n",
      "          vf_explained_var: -0.16085346043109894\n",
      "          vf_loss: 0.012500954309426661\n",
      "    num_agent_steps_sampled: 766000\n",
      "    num_agent_steps_trained: 766000\n",
      "    num_steps_sampled: 766000\n",
      "    num_steps_trained: 766000\n",
      "  iterations_since_restore: 766\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.03720930232558\n",
      "    ram_util_percent: 51.358139534883726\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871928230078307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.66831808991381\n",
      "    mean_inference_ms: 1.930353649687595\n",
      "    mean_raw_obs_processing_ms: 2.1906659091447067\n",
      "  time_since_restore: 19675.74227833748\n",
      "  time_this_iter_s: 30.610538721084595\n",
      "  time_total_s: 19675.74227833748\n",
      "  timers:\n",
      "    learn_throughput: 1321.721\n",
      "    learn_time_ms: 756.59\n",
      "    load_throughput: 49742.4\n",
      "    load_time_ms: 20.104\n",
      "    sample_throughput: 38.795\n",
      "    sample_time_ms: 25776.759\n",
      "    update_time_ms: 7.632\n",
      "  timestamp: 1635082555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 766000\n",
      "  training_iteration: 766\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   766</td><td style=\"text-align: right;\">         19675.7</td><td style=\"text-align: right;\">766000</td><td style=\"text-align: right;\"> -2.8136</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            281.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 767000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-36-43\n",
      "  done: false\n",
      "  episode_len_mean: 284.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.849499999999984\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2670\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0640727122571332e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5914654506577386\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.047263854740263234\n",
      "          policy_loss: 0.017302289770709144\n",
      "          total_loss: 0.008196293645434909\n",
      "          vf_explained_var: 0.6183378100395203\n",
      "          vf_loss: 0.006808658004997091\n",
      "    num_agent_steps_sampled: 767000\n",
      "    num_agent_steps_trained: 767000\n",
      "    num_steps_sampled: 767000\n",
      "    num_steps_trained: 767000\n",
      "  iterations_since_restore: 767\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.38985507246376\n",
      "    ram_util_percent: 51.146376811594195\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872685845780076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.67868006755392\n",
      "    mean_inference_ms: 1.930549758897128\n",
      "    mean_raw_obs_processing_ms: 2.1915974880373748\n",
      "  time_since_restore: 19723.633685827255\n",
      "  time_this_iter_s: 47.89140748977661\n",
      "  time_total_s: 19723.633685827255\n",
      "  timers:\n",
      "    learn_throughput: 1315.794\n",
      "    learn_time_ms: 759.998\n",
      "    load_throughput: 52393.173\n",
      "    load_time_ms: 19.086\n",
      "    sample_throughput: 37.472\n",
      "    sample_time_ms: 26686.327\n",
      "    update_time_ms: 8.069\n",
      "  timestamp: 1635082603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 767000\n",
      "  training_iteration: 767\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   767</td><td style=\"text-align: right;\">         19723.6</td><td style=\"text-align: right;\">767000</td><td style=\"text-align: right;\"> -2.8495</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            284.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-37-14\n",
      "  done: false\n",
      "  episode_len_mean: 287.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.877199999999982\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2672\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5961090683857002e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.845238783624437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015226380668608385\n",
      "          policy_loss: -0.10129604422383838\n",
      "          total_loss: -0.11523924602402581\n",
      "          vf_explained_var: 0.20396021008491516\n",
      "          vf_loss: 0.004509181614008008\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_agent_steps_trained: 768000\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "  iterations_since_restore: 768\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.85116279069769\n",
      "    ram_util_percent: 50.69302325581395\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873066637342068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.683959515083117\n",
      "    mean_inference_ms: 1.9306488879703778\n",
      "    mean_raw_obs_processing_ms: 2.1920371859431715\n",
      "  time_since_restore: 19753.836509227753\n",
      "  time_this_iter_s: 30.202823400497437\n",
      "  time_total_s: 19753.836509227753\n",
      "  timers:\n",
      "    learn_throughput: 1317.755\n",
      "    learn_time_ms: 758.866\n",
      "    load_throughput: 49875.072\n",
      "    load_time_ms: 20.05\n",
      "    sample_throughput: 35.652\n",
      "    sample_time_ms: 28049.079\n",
      "    update_time_ms: 8.359\n",
      "  timestamp: 1635082634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 768\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   768</td><td style=\"text-align: right;\">         19753.8</td><td style=\"text-align: right;\">768000</td><td style=\"text-align: right;\"> -2.8772</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            287.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 769000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-37-47\n",
      "  done: false\n",
      "  episode_len_mean: 291.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.9312999999999816\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2676\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5961090683857002e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5502185146013896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019958271155723118\n",
      "          policy_loss: -0.09276651077800327\n",
      "          total_loss: -0.09851634816990959\n",
      "          vf_explained_var: 0.886216402053833\n",
      "          vf_loss: 0.00975235073775467\n",
      "    num_agent_steps_sampled: 769000\n",
      "    num_agent_steps_trained: 769000\n",
      "    num_steps_sampled: 769000\n",
      "    num_steps_trained: 769000\n",
      "  iterations_since_restore: 769\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.2125\n",
      "    ram_util_percent: 50.93958333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873838076303633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.694620406537414\n",
      "    mean_inference_ms: 1.9308498208212788\n",
      "    mean_raw_obs_processing_ms: 2.1929195353397803\n",
      "  time_since_restore: 19787.67909502983\n",
      "  time_this_iter_s: 33.84258580207825\n",
      "  time_total_s: 19787.67909502983\n",
      "  timers:\n",
      "    learn_throughput: 1318.149\n",
      "    learn_time_ms: 758.639\n",
      "    load_throughput: 49936.471\n",
      "    load_time_ms: 20.025\n",
      "    sample_throughput: 33.57\n",
      "    sample_time_ms: 29788.336\n",
      "    update_time_ms: 8.449\n",
      "  timestamp: 1635082667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 769000\n",
      "  training_iteration: 769\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   769</td><td style=\"text-align: right;\">         19787.7</td><td style=\"text-align: right;\">769000</td><td style=\"text-align: right;\"> -2.9313</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            291.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 770000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-38-20\n",
      "  done: false\n",
      "  episode_len_mean: 294.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9000000000000015\n",
      "  episode_reward_mean: -2.967399999999981\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2679\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5961090683857002e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.698550898498959\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013691867303100342\n",
      "          policy_loss: -0.0835406202202042\n",
      "          total_loss: -0.09652119369970427\n",
      "          vf_explained_var: 0.927364706993103\n",
      "          vf_loss: 0.004004940059449937\n",
      "    num_agent_steps_sampled: 770000\n",
      "    num_agent_steps_trained: 770000\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "  iterations_since_restore: 770\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.55869565217391\n",
      "    ram_util_percent: 51.1086956521739\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874405939564376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.70203325570903\n",
      "    mean_inference_ms: 1.9310013708658398\n",
      "    mean_raw_obs_processing_ms: 2.1927354886714863\n",
      "  time_since_restore: 19819.896449804306\n",
      "  time_this_iter_s: 32.2173547744751\n",
      "  time_total_s: 19819.896449804306\n",
      "  timers:\n",
      "    learn_throughput: 1311.654\n",
      "    learn_time_ms: 762.396\n",
      "    load_throughput: 51520.051\n",
      "    load_time_ms: 19.41\n",
      "    sample_throughput: 32.183\n",
      "    sample_time_ms: 31071.998\n",
      "    update_time_ms: 9.013\n",
      "  timestamp: 1635082700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 770\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   770</td><td style=\"text-align: right;\">         19819.9</td><td style=\"text-align: right;\">770000</td><td style=\"text-align: right;\"> -2.9674</td><td style=\"text-align: right;\">                -1.9</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            294.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 771000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-38-53\n",
      "  done: false\n",
      "  episode_len_mean: 297.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -2.99179999999998\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2681\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5961090683857002e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6432768755488925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009285331514020119\n",
      "          policy_loss: -0.16741871949699189\n",
      "          total_loss: -0.18092896288467777\n",
      "          vf_explained_var: 0.9520081281661987\n",
      "          vf_loss: 0.0029225203182755245\n",
      "    num_agent_steps_sampled: 771000\n",
      "    num_agent_steps_trained: 771000\n",
      "    num_steps_sampled: 771000\n",
      "    num_steps_trained: 771000\n",
      "  iterations_since_restore: 771\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.34166666666665\n",
      "    ram_util_percent: 51.41874999999999\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874800702331848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.707212058735784\n",
      "    mean_inference_ms: 1.931107533959568\n",
      "    mean_raw_obs_processing_ms: 2.1925614651946086\n",
      "  time_since_restore: 19853.12470817566\n",
      "  time_this_iter_s: 33.22825837135315\n",
      "  time_total_s: 19853.12470817566\n",
      "  timers:\n",
      "    learn_throughput: 1320.807\n",
      "    learn_time_ms: 757.113\n",
      "    load_throughput: 51446.304\n",
      "    load_time_ms: 19.438\n",
      "    sample_throughput: 31.398\n",
      "    sample_time_ms: 31849.248\n",
      "    update_time_ms: 8.756\n",
      "  timestamp: 1635082733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 771000\n",
      "  training_iteration: 771\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   771</td><td style=\"text-align: right;\">         19853.1</td><td style=\"text-align: right;\">771000</td><td style=\"text-align: right;\"> -2.9918</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            297.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 772000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-39-27\n",
      "  done: false\n",
      "  episode_len_mean: 300.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.0214999999999805\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2685\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5961090683857002e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5853474497795106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03803835327645449\n",
      "          policy_loss: -0.0009996000263426039\n",
      "          total_loss: -0.012696083883444469\n",
      "          vf_explained_var: 0.9154582619667053\n",
      "          vf_loss: 0.004156992312831184\n",
      "    num_agent_steps_sampled: 772000\n",
      "    num_agent_steps_trained: 772000\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "  iterations_since_restore: 772\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.22083333333335\n",
      "    ram_util_percent: 51.34375\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387559703862423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.717286685156918\n",
      "    mean_inference_ms: 1.9313148807739047\n",
      "    mean_raw_obs_processing_ms: 2.1922172740240615\n",
      "  time_since_restore: 19886.99811387062\n",
      "  time_this_iter_s: 33.87340569496155\n",
      "  time_total_s: 19886.99811387062\n",
      "  timers:\n",
      "    learn_throughput: 1312.797\n",
      "    learn_time_ms: 761.732\n",
      "    load_throughput: 49422.899\n",
      "    load_time_ms: 20.234\n",
      "    sample_throughput: 30.682\n",
      "    sample_time_ms: 32592.726\n",
      "    update_time_ms: 9.484\n",
      "  timestamp: 1635082767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 772\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   772</td><td style=\"text-align: right;\">           19887</td><td style=\"text-align: right;\">772000</td><td style=\"text-align: right;\"> -3.0215</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            300.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 773000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-40-01\n",
      "  done: false\n",
      "  episode_len_mean: 302.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.057199999999979\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2688\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.39416360257855e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5267283704545762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016658260050860938\n",
      "          policy_loss: 0.015156684070825576\n",
      "          total_loss: 0.014098453356160058\n",
      "          vf_explained_var: 0.7180027365684509\n",
      "          vf_loss: 0.01420905365763853\n",
      "    num_agent_steps_sampled: 773000\n",
      "    num_agent_steps_trained: 773000\n",
      "    num_steps_sampled: 773000\n",
      "    num_steps_trained: 773000\n",
      "  iterations_since_restore: 773\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.07142857142857\n",
      "    ram_util_percent: 51.712244897959195\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876198502075635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.724619083136773\n",
      "    mean_inference_ms: 1.931470304381993\n",
      "    mean_raw_obs_processing_ms: 2.1919640282624475\n",
      "  time_since_restore: 19921.146529197693\n",
      "  time_this_iter_s: 34.148415327072144\n",
      "  time_total_s: 19921.146529197693\n",
      "  timers:\n",
      "    learn_throughput: 1302.525\n",
      "    learn_time_ms: 767.74\n",
      "    load_throughput: 49721.82\n",
      "    load_time_ms: 20.112\n",
      "    sample_throughput: 30.395\n",
      "    sample_time_ms: 32900.322\n",
      "    update_time_ms: 9.783\n",
      "  timestamp: 1635082801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 773000\n",
      "  training_iteration: 773\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   773</td><td style=\"text-align: right;\">         19921.1</td><td style=\"text-align: right;\">773000</td><td style=\"text-align: right;\"> -3.0572</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            302.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 774000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-40-40\n",
      "  done: false\n",
      "  episode_len_mean: 304.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.0790999999999786\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2691\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.39416360257855e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5557236591974895\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028286458694804197\n",
      "          policy_loss: 0.033071016354693304\n",
      "          total_loss: 0.025237288574377695\n",
      "          vf_explained_var: 0.6583817005157471\n",
      "          vf_loss: 0.00772350924089551\n",
      "    num_agent_steps_sampled: 774000\n",
      "    num_agent_steps_trained: 774000\n",
      "    num_steps_sampled: 774000\n",
      "    num_steps_trained: 774000\n",
      "  iterations_since_restore: 774\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.36181818181821\n",
      "    ram_util_percent: 51.5890909090909\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876783895867694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.732164719342826\n",
      "    mean_inference_ms: 1.9316288898003462\n",
      "    mean_raw_obs_processing_ms: 2.1916877253351092\n",
      "  time_since_restore: 19959.7946228981\n",
      "  time_this_iter_s: 38.648093700408936\n",
      "  time_total_s: 19959.7946228981\n",
      "  timers:\n",
      "    learn_throughput: 1290.406\n",
      "    learn_time_ms: 774.95\n",
      "    load_throughput: 49459.266\n",
      "    load_time_ms: 20.219\n",
      "    sample_throughput: 30.012\n",
      "    sample_time_ms: 33319.919\n",
      "    update_time_ms: 9.067\n",
      "  timestamp: 1635082840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 774000\n",
      "  training_iteration: 774\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   774</td><td style=\"text-align: right;\">         19959.8</td><td style=\"text-align: right;\">774000</td><td style=\"text-align: right;\"> -3.0791</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            304.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 775000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-41-14\n",
      "  done: false\n",
      "  episode_len_mean: 307.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.108399999999977\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2694\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.591245403867824e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6699435459242926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023494190448028165\n",
      "          policy_loss: 0.018183358924256432\n",
      "          total_loss: 0.005722740499509706\n",
      "          vf_explained_var: 0.8583396077156067\n",
      "          vf_loss: 0.00423881169473235\n",
      "    num_agent_steps_sampled: 775000\n",
      "    num_agent_steps_trained: 775000\n",
      "    num_steps_sampled: 775000\n",
      "    num_steps_trained: 775000\n",
      "  iterations_since_restore: 775\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.13673469387754\n",
      "    ram_util_percent: 51.62040816326532\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877372197863635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.739787145501325\n",
      "    mean_inference_ms: 1.9317901607207246\n",
      "    mean_raw_obs_processing_ms: 2.191387880520366\n",
      "  time_since_restore: 19994.37422657013\n",
      "  time_this_iter_s: 34.57960367202759\n",
      "  time_total_s: 19994.37422657013\n",
      "  timers:\n",
      "    learn_throughput: 1289.013\n",
      "    learn_time_ms: 775.787\n",
      "    load_throughput: 52077.472\n",
      "    load_time_ms: 19.202\n",
      "    sample_throughput: 29.313\n",
      "    sample_time_ms: 34114.11\n",
      "    update_time_ms: 9.637\n",
      "  timestamp: 1635082874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 775000\n",
      "  training_iteration: 775\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   775</td><td style=\"text-align: right;\">         19994.4</td><td style=\"text-align: right;\">775000</td><td style=\"text-align: right;\"> -3.1084</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            307.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-41-49\n",
      "  done: false\n",
      "  episode_len_mean: 309.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.1274999999999773\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2697\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5996949672698975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010982262362218817\n",
      "          policy_loss: -0.14074429290162194\n",
      "          total_loss: -0.150295546816455\n",
      "          vf_explained_var: 0.7309721112251282\n",
      "          vf_loss: 0.006445695423624582\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "  iterations_since_restore: 776\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.39387755102041\n",
      "    ram_util_percent: 51.55102040816327\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038779604638980454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.747501090449944\n",
      "    mean_inference_ms: 1.931951074514035\n",
      "    mean_raw_obs_processing_ms: 2.191063299115673\n",
      "  time_since_restore: 20028.706233739853\n",
      "  time_this_iter_s: 34.33200716972351\n",
      "  time_total_s: 20028.706233739853\n",
      "  timers:\n",
      "    learn_throughput: 1289.649\n",
      "    learn_time_ms: 775.405\n",
      "    load_throughput: 52210.365\n",
      "    load_time_ms: 19.153\n",
      "    sample_throughput: 28.994\n",
      "    sample_time_ms: 34489.33\n",
      "    update_time_ms: 7.69\n",
      "  timestamp: 1635082909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 776\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   776</td><td style=\"text-align: right;\">         20028.7</td><td style=\"text-align: right;\">776000</td><td style=\"text-align: right;\"> -3.1275</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">             309.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 777000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-42-40\n",
      "  done: false\n",
      "  episode_len_mean: 312.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.149899999999976\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2701\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5775165888998244\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010003643876723424\n",
      "          policy_loss: 0.07225088675816854\n",
      "          total_loss: 0.06262593964735667\n",
      "          vf_explained_var: 0.7316579222679138\n",
      "          vf_loss: 0.0061502180527895686\n",
      "    num_agent_steps_sampled: 777000\n",
      "    num_agent_steps_trained: 777000\n",
      "    num_steps_sampled: 777000\n",
      "    num_steps_trained: 777000\n",
      "  iterations_since_restore: 777\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.5082191780822\n",
      "    ram_util_percent: 51.409589041095884\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038787399639074634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.757847026445717\n",
      "    mean_inference_ms: 1.9321643507108626\n",
      "    mean_raw_obs_processing_ms: 2.191480914797469\n",
      "  time_since_restore: 20079.532720804214\n",
      "  time_this_iter_s: 50.82648706436157\n",
      "  time_total_s: 20079.532720804214\n",
      "  timers:\n",
      "    learn_throughput: 1290.138\n",
      "    learn_time_ms: 775.111\n",
      "    load_throughput: 50390.141\n",
      "    load_time_ms: 19.845\n",
      "    sample_throughput: 28.75\n",
      "    sample_time_ms: 34782.444\n",
      "    update_time_ms: 7.664\n",
      "  timestamp: 1635082960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 777000\n",
      "  training_iteration: 777\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   777</td><td style=\"text-align: right;\">         20079.5</td><td style=\"text-align: right;\">777000</td><td style=\"text-align: right;\"> -3.1499</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            312.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 778000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 313.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.1667999999999763\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2704\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.600071402390798\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008727857193966947\n",
      "          policy_loss: 0.025836302505599127\n",
      "          total_loss: 0.013754149857494567\n",
      "          vf_explained_var: 0.8259048461914062\n",
      "          vf_loss: 0.003918561301866753\n",
      "    num_agent_steps_sampled: 778000\n",
      "    num_agent_steps_trained: 778000\n",
      "    num_steps_sampled: 778000\n",
      "    num_steps_trained: 778000\n",
      "  iterations_since_restore: 778\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.40208333333332\n",
      "    ram_util_percent: 51.71875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387932237567758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.765663580646546\n",
      "    mean_inference_ms: 1.9323232979740026\n",
      "    mean_raw_obs_processing_ms: 2.1917898867206933\n",
      "  time_since_restore: 20112.90867972374\n",
      "  time_this_iter_s: 33.37595891952515\n",
      "  time_total_s: 20112.90867972374\n",
      "  timers:\n",
      "    learn_throughput: 1286.693\n",
      "    learn_time_ms: 777.186\n",
      "    load_throughput: 50561.318\n",
      "    load_time_ms: 19.778\n",
      "    sample_throughput: 28.491\n",
      "    sample_time_ms: 35098.426\n",
      "    update_time_ms: 6.777\n",
      "  timestamp: 1635082993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 778000\n",
      "  training_iteration: 778\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   778</td><td style=\"text-align: right;\">         20112.9</td><td style=\"text-align: right;\">778000</td><td style=\"text-align: right;\"> -3.1668</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            313.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 779000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-43-46\n",
      "  done: false\n",
      "  episode_len_mean: 313.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.1666999999999774\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2707\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5366630567444695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019341136285207245\n",
      "          policy_loss: -0.13200181755754684\n",
      "          total_loss: -0.14018076575464672\n",
      "          vf_explained_var: 0.7000134587287903\n",
      "          vf_loss: 0.007187680398217506\n",
      "    num_agent_steps_sampled: 779000\n",
      "    num_agent_steps_trained: 779000\n",
      "    num_steps_sampled: 779000\n",
      "    num_steps_trained: 779000\n",
      "  iterations_since_restore: 779\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.94347826086955\n",
      "    ram_util_percent: 51.93260869565218\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038799118278874556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.773652100999094\n",
      "    mean_inference_ms: 1.932482174653925\n",
      "    mean_raw_obs_processing_ms: 2.192101569024752\n",
      "  time_since_restore: 20145.557047843933\n",
      "  time_this_iter_s: 32.64836812019348\n",
      "  time_total_s: 20145.557047843933\n",
      "  timers:\n",
      "    learn_throughput: 1286.413\n",
      "    learn_time_ms: 777.355\n",
      "    load_throughput: 48115.653\n",
      "    load_time_ms: 20.783\n",
      "    sample_throughput: 28.589\n",
      "    sample_time_ms: 34978.45\n",
      "    update_time_ms: 6.352\n",
      "  timestamp: 1635083026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 779000\n",
      "  training_iteration: 779\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   779</td><td style=\"text-align: right;\">         20145.6</td><td style=\"text-align: right;\">779000</td><td style=\"text-align: right;\"> -3.1667</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            313.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-44-18\n",
      "  done: false\n",
      "  episode_len_mean: 314.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.171999999999976\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2711\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6405035191112094\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016511541657382386\n",
      "          policy_loss: 0.06822539079520437\n",
      "          total_loss: 0.05911693792376253\n",
      "          vf_explained_var: 0.716736376285553\n",
      "          vf_loss: 0.007296584634524253\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_agent_steps_trained: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "  iterations_since_restore: 780\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.0608695652174\n",
      "    ram_util_percent: 52.0\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880691174077023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.78445801832309\n",
      "    mean_inference_ms: 1.932693623006408\n",
      "    mean_raw_obs_processing_ms: 2.1920444708544506\n",
      "  time_since_restore: 20177.954954385757\n",
      "  time_this_iter_s: 32.39790654182434\n",
      "  time_total_s: 20177.954954385757\n",
      "  timers:\n",
      "    learn_throughput: 1290.893\n",
      "    learn_time_ms: 774.658\n",
      "    load_throughput: 46663.73\n",
      "    load_time_ms: 21.43\n",
      "    sample_throughput: 28.572\n",
      "    sample_time_ms: 34999.222\n",
      "    update_time_ms: 5.555\n",
      "  timestamp: 1635083058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 780\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   780</td><td style=\"text-align: right;\">           20178</td><td style=\"text-align: right;\">780000</td><td style=\"text-align: right;\">  -3.172</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            314.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 781000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-44-49\n",
      "  done: false\n",
      "  episode_len_mean: 315.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.1867999999999754\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2714\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7242835720380147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012930128928887698\n",
      "          policy_loss: 0.051116178184747695\n",
      "          total_loss: 0.04013370532128546\n",
      "          vf_explained_var: 0.725238025188446\n",
      "          vf_loss: 0.006260360493454047\n",
      "    num_agent_steps_sampled: 781000\n",
      "    num_agent_steps_trained: 781000\n",
      "    num_steps_sampled: 781000\n",
      "    num_steps_trained: 781000\n",
      "  iterations_since_restore: 781\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.36666666666666\n",
      "    ram_util_percent: 51.92222222222223\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038812621742370164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.792334716616747\n",
      "    mean_inference_ms: 1.9328494224876602\n",
      "    mean_raw_obs_processing_ms: 2.1916708527414785\n",
      "  time_since_restore: 20209.073878526688\n",
      "  time_this_iter_s: 31.118924140930176\n",
      "  time_total_s: 20209.073878526688\n",
      "  timers:\n",
      "    learn_throughput: 1296.282\n",
      "    learn_time_ms: 771.437\n",
      "    load_throughput: 46889.875\n",
      "    load_time_ms: 21.327\n",
      "    sample_throughput: 28.742\n",
      "    sample_time_ms: 34791.822\n",
      "    update_time_ms: 5.471\n",
      "  timestamp: 1635083089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 781000\n",
      "  training_iteration: 781\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   781</td><td style=\"text-align: right;\">         20209.1</td><td style=\"text-align: right;\">781000</td><td style=\"text-align: right;\"> -3.1868</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            315.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 782000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-45-20\n",
      "  done: false\n",
      "  episode_len_mean: 317.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.205099999999976\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2717\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7044996552997165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008635426339779784\n",
      "          policy_loss: 0.0687801664074262\n",
      "          total_loss: 0.05889159755574332\n",
      "          vf_explained_var: 0.6374136805534363\n",
      "          vf_loss: 0.007156427392813688\n",
      "    num_agent_steps_sampled: 782000\n",
      "    num_agent_steps_trained: 782000\n",
      "    num_steps_sampled: 782000\n",
      "    num_steps_trained: 782000\n",
      "  iterations_since_restore: 782\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.08636363636364\n",
      "    ram_util_percent: 51.838636363636354\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881829505783492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.800169013698067\n",
      "    mean_inference_ms: 1.9330049981063093\n",
      "    mean_raw_obs_processing_ms: 2.191273131273037\n",
      "  time_since_restore: 20239.821544885635\n",
      "  time_this_iter_s: 30.747666358947754\n",
      "  time_total_s: 20239.821544885635\n",
      "  timers:\n",
      "    learn_throughput: 1306.95\n",
      "    learn_time_ms: 765.14\n",
      "    load_throughput: 46216.443\n",
      "    load_time_ms: 21.637\n",
      "    sample_throughput: 28.997\n",
      "    sample_time_ms: 34485.912\n",
      "    update_time_ms: 4.927\n",
      "  timestamp: 1635083120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 782000\n",
      "  training_iteration: 782\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   782</td><td style=\"text-align: right;\">         20239.8</td><td style=\"text-align: right;\">782000</td><td style=\"text-align: right;\"> -3.2051</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            317.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 783000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 319.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.225499999999975\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2720\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6524101376533509\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012358280313648735\n",
      "          policy_loss: 0.03740560900833872\n",
      "          total_loss: 0.02899562335676617\n",
      "          vf_explained_var: 0.5511165857315063\n",
      "          vf_loss: 0.008114115354126422\n",
      "    num_agent_steps_sampled: 783000\n",
      "    num_agent_steps_trained: 783000\n",
      "    num_steps_sampled: 783000\n",
      "    num_steps_trained: 783000\n",
      "  iterations_since_restore: 783\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.17391304347827\n",
      "    ram_util_percent: 51.8608695652174\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038823967448697136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.808045152836243\n",
      "    mean_inference_ms: 1.933161921601312\n",
      "    mean_raw_obs_processing_ms: 2.1908514385141973\n",
      "  time_since_restore: 20272.055468797684\n",
      "  time_this_iter_s: 32.23392391204834\n",
      "  time_total_s: 20272.055468797684\n",
      "  timers:\n",
      "    learn_throughput: 1337.068\n",
      "    learn_time_ms: 747.905\n",
      "    load_throughput: 44352.489\n",
      "    load_time_ms: 22.547\n",
      "    sample_throughput: 29.145\n",
      "    sample_time_ms: 34311.431\n",
      "    update_time_ms: 4.27\n",
      "  timestamp: 1635083152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 783000\n",
      "  training_iteration: 783\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   783</td><td style=\"text-align: right;\">         20272.1</td><td style=\"text-align: right;\">783000</td><td style=\"text-align: right;\"> -3.2255</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">             319.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-46-23\n",
      "  done: false\n",
      "  episode_len_mean: 321.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.2430999999999752\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2723\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6024370484881931\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015802153460181264\n",
      "          policy_loss: -0.10103229615423415\n",
      "          total_loss: -0.1042971808049414\n",
      "          vf_explained_var: 0.3550052344799042\n",
      "          vf_loss: 0.012759481106574337\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_agent_steps_trained: 784000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "  iterations_since_restore: 784\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.8\n",
      "    ram_util_percent: 51.76976744186045\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882954184426131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.815779027629706\n",
      "    mean_inference_ms: 1.9333145579824644\n",
      "    mean_raw_obs_processing_ms: 2.19043132537617\n",
      "  time_since_restore: 20302.44709300995\n",
      "  time_this_iter_s: 30.391624212265015\n",
      "  time_total_s: 20302.44709300995\n",
      "  timers:\n",
      "    learn_throughput: 1360.735\n",
      "    learn_time_ms: 734.897\n",
      "    load_throughput: 42859.417\n",
      "    load_time_ms: 23.332\n",
      "    sample_throughput: 29.853\n",
      "    sample_time_ms: 33498.005\n",
      "    update_time_ms: 4.174\n",
      "  timestamp: 1635083183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 784\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   784</td><td style=\"text-align: right;\">         20302.4</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\"> -3.2431</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            321.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 785000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-46-54\n",
      "  done: false\n",
      "  episode_len_mean: 323.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.261099999999974\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2727\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6311918709013198\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015683284698058998\n",
      "          policy_loss: 0.0064287576410505505\n",
      "          total_loss: 0.00019636986156304677\n",
      "          vf_explained_var: 0.5756270885467529\n",
      "          vf_loss: 0.01007953301175601\n",
      "    num_agent_steps_sampled: 785000\n",
      "    num_agent_steps_trained: 785000\n",
      "    num_steps_sampled: 785000\n",
      "    num_steps_trained: 785000\n",
      "  iterations_since_restore: 785\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.57173913043479\n",
      "    ram_util_percent: 51.55869565217392\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883705157022016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.826108674871577\n",
      "    mean_inference_ms: 1.9335181121337677\n",
      "    mean_raw_obs_processing_ms: 2.18985080148087\n",
      "  time_since_restore: 20334.202293395996\n",
      "  time_this_iter_s: 31.755200386047363\n",
      "  time_total_s: 20334.202293395996\n",
      "  timers:\n",
      "    learn_throughput: 1360.769\n",
      "    learn_time_ms: 734.879\n",
      "    load_throughput: 42852.28\n",
      "    load_time_ms: 23.336\n",
      "    sample_throughput: 30.106\n",
      "    sample_time_ms: 33215.783\n",
      "    update_time_ms: 4.086\n",
      "  timestamp: 1635083214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 785000\n",
      "  training_iteration: 785\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   785</td><td style=\"text-align: right;\">         20334.2</td><td style=\"text-align: right;\">785000</td><td style=\"text-align: right;\"> -3.2611</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            323.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 786000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-47-43\n",
      "  done: false\n",
      "  episode_len_mean: 324.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.2729999999999744\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2730\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6458908478418985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010261733764244517\n",
      "          policy_loss: 0.0785612240433693\n",
      "          total_loss: 0.07064186872707473\n",
      "          vf_explained_var: 0.3915586769580841\n",
      "          vf_loss: 0.008539552537129364\n",
      "    num_agent_steps_sampled: 786000\n",
      "    num_agent_steps_trained: 786000\n",
      "    num_steps_sampled: 786000\n",
      "    num_steps_trained: 786000\n",
      "  iterations_since_restore: 786\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.84057971014492\n",
      "    ram_util_percent: 51.591304347826096\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038842665029399315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.833853665973443\n",
      "    mean_inference_ms: 1.9336701328606052\n",
      "    mean_raw_obs_processing_ms: 2.1900486001820587\n",
      "  time_since_restore: 20382.68217420578\n",
      "  time_this_iter_s: 48.479880809783936\n",
      "  time_total_s: 20382.68217420578\n",
      "  timers:\n",
      "    learn_throughput: 1352.932\n",
      "    learn_time_ms: 739.136\n",
      "    load_throughput: 41422.484\n",
      "    load_time_ms: 24.141\n",
      "    sample_throughput: 28.881\n",
      "    sample_time_ms: 34624.51\n",
      "    update_time_ms: 4.933\n",
      "  timestamp: 1635083263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 786000\n",
      "  training_iteration: 786\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   786</td><td style=\"text-align: right;\">         20382.7</td><td style=\"text-align: right;\">786000</td><td style=\"text-align: right;\">  -3.273</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            324.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 787000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-48-17\n",
      "  done: false\n",
      "  episode_len_mean: 325.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.2837999999999745\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2733\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.660279683272044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00882566047660889\n",
      "          policy_loss: 0.005619012481636472\n",
      "          total_loss: -0.0012204719914330375\n",
      "          vf_explained_var: 0.46324747800827026\n",
      "          vf_loss: 0.009763310487485594\n",
      "    num_agent_steps_sampled: 787000\n",
      "    num_agent_steps_trained: 787000\n",
      "    num_steps_sampled: 787000\n",
      "    num_steps_trained: 787000\n",
      "  iterations_since_restore: 787\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.93333333333332\n",
      "    ram_util_percent: 51.89166666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038848316738021624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.841758167556687\n",
      "    mean_inference_ms: 1.9338221409872745\n",
      "    mean_raw_obs_processing_ms: 2.190223466904474\n",
      "  time_since_restore: 20416.401272773743\n",
      "  time_this_iter_s: 33.71909856796265\n",
      "  time_total_s: 20416.401272773743\n",
      "  timers:\n",
      "    learn_throughput: 1321.164\n",
      "    learn_time_ms: 756.909\n",
      "    load_throughput: 42735.092\n",
      "    load_time_ms: 23.4\n",
      "    sample_throughput: 30.398\n",
      "    sample_time_ms: 32897.019\n",
      "    update_time_ms: 4.663\n",
      "  timestamp: 1635083297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 787000\n",
      "  training_iteration: 787\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   787</td><td style=\"text-align: right;\">         20416.4</td><td style=\"text-align: right;\">787000</td><td style=\"text-align: right;\"> -3.2838</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            325.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 788000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-48-48\n",
      "  done: false\n",
      "  episode_len_mean: 326.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.259999999999996\n",
      "  episode_reward_mean: -3.294799999999974\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2736\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.3868681058017375e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6641796112060547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0817338523102617\n",
      "          policy_loss: -0.1139512225985527\n",
      "          total_loss: -0.1202177365620931\n",
      "          vf_explained_var: 0.5141204595565796\n",
      "          vf_loss: 0.010375282687083301\n",
      "    num_agent_steps_sampled: 788000\n",
      "    num_agent_steps_trained: 788000\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "  iterations_since_restore: 788\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.77777777777777\n",
      "    ram_util_percent: 52.11555555555554\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03885395979046824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.849658912617933\n",
      "    mean_inference_ms: 1.933973111379277\n",
      "    mean_raw_obs_processing_ms: 2.1904002037333625\n",
      "  time_since_restore: 20448.110788345337\n",
      "  time_this_iter_s: 31.70951557159424\n",
      "  time_total_s: 20448.110788345337\n",
      "  timers:\n",
      "    learn_throughput: 1274.659\n",
      "    learn_time_ms: 784.524\n",
      "    load_throughput: 43124.168\n",
      "    load_time_ms: 23.189\n",
      "    sample_throughput: 30.578\n",
      "    sample_time_ms: 32702.792\n",
      "    update_time_ms: 5.022\n",
      "  timestamp: 1635083328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 788\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   788</td><td style=\"text-align: right;\">         20448.1</td><td style=\"text-align: right;\">788000</td><td style=\"text-align: right;\"> -3.2948</td><td style=\"text-align: right;\">               -2.26</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            326.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 789000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-49-24\n",
      "  done: false\n",
      "  episode_len_mean: 328.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.3177999999999725\n",
      "  episode_reward_min: -5.809999999999921\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2740\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.080302158702608e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5759545299741957\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.05994738577403354\n",
      "          policy_loss: -0.07380693952242533\n",
      "          total_loss: -0.0795178134408262\n",
      "          vf_explained_var: 0.5875658988952637\n",
      "          vf_loss: 0.010048667657085592\n",
      "    num_agent_steps_sampled: 789000\n",
      "    num_agent_steps_trained: 789000\n",
      "    num_steps_sampled: 789000\n",
      "    num_steps_trained: 789000\n",
      "  iterations_since_restore: 789\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.25294117647059\n",
      "    ram_util_percent: 52.205882352941174\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886155673901918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.860569061407528\n",
      "    mean_inference_ms: 1.934177704619554\n",
      "    mean_raw_obs_processing_ms: 2.190405481746738\n",
      "  time_since_restore: 20483.608952760696\n",
      "  time_this_iter_s: 35.4981644153595\n",
      "  time_total_s: 20483.608952760696\n",
      "  timers:\n",
      "    learn_throughput: 1266.115\n",
      "    learn_time_ms: 789.818\n",
      "    load_throughput: 45090.637\n",
      "    load_time_ms: 22.178\n",
      "    sample_throughput: 30.319\n",
      "    sample_time_ms: 32982.598\n",
      "    update_time_ms: 5.813\n",
      "  timestamp: 1635083364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 789000\n",
      "  training_iteration: 789\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   789</td><td style=\"text-align: right;\">         20483.6</td><td style=\"text-align: right;\">789000</td><td style=\"text-align: right;\"> -3.3178</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -5.81</td><td style=\"text-align: right;\">            328.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 790000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 323.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.266099999999974\n",
      "  episode_reward_min: -5.739999999999922\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2743\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2120453238053906e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5506870455212063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019783780017461478\n",
      "          policy_loss: -0.04727967547045814\n",
      "          total_loss: -0.058337340669499506\n",
      "          vf_explained_var: 0.852529764175415\n",
      "          vf_loss: 0.004449203095605804\n",
      "    num_agent_steps_sampled: 790000\n",
      "    num_agent_steps_trained: 790000\n",
      "    num_steps_sampled: 790000\n",
      "    num_steps_trained: 790000\n",
      "  iterations_since_restore: 790\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.002\n",
      "    ram_util_percent: 52.528\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038867308678194636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.86933690197814\n",
      "    mean_inference_ms: 1.9343340055587384\n",
      "    mean_raw_obs_processing_ms: 2.189910073774047\n",
      "  time_since_restore: 20519.086079597473\n",
      "  time_this_iter_s: 35.47712683677673\n",
      "  time_total_s: 20519.086079597473\n",
      "  timers:\n",
      "    learn_throughput: 1260.166\n",
      "    learn_time_ms: 793.546\n",
      "    load_throughput: 45860.639\n",
      "    load_time_ms: 21.805\n",
      "    sample_throughput: 30.043\n",
      "    sample_time_ms: 33285.769\n",
      "    update_time_ms: 6.838\n",
      "  timestamp: 1635083399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 790000\n",
      "  training_iteration: 790\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   790</td><td style=\"text-align: right;\">         20519.1</td><td style=\"text-align: right;\">790000</td><td style=\"text-align: right;\"> -3.2661</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -5.74</td><td style=\"text-align: right;\">            323.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 791000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 316.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.1965999999999766\n",
      "  episode_reward_min: -4.779999999999963\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2746\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2120453238053906e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4813173505995008\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0148484169966603\n",
      "          policy_loss: -0.1395120798713631\n",
      "          total_loss: -0.148926112966405\n",
      "          vf_explained_var: 0.8218781352043152\n",
      "          vf_loss: 0.005399138837431868\n",
      "    num_agent_steps_sampled: 791000\n",
      "    num_agent_steps_trained: 791000\n",
      "    num_steps_sampled: 791000\n",
      "    num_steps_trained: 791000\n",
      "  iterations_since_restore: 791\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.56400000000002\n",
      "    ram_util_percent: 52.352000000000004\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887297217434626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.878951723764295\n",
      "    mean_inference_ms: 1.9344893080527705\n",
      "    mean_raw_obs_processing_ms: 2.1894735855377414\n",
      "  time_since_restore: 20553.931343078613\n",
      "  time_this_iter_s: 34.84526348114014\n",
      "  time_total_s: 20553.931343078613\n",
      "  timers:\n",
      "    learn_throughput: 1258.587\n",
      "    learn_time_ms: 794.542\n",
      "    load_throughput: 47658.321\n",
      "    load_time_ms: 20.983\n",
      "    sample_throughput: 29.71\n",
      "    sample_time_ms: 33658.242\n",
      "    update_time_ms: 6.661\n",
      "  timestamp: 1635083434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 791000\n",
      "  training_iteration: 791\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   791</td><td style=\"text-align: right;\">         20553.9</td><td style=\"text-align: right;\">791000</td><td style=\"text-align: right;\"> -3.1966</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">            316.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-51-05\n",
      "  done: false\n",
      "  episode_len_mean: 312.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.153199999999976\n",
      "  episode_reward_min: -4.779999999999963\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2750\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2120453238053906e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4730863518185087\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.05758260811377222\n",
      "          policy_loss: 0.06503781684570842\n",
      "          total_loss: 0.05556300534970231\n",
      "          vf_explained_var: 0.8143094182014465\n",
      "          vf_loss: 0.005256048737404247\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "  iterations_since_restore: 792\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.38409090909092\n",
      "    ram_util_percent: 52.265909090909076\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038880275771000956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.89235159336548\n",
      "    mean_inference_ms: 1.9346898669904902\n",
      "    mean_raw_obs_processing_ms: 2.188950668910914\n",
      "  time_since_restore: 20584.649220705032\n",
      "  time_this_iter_s: 30.717877626419067\n",
      "  time_total_s: 20584.649220705032\n",
      "  timers:\n",
      "    learn_throughput: 1259.591\n",
      "    learn_time_ms: 793.909\n",
      "    load_throughput: 47524.616\n",
      "    load_time_ms: 21.042\n",
      "    sample_throughput: 29.712\n",
      "    sample_time_ms: 33656.255\n",
      "    update_time_ms: 6.268\n",
      "  timestamp: 1635083465\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 792\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   792</td><td style=\"text-align: right;\">         20584.6</td><td style=\"text-align: right;\">792000</td><td style=\"text-align: right;\"> -3.1532</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">            312.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 793000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-51-39\n",
      "  done: false\n",
      "  episode_len_mean: 311.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.1409999999999774\n",
      "  episode_reward_min: -4.779999999999963\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2753\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.818067985708087e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3426658471425374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009729036927925467\n",
      "          policy_loss: -0.014905399746365018\n",
      "          total_loss: -0.023176652027501\n",
      "          vf_explained_var: 0.8114170432090759\n",
      "          vf_loss: 0.0051554047683667805\n",
      "    num_agent_steps_sampled: 793000\n",
      "    num_agent_steps_trained: 793000\n",
      "    num_steps_sampled: 793000\n",
      "    num_steps_trained: 793000\n",
      "  iterations_since_restore: 793\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.03541666666666\n",
      "    ram_util_percent: 52.22083333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038885318356520274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.90267471595562\n",
      "    mean_inference_ms: 1.9348384724706522\n",
      "    mean_raw_obs_processing_ms: 2.1885745930887146\n",
      "  time_since_restore: 20618.60793375969\n",
      "  time_this_iter_s: 33.95871305465698\n",
      "  time_total_s: 20618.60793375969\n",
      "  timers:\n",
      "    learn_throughput: 1257.693\n",
      "    learn_time_ms: 795.107\n",
      "    load_throughput: 47592.185\n",
      "    load_time_ms: 21.012\n",
      "    sample_throughput: 29.562\n",
      "    sample_time_ms: 33827.63\n",
      "    update_time_ms: 6.231\n",
      "  timestamp: 1635083499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 793000\n",
      "  training_iteration: 793\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   793</td><td style=\"text-align: right;\">         20618.6</td><td style=\"text-align: right;\">793000</td><td style=\"text-align: right;\">  -3.141</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">            311.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 794000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-52-13\n",
      "  done: false\n",
      "  episode_len_mean: 309.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.1284999999999767\n",
      "  episode_reward_min: -4.779999999999963\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2757\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.818067985708087e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2779460456636218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026026834333702058\n",
      "          policy_loss: -0.0059776623215940265\n",
      "          total_loss: -0.011990679800510407\n",
      "          vf_explained_var: 0.724766731262207\n",
      "          vf_loss: 0.006766439938089914\n",
      "    num_agent_steps_sampled: 794000\n",
      "    num_agent_steps_trained: 794000\n",
      "    num_steps_sampled: 794000\n",
      "    num_steps_trained: 794000\n",
      "  iterations_since_restore: 794\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.83749999999999\n",
      "    ram_util_percent: 52.20208333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03889165733095213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.916592134596517\n",
      "    mean_inference_ms: 1.9350323785166057\n",
      "    mean_raw_obs_processing_ms: 2.188087409855847\n",
      "  time_since_restore: 20652.157509326935\n",
      "  time_this_iter_s: 33.54957556724548\n",
      "  time_total_s: 20652.157509326935\n",
      "  timers:\n",
      "    learn_throughput: 1251.547\n",
      "    learn_time_ms: 799.011\n",
      "    load_throughput: 47384.859\n",
      "    load_time_ms: 21.104\n",
      "    sample_throughput: 29.292\n",
      "    sample_time_ms: 34139.242\n",
      "    update_time_ms: 6.421\n",
      "  timestamp: 1635083533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 794000\n",
      "  training_iteration: 794\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   794</td><td style=\"text-align: right;\">         20652.2</td><td style=\"text-align: right;\">794000</td><td style=\"text-align: right;\"> -3.1285</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">             309.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 795000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-53-02\n",
      "  done: false\n",
      "  episode_len_mean: 310.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.134599999999977\n",
      "  episode_reward_min: -4.779999999999963\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2760\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.72710197856213e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4158738851547241\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010075650585547812\n",
      "          policy_loss: -0.04514281832509571\n",
      "          total_loss: -0.05144121530983183\n",
      "          vf_explained_var: 0.579245388507843\n",
      "          vf_loss: 0.00786034255870618\n",
      "    num_agent_steps_sampled: 795000\n",
      "    num_agent_steps_trained: 795000\n",
      "    num_steps_sampled: 795000\n",
      "    num_steps_trained: 795000\n",
      "  iterations_since_restore: 795\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.35492957746479\n",
      "    ram_util_percent: 52.13943661971831\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03889635997841192\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.926884230913227\n",
      "    mean_inference_ms: 1.9351733073930868\n",
      "    mean_raw_obs_processing_ms: 2.188378055669316\n",
      "  time_since_restore: 20701.989715337753\n",
      "  time_this_iter_s: 49.83220601081848\n",
      "  time_total_s: 20701.989715337753\n",
      "  timers:\n",
      "    learn_throughput: 1250.944\n",
      "    learn_time_ms: 799.396\n",
      "    load_throughput: 47342.125\n",
      "    load_time_ms: 21.123\n",
      "    sample_throughput: 27.819\n",
      "    sample_time_ms: 35946.869\n",
      "    update_time_ms: 6.194\n",
      "  timestamp: 1635083582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 795000\n",
      "  training_iteration: 795\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   795</td><td style=\"text-align: right;\">           20702</td><td style=\"text-align: right;\">795000</td><td style=\"text-align: right;\"> -3.1346</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">            310.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 796000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-53-38\n",
      "  done: false\n",
      "  episode_len_mean: 309.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.122399999999977\n",
      "  episode_reward_min: -4.779999999999963\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2764\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.72710197856213e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4346382273568048\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012499310646487485\n",
      "          policy_loss: 0.0004241079092025757\n",
      "          total_loss: -0.005348840397265222\n",
      "          vf_explained_var: 0.5478571057319641\n",
      "          vf_loss: 0.008573432753069533\n",
      "    num_agent_steps_sampled: 796000\n",
      "    num_agent_steps_trained: 796000\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "  iterations_since_restore: 796\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.7627450980392\n",
      "    ram_util_percent: 52.31764705882353\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038902655353255024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.94106561525215\n",
      "    mean_inference_ms: 1.9353633844715026\n",
      "    mean_raw_obs_processing_ms: 2.1887453987891585\n",
      "  time_since_restore: 20737.669917345047\n",
      "  time_this_iter_s: 35.6802020072937\n",
      "  time_total_s: 20737.669917345047\n",
      "  timers:\n",
      "    learn_throughput: 1238.555\n",
      "    learn_time_ms: 807.392\n",
      "    load_throughput: 46450.864\n",
      "    load_time_ms: 21.528\n",
      "    sample_throughput: 28.853\n",
      "    sample_time_ms: 34658.389\n",
      "    update_time_ms: 6.31\n",
      "  timestamp: 1635083618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 796\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   796</td><td style=\"text-align: right;\">         20737.7</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> -3.1224</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">            309.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 797000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-54-15\n",
      "  done: false\n",
      "  episode_len_mean: 307.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.1069999999999784\n",
      "  episode_reward_min: -4.779999999999963\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2767\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.72710197856213e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5267807417445713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01174347427880412\n",
      "          policy_loss: 0.0008953005903297001\n",
      "          total_loss: -0.0071014673345618775\n",
      "          vf_explained_var: 0.45668724179267883\n",
      "          vf_loss: 0.0072710392065346244\n",
      "    num_agent_steps_sampled: 797000\n",
      "    num_agent_steps_trained: 797000\n",
      "    num_steps_sampled: 797000\n",
      "    num_steps_trained: 797000\n",
      "  iterations_since_restore: 797\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.47358490566039\n",
      "    ram_util_percent: 52.632075471698116\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0389073308350484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.951773404733927\n",
      "    mean_inference_ms: 1.9355063201289635\n",
      "    mean_raw_obs_processing_ms: 2.1888486456729144\n",
      "  time_since_restore: 20774.154664993286\n",
      "  time_this_iter_s: 36.484747648239136\n",
      "  time_total_s: 20774.154664993286\n",
      "  timers:\n",
      "    learn_throughput: 1257.907\n",
      "    learn_time_ms: 794.971\n",
      "    load_throughput: 44765.218\n",
      "    load_time_ms: 22.339\n",
      "    sample_throughput: 28.615\n",
      "    sample_time_ms: 34946.772\n",
      "    update_time_ms: 6.207\n",
      "  timestamp: 1635083655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 797000\n",
      "  training_iteration: 797\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   797</td><td style=\"text-align: right;\">         20774.2</td><td style=\"text-align: right;\">797000</td><td style=\"text-align: right;\">  -3.107</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">            307.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 798000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-54-47\n",
      "  done: false\n",
      "  episode_len_mean: 305.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.089299999999978\n",
      "  episode_reward_min: -4.779999999999963\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2771\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.72710197856213e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3913313190142313\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01093003230931973\n",
      "          policy_loss: -0.002625896698898739\n",
      "          total_loss: -0.00786510809428162\n",
      "          vf_explained_var: 0.5248320698738098\n",
      "          vf_loss: 0.008674101112410426\n",
      "    num_agent_steps_sampled: 798000\n",
      "    num_agent_steps_trained: 798000\n",
      "    num_steps_sampled: 798000\n",
      "    num_steps_trained: 798000\n",
      "  iterations_since_restore: 798\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.72608695652174\n",
      "    ram_util_percent: 52.50652173913044\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03891350227467859\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.96614055998891\n",
      "    mean_inference_ms: 1.9356995468197522\n",
      "    mean_raw_obs_processing_ms: 2.188393388258416\n",
      "  time_since_restore: 20806.791229248047\n",
      "  time_this_iter_s: 32.63656425476074\n",
      "  time_total_s: 20806.791229248047\n",
      "  timers:\n",
      "    learn_throughput: 1306.961\n",
      "    learn_time_ms: 765.134\n",
      "    load_throughput: 46163.237\n",
      "    load_time_ms: 21.662\n",
      "    sample_throughput: 28.514\n",
      "    sample_time_ms: 35069.988\n",
      "    update_time_ms: 6.074\n",
      "  timestamp: 1635083687\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 798000\n",
      "  training_iteration: 798\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   798</td><td style=\"text-align: right;\">         20806.8</td><td style=\"text-align: right;\">798000</td><td style=\"text-align: right;\"> -3.0893</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">            305.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 799000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-55-18\n",
      "  done: false\n",
      "  episode_len_mean: 304.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0785999999999785\n",
      "  episode_reward_min: -4.779999999999963\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2774\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.72710197856213e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1653160439597237\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010752599473344482\n",
      "          policy_loss: 0.016292994634972677\n",
      "          total_loss: 0.011822712752554152\n",
      "          vf_explained_var: 0.4535139203071594\n",
      "          vf_loss: 0.00718287501949817\n",
      "    num_agent_steps_sampled: 799000\n",
      "    num_agent_steps_trained: 799000\n",
      "    num_steps_sampled: 799000\n",
      "    num_steps_trained: 799000\n",
      "  iterations_since_restore: 799\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.92954545454545\n",
      "    ram_util_percent: 52.343181818181804\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038918073710505596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.976721774810827\n",
      "    mean_inference_ms: 1.9358432363400309\n",
      "    mean_raw_obs_processing_ms: 2.1880870704023825\n",
      "  time_since_restore: 20837.656577825546\n",
      "  time_this_iter_s: 30.86534857749939\n",
      "  time_total_s: 20837.656577825546\n",
      "  timers:\n",
      "    learn_throughput: 1320.547\n",
      "    learn_time_ms: 757.262\n",
      "    load_throughput: 46245.591\n",
      "    load_time_ms: 21.624\n",
      "    sample_throughput: 28.889\n",
      "    sample_time_ms: 34615.648\n",
      "    update_time_ms: 5.202\n",
      "  timestamp: 1635083718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 799000\n",
      "  training_iteration: 799\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   799</td><td style=\"text-align: right;\">         20837.7</td><td style=\"text-align: right;\">799000</td><td style=\"text-align: right;\"> -3.0786</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">            304.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 304.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0522999999999785\n",
      "  episode_reward_min: -4.209999999999954\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2777\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.72710197856213e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2684908628463745\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013620958969009797\n",
      "          policy_loss: -0.11376898503965802\n",
      "          total_loss: -0.11669557094573975\n",
      "          vf_explained_var: 0.3422700762748718\n",
      "          vf_loss: 0.009758322111641368\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_agent_steps_trained: 800000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "  iterations_since_restore: 800\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.78181818181818\n",
      "    ram_util_percent: 52.1840909090909\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03892258894048934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.987303886886785\n",
      "    mean_inference_ms: 1.9359861135394483\n",
      "    mean_raw_obs_processing_ms: 2.18775806916188\n",
      "  time_since_restore: 20868.21803689003\n",
      "  time_this_iter_s: 30.561459064483643\n",
      "  time_total_s: 20868.21803689003\n",
      "  timers:\n",
      "    learn_throughput: 1329.967\n",
      "    learn_time_ms: 751.899\n",
      "    load_throughput: 45581.244\n",
      "    load_time_ms: 21.939\n",
      "    sample_throughput: 29.3\n",
      "    sample_time_ms: 34130.255\n",
      "    update_time_ms: 4.742\n",
      "  timestamp: 1635083749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 800\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   800</td><td style=\"text-align: right;\">         20868.2</td><td style=\"text-align: right;\">800000</td><td style=\"text-align: right;\"> -3.0523</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.21</td><td style=\"text-align: right;\">            304.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 801000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-56-20\n",
      "  done: false\n",
      "  episode_len_mean: 302.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0360999999999785\n",
      "  episode_reward_min: -4.209999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2781\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.72710197856213e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1910893042882285\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016626177709481264\n",
      "          policy_loss: 0.01601391140785482\n",
      "          total_loss: 0.016064710128638478\n",
      "          vf_explained_var: 0.2664908766746521\n",
      "          vf_loss: 0.01196169105047981\n",
      "    num_agent_steps_sampled: 801000\n",
      "    num_agent_steps_trained: 801000\n",
      "    num_steps_sampled: 801000\n",
      "    num_steps_trained: 801000\n",
      "  iterations_since_restore: 801\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.92727272727274\n",
      "    ram_util_percent: 52.218181818181804\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038928338552712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.001230140036963\n",
      "    mean_inference_ms: 1.9361700383954277\n",
      "    mean_raw_obs_processing_ms: 2.1873435112966964\n",
      "  time_since_restore: 20899.28246974945\n",
      "  time_this_iter_s: 31.064432859420776\n",
      "  time_total_s: 20899.28246974945\n",
      "  timers:\n",
      "    learn_throughput: 1331.861\n",
      "    learn_time_ms: 750.829\n",
      "    load_throughput: 43861.269\n",
      "    load_time_ms: 22.799\n",
      "    sample_throughput: 29.628\n",
      "    sample_time_ms: 33752.303\n",
      "    update_time_ms: 4.9\n",
      "  timestamp: 1635083780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 801000\n",
      "  training_iteration: 801\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   801</td><td style=\"text-align: right;\">         20899.3</td><td style=\"text-align: right;\">801000</td><td style=\"text-align: right;\"> -3.0361</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.21</td><td style=\"text-align: right;\">            302.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 802000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-56-54\n",
      "  done: false\n",
      "  episode_len_mean: 301.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.029599999999979\n",
      "  episode_reward_min: -4.209999999999954\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2784\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.72710197856213e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.208859501944648\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010923391008218965\n",
      "          policy_loss: 0.0588922461701764\n",
      "          total_loss: 0.055699704256322646\n",
      "          vf_explained_var: 0.21114476025104523\n",
      "          vf_loss: 0.008896054037743144\n",
      "    num_agent_steps_sampled: 802000\n",
      "    num_agent_steps_trained: 802000\n",
      "    num_steps_sampled: 802000\n",
      "    num_steps_trained: 802000\n",
      "  iterations_since_restore: 802\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.72083333333332\n",
      "    ram_util_percent: 52.28125\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03893247217300264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.011467699807998\n",
      "    mean_inference_ms: 1.9363034428830992\n",
      "    mean_raw_obs_processing_ms: 2.187072514027628\n",
      "  time_since_restore: 20932.94012451172\n",
      "  time_this_iter_s: 33.657654762268066\n",
      "  time_total_s: 20932.94012451172\n",
      "  timers:\n",
      "    learn_throughput: 1330.473\n",
      "    learn_time_ms: 751.613\n",
      "    load_throughput: 44334.159\n",
      "    load_time_ms: 22.556\n",
      "    sample_throughput: 29.372\n",
      "    sample_time_ms: 34045.686\n",
      "    update_time_ms: 4.969\n",
      "  timestamp: 1635083814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 802000\n",
      "  training_iteration: 802\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   802</td><td style=\"text-align: right;\">         20932.9</td><td style=\"text-align: right;\">802000</td><td style=\"text-align: right;\"> -3.0296</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -4.21</td><td style=\"text-align: right;\">            301.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 803000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 300.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.00939999999998\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2788\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.72710197856213e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1311764816443126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00491776611151301\n",
      "          policy_loss: -0.008846146861712138\n",
      "          total_loss: -0.00651830674873458\n",
      "          vf_explained_var: 0.21833781898021698\n",
      "          vf_loss: 0.013639605986989206\n",
      "    num_agent_steps_sampled: 803000\n",
      "    num_agent_steps_trained: 803000\n",
      "    num_steps_sampled: 803000\n",
      "    num_steps_trained: 803000\n",
      "  iterations_since_restore: 803\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.12127659574469\n",
      "    ram_util_percent: 52.144680851063825\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038937907971055626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.02515876582293\n",
      "    mean_inference_ms: 1.9364803437456835\n",
      "    mean_raw_obs_processing_ms: 2.186689426809177\n",
      "  time_since_restore: 20965.848570108414\n",
      "  time_this_iter_s: 32.908445596694946\n",
      "  time_total_s: 20965.848570108414\n",
      "  timers:\n",
      "    learn_throughput: 1325.719\n",
      "    learn_time_ms: 754.308\n",
      "    load_throughput: 44520.652\n",
      "    load_time_ms: 22.461\n",
      "    sample_throughput: 29.465\n",
      "    sample_time_ms: 33938.113\n",
      "    update_time_ms: 4.901\n",
      "  timestamp: 1635083846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 803000\n",
      "  training_iteration: 803\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   803</td><td style=\"text-align: right;\">         20965.8</td><td style=\"text-align: right;\">803000</td><td style=\"text-align: right;\"> -3.0094</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            300.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 804000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 300.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0001999999999804\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2791\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.363550989281065e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.153990450170305\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01070345366295192\n",
      "          policy_loss: 0.02450390557448069\n",
      "          total_loss: 0.022026515503724416\n",
      "          vf_explained_var: 0.3939598500728607\n",
      "          vf_loss: 0.009062514030503937\n",
      "    num_agent_steps_sampled: 804000\n",
      "    num_agent_steps_trained: 804000\n",
      "    num_steps_sampled: 804000\n",
      "    num_steps_trained: 804000\n",
      "  iterations_since_restore: 804\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.73287671232876\n",
      "    ram_util_percent: 52.11917808219179\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038941915635162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.035116022647962\n",
      "    mean_inference_ms: 1.93660853389999\n",
      "    mean_raw_obs_processing_ms: 2.1870624944802812\n",
      "  time_since_restore: 21016.585542678833\n",
      "  time_this_iter_s: 50.73697257041931\n",
      "  time_total_s: 21016.585542678833\n",
      "  timers:\n",
      "    learn_throughput: 1321.214\n",
      "    learn_time_ms: 756.88\n",
      "    load_throughput: 46370.341\n",
      "    load_time_ms: 21.566\n",
      "    sample_throughput: 28.046\n",
      "    sample_time_ms: 35655.219\n",
      "    update_time_ms: 4.798\n",
      "  timestamp: 1635083897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 804000\n",
      "  training_iteration: 804\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   804</td><td style=\"text-align: right;\">         21016.6</td><td style=\"text-align: right;\">804000</td><td style=\"text-align: right;\"> -3.0002</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            300.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 805000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-58-49\n",
      "  done: false\n",
      "  episode_len_mean: 299.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9933999999999794\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2794\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.363550989281065e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2228176911671957\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009657202232230589\n",
      "          policy_loss: -0.10899533977111181\n",
      "          total_loss: -0.1086574438545439\n",
      "          vf_explained_var: 0.24696150422096252\n",
      "          vf_loss: 0.012566075173930989\n",
      "    num_agent_steps_sampled: 805000\n",
      "    num_agent_steps_trained: 805000\n",
      "    num_steps_sampled: 805000\n",
      "    num_steps_trained: 805000\n",
      "  iterations_since_restore: 805\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.47727272727272\n",
      "    ram_util_percent: 52.234090909090924\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038945809465099075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.044928449581363\n",
      "    mean_inference_ms: 1.9367342983069873\n",
      "    mean_raw_obs_processing_ms: 2.1874371583560643\n",
      "  time_since_restore: 21047.930738687515\n",
      "  time_this_iter_s: 31.34519600868225\n",
      "  time_total_s: 21047.930738687515\n",
      "  timers:\n",
      "    learn_throughput: 1323.027\n",
      "    learn_time_ms: 755.842\n",
      "    load_throughput: 46284.273\n",
      "    load_time_ms: 21.606\n",
      "    sample_throughput: 29.58\n",
      "    sample_time_ms: 33806.717\n",
      "    update_time_ms: 5.62\n",
      "  timestamp: 1635083929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 805000\n",
      "  training_iteration: 805\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   805</td><td style=\"text-align: right;\">         21047.9</td><td style=\"text-align: right;\">805000</td><td style=\"text-align: right;\"> -2.9934</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            299.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 806000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-59-20\n",
      "  done: false\n",
      "  episode_len_mean: 299.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.993299999999979\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2798\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.363550989281065e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1096689820289611\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010678875652147665\n",
      "          policy_loss: 0.0008935411771138509\n",
      "          total_loss: 0.0018590984245141348\n",
      "          vf_explained_var: 0.2791752815246582\n",
      "          vf_loss: 0.012062248815264966\n",
      "    num_agent_steps_sampled: 806000\n",
      "    num_agent_steps_trained: 806000\n",
      "    num_steps_sampled: 806000\n",
      "    num_steps_trained: 806000\n",
      "  iterations_since_restore: 806\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.13333333333334\n",
      "    ram_util_percent: 52.2777777777778\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03895098553111815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.057686164034468\n",
      "    mean_inference_ms: 1.936901332024323\n",
      "    mean_raw_obs_processing_ms: 2.1877474653437874\n",
      "  time_since_restore: 21079.0804605484\n",
      "  time_this_iter_s: 31.14972186088562\n",
      "  time_total_s: 21079.0804605484\n",
      "  timers:\n",
      "    learn_throughput: 1338.033\n",
      "    learn_time_ms: 747.366\n",
      "    load_throughput: 48821.558\n",
      "    load_time_ms: 20.483\n",
      "    sample_throughput: 29.973\n",
      "    sample_time_ms: 33362.893\n",
      "    update_time_ms: 5.541\n",
      "  timestamp: 1635083960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 806000\n",
      "  training_iteration: 806\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   806</td><td style=\"text-align: right;\">         21079.1</td><td style=\"text-align: right;\">806000</td><td style=\"text-align: right;\"> -2.9933</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            299.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 807000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-59-53\n",
      "  done: false\n",
      "  episode_len_mean: 299.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.995999999999979\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2801\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.363550989281065e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2325221644507514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011542643321592137\n",
      "          policy_loss: 0.05329677859942118\n",
      "          total_loss: 0.05174435998002688\n",
      "          vf_explained_var: 0.10716021060943604\n",
      "          vf_loss: 0.010772799597018295\n",
      "    num_agent_steps_sampled: 807000\n",
      "    num_agent_steps_trained: 807000\n",
      "    num_steps_sampled: 807000\n",
      "    num_steps_trained: 807000\n",
      "  iterations_since_restore: 807\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.49787234042554\n",
      "    ram_util_percent: 52.42978723404256\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03895491616141073\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.067289993328846\n",
      "    mean_inference_ms: 1.9370288791715655\n",
      "    mean_raw_obs_processing_ms: 2.187472188644705\n",
      "  time_since_restore: 21112.081559419632\n",
      "  time_this_iter_s: 33.00109887123108\n",
      "  time_total_s: 21112.081559419632\n",
      "  timers:\n",
      "    learn_throughput: 1349.021\n",
      "    learn_time_ms: 741.278\n",
      "    load_throughput: 47841.68\n",
      "    load_time_ms: 20.902\n",
      "    sample_throughput: 30.285\n",
      "    sample_time_ms: 33019.267\n",
      "    update_time_ms: 6.47\n",
      "  timestamp: 1635083993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 807000\n",
      "  training_iteration: 807\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   807</td><td style=\"text-align: right;\">         21112.1</td><td style=\"text-align: right;\">807000</td><td style=\"text-align: right;\">  -2.996</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">             299.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 808000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-00-27\n",
      "  done: false\n",
      "  episode_len_mean: 299.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9916999999999803\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2804\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.363550989281065e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9845189147525364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008072051989668694\n",
      "          policy_loss: -0.09896096967988544\n",
      "          total_loss: -0.09613819652133518\n",
      "          vf_explained_var: 0.3690199553966522\n",
      "          vf_loss: 0.012667959814684259\n",
      "    num_agent_steps_sampled: 808000\n",
      "    num_agent_steps_trained: 808000\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 808000\n",
      "  iterations_since_restore: 808\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.76530612244898\n",
      "    ram_util_percent: 52.37755102040816\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03895886857633182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.07691199674578\n",
      "    mean_inference_ms: 1.9371574345111653\n",
      "    mean_raw_obs_processing_ms: 2.187201035952329\n",
      "  time_since_restore: 21146.72689127922\n",
      "  time_this_iter_s: 34.64533185958862\n",
      "  time_total_s: 21146.72689127922\n",
      "  timers:\n",
      "    learn_throughput: 1340.688\n",
      "    learn_time_ms: 745.886\n",
      "    load_throughput: 45526.277\n",
      "    load_time_ms: 21.965\n",
      "    sample_throughput: 30.108\n",
      "    sample_time_ms: 33214.28\n",
      "    update_time_ms: 6.596\n",
      "  timestamp: 1635084027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 808\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   808</td><td style=\"text-align: right;\">         21146.7</td><td style=\"text-align: right;\">808000</td><td style=\"text-align: right;\"> -2.9917</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            299.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 809000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 298.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.98739999999998\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2808\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.363550989281065e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.052888591421975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012479994669468258\n",
      "          policy_loss: 0.013354011376698812\n",
      "          total_loss: 0.014411403404341803\n",
      "          vf_explained_var: 0.4397616386413574\n",
      "          vf_loss: 0.011586278014712863\n",
      "    num_agent_steps_sampled: 809000\n",
      "    num_agent_steps_trained: 809000\n",
      "    num_steps_sampled: 809000\n",
      "    num_steps_trained: 809000\n",
      "  iterations_since_restore: 809\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.36666666666667\n",
      "    ram_util_percent: 52.21666666666666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03896394726051811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.089633072962926\n",
      "    mean_inference_ms: 1.937326633926098\n",
      "    mean_raw_obs_processing_ms: 2.186867922172122\n",
      "  time_since_restore: 21179.951169490814\n",
      "  time_this_iter_s: 33.22427821159363\n",
      "  time_total_s: 21179.951169490814\n",
      "  timers:\n",
      "    learn_throughput: 1333.751\n",
      "    learn_time_ms: 749.765\n",
      "    load_throughput: 43418.712\n",
      "    load_time_ms: 23.032\n",
      "    sample_throughput: 29.9\n",
      "    sample_time_ms: 33445.159\n",
      "    update_time_ms: 6.612\n",
      "  timestamp: 1635084061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 809000\n",
      "  training_iteration: 809\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   809</td><td style=\"text-align: right;\">           21180</td><td style=\"text-align: right;\">809000</td><td style=\"text-align: right;\"> -2.9874</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            298.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 810000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 298.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9849999999999794\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2811\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.363550989281065e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9786860518985324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0065430475091485365\n",
      "          policy_loss: -0.05104437039958106\n",
      "          total_loss: -0.04982565575175815\n",
      "          vf_explained_var: 0.39285561442375183\n",
      "          vf_loss: 0.011005576461967495\n",
      "    num_agent_steps_sampled: 810000\n",
      "    num_agent_steps_trained: 810000\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "  iterations_since_restore: 810\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.902\n",
      "    ram_util_percent: 52.342000000000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03896779359551285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.099318433226834\n",
      "    mean_inference_ms: 1.9374552809537149\n",
      "    mean_raw_obs_processing_ms: 2.1866014602654187\n",
      "  time_since_restore: 21214.80563211441\n",
      "  time_this_iter_s: 34.85446262359619\n",
      "  time_total_s: 21214.80563211441\n",
      "  timers:\n",
      "    learn_throughput: 1318.674\n",
      "    learn_time_ms: 758.337\n",
      "    load_throughput: 43941.039\n",
      "    load_time_ms: 22.758\n",
      "    sample_throughput: 29.528\n",
      "    sample_time_ms: 33866.365\n",
      "    update_time_ms: 6.389\n",
      "  timestamp: 1635084096\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 810\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   810</td><td style=\"text-align: right;\">         21214.8</td><td style=\"text-align: right;\">810000</td><td style=\"text-align: right;\">  -2.985</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">             298.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 811000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-02-10\n",
      "  done: false\n",
      "  episode_len_mean: 297.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9742999999999813\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2815\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.363550989281065e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1708005640241834\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017178668353457344\n",
      "          policy_loss: 0.031082888402872617\n",
      "          total_loss: 0.030086131021380424\n",
      "          vf_explained_var: 0.6185087561607361\n",
      "          vf_loss: 0.010711247153166268\n",
      "    num_agent_steps_sampled: 811000\n",
      "    num_agent_steps_trained: 811000\n",
      "    num_steps_sampled: 811000\n",
      "    num_steps_trained: 811000\n",
      "  iterations_since_restore: 811\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.81224489795919\n",
      "    ram_util_percent: 52.38979591836735\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03897294395011583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.112295620324343\n",
      "    mean_inference_ms: 1.9376262307831917\n",
      "    mean_raw_obs_processing_ms: 2.1862781699240355\n",
      "  time_since_restore: 21249.623378038406\n",
      "  time_this_iter_s: 34.81774592399597\n",
      "  time_total_s: 21249.623378038406\n",
      "  timers:\n",
      "    learn_throughput: 1316.851\n",
      "    learn_time_ms: 759.388\n",
      "    load_throughput: 43899.877\n",
      "    load_time_ms: 22.779\n",
      "    sample_throughput: 29.205\n",
      "    sample_time_ms: 34240.801\n",
      "    update_time_ms: 6.259\n",
      "  timestamp: 1635084130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 811000\n",
      "  training_iteration: 811\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   811</td><td style=\"text-align: right;\">         21249.6</td><td style=\"text-align: right;\">811000</td><td style=\"text-align: right;\"> -2.9743</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            297.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 812000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-02-44\n",
      "  done: false\n",
      "  episode_len_mean: 296.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.96219999999998\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2818\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.363550989281065e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0038144965966542\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003062113045811543\n",
      "          policy_loss: 0.04050092101097107\n",
      "          total_loss: 0.03776545971632004\n",
      "          vf_explained_var: 0.6371682286262512\n",
      "          vf_loss: 0.007302683809151252\n",
      "    num_agent_steps_sampled: 812000\n",
      "    num_agent_steps_trained: 812000\n",
      "    num_steps_sampled: 812000\n",
      "    num_steps_trained: 812000\n",
      "  iterations_since_restore: 812\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.30416666666666\n",
      "    ram_util_percent: 52.364583333333336\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03897675121378699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.122076149876406\n",
      "    mean_inference_ms: 1.937753274060712\n",
      "    mean_raw_obs_processing_ms: 2.1860449969085387\n",
      "  time_since_restore: 21283.30389690399\n",
      "  time_this_iter_s: 33.68051886558533\n",
      "  time_total_s: 21283.30389690399\n",
      "  timers:\n",
      "    learn_throughput: 1316.483\n",
      "    learn_time_ms: 759.6\n",
      "    load_throughput: 43927.279\n",
      "    load_time_ms: 22.765\n",
      "    sample_throughput: 29.203\n",
      "    sample_time_ms: 34242.719\n",
      "    update_time_ms: 6.194\n",
      "  timestamp: 1635084164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 812000\n",
      "  training_iteration: 812\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   812</td><td style=\"text-align: right;\">         21283.3</td><td style=\"text-align: right;\">812000</td><td style=\"text-align: right;\"> -2.9622</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            296.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 813000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-03-36\n",
      "  done: false\n",
      "  episode_len_mean: 295.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9504999999999812\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2822\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.817754946405326e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0428522573577033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011268550048267024\n",
      "          policy_loss: 0.014833258920245701\n",
      "          total_loss: 0.014936356287863519\n",
      "          vf_explained_var: 0.5241327881813049\n",
      "          vf_loss: 0.01053162167987062\n",
      "    num_agent_steps_sampled: 813000\n",
      "    num_agent_steps_trained: 813000\n",
      "    num_steps_sampled: 813000\n",
      "    num_steps_trained: 813000\n",
      "  iterations_since_restore: 813\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.64999999999999\n",
      "    ram_util_percent: 52.25135135135136\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03898174705013471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.135198384113195\n",
      "    mean_inference_ms: 1.9379218230130315\n",
      "    mean_raw_obs_processing_ms: 2.186576564187911\n",
      "  time_since_restore: 21335.021591186523\n",
      "  time_this_iter_s: 51.71769428253174\n",
      "  time_total_s: 21335.021591186523\n",
      "  timers:\n",
      "    learn_throughput: 1321.221\n",
      "    learn_time_ms: 756.876\n",
      "    load_throughput: 45509.087\n",
      "    load_time_ms: 21.974\n",
      "    sample_throughput: 27.68\n",
      "    sample_time_ms: 36127.114\n",
      "    update_time_ms: 6.194\n",
      "  timestamp: 1635084216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 813000\n",
      "  training_iteration: 813\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   813</td><td style=\"text-align: right;\">           21335</td><td style=\"text-align: right;\">813000</td><td style=\"text-align: right;\"> -2.9505</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            295.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 814000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-04-08\n",
      "  done: false\n",
      "  episode_len_mean: 294.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9445999999999812\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2825\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.817754946405326e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9681215544541677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03252030164885369\n",
      "          policy_loss: -0.03566189722882377\n",
      "          total_loss: -0.03687481962972217\n",
      "          vf_explained_var: 0.5125224590301514\n",
      "          vf_loss: 0.008468290928026869\n",
      "    num_agent_steps_sampled: 814000\n",
      "    num_agent_steps_trained: 814000\n",
      "    num_steps_sampled: 814000\n",
      "    num_steps_trained: 814000\n",
      "  iterations_since_restore: 814\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.56666666666668\n",
      "    ram_util_percent: 52.38222222222222\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038985435692249513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.14499571670899\n",
      "    mean_inference_ms: 1.938046721468515\n",
      "    mean_raw_obs_processing_ms: 2.186987834966509\n",
      "  time_since_restore: 21366.690695762634\n",
      "  time_this_iter_s: 31.66910457611084\n",
      "  time_total_s: 21366.690695762634\n",
      "  timers:\n",
      "    learn_throughput: 1333.138\n",
      "    learn_time_ms: 750.11\n",
      "    load_throughput: 43862.003\n",
      "    load_time_ms: 22.799\n",
      "    sample_throughput: 29.217\n",
      "    sample_time_ms: 34226.565\n",
      "    update_time_ms: 6.11\n",
      "  timestamp: 1635084248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 814000\n",
      "  training_iteration: 814\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   814</td><td style=\"text-align: right;\">         21366.7</td><td style=\"text-align: right;\">814000</td><td style=\"text-align: right;\"> -2.9446</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            294.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 815000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 294.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9421999999999815\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2828\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0226632419607988e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0038778457376691\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01171707762750521\n",
      "          policy_loss: -0.15488775024811427\n",
      "          total_loss: -0.15575557781590355\n",
      "          vf_explained_var: 0.5534805059432983\n",
      "          vf_loss: 0.009170946266709102\n",
      "    num_agent_steps_sampled: 815000\n",
      "    num_agent_steps_trained: 815000\n",
      "    num_steps_sampled: 815000\n",
      "    num_steps_trained: 815000\n",
      "  iterations_since_restore: 815\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.77045454545454\n",
      "    ram_util_percent: 52.25681818181818\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0389891278614994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.154815425939923\n",
      "    mean_inference_ms: 1.938171385839173\n",
      "    mean_raw_obs_processing_ms: 2.1871630279488836\n",
      "  time_since_restore: 21396.955698490143\n",
      "  time_this_iter_s: 30.265002727508545\n",
      "  time_total_s: 21396.955698490143\n",
      "  timers:\n",
      "    learn_throughput: 1333.75\n",
      "    learn_time_ms: 749.766\n",
      "    load_throughput: 43994.135\n",
      "    load_time_ms: 22.73\n",
      "    sample_throughput: 29.309\n",
      "    sample_time_ms: 34119.566\n",
      "    update_time_ms: 5.523\n",
      "  timestamp: 1635084278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 815000\n",
      "  training_iteration: 815\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   815</td><td style=\"text-align: right;\">           21397</td><td style=\"text-align: right;\">815000</td><td style=\"text-align: right;\"> -2.9422</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            294.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 816000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-05-07\n",
      "  done: false\n",
      "  episode_len_mean: 295.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.952899999999981\n",
      "  episode_reward_min: -3.709999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2831\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0226632419607988e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9738003008895451\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017713029461234934\n",
      "          policy_loss: -0.14627193162838617\n",
      "          total_loss: -0.145665093511343\n",
      "          vf_explained_var: 0.39105525612831116\n",
      "          vf_loss: 0.010344839158157508\n",
      "    num_agent_steps_sampled: 816000\n",
      "    num_agent_steps_trained: 816000\n",
      "    num_steps_sampled: 816000\n",
      "    num_steps_trained: 816000\n",
      "  iterations_since_restore: 816\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.0390243902439\n",
      "    ram_util_percent: 52.29024390243902\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03899280607509904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.16448006414688\n",
      "    mean_inference_ms: 1.9382952980217159\n",
      "    mean_raw_obs_processing_ms: 2.186915616464644\n",
      "  time_since_restore: 21425.747426509857\n",
      "  time_this_iter_s: 28.791728019714355\n",
      "  time_total_s: 21425.747426509857\n",
      "  timers:\n",
      "    learn_throughput: 1333.658\n",
      "    learn_time_ms: 749.818\n",
      "    load_throughput: 42665.017\n",
      "    load_time_ms: 23.438\n",
      "    sample_throughput: 29.515\n",
      "    sample_time_ms: 33881.348\n",
      "    update_time_ms: 5.65\n",
      "  timestamp: 1635084307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 816000\n",
      "  training_iteration: 816\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   816</td><td style=\"text-align: right;\">         21425.7</td><td style=\"text-align: right;\">816000</td><td style=\"text-align: right;\"> -2.9529</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.71</td><td style=\"text-align: right;\">            295.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 817000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-05-37\n",
      "  done: false\n",
      "  episode_len_mean: 295.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9504999999999812\n",
      "  episode_reward_min: -3.709999999999965\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2835\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0226632419607988e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0779797004328833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011409633876348834\n",
      "          policy_loss: 0.009401664675937758\n",
      "          total_loss: 0.008972332916325994\n",
      "          vf_explained_var: 0.3058617413043976\n",
      "          vf_loss: 0.010350465971148677\n",
      "    num_agent_steps_sampled: 817000\n",
      "    num_agent_steps_trained: 817000\n",
      "    num_steps_sampled: 817000\n",
      "    num_steps_trained: 817000\n",
      "  iterations_since_restore: 817\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.92325581395349\n",
      "    ram_util_percent: 52.365116279069774\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03899763568657932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.17713197315678\n",
      "    mean_inference_ms: 1.9384587256923007\n",
      "    mean_raw_obs_processing_ms: 2.1866079883142846\n",
      "  time_since_restore: 21455.736605405807\n",
      "  time_this_iter_s: 29.989178895950317\n",
      "  time_total_s: 21455.736605405807\n",
      "  timers:\n",
      "    learn_throughput: 1332.01\n",
      "    learn_time_ms: 750.745\n",
      "    load_throughput: 44008.306\n",
      "    load_time_ms: 22.723\n",
      "    sample_throughput: 29.779\n",
      "    sample_time_ms: 33580.858\n",
      "    update_time_ms: 4.701\n",
      "  timestamp: 1635084337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 817000\n",
      "  training_iteration: 817\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   817</td><td style=\"text-align: right;\">         21455.7</td><td style=\"text-align: right;\">817000</td><td style=\"text-align: right;\"> -2.9505</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.71</td><td style=\"text-align: right;\">            295.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 818000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-06-04\n",
      "  done: false\n",
      "  episode_len_mean: 295.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9544999999999813\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2837\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0226632419607988e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.087579039732615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012969416508182185\n",
      "          policy_loss: -0.10681812034712898\n",
      "          total_loss: -0.10922404593891567\n",
      "          vf_explained_var: 0.33674538135528564\n",
      "          vf_loss: 0.008469861982545505\n",
      "    num_agent_steps_sampled: 818000\n",
      "    num_agent_steps_trained: 818000\n",
      "    num_steps_sampled: 818000\n",
      "    num_steps_trained: 818000\n",
      "  iterations_since_restore: 818\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.31794871794872\n",
      "    ram_util_percent: 52.351282051282055\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03900003703322714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.183298379583416\n",
      "    mean_inference_ms: 1.9385396376591892\n",
      "    mean_raw_obs_processing_ms: 2.1864546999979275\n",
      "  time_since_restore: 21483.219310760498\n",
      "  time_this_iter_s: 27.48270535469055\n",
      "  time_total_s: 21483.219310760498\n",
      "  timers:\n",
      "    learn_throughput: 1337.731\n",
      "    learn_time_ms: 747.534\n",
      "    load_throughput: 44114.58\n",
      "    load_time_ms: 22.668\n",
      "    sample_throughput: 30.425\n",
      "    sample_time_ms: 32868.058\n",
      "    update_time_ms: 4.495\n",
      "  timestamp: 1635084364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 818000\n",
      "  training_iteration: 818\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   818</td><td style=\"text-align: right;\">         21483.2</td><td style=\"text-align: right;\">818000</td><td style=\"text-align: right;\"> -2.9545</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            295.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 819000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-06-33\n",
      "  done: false\n",
      "  episode_len_mean: 296.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9657999999999816\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2840\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0226632419607988e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.036473337146971\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01141399358441725\n",
      "          policy_loss: -0.10668763750129276\n",
      "          total_loss: -0.10773021992709901\n",
      "          vf_explained_var: 0.30629056692123413\n",
      "          vf_loss: 0.009322152928345734\n",
      "    num_agent_steps_sampled: 819000\n",
      "    num_agent_steps_trained: 819000\n",
      "    num_steps_sampled: 819000\n",
      "    num_steps_trained: 819000\n",
      "  iterations_since_restore: 819\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.03999999999999\n",
      "    ram_util_percent: 52.370000000000005\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039003652861368726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.19247434164163\n",
      "    mean_inference_ms: 1.9386615902556286\n",
      "    mean_raw_obs_processing_ms: 2.1861901306080864\n",
      "  time_since_restore: 21511.585422039032\n",
      "  time_this_iter_s: 28.366111278533936\n",
      "  time_total_s: 21511.585422039032\n",
      "  timers:\n",
      "    learn_throughput: 1327.398\n",
      "    learn_time_ms: 753.353\n",
      "    load_throughput: 44191.178\n",
      "    load_time_ms: 22.629\n",
      "    sample_throughput: 30.888\n",
      "    sample_time_ms: 32374.59\n",
      "    update_time_ms: 5.958\n",
      "  timestamp: 1635084393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 819000\n",
      "  training_iteration: 819\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   819</td><td style=\"text-align: right;\">         21511.6</td><td style=\"text-align: right;\">819000</td><td style=\"text-align: right;\"> -2.9658</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            296.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 820000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-07-02\n",
      "  done: false\n",
      "  episode_len_mean: 298.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.98149999999998\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2844\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0226632419607988e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0723846521642473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.034839250128132764\n",
      "          policy_loss: 0.010217192355129454\n",
      "          total_loss: 0.00981936508582698\n",
      "          vf_explained_var: 0.38548609614372253\n",
      "          vf_loss: 0.010326017460061444\n",
      "    num_agent_steps_sampled: 820000\n",
      "    num_agent_steps_trained: 820000\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "  iterations_since_restore: 820\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.33255813953488\n",
      "    ram_util_percent: 52.4\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039008387108719685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.204262271210144\n",
      "    mean_inference_ms: 1.9388200280063277\n",
      "    mean_raw_obs_processing_ms: 2.185867209920981\n",
      "  time_since_restore: 21541.311280727386\n",
      "  time_this_iter_s: 29.725858688354492\n",
      "  time_total_s: 21541.311280727386\n",
      "  timers:\n",
      "    learn_throughput: 1340.461\n",
      "    learn_time_ms: 746.012\n",
      "    load_throughput: 43037.174\n",
      "    load_time_ms: 23.236\n",
      "    sample_throughput: 31.379\n",
      "    sample_time_ms: 31868.627\n",
      "    update_time_ms: 5.584\n",
      "  timestamp: 1635084422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 820\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   820</td><td style=\"text-align: right;\">         21541.3</td><td style=\"text-align: right;\">820000</td><td style=\"text-align: right;\"> -2.9815</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            298.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 821000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-07-33\n",
      "  done: false\n",
      "  episode_len_mean: 298.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9848999999999806\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2847\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5339948629411983e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0535448332627615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009203924984630005\n",
      "          policy_loss: 0.05390461170011097\n",
      "          total_loss: 0.05206727700101005\n",
      "          vf_explained_var: 0.4450747072696686\n",
      "          vf_loss: 0.008698114201736946\n",
      "    num_agent_steps_sampled: 821000\n",
      "    num_agent_steps_trained: 821000\n",
      "    num_steps_sampled: 821000\n",
      "    num_steps_trained: 821000\n",
      "  iterations_since_restore: 821\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.07906976744187\n",
      "    ram_util_percent: 52.37441860465116\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901196647109839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.212936726236293\n",
      "    mean_inference_ms: 1.9389380220075214\n",
      "    mean_raw_obs_processing_ms: 2.1856337286565175\n",
      "  time_since_restore: 21571.773504257202\n",
      "  time_this_iter_s: 30.462223529815674\n",
      "  time_total_s: 21571.773504257202\n",
      "  timers:\n",
      "    learn_throughput: 1338.79\n",
      "    learn_time_ms: 746.943\n",
      "    load_throughput: 45008.381\n",
      "    load_time_ms: 22.218\n",
      "    sample_throughput: 31.814\n",
      "    sample_time_ms: 31432.925\n",
      "    update_time_ms: 5.71\n",
      "  timestamp: 1635084453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 821000\n",
      "  training_iteration: 821\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   821</td><td style=\"text-align: right;\">         21571.8</td><td style=\"text-align: right;\">821000</td><td style=\"text-align: right;\"> -2.9849</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            298.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 822000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 298.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.9851999999999803\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2850\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5339948629411983e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0312550054656136\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010688157765615674\n",
      "          policy_loss: -0.08224083011349043\n",
      "          total_loss: -0.08057408514950011\n",
      "          vf_explained_var: 0.22652968764305115\n",
      "          vf_loss: 0.011979296327465111\n",
      "    num_agent_steps_sampled: 822000\n",
      "    num_agent_steps_trained: 822000\n",
      "    num_steps_sampled: 822000\n",
      "    num_steps_trained: 822000\n",
      "  iterations_since_restore: 822\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.36617647058824\n",
      "    ram_util_percent: 52.36470588235293\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901558770297484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.221683625262337\n",
      "    mean_inference_ms: 1.9390573570358853\n",
      "    mean_raw_obs_processing_ms: 2.185993451952738\n",
      "  time_since_restore: 21619.609028816223\n",
      "  time_this_iter_s: 47.835524559020996\n",
      "  time_total_s: 21619.609028816223\n",
      "  timers:\n",
      "    learn_throughput: 1342.882\n",
      "    learn_time_ms: 744.667\n",
      "    load_throughput: 45226.58\n",
      "    load_time_ms: 22.111\n",
      "    sample_throughput: 30.441\n",
      "    sample_time_ms: 32850.613\n",
      "    update_time_ms: 6.135\n",
      "  timestamp: 1635084501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 822000\n",
      "  training_iteration: 822\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   822</td><td style=\"text-align: right;\">         21619.6</td><td style=\"text-align: right;\">822000</td><td style=\"text-align: right;\"> -2.9852</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            298.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 823000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 298.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.98259999999998\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2854\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5339948629411983e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9941312465402815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0187864780915928\n",
      "          policy_loss: 0.009696690117319424\n",
      "          total_loss: 0.011182642065816456\n",
      "          vf_explained_var: 0.29774999618530273\n",
      "          vf_loss: 0.011427263584401872\n",
      "    num_agent_steps_sampled: 823000\n",
      "    num_agent_steps_trained: 823000\n",
      "    num_steps_sampled: 823000\n",
      "    num_steps_trained: 823000\n",
      "  iterations_since_restore: 823\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.31599999999999\n",
      "    ram_util_percent: 52.298\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039020241067718835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.233221058083657\n",
      "    mean_inference_ms: 1.9392156012212787\n",
      "    mean_raw_obs_processing_ms: 2.1865025840573953\n",
      "  time_since_restore: 21654.092612028122\n",
      "  time_this_iter_s: 34.483583211898804\n",
      "  time_total_s: 21654.092612028122\n",
      "  timers:\n",
      "    learn_throughput: 1344.185\n",
      "    learn_time_ms: 743.945\n",
      "    load_throughput: 44304.281\n",
      "    load_time_ms: 22.571\n",
      "    sample_throughput: 32.127\n",
      "    sample_time_ms: 31126.861\n",
      "    update_time_ms: 6.614\n",
      "  timestamp: 1635084535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 823000\n",
      "  training_iteration: 823\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   823</td><td style=\"text-align: right;\">         21654.1</td><td style=\"text-align: right;\">823000</td><td style=\"text-align: right;\"> -2.9826</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            298.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 824000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-09-27\n",
      "  done: false\n",
      "  episode_len_mean: 298.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.98289999999998\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2857\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5339948629411983e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.011530602640576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006254589033282403\n",
      "          policy_loss: -0.11781252788172827\n",
      "          total_loss: -0.11539139101902644\n",
      "          vf_explained_var: 0.1494928002357483\n",
      "          vf_loss: 0.01253644216598736\n",
      "    num_agent_steps_sampled: 824000\n",
      "    num_agent_steps_trained: 824000\n",
      "    num_steps_sampled: 824000\n",
      "    num_steps_trained: 824000\n",
      "  iterations_since_restore: 824\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.07999999999998\n",
      "    ram_util_percent: 52.3888888888889\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03902371315077549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.241884644392304\n",
      "    mean_inference_ms: 1.9393356455029414\n",
      "    mean_raw_obs_processing_ms: 2.186867479905917\n",
      "  time_since_restore: 21686.143934249878\n",
      "  time_this_iter_s: 32.05132222175598\n",
      "  time_total_s: 21686.143934249878\n",
      "  timers:\n",
      "    learn_throughput: 1335.943\n",
      "    learn_time_ms: 748.535\n",
      "    load_throughput: 46041.417\n",
      "    load_time_ms: 21.72\n",
      "    sample_throughput: 32.091\n",
      "    sample_time_ms: 31161.128\n",
      "    update_time_ms: 6.768\n",
      "  timestamp: 1635084567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 824000\n",
      "  training_iteration: 824\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   824</td><td style=\"text-align: right;\">         21686.1</td><td style=\"text-align: right;\">824000</td><td style=\"text-align: right;\"> -2.9829</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            298.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 825000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-10-00\n",
      "  done: false\n",
      "  episode_len_mean: 298.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.98799999999998\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2861\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5339948629411983e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1953377710448372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01227228631481836\n",
      "          policy_loss: 0.029447115378247367\n",
      "          total_loss: 0.028085809739099608\n",
      "          vf_explained_var: 0.4302710294723511\n",
      "          vf_loss: 0.010592068814569049\n",
      "    num_agent_steps_sampled: 825000\n",
      "    num_agent_steps_trained: 825000\n",
      "    num_steps_sampled: 825000\n",
      "    num_steps_trained: 825000\n",
      "  iterations_since_restore: 825\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.15957446808511\n",
      "    ram_util_percent: 52.470212765957434\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039028212868172305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.2532358878392\n",
      "    mean_inference_ms: 1.9394937877782723\n",
      "    mean_raw_obs_processing_ms: 2.186535410055414\n",
      "  time_since_restore: 21718.564371347427\n",
      "  time_this_iter_s: 32.42043709754944\n",
      "  time_total_s: 21718.564371347427\n",
      "  timers:\n",
      "    learn_throughput: 1336.86\n",
      "    learn_time_ms: 748.022\n",
      "    load_throughput: 44817.404\n",
      "    load_time_ms: 22.313\n",
      "    sample_throughput: 31.871\n",
      "    sample_time_ms: 31376.537\n",
      "    update_time_ms: 6.69\n",
      "  timestamp: 1635084600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 825000\n",
      "  training_iteration: 825\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   825</td><td style=\"text-align: right;\">         21718.6</td><td style=\"text-align: right;\">825000</td><td style=\"text-align: right;\">  -2.988</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">             298.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 826000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-10-30\n",
      "  done: false\n",
      "  episode_len_mean: 299.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -2.99409999999998\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2864\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5339948629411983e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2017487261030408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.044025128815993894\n",
      "          policy_loss: 0.05104037490155962\n",
      "          total_loss: 0.047433480620384216\n",
      "          vf_explained_var: 0.3574709892272949\n",
      "          vf_loss: 0.008410587126854807\n",
      "    num_agent_steps_sampled: 826000\n",
      "    num_agent_steps_trained: 826000\n",
      "    num_steps_sampled: 826000\n",
      "    num_steps_trained: 826000\n",
      "  iterations_since_restore: 826\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.10227272727272\n",
      "    ram_util_percent: 52.50909090909091\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039031603236591036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.261652356919956\n",
      "    mean_inference_ms: 1.9396123128107043\n",
      "    mean_raw_obs_processing_ms: 2.1862698682866206\n",
      "  time_since_restore: 21749.213800430298\n",
      "  time_this_iter_s: 30.649429082870483\n",
      "  time_total_s: 21749.213800430298\n",
      "  timers:\n",
      "    learn_throughput: 1336.558\n",
      "    learn_time_ms: 748.191\n",
      "    load_throughput: 44404.939\n",
      "    load_time_ms: 22.52\n",
      "    sample_throughput: 31.681\n",
      "    sample_time_ms: 31564.786\n",
      "    update_time_ms: 5.819\n",
      "  timestamp: 1635084630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 826000\n",
      "  training_iteration: 826\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   826</td><td style=\"text-align: right;\">         21749.2</td><td style=\"text-align: right;\">826000</td><td style=\"text-align: right;\"> -2.9941</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            299.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 827000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 300.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0015999999999803\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2867\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3009922944117966e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2731255385610791\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027515283716720754\n",
      "          policy_loss: 0.023457959211534925\n",
      "          total_loss: 0.020047931869824728\n",
      "          vf_explained_var: 0.20763395726680756\n",
      "          vf_loss: 0.009321224715353713\n",
      "    num_agent_steps_sampled: 827000\n",
      "    num_agent_steps_trained: 827000\n",
      "    num_steps_sampled: 827000\n",
      "    num_steps_trained: 827000\n",
      "  iterations_since_restore: 827\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.33953488372094\n",
      "    ram_util_percent: 52.5325581395349\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903495247097143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.269818912529303\n",
      "    mean_inference_ms: 1.9397278520405337\n",
      "    mean_raw_obs_processing_ms: 2.1860061356059486\n",
      "  time_since_restore: 21779.410875320435\n",
      "  time_this_iter_s: 30.19707489013672\n",
      "  time_total_s: 21779.410875320435\n",
      "  timers:\n",
      "    learn_throughput: 1332.798\n",
      "    learn_time_ms: 750.301\n",
      "    load_throughput: 43398.855\n",
      "    load_time_ms: 23.042\n",
      "    sample_throughput: 31.663\n",
      "    sample_time_ms: 31582.907\n",
      "    update_time_ms: 5.832\n",
      "  timestamp: 1635084661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 827000\n",
      "  training_iteration: 827\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   827</td><td style=\"text-align: right;\">         21779.4</td><td style=\"text-align: right;\">827000</td><td style=\"text-align: right;\"> -3.0016</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            300.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 828000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-11-30\n",
      "  done: false\n",
      "  episode_len_mean: 301.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0115999999999796\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2870\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4514884416176967e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2714800715446473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01490237103284097\n",
      "          policy_loss: -0.10376206222507689\n",
      "          total_loss: -0.1044488449063566\n",
      "          vf_explained_var: 0.28051331639289856\n",
      "          vf_loss: 0.01202801608790954\n",
      "    num_agent_steps_sampled: 828000\n",
      "    num_agent_steps_trained: 828000\n",
      "    num_steps_sampled: 828000\n",
      "    num_steps_trained: 828000\n",
      "  iterations_since_restore: 828\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.19268292682926\n",
      "    ram_util_percent: 52.48780487804878\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039038275529920764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.277834922681933\n",
      "    mean_inference_ms: 1.939841809209075\n",
      "    mean_raw_obs_processing_ms: 2.1857451081676613\n",
      "  time_since_restore: 21808.479510307312\n",
      "  time_this_iter_s: 29.06863498687744\n",
      "  time_total_s: 21808.479510307312\n",
      "  timers:\n",
      "    learn_throughput: 1330.076\n",
      "    learn_time_ms: 751.837\n",
      "    load_throughput: 43315.715\n",
      "    load_time_ms: 23.086\n",
      "    sample_throughput: 31.506\n",
      "    sample_time_ms: 31739.767\n",
      "    update_time_ms: 6.106\n",
      "  timestamp: 1635084690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 828000\n",
      "  training_iteration: 828\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   828</td><td style=\"text-align: right;\">         21808.5</td><td style=\"text-align: right;\">828000</td><td style=\"text-align: right;\"> -3.0116</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            301.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 829000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-12-00\n",
      "  done: false\n",
      "  episode_len_mean: 302.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.020599999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2873\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4514884416176967e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.205049455165863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00643809125421334\n",
      "          policy_loss: -0.0934809788233704\n",
      "          total_loss: -0.09432101473212243\n",
      "          vf_explained_var: 0.2790212333202362\n",
      "          vf_loss: 0.01121045420360234\n",
      "    num_agent_steps_sampled: 829000\n",
      "    num_agent_steps_trained: 829000\n",
      "    num_steps_sampled: 829000\n",
      "    num_steps_trained: 829000\n",
      "  iterations_since_restore: 829\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.06363636363636\n",
      "    ram_util_percent: 52.45227272727273\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0390416475025038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.28589878105908\n",
      "    mean_inference_ms: 1.93995773336795\n",
      "    mean_raw_obs_processing_ms: 2.1854637608077088\n",
      "  time_since_restore: 21839.026235818863\n",
      "  time_this_iter_s: 30.546725511550903\n",
      "  time_total_s: 21839.026235818863\n",
      "  timers:\n",
      "    learn_throughput: 1339.505\n",
      "    learn_time_ms: 746.545\n",
      "    load_throughput: 43720.725\n",
      "    load_time_ms: 22.872\n",
      "    sample_throughput: 31.285\n",
      "    sample_time_ms: 31964.577\n",
      "    update_time_ms: 5.151\n",
      "  timestamp: 1635084720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 829000\n",
      "  training_iteration: 829\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   829</td><td style=\"text-align: right;\">           21839</td><td style=\"text-align: right;\">829000</td><td style=\"text-align: right;\"> -3.0206</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            302.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 830000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-12-31\n",
      "  done: false\n",
      "  episode_len_mean: 302.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0220999999999787\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2877\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4514884416176967e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2258878416485257\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013309316934092496\n",
      "          policy_loss: 0.018760150174299876\n",
      "          total_loss: 0.019010857244332633\n",
      "          vf_explained_var: 0.27578049898147583\n",
      "          vf_loss: 0.012509587282935778\n",
      "    num_agent_steps_sampled: 830000\n",
      "    num_agent_steps_trained: 830000\n",
      "    num_steps_sampled: 830000\n",
      "    num_steps_trained: 830000\n",
      "  iterations_since_restore: 830\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.28181818181818\n",
      "    ram_util_percent: 52.368181818181824\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03904626327585942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.296605645127784\n",
      "    mean_inference_ms: 1.9401149951760734\n",
      "    mean_raw_obs_processing_ms: 2.1851028867888433\n",
      "  time_since_restore: 21869.936507940292\n",
      "  time_this_iter_s: 30.910272121429443\n",
      "  time_total_s: 21869.936507940292\n",
      "  timers:\n",
      "    learn_throughput: 1339.161\n",
      "    learn_time_ms: 746.736\n",
      "    load_throughput: 46229.79\n",
      "    load_time_ms: 21.631\n",
      "    sample_throughput: 31.168\n",
      "    sample_time_ms: 32083.699\n",
      "    update_time_ms: 5.61\n",
      "  timestamp: 1635084751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 830000\n",
      "  training_iteration: 830\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   830</td><td style=\"text-align: right;\">         21869.9</td><td style=\"text-align: right;\">830000</td><td style=\"text-align: right;\"> -3.0221</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            302.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 831000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-13-18\n",
      "  done: false\n",
      "  episode_len_mean: 301.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.016299999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2880\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4514884416176967e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2501302149560716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01167123585715078\n",
      "          policy_loss: -0.03231607046392229\n",
      "          total_loss: -0.03498509600758552\n",
      "          vf_explained_var: 0.2520853579044342\n",
      "          vf_loss: 0.009832274944831928\n",
      "    num_agent_steps_sampled: 831000\n",
      "    num_agent_steps_trained: 831000\n",
      "    num_steps_sampled: 831000\n",
      "    num_steps_trained: 831000\n",
      "  iterations_since_restore: 831\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.10909090909091\n",
      "    ram_util_percent: 52.260606060606065\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039049660870507846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.30449742863077\n",
      "    mean_inference_ms: 1.9402306628562365\n",
      "    mean_raw_obs_processing_ms: 2.1854705728902433\n",
      "  time_since_restore: 21916.363869190216\n",
      "  time_this_iter_s: 46.427361249923706\n",
      "  time_total_s: 21916.363869190216\n",
      "  timers:\n",
      "    learn_throughput: 1339.828\n",
      "    learn_time_ms: 746.364\n",
      "    load_throughput: 46404.611\n",
      "    load_time_ms: 21.55\n",
      "    sample_throughput: 29.691\n",
      "    sample_time_ms: 33680.69\n",
      "    update_time_ms: 5.653\n",
      "  timestamp: 1635084798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 831000\n",
      "  training_iteration: 831\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   831</td><td style=\"text-align: right;\">         21916.4</td><td style=\"text-align: right;\">831000</td><td style=\"text-align: right;\"> -3.0163</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            301.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 832000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-13-52\n",
      "  done: false\n",
      "  episode_len_mean: 301.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.015999999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2884\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4514884416176967e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1407834092775981\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027724158549435333\n",
      "          policy_loss: -0.03808104279968474\n",
      "          total_loss: -0.037787234596908095\n",
      "          vf_explained_var: 0.4388044476509094\n",
      "          vf_loss: 0.011701636502726211\n",
      "    num_agent_steps_sampled: 832000\n",
      "    num_agent_steps_trained: 832000\n",
      "    num_steps_sampled: 832000\n",
      "    num_steps_trained: 832000\n",
      "  iterations_since_restore: 832\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.53800000000001\n",
      "    ram_util_percent: 52.46\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905444394000601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.315156373670355\n",
      "    mean_inference_ms: 1.9403857346116147\n",
      "    mean_raw_obs_processing_ms: 2.1859407465696123\n",
      "  time_since_restore: 21951.066387176514\n",
      "  time_this_iter_s: 34.70251798629761\n",
      "  time_total_s: 21951.066387176514\n",
      "  timers:\n",
      "    learn_throughput: 1338.003\n",
      "    learn_time_ms: 747.383\n",
      "    load_throughput: 47941.423\n",
      "    load_time_ms: 20.859\n",
      "    sample_throughput: 30.896\n",
      "    sample_time_ms: 32366.544\n",
      "    update_time_ms: 6.121\n",
      "  timestamp: 1635084832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 832000\n",
      "  training_iteration: 832\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   832</td><td style=\"text-align: right;\">         21951.1</td><td style=\"text-align: right;\">832000</td><td style=\"text-align: right;\">  -3.016</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">             301.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 833000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 301.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.01779999999998\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2887\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2018525388505723\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009995695521802933\n",
      "          policy_loss: 0.06148504209187296\n",
      "          total_loss: 0.058541967802577546\n",
      "          vf_explained_var: 0.45133349299430847\n",
      "          vf_loss: 0.009075449332724222\n",
      "    num_agent_steps_sampled: 833000\n",
      "    num_agent_steps_trained: 833000\n",
      "    num_steps_sampled: 833000\n",
      "    num_steps_trained: 833000\n",
      "  iterations_since_restore: 833\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.99565217391304\n",
      "    ram_util_percent: 52.73695652173911\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039058109889062026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.323022124709215\n",
      "    mean_inference_ms: 1.9405023284482688\n",
      "    mean_raw_obs_processing_ms: 2.1863124959079805\n",
      "  time_since_restore: 21983.458024024963\n",
      "  time_this_iter_s: 32.39163684844971\n",
      "  time_total_s: 21983.458024024963\n",
      "  timers:\n",
      "    learn_throughput: 1326.331\n",
      "    learn_time_ms: 753.959\n",
      "    load_throughput: 47486.253\n",
      "    load_time_ms: 21.059\n",
      "    sample_throughput: 31.103\n",
      "    sample_time_ms: 32150.924\n",
      "    update_time_ms: 5.925\n",
      "  timestamp: 1635084865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 833000\n",
      "  training_iteration: 833\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   833</td><td style=\"text-align: right;\">         21983.5</td><td style=\"text-align: right;\">833000</td><td style=\"text-align: right;\"> -3.0178</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            301.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 834000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-14-56\n",
      "  done: false\n",
      "  episode_len_mean: 302.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.028699999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2890\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1658782323201498\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010528033397565018\n",
      "          policy_loss: 0.04220283553004265\n",
      "          total_loss: 0.03960819376839532\n",
      "          vf_explained_var: 0.3525668680667877\n",
      "          vf_loss: 0.009064138686517254\n",
      "    num_agent_steps_sampled: 834000\n",
      "    num_agent_steps_trained: 834000\n",
      "    num_steps_sampled: 834000\n",
      "    num_steps_trained: 834000\n",
      "  iterations_since_restore: 834\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.88636363636364\n",
      "    ram_util_percent: 52.80227272727271\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039061851401509126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.330879167242802\n",
      "    mean_inference_ms: 1.9406203021062465\n",
      "    mean_raw_obs_processing_ms: 2.1862369604036114\n",
      "  time_since_restore: 22014.297451734543\n",
      "  time_this_iter_s: 30.839427709579468\n",
      "  time_total_s: 22014.297451734543\n",
      "  timers:\n",
      "    learn_throughput: 1319.137\n",
      "    learn_time_ms: 758.071\n",
      "    load_throughput: 46577.346\n",
      "    load_time_ms: 21.47\n",
      "    sample_throughput: 31.226\n",
      "    sample_time_ms: 32024.287\n",
      "    update_time_ms: 6.842\n",
      "  timestamp: 1635084896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 834000\n",
      "  training_iteration: 834\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   834</td><td style=\"text-align: right;\">         22014.3</td><td style=\"text-align: right;\">834000</td><td style=\"text-align: right;\"> -3.0287</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            302.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 835000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-15-26\n",
      "  done: false\n",
      "  episode_len_mean: 303.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0346999999999786\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2893\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2227878385119968\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013570538194273615\n",
      "          policy_loss: -0.03466403020752801\n",
      "          total_loss: -0.036336799959341685\n",
      "          vf_explained_var: 0.14859938621520996\n",
      "          vf_loss: 0.010555107958821787\n",
      "    num_agent_steps_sampled: 835000\n",
      "    num_agent_steps_trained: 835000\n",
      "    num_steps_sampled: 835000\n",
      "    num_steps_trained: 835000\n",
      "  iterations_since_restore: 835\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17906976744187\n",
      "    ram_util_percent: 52.82558139534885\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906557864127968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.338645188370148\n",
      "    mean_inference_ms: 1.9407382208864203\n",
      "    mean_raw_obs_processing_ms: 2.185951714407174\n",
      "  time_since_restore: 22044.43080019951\n",
      "  time_this_iter_s: 30.13334846496582\n",
      "  time_total_s: 22044.43080019951\n",
      "  timers:\n",
      "    learn_throughput: 1319.596\n",
      "    learn_time_ms: 757.808\n",
      "    load_throughput: 45988.511\n",
      "    load_time_ms: 21.745\n",
      "    sample_throughput: 31.451\n",
      "    sample_time_ms: 31795.625\n",
      "    update_time_ms: 6.872\n",
      "  timestamp: 1635084926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 835000\n",
      "  training_iteration: 835\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   835</td><td style=\"text-align: right;\">         22044.4</td><td style=\"text-align: right;\">835000</td><td style=\"text-align: right;\"> -3.0347</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            303.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 836000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-15-58\n",
      "  done: false\n",
      "  episode_len_mean: 303.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.038099999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2896\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1470203147994147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009074984693783409\n",
      "          policy_loss: -0.09748465857572025\n",
      "          total_loss: -0.09627792040506998\n",
      "          vf_explained_var: 0.18601594865322113\n",
      "          vf_loss: 0.012676936098270947\n",
      "    num_agent_steps_sampled: 836000\n",
      "    num_agent_steps_trained: 836000\n",
      "    num_steps_sampled: 836000\n",
      "    num_steps_trained: 836000\n",
      "  iterations_since_restore: 836\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.62340425531916\n",
      "    ram_util_percent: 52.7468085106383\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039069609867842775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.34644007565304\n",
      "    mean_inference_ms: 1.9408571582504126\n",
      "    mean_raw_obs_processing_ms: 2.1856699716841965\n",
      "  time_since_restore: 22077.0486369133\n",
      "  time_this_iter_s: 32.617836713790894\n",
      "  time_total_s: 22077.0486369133\n",
      "  timers:\n",
      "    learn_throughput: 1317.455\n",
      "    learn_time_ms: 759.039\n",
      "    load_throughput: 47465.725\n",
      "    load_time_ms: 21.068\n",
      "    sample_throughput: 31.258\n",
      "    sample_time_ms: 31991.613\n",
      "    update_time_ms: 7.212\n",
      "  timestamp: 1635084958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 836000\n",
      "  training_iteration: 836\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   836</td><td style=\"text-align: right;\">           22077</td><td style=\"text-align: right;\">836000</td><td style=\"text-align: right;\"> -3.0381</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            303.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 837000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-16-30\n",
      "  done: false\n",
      "  episode_len_mean: 303.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.038999999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2900\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1035032682948642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010777756195532914\n",
      "          policy_loss: 0.01958028193977144\n",
      "          total_loss: 0.02142199402054151\n",
      "          vf_explained_var: 0.22594352066516876\n",
      "          vf_loss: 0.01287674384398593\n",
      "    num_agent_steps_sampled: 837000\n",
      "    num_agent_steps_trained: 837000\n",
      "    num_steps_sampled: 837000\n",
      "    num_steps_trained: 837000\n",
      "  iterations_since_restore: 837\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.74090909090908\n",
      "    ram_util_percent: 52.69772727272727\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03907502331446484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.356829918697702\n",
      "    mean_inference_ms: 1.9410177325895983\n",
      "    mean_raw_obs_processing_ms: 2.185283931989661\n",
      "  time_since_restore: 22108.496998786926\n",
      "  time_this_iter_s: 31.44836187362671\n",
      "  time_total_s: 22108.496998786926\n",
      "  timers:\n",
      "    learn_throughput: 1320.707\n",
      "    learn_time_ms: 757.17\n",
      "    load_throughput: 49089.488\n",
      "    load_time_ms: 20.371\n",
      "    sample_throughput: 31.135\n",
      "    sample_time_ms: 32118.462\n",
      "    update_time_ms: 8.077\n",
      "  timestamp: 1635084990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 837000\n",
      "  training_iteration: 837\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   837</td><td style=\"text-align: right;\">         22108.5</td><td style=\"text-align: right;\">837000</td><td style=\"text-align: right;\">  -3.039</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">             303.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 838000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 303.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.036999999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2903\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1222192247708638\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005866464051716886\n",
      "          policy_loss: 0.03642935703198115\n",
      "          total_loss: 0.03468958934148152\n",
      "          vf_explained_var: 0.3281368911266327\n",
      "          vf_loss: 0.009482426528120414\n",
      "    num_agent_steps_sampled: 838000\n",
      "    num_agent_steps_trained: 838000\n",
      "    num_steps_sampled: 838000\n",
      "    num_steps_trained: 838000\n",
      "  iterations_since_restore: 838\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.66170212765957\n",
      "    ram_util_percent: 52.57659574468084\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03907907701828105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.364498679738116\n",
      "    mean_inference_ms: 1.9411360765518928\n",
      "    mean_raw_obs_processing_ms: 2.1850091276229207\n",
      "  time_since_restore: 22141.14404153824\n",
      "  time_this_iter_s: 32.647042751312256\n",
      "  time_total_s: 22141.14404153824\n",
      "  timers:\n",
      "    learn_throughput: 1330.769\n",
      "    learn_time_ms: 751.445\n",
      "    load_throughput: 52031.152\n",
      "    load_time_ms: 19.219\n",
      "    sample_throughput: 30.785\n",
      "    sample_time_ms: 32483.771\n",
      "    update_time_ms: 7.628\n",
      "  timestamp: 1635085023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 838000\n",
      "  training_iteration: 838\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   838</td><td style=\"text-align: right;\">         22141.1</td><td style=\"text-align: right;\">838000</td><td style=\"text-align: right;\">  -3.037</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">             303.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 839000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-17-34\n",
      "  done: false\n",
      "  episode_len_mean: 303.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.034699999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2907\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.092124303181966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01080919698675255\n",
      "          policy_loss: 0.013004951261811786\n",
      "          total_loss: 0.01549281585547659\n",
      "          vf_explained_var: 0.24725140631198883\n",
      "          vf_loss: 0.013409106836964687\n",
      "    num_agent_steps_sampled: 839000\n",
      "    num_agent_steps_trained: 839000\n",
      "    num_steps_sampled: 839000\n",
      "    num_steps_trained: 839000\n",
      "  iterations_since_restore: 839\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.48222222222222\n",
      "    ram_util_percent: 52.5111111111111\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03908445007737896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.374541267218127\n",
      "    mean_inference_ms: 1.9412911963581352\n",
      "    mean_raw_obs_processing_ms: 2.184655113801997\n",
      "  time_since_restore: 22172.348187208176\n",
      "  time_this_iter_s: 31.204145669937134\n",
      "  time_total_s: 22172.348187208176\n",
      "  timers:\n",
      "    learn_throughput: 1333.585\n",
      "    learn_time_ms: 749.858\n",
      "    load_throughput: 51411.306\n",
      "    load_time_ms: 19.451\n",
      "    sample_throughput: 30.721\n",
      "    sample_time_ms: 32550.933\n",
      "    update_time_ms: 7.604\n",
      "  timestamp: 1635085054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 839000\n",
      "  training_iteration: 839\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   839</td><td style=\"text-align: right;\">         22172.3</td><td style=\"text-align: right;\">839000</td><td style=\"text-align: right;\"> -3.0347</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            303.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-18-25\n",
      "  done: false\n",
      "  episode_len_mean: 303.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.033099999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2910\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1016767263412475\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013763868220652418\n",
      "          policy_loss: -0.012457702888382806\n",
      "          total_loss: -0.013582099808586968\n",
      "          vf_explained_var: 0.33454012870788574\n",
      "          vf_loss: 0.009892368482542224\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "  iterations_since_restore: 840\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.15416666666667\n",
      "    ram_util_percent: 52.42222222222222\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03908848847907184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.38202483725357\n",
      "    mean_inference_ms: 1.941407638504327\n",
      "    mean_raw_obs_processing_ms: 2.185029402287119\n",
      "  time_since_restore: 22223.275119543076\n",
      "  time_this_iter_s: 50.9269323348999\n",
      "  time_total_s: 22223.275119543076\n",
      "  timers:\n",
      "    learn_throughput: 1305.224\n",
      "    learn_time_ms: 766.152\n",
      "    load_throughput: 51675.946\n",
      "    load_time_ms: 19.351\n",
      "    sample_throughput: 28.955\n",
      "    sample_time_ms: 34536.561\n",
      "    update_time_ms: 7.587\n",
      "  timestamp: 1635085105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 840\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   840</td><td style=\"text-align: right;\">         22223.3</td><td style=\"text-align: right;\">840000</td><td style=\"text-align: right;\"> -3.0331</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            303.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 841000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-18-57\n",
      "  done: false\n",
      "  episode_len_mean: 303.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.036099999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2914\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.138350952996148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015945276634026898\n",
      "          policy_loss: -0.01278728089398808\n",
      "          total_loss: -0.01148896167675654\n",
      "          vf_explained_var: 0.19995729625225067\n",
      "          vf_loss: 0.012681826586938567\n",
      "    num_agent_steps_sampled: 841000\n",
      "    num_agent_steps_trained: 841000\n",
      "    num_steps_sampled: 841000\n",
      "    num_steps_trained: 841000\n",
      "  iterations_since_restore: 841\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.77872340425532\n",
      "    ram_util_percent: 52.4595744680851\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03909380840719493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.391820538736347\n",
      "    mean_inference_ms: 1.9415614720306746\n",
      "    mean_raw_obs_processing_ms: 2.185539315703457\n",
      "  time_since_restore: 22255.74566102028\n",
      "  time_this_iter_s: 32.47054147720337\n",
      "  time_total_s: 22255.74566102028\n",
      "  timers:\n",
      "    learn_throughput: 1307.043\n",
      "    learn_time_ms: 765.086\n",
      "    load_throughput: 48519.241\n",
      "    load_time_ms: 20.61\n",
      "    sample_throughput: 30.174\n",
      "    sample_time_ms: 33140.911\n",
      "    update_time_ms: 7.431\n",
      "  timestamp: 1635085137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 841000\n",
      "  training_iteration: 841\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   841</td><td style=\"text-align: right;\">         22255.7</td><td style=\"text-align: right;\">841000</td><td style=\"text-align: right;\"> -3.0361</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            303.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 842000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-19-29\n",
      "  done: false\n",
      "  episode_len_mean: 303.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.039399999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2917\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1423868629667493\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0077906608500774395\n",
      "          policy_loss: 0.049740736931562425\n",
      "          total_loss: 0.048802559326092405\n",
      "          vf_explained_var: 0.2746739387512207\n",
      "          vf_loss: 0.010485686902474199\n",
      "    num_agent_steps_sampled: 842000\n",
      "    num_agent_steps_trained: 842000\n",
      "    num_steps_sampled: 842000\n",
      "    num_steps_trained: 842000\n",
      "  iterations_since_restore: 842\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.41777777777779\n",
      "    ram_util_percent: 52.577777777777776\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03909779861984371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.399091066135046\n",
      "    mean_inference_ms: 1.9416772441507695\n",
      "    mean_raw_obs_processing_ms: 2.1859174558595624\n",
      "  time_since_restore: 22287.382021665573\n",
      "  time_this_iter_s: 31.63636064529419\n",
      "  time_total_s: 22287.382021665573\n",
      "  timers:\n",
      "    learn_throughput: 1304.13\n",
      "    learn_time_ms: 766.795\n",
      "    load_throughput: 46574.864\n",
      "    load_time_ms: 21.471\n",
      "    sample_throughput: 30.458\n",
      "    sample_time_ms: 32831.807\n",
      "    update_time_ms: 7.331\n",
      "  timestamp: 1635085169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 842000\n",
      "  training_iteration: 842\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   842</td><td style=\"text-align: right;\">         22287.4</td><td style=\"text-align: right;\">842000</td><td style=\"text-align: right;\"> -3.0394</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            303.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 843000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-20-02\n",
      "  done: false\n",
      "  episode_len_mean: 304.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.0447999999999786\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2920\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1651702801386514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009569742209233172\n",
      "          policy_loss: -0.05310171677006616\n",
      "          total_loss: -0.051697463128301833\n",
      "          vf_explained_var: 0.15917806327342987\n",
      "          vf_loss: 0.013055953942239285\n",
      "    num_agent_steps_sampled: 843000\n",
      "    num_agent_steps_trained: 843000\n",
      "    num_steps_sampled: 843000\n",
      "    num_steps_trained: 843000\n",
      "  iterations_since_restore: 843\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.44255319148937\n",
      "    ram_util_percent: 52.67872340425532\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039101796166383415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.406277981690128\n",
      "    mean_inference_ms: 1.9417932149162718\n",
      "    mean_raw_obs_processing_ms: 2.1858876113927392\n",
      "  time_since_restore: 22320.278173208237\n",
      "  time_this_iter_s: 32.896151542663574\n",
      "  time_total_s: 22320.278173208237\n",
      "  timers:\n",
      "    learn_throughput: 1315.098\n",
      "    learn_time_ms: 760.4\n",
      "    load_throughput: 46640.742\n",
      "    load_time_ms: 21.44\n",
      "    sample_throughput: 30.406\n",
      "    sample_time_ms: 32888.105\n",
      "    update_time_ms: 7.88\n",
      "  timestamp: 1635085202\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 843000\n",
      "  training_iteration: 843\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   843</td><td style=\"text-align: right;\">         22320.3</td><td style=\"text-align: right;\">843000</td><td style=\"text-align: right;\"> -3.0448</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            304.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 844000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-20-34\n",
      "  done: false\n",
      "  episode_len_mean: 304.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.0437999999999783\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2924\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.16817225350274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013965246384159337\n",
      "          policy_loss: 0.010567739026414023\n",
      "          total_loss: 0.013095755378405254\n",
      "          vf_explained_var: 0.25640198588371277\n",
      "          vf_loss: 0.01420973568326897\n",
      "    num_agent_steps_sampled: 844000\n",
      "    num_agent_steps_trained: 844000\n",
      "    num_steps_sampled: 844000\n",
      "    num_steps_trained: 844000\n",
      "  iterations_since_restore: 844\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.91956521739131\n",
      "    ram_util_percent: 52.660869565217396\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910736598416844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.415868343309917\n",
      "    mean_inference_ms: 1.9419524792097511\n",
      "    mean_raw_obs_processing_ms: 2.1855647081441374\n",
      "  time_since_restore: 22352.756892442703\n",
      "  time_this_iter_s: 32.47871923446655\n",
      "  time_total_s: 22352.756892442703\n",
      "  timers:\n",
      "    learn_throughput: 1318.978\n",
      "    learn_time_ms: 758.162\n",
      "    load_throughput: 45465.379\n",
      "    load_time_ms: 21.995\n",
      "    sample_throughput: 30.253\n",
      "    sample_time_ms: 33054.536\n",
      "    update_time_ms: 7.061\n",
      "  timestamp: 1635085234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 844000\n",
      "  training_iteration: 844\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   844</td><td style=\"text-align: right;\">         22352.8</td><td style=\"text-align: right;\">844000</td><td style=\"text-align: right;\"> -3.0438</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            304.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 845000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-21-07\n",
      "  done: false\n",
      "  episode_len_mean: 304.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.043099999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2927\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1640701002544827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009835497064820705\n",
      "          policy_loss: 0.03593531482749515\n",
      "          total_loss: 0.03422314309411579\n",
      "          vf_explained_var: 0.23579257726669312\n",
      "          vf_loss: 0.009928527528730531\n",
      "    num_agent_steps_sampled: 845000\n",
      "    num_agent_steps_trained: 845000\n",
      "    num_steps_sampled: 845000\n",
      "    num_steps_trained: 845000\n",
      "  iterations_since_restore: 845\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.40434782608696\n",
      "    ram_util_percent: 52.59565217391305\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911156454006992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.423053999586806\n",
      "    mean_inference_ms: 1.9420730104380084\n",
      "    mean_raw_obs_processing_ms: 2.185338164069959\n",
      "  time_since_restore: 22385.15893149376\n",
      "  time_this_iter_s: 32.40203905105591\n",
      "  time_total_s: 22385.15893149376\n",
      "  timers:\n",
      "    learn_throughput: 1315.919\n",
      "    learn_time_ms: 759.925\n",
      "    load_throughput: 45390.641\n",
      "    load_time_ms: 22.031\n",
      "    sample_throughput: 30.049\n",
      "    sample_time_ms: 33279.471\n",
      "    update_time_ms: 7.162\n",
      "  timestamp: 1635085267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 845000\n",
      "  training_iteration: 845\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   845</td><td style=\"text-align: right;\">         22385.2</td><td style=\"text-align: right;\">845000</td><td style=\"text-align: right;\"> -3.0431</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            304.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 846000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-21-37\n",
      "  done: false\n",
      "  episode_len_mean: 303.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.037999999999979\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2930\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1724518524275886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0128826537696768\n",
      "          policy_loss: -0.11851653647091653\n",
      "          total_loss: -0.11631747335195541\n",
      "          vf_explained_var: 0.20289120078086853\n",
      "          vf_loss: 0.013923579526858198\n",
      "    num_agent_steps_sampled: 846000\n",
      "    num_agent_steps_trained: 846000\n",
      "    num_steps_sampled: 846000\n",
      "    num_steps_trained: 846000\n",
      "  iterations_since_restore: 846\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.85000000000001\n",
      "    ram_util_percent: 52.527272727272724\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039115712052262205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.430269632114182\n",
      "    mean_inference_ms: 1.9421924957271506\n",
      "    mean_raw_obs_processing_ms: 2.1851146625562525\n",
      "  time_since_restore: 22415.672929525375\n",
      "  time_this_iter_s: 30.51399803161621\n",
      "  time_total_s: 22415.672929525375\n",
      "  timers:\n",
      "    learn_throughput: 1319.03\n",
      "    learn_time_ms: 758.133\n",
      "    load_throughput: 43845.404\n",
      "    load_time_ms: 22.807\n",
      "    sample_throughput: 30.239\n",
      "    sample_time_ms: 33069.983\n",
      "    update_time_ms: 7.248\n",
      "  timestamp: 1635085297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 846000\n",
      "  training_iteration: 846\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   846</td><td style=\"text-align: right;\">         22415.7</td><td style=\"text-align: right;\">846000</td><td style=\"text-align: right;\">  -3.038</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">             303.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 847000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 302.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.0238999999999794\n",
      "  episode_reward_min: -3.9699999999999593\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2934\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2223837494850158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009110982215029395\n",
      "          policy_loss: 0.04296711766057544\n",
      "          total_loss: 0.04337434619665146\n",
      "          vf_explained_var: 0.19404661655426025\n",
      "          vf_loss: 0.012631061160936952\n",
      "    num_agent_steps_sampled: 847000\n",
      "    num_agent_steps_trained: 847000\n",
      "    num_steps_sampled: 847000\n",
      "    num_steps_trained: 847000\n",
      "  iterations_since_restore: 847\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.3608695652174\n",
      "    ram_util_percent: 52.42391304347827\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03912124906558885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.439973414643774\n",
      "    mean_inference_ms: 1.9423510605343615\n",
      "    mean_raw_obs_processing_ms: 2.1848286436095714\n",
      "  time_since_restore: 22447.91681909561\n",
      "  time_this_iter_s: 32.243889570236206\n",
      "  time_total_s: 22447.91681909561\n",
      "  timers:\n",
      "    learn_throughput: 1319.332\n",
      "    learn_time_ms: 757.96\n",
      "    load_throughput: 43538.283\n",
      "    load_time_ms: 22.968\n",
      "    sample_throughput: 30.166\n",
      "    sample_time_ms: 33150.281\n",
      "    update_time_ms: 6.462\n",
      "  timestamp: 1635085330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 847000\n",
      "  training_iteration: 847\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   847</td><td style=\"text-align: right;\">         22447.9</td><td style=\"text-align: right;\">847000</td><td style=\"text-align: right;\"> -3.0239</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -3.97</td><td style=\"text-align: right;\">            302.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 848000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-22-42\n",
      "  done: false\n",
      "  episode_len_mean: 301.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.013699999999979\n",
      "  episode_reward_min: -3.6799999999999655\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2937\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2693618986341688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011536464919163257\n",
      "          policy_loss: -0.013530953311257893\n",
      "          total_loss: -0.01501491086350547\n",
      "          vf_explained_var: 0.1903633326292038\n",
      "          vf_loss: 0.011209661955945194\n",
      "    num_agent_steps_sampled: 848000\n",
      "    num_agent_steps_trained: 848000\n",
      "    num_steps_sampled: 848000\n",
      "    num_steps_trained: 848000\n",
      "  iterations_since_restore: 848\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.49574468085106\n",
      "    ram_util_percent: 52.43191489361703\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03912542352173089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.447405924067784\n",
      "    mean_inference_ms: 1.9424700205555765\n",
      "    mean_raw_obs_processing_ms: 2.184612050309785\n",
      "  time_since_restore: 22480.55644607544\n",
      "  time_this_iter_s: 32.63962697982788\n",
      "  time_total_s: 22480.55644607544\n",
      "  timers:\n",
      "    learn_throughput: 1311.765\n",
      "    learn_time_ms: 762.332\n",
      "    load_throughput: 41615.608\n",
      "    load_time_ms: 24.029\n",
      "    sample_throughput: 30.172\n",
      "    sample_time_ms: 33143.837\n",
      "    update_time_ms: 6.684\n",
      "  timestamp: 1635085362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 848000\n",
      "  training_iteration: 848\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   848</td><td style=\"text-align: right;\">         22480.6</td><td style=\"text-align: right;\">848000</td><td style=\"text-align: right;\"> -3.0137</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -3.68</td><td style=\"text-align: right;\">            301.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 849000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 300.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.0041999999999796\n",
      "  episode_reward_min: -3.6799999999999655\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2940\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2056357476446364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00741547029336809\n",
      "          policy_loss: -0.10476919180817074\n",
      "          total_loss: -0.10479369991355472\n",
      "          vf_explained_var: 0.210309237241745\n",
      "          vf_loss: 0.012031847181626492\n",
      "    num_agent_steps_sampled: 849000\n",
      "    num_agent_steps_trained: 849000\n",
      "    num_steps_sampled: 849000\n",
      "    num_steps_trained: 849000\n",
      "  iterations_since_restore: 849\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.48840579710146\n",
      "    ram_util_percent: 52.36811594202897\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03912961471502227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.454909075688988\n",
      "    mean_inference_ms: 1.9425881437958774\n",
      "    mean_raw_obs_processing_ms: 2.1850201016937736\n",
      "  time_since_restore: 22529.242030858994\n",
      "  time_this_iter_s: 48.68558478355408\n",
      "  time_total_s: 22529.242030858994\n",
      "  timers:\n",
      "    learn_throughput: 1311.353\n",
      "    learn_time_ms: 762.571\n",
      "    load_throughput: 43331.109\n",
      "    load_time_ms: 23.078\n",
      "    sample_throughput: 28.66\n",
      "    sample_time_ms: 34891.825\n",
      "    update_time_ms: 7.15\n",
      "  timestamp: 1635085411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 849000\n",
      "  training_iteration: 849\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   849</td><td style=\"text-align: right;\">         22529.2</td><td style=\"text-align: right;\">849000</td><td style=\"text-align: right;\"> -3.0042</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -3.68</td><td style=\"text-align: right;\">            300.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 850000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-24-00\n",
      "  done: false\n",
      "  episode_len_mean: 300.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.0040999999999793\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2943\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.146942060523563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014927168507668754\n",
      "          policy_loss: -0.12197377731402716\n",
      "          total_loss: -0.11831879996591144\n",
      "          vf_explained_var: 0.18100637197494507\n",
      "          vf_loss: 0.01512439777660701\n",
      "    num_agent_steps_sampled: 850000\n",
      "    num_agent_steps_trained: 850000\n",
      "    num_steps_sampled: 850000\n",
      "    num_steps_trained: 850000\n",
      "  iterations_since_restore: 850\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.18571428571428\n",
      "    ram_util_percent: 52.62380952380952\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913382564579535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.46237830804577\n",
      "    mean_inference_ms: 1.9427066165606386\n",
      "    mean_raw_obs_processing_ms: 2.1854297753355687\n",
      "  time_since_restore: 22558.39791083336\n",
      "  time_this_iter_s: 29.155879974365234\n",
      "  time_total_s: 22558.39791083336\n",
      "  timers:\n",
      "    learn_throughput: 1345.499\n",
      "    learn_time_ms: 743.219\n",
      "    load_throughput: 43023.621\n",
      "    load_time_ms: 23.243\n",
      "    sample_throughput: 30.549\n",
      "    sample_time_ms: 32734.155\n",
      "    update_time_ms: 6.806\n",
      "  timestamp: 1635085440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 850000\n",
      "  training_iteration: 850\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   850</td><td style=\"text-align: right;\">         22558.4</td><td style=\"text-align: right;\">850000</td><td style=\"text-align: right;\"> -3.0041</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            300.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 851000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-24-30\n",
      "  done: false\n",
      "  episode_len_mean: 300.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.009599999999979\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2947\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0724093033207787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006942626478449417\n",
      "          policy_loss: 0.008585747828086217\n",
      "          total_loss: 0.012394519067472881\n",
      "          vf_explained_var: 0.17548911273479462\n",
      "          vf_loss: 0.014532861279116736\n",
      "    num_agent_steps_sampled: 851000\n",
      "    num_agent_steps_trained: 851000\n",
      "    num_steps_sampled: 851000\n",
      "    num_steps_trained: 851000\n",
      "  iterations_since_restore: 851\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.48809523809524\n",
      "    ram_util_percent: 52.61904761904762\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913943055113637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.472356916676148\n",
      "    mean_inference_ms: 1.942867348518195\n",
      "    mean_raw_obs_processing_ms: 2.1859582326185034\n",
      "  time_since_restore: 22588.297917842865\n",
      "  time_this_iter_s: 29.900007009506226\n",
      "  time_total_s: 22588.297917842865\n",
      "  timers:\n",
      "    learn_throughput: 1345.073\n",
      "    learn_time_ms: 743.454\n",
      "    load_throughput: 43507.166\n",
      "    load_time_ms: 22.985\n",
      "    sample_throughput: 30.791\n",
      "    sample_time_ms: 32477.056\n",
      "    update_time_ms: 6.806\n",
      "  timestamp: 1635085470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 851000\n",
      "  training_iteration: 851\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   851</td><td style=\"text-align: right;\">         22588.3</td><td style=\"text-align: right;\">851000</td><td style=\"text-align: right;\"> -3.0096</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            300.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 852000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-24-59\n",
      "  done: false\n",
      "  episode_len_mean: 301.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.0177999999999794\n",
      "  episode_reward_min: -3.5299999999999687\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2949\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0770132939020793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011240585750015776\n",
      "          policy_loss: -0.1095571239789327\n",
      "          total_loss: -0.10960904955863952\n",
      "          vf_explained_var: 0.2674984335899353\n",
      "          vf_loss: 0.010718211795837205\n",
      "    num_agent_steps_sampled: 852000\n",
      "    num_agent_steps_trained: 852000\n",
      "    num_steps_sampled: 852000\n",
      "    num_steps_trained: 852000\n",
      "  iterations_since_restore: 852\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.36341463414634\n",
      "    ram_util_percent: 52.74146341463415\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03914226737158459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.477244159291605\n",
      "    mean_inference_ms: 1.9429483014519975\n",
      "    mean_raw_obs_processing_ms: 2.1858252110360414\n",
      "  time_since_restore: 22617.02050256729\n",
      "  time_this_iter_s: 28.72258472442627\n",
      "  time_total_s: 22617.02050256729\n",
      "  timers:\n",
      "    learn_throughput: 1345.846\n",
      "    learn_time_ms: 743.027\n",
      "    load_throughput: 43569.761\n",
      "    load_time_ms: 22.952\n",
      "    sample_throughput: 31.069\n",
      "    sample_time_ms: 32186.644\n",
      "    update_time_ms: 6.352\n",
      "  timestamp: 1635085499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 852000\n",
      "  training_iteration: 852\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   852</td><td style=\"text-align: right;\">           22617</td><td style=\"text-align: right;\">852000</td><td style=\"text-align: right;\"> -3.0178</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">            301.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 853000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 303.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.0352999999999795\n",
      "  episode_reward_min: -3.5299999999999687\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2952\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0293932769033645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006824065120119372\n",
      "          policy_loss: -0.10629443559381697\n",
      "          total_loss: -0.10372963945070902\n",
      "          vf_explained_var: 0.15109558403491974\n",
      "          vf_loss: 0.01285872686550849\n",
      "    num_agent_steps_sampled: 853000\n",
      "    num_agent_steps_trained: 853000\n",
      "    num_steps_sampled: 853000\n",
      "    num_steps_trained: 853000\n",
      "  iterations_since_restore: 853\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.70952380952379\n",
      "    ram_util_percent: 52.72619047619047\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03914657484170137\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.4844719034713\n",
      "    mean_inference_ms: 1.9430705626442626\n",
      "    mean_raw_obs_processing_ms: 2.1856045682888396\n",
      "  time_since_restore: 22645.81752181053\n",
      "  time_this_iter_s: 28.797019243240356\n",
      "  time_total_s: 22645.81752181053\n",
      "  timers:\n",
      "    learn_throughput: 1332.11\n",
      "    learn_time_ms: 750.689\n",
      "    load_throughput: 44136.398\n",
      "    load_time_ms: 22.657\n",
      "    sample_throughput: 31.477\n",
      "    sample_time_ms: 31769.492\n",
      "    update_time_ms: 6.167\n",
      "  timestamp: 1635085528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 853000\n",
      "  training_iteration: 853\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   853</td><td style=\"text-align: right;\">         22645.8</td><td style=\"text-align: right;\">853000</td><td style=\"text-align: right;\"> -3.0353</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">            303.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 854000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-25-55\n",
      "  done: false\n",
      "  episode_len_mean: 305.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.050199999999979\n",
      "  episode_reward_min: -3.5299999999999687\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2956\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9819774581326379\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009387056393524535\n",
      "          policy_loss: 0.006314543262124062\n",
      "          total_loss: 0.011875573048988978\n",
      "          vf_explained_var: 0.12430036067962646\n",
      "          vf_loss: 0.015380801984833346\n",
      "    num_agent_steps_sampled: 854000\n",
      "    num_agent_steps_trained: 854000\n",
      "    num_steps_sampled: 854000\n",
      "    num_steps_trained: 854000\n",
      "  iterations_since_restore: 854\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.76923076923077\n",
      "    ram_util_percent: 52.67435897435895\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0391523247101753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.49389530992381\n",
      "    mean_inference_ms: 1.9432315836445284\n",
      "    mean_raw_obs_processing_ms: 2.185300810346311\n",
      "  time_since_restore: 22673.391359567642\n",
      "  time_this_iter_s: 27.573837757110596\n",
      "  time_total_s: 22673.391359567642\n",
      "  timers:\n",
      "    learn_throughput: 1331.949\n",
      "    learn_time_ms: 750.78\n",
      "    load_throughput: 43942.42\n",
      "    load_time_ms: 22.757\n",
      "    sample_throughput: 31.97\n",
      "    sample_time_ms: 31279.021\n",
      "    update_time_ms: 5.994\n",
      "  timestamp: 1635085555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 854000\n",
      "  training_iteration: 854\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   854</td><td style=\"text-align: right;\">         22673.4</td><td style=\"text-align: right;\">854000</td><td style=\"text-align: right;\"> -3.0502</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">            305.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 855000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-26-25\n",
      "  done: false\n",
      "  episode_len_mean: 305.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.055399999999979\n",
      "  episode_reward_min: -3.5299999999999687\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2959\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.177232662426543e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9699491004149119\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004126810902046824\n",
      "          policy_loss: 0.0936762112710211\n",
      "          total_loss: 0.09167253606849246\n",
      "          vf_explained_var: 0.22939088940620422\n",
      "          vf_loss: 0.007695812798273336\n",
      "    num_agent_steps_sampled: 855000\n",
      "    num_agent_steps_trained: 855000\n",
      "    num_steps_sampled: 855000\n",
      "    num_steps_trained: 855000\n",
      "  iterations_since_restore: 855\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.11627906976744\n",
      "    ram_util_percent: 52.57674418604652\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03915661443602727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.500810992852895\n",
      "    mean_inference_ms: 1.9433511948409699\n",
      "    mean_raw_obs_processing_ms: 2.1850879104874443\n",
      "  time_since_restore: 22703.23364853859\n",
      "  time_this_iter_s: 29.842288970947266\n",
      "  time_total_s: 22703.23364853859\n",
      "  timers:\n",
      "    learn_throughput: 1330.097\n",
      "    learn_time_ms: 751.825\n",
      "    load_throughput: 43815.266\n",
      "    load_time_ms: 22.823\n",
      "    sample_throughput: 32.235\n",
      "    sample_time_ms: 31022.374\n",
      "    update_time_ms: 5.576\n",
      "  timestamp: 1635085585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 855000\n",
      "  training_iteration: 855\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   855</td><td style=\"text-align: right;\">         22703.2</td><td style=\"text-align: right;\">855000</td><td style=\"text-align: right;\"> -3.0554</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">            305.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 856000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 306.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.0616999999999788\n",
      "  episode_reward_min: -3.5299999999999687\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2962\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.5886163312132717e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8871711572011312\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008444065228292965\n",
      "          policy_loss: 0.05263581110371484\n",
      "          total_loss: 0.05556047757466634\n",
      "          vf_explained_var: 0.10237066447734833\n",
      "          vf_loss: 0.011796380879564418\n",
      "    num_agent_steps_sampled: 856000\n",
      "    num_agent_steps_trained: 856000\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 856000\n",
      "  iterations_since_restore: 856\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.98478260869565\n",
      "    ram_util_percent: 52.60652173913043\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03916106839056844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.507819466842903\n",
      "    mean_inference_ms: 1.9434717428923292\n",
      "    mean_raw_obs_processing_ms: 2.1848540210440186\n",
      "  time_since_restore: 22735.504019975662\n",
      "  time_this_iter_s: 32.270371437072754\n",
      "  time_total_s: 22735.504019975662\n",
      "  timers:\n",
      "    learn_throughput: 1328.92\n",
      "    learn_time_ms: 752.491\n",
      "    load_throughput: 42525.038\n",
      "    load_time_ms: 23.516\n",
      "    sample_throughput: 32.054\n",
      "    sample_time_ms: 31196.991\n",
      "    update_time_ms: 5.198\n",
      "  timestamp: 1635085617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 856\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   856</td><td style=\"text-align: right;\">         22735.5</td><td style=\"text-align: right;\">856000</td><td style=\"text-align: right;\"> -3.0617</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">            306.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 857000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-27-29\n",
      "  done: false\n",
      "  episode_len_mean: 306.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.0607999999999786\n",
      "  episode_reward_min: -3.5299999999999687\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2965\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.5886163312132717e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8562553736898634\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007229153295681377\n",
      "          policy_loss: -0.09635603725910187\n",
      "          total_loss: -0.08831552995575799\n",
      "          vf_explained_var: 0.10808359086513519\n",
      "          vf_loss: 0.016603057976398203\n",
      "    num_agent_steps_sampled: 857000\n",
      "    num_agent_steps_trained: 857000\n",
      "    num_steps_sampled: 857000\n",
      "    num_steps_trained: 857000\n",
      "  iterations_since_restore: 857\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.05\n",
      "    ram_util_percent: 52.599999999999994\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039165500231610445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.51484382019245\n",
      "    mean_inference_ms: 1.9435915269411816\n",
      "    mean_raw_obs_processing_ms: 2.1846234752387104\n",
      "  time_since_restore: 22766.9057636261\n",
      "  time_this_iter_s: 31.4017436504364\n",
      "  time_total_s: 22766.9057636261\n",
      "  timers:\n",
      "    learn_throughput: 1329.188\n",
      "    learn_time_ms: 752.339\n",
      "    load_throughput: 41005.811\n",
      "    load_time_ms: 24.387\n",
      "    sample_throughput: 32.143\n",
      "    sample_time_ms: 31111.244\n",
      "    update_time_ms: 6.086\n",
      "  timestamp: 1635085649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 857000\n",
      "  training_iteration: 857\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   857</td><td style=\"text-align: right;\">         22766.9</td><td style=\"text-align: right;\">857000</td><td style=\"text-align: right;\"> -3.0608</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">            306.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 858000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-28-00\n",
      "  done: false\n",
      "  episode_len_mean: 305.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.051799999999979\n",
      "  episode_reward_min: -3.5299999999999687\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2969\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.5886163312132717e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8017030696074168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019338059520226065\n",
      "          policy_loss: -0.0076943774190213945\n",
      "          total_loss: -0.0005509047044648064\n",
      "          vf_explained_var: 0.19855526089668274\n",
      "          vf_loss: 0.015160501479274696\n",
      "    num_agent_steps_sampled: 858000\n",
      "    num_agent_steps_trained: 858000\n",
      "    num_steps_sampled: 858000\n",
      "    num_steps_trained: 858000\n",
      "  iterations_since_restore: 858\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.03777777777779\n",
      "    ram_util_percent: 52.49333333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917135199772556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.524221575998503\n",
      "    mean_inference_ms: 1.9437492756570538\n",
      "    mean_raw_obs_processing_ms: 2.184337061997381\n",
      "  time_since_restore: 22798.17490553856\n",
      "  time_this_iter_s: 31.269141912460327\n",
      "  time_total_s: 22798.17490553856\n",
      "  timers:\n",
      "    learn_throughput: 1330.516\n",
      "    learn_time_ms: 751.588\n",
      "    load_throughput: 42722.599\n",
      "    load_time_ms: 23.407\n",
      "    sample_throughput: 32.283\n",
      "    sample_time_ms: 30975.995\n",
      "    update_time_ms: 5.924\n",
      "  timestamp: 1635085680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 858000\n",
      "  training_iteration: 858\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   858</td><td style=\"text-align: right;\">         22798.2</td><td style=\"text-align: right;\">858000</td><td style=\"text-align: right;\"> -3.0518</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">            305.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 859000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-28-48\n",
      "  done: false\n",
      "  episode_len_mean: 304.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.049199999999979\n",
      "  episode_reward_min: -3.5299999999999687\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2972\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.5886163312132717e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9461044775115119\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022606605580128857\n",
      "          policy_loss: 0.05365470167663362\n",
      "          total_loss: 0.0549973103735182\n",
      "          vf_explained_var: 0.4205946624279022\n",
      "          vf_loss: 0.010803648998909112\n",
      "    num_agent_steps_sampled: 859000\n",
      "    num_agent_steps_trained: 859000\n",
      "    num_steps_sampled: 859000\n",
      "    num_steps_trained: 859000\n",
      "  iterations_since_restore: 859\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.52463768115943\n",
      "    ram_util_percent: 52.41739130434783\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917573826481289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.531256212928046\n",
      "    mean_inference_ms: 1.9438670937167708\n",
      "    mean_raw_obs_processing_ms: 2.184723584913276\n",
      "  time_since_restore: 22846.283772945404\n",
      "  time_this_iter_s: 48.10886740684509\n",
      "  time_total_s: 22846.283772945404\n",
      "  timers:\n",
      "    learn_throughput: 1332.165\n",
      "    learn_time_ms: 750.658\n",
      "    load_throughput: 41234.913\n",
      "    load_time_ms: 24.251\n",
      "    sample_throughput: 32.342\n",
      "    sample_time_ms: 30919.158\n",
      "    update_time_ms: 5.653\n",
      "  timestamp: 1635085728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 859000\n",
      "  training_iteration: 859\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   859</td><td style=\"text-align: right;\">         22846.3</td><td style=\"text-align: right;\">859000</td><td style=\"text-align: right;\"> -3.0492</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -3.53</td><td style=\"text-align: right;\">            304.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 860000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-29-17\n",
      "  done: false\n",
      "  episode_len_mean: 306.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.0634999999999786\n",
      "  episode_reward_min: -4.1699999999999555\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2975\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.8829244968199073e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0360566371017033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023801833123099116\n",
      "          policy_loss: 0.09900799385375447\n",
      "          total_loss: 0.09902795826395352\n",
      "          vf_explained_var: 0.5287227630615234\n",
      "          vf_loss: 0.010380523776014646\n",
      "    num_agent_steps_sampled: 860000\n",
      "    num_agent_steps_trained: 860000\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "  iterations_since_restore: 860\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17999999999999\n",
      "    ram_util_percent: 52.5875\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039180078705955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.538194941457974\n",
      "    mean_inference_ms: 1.9439834793910054\n",
      "    mean_raw_obs_processing_ms: 2.1851109531745196\n",
      "  time_since_restore: 22874.785614013672\n",
      "  time_this_iter_s: 28.501841068267822\n",
      "  time_total_s: 22874.785614013672\n",
      "  timers:\n",
      "    learn_throughput: 1313.681\n",
      "    learn_time_ms: 761.22\n",
      "    load_throughput: 39841.898\n",
      "    load_time_ms: 25.099\n",
      "    sample_throughput: 32.423\n",
      "    sample_time_ms: 30842.262\n",
      "    update_time_ms: 5.787\n",
      "  timestamp: 1635085757\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 860\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   860</td><td style=\"text-align: right;\">         22874.8</td><td style=\"text-align: right;\">860000</td><td style=\"text-align: right;\"> -3.0635</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -4.17</td><td style=\"text-align: right;\">            306.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 861000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-29-43\n",
      "  done: false\n",
      "  episode_len_mean: 308.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.0850999999999784\n",
      "  episode_reward_min: -4.6399999999999455\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2977\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.824386745229861e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3976129035154978\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023207801429845965\n",
      "          policy_loss: -0.028022880189948612\n",
      "          total_loss: -0.03185253027412627\n",
      "          vf_explained_var: 0.3988505005836487\n",
      "          vf_loss: 0.010146467722693665\n",
      "    num_agent_steps_sampled: 861000\n",
      "    num_agent_steps_trained: 861000\n",
      "    num_steps_sampled: 861000\n",
      "    num_steps_trained: 861000\n",
      "  iterations_since_restore: 861\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.22368421052629\n",
      "    ram_util_percent: 52.74473684210527\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039183067588886414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.542796854518084\n",
      "    mean_inference_ms: 1.9440633991925944\n",
      "    mean_raw_obs_processing_ms: 2.185339174655537\n",
      "  time_since_restore: 22900.83033466339\n",
      "  time_this_iter_s: 26.04472064971924\n",
      "  time_total_s: 22900.83033466339\n",
      "  timers:\n",
      "    learn_throughput: 1306.586\n",
      "    learn_time_ms: 765.353\n",
      "    load_throughput: 41438.731\n",
      "    load_time_ms: 24.132\n",
      "    sample_throughput: 32.838\n",
      "    sample_time_ms: 30452.869\n",
      "    update_time_ms: 6.472\n",
      "  timestamp: 1635085783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 861000\n",
      "  training_iteration: 861\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   861</td><td style=\"text-align: right;\">         22900.8</td><td style=\"text-align: right;\">861000</td><td style=\"text-align: right;\"> -3.0851</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -4.64</td><td style=\"text-align: right;\">            308.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 862000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-30-09\n",
      "  done: false\n",
      "  episode_len_mean: 311.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.116399999999977\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2979\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.736580117844795e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6200096395280625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014552724616912174\n",
      "          policy_loss: -0.05820018384191725\n",
      "          total_loss: -0.058822150197294025\n",
      "          vf_explained_var: -0.2850101590156555\n",
      "          vf_loss: 0.015578125562751666\n",
      "    num_agent_steps_sampled: 862000\n",
      "    num_agent_steps_trained: 862000\n",
      "    num_steps_sampled: 862000\n",
      "    num_steps_trained: 862000\n",
      "  iterations_since_restore: 862\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.82894736842104\n",
      "    ram_util_percent: 52.934210526315795\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039186215050253645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.547324664834395\n",
      "    mean_inference_ms: 1.9441457611534958\n",
      "    mean_raw_obs_processing_ms: 2.185155790730452\n",
      "  time_since_restore: 22927.44990682602\n",
      "  time_this_iter_s: 26.619572162628174\n",
      "  time_total_s: 22927.44990682602\n",
      "  timers:\n",
      "    learn_throughput: 1307.321\n",
      "    learn_time_ms: 764.923\n",
      "    load_throughput: 39635.838\n",
      "    load_time_ms: 25.23\n",
      "    sample_throughput: 33.067\n",
      "    sample_time_ms: 30241.976\n",
      "    update_time_ms: 6.392\n",
      "  timestamp: 1635085809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 862000\n",
      "  training_iteration: 862\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   862</td><td style=\"text-align: right;\">         22927.4</td><td style=\"text-align: right;\">862000</td><td style=\"text-align: right;\"> -3.1164</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            311.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 863000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-30-37\n",
      "  done: false\n",
      "  episode_len_mean: 314.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.145199999999976\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2982\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.736580117844795e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5306568834516736\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02349711643280218\n",
      "          policy_loss: 0.03368128774066766\n",
      "          total_loss: 0.035247601278954085\n",
      "          vf_explained_var: 0.031316984444856644\n",
      "          vf_loss: 0.016872862672122815\n",
      "    num_agent_steps_sampled: 863000\n",
      "    num_agent_steps_trained: 863000\n",
      "    num_steps_sampled: 863000\n",
      "    num_steps_trained: 863000\n",
      "  iterations_since_restore: 863\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.73076923076921\n",
      "    ram_util_percent: 52.794871794871796\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039190836975335666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.553955974826223\n",
      "    mean_inference_ms: 1.9442708686800436\n",
      "    mean_raw_obs_processing_ms: 2.184861167634967\n",
      "  time_since_restore: 22954.725045681\n",
      "  time_this_iter_s: 27.27513885498047\n",
      "  time_total_s: 22954.725045681\n",
      "  timers:\n",
      "    learn_throughput: 1319.436\n",
      "    learn_time_ms: 757.899\n",
      "    load_throughput: 40211.3\n",
      "    load_time_ms: 24.869\n",
      "    sample_throughput: 33.226\n",
      "    sample_time_ms: 30096.778\n",
      "    update_time_ms: 6.59\n",
      "  timestamp: 1635085837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 863000\n",
      "  training_iteration: 863\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   863</td><td style=\"text-align: right;\">         22954.7</td><td style=\"text-align: right;\">863000</td><td style=\"text-align: right;\"> -3.1452</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            314.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 864000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-31-03\n",
      "  done: false\n",
      "  episode_len_mean: 317.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.155999999999976\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2984\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3104870176767186e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.524468794133928\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.029282041405867914\n",
      "          policy_loss: -0.07322262575229009\n",
      "          total_loss: 0.11300045458806886\n",
      "          vf_explained_var: 0.17259731888771057\n",
      "          vf_loss: 0.20146773163643147\n",
      "    num_agent_steps_sampled: 864000\n",
      "    num_agent_steps_trained: 864000\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "  iterations_since_restore: 864\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.10540540540542\n",
      "    ram_util_percent: 52.78648648648649\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919395675568416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.55831808931163\n",
      "    mean_inference_ms: 1.9443568080023437\n",
      "    mean_raw_obs_processing_ms: 2.1846348086292338\n",
      "  time_since_restore: 22980.650253534317\n",
      "  time_this_iter_s: 25.92520785331726\n",
      "  time_total_s: 22980.650253534317\n",
      "  timers:\n",
      "    learn_throughput: 1310.83\n",
      "    learn_time_ms: 762.875\n",
      "    load_throughput: 41105.555\n",
      "    load_time_ms: 24.328\n",
      "    sample_throughput: 33.415\n",
      "    sample_time_ms: 29926.9\n",
      "    update_time_ms: 7.19\n",
      "  timestamp: 1635085863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 864\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   864</td><td style=\"text-align: right;\">         22980.7</td><td style=\"text-align: right;\">864000</td><td style=\"text-align: right;\">  -3.156</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            317.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 865000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-31-33\n",
      "  done: false\n",
      "  episode_len_mean: 320.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.1777999999999764\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2987\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9657305265150787e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1178966290420955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01901800104753559\n",
      "          policy_loss: 0.008658425716890229\n",
      "          total_loss: 0.015143335941765044\n",
      "          vf_explained_var: 0.5019263029098511\n",
      "          vf_loss: 0.017663839945776597\n",
      "    num_agent_steps_sampled: 865000\n",
      "    num_agent_steps_trained: 865000\n",
      "    num_steps_sampled: 865000\n",
      "    num_steps_trained: 865000\n",
      "  iterations_since_restore: 865\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.80930232558138\n",
      "    ram_util_percent: 52.888372093023264\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039198564453365094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.564781513312667\n",
      "    mean_inference_ms: 1.9444869427474594\n",
      "    mean_raw_obs_processing_ms: 2.1842992561155876\n",
      "  time_since_restore: 23011.275011777878\n",
      "  time_this_iter_s: 30.62475824356079\n",
      "  time_total_s: 23011.275011777878\n",
      "  timers:\n",
      "    learn_throughput: 1314.623\n",
      "    learn_time_ms: 760.674\n",
      "    load_throughput: 41030.642\n",
      "    load_time_ms: 24.372\n",
      "    sample_throughput: 33.325\n",
      "    sample_time_ms: 30007.393\n",
      "    update_time_ms: 7.081\n",
      "  timestamp: 1635085893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 865000\n",
      "  training_iteration: 865\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   865</td><td style=\"text-align: right;\">         23011.3</td><td style=\"text-align: right;\">865000</td><td style=\"text-align: right;\"> -3.1778</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            320.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 866000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-32-05\n",
      "  done: false\n",
      "  episode_len_mean: 320.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.182699999999976\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2990\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9657305265150787e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9690940810574425\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011258081924870014\n",
      "          policy_loss: 0.05582780759367678\n",
      "          total_loss: 0.055695661819643444\n",
      "          vf_explained_var: 0.606826901435852\n",
      "          vf_loss: 0.009558777332616349\n",
      "    num_agent_steps_sampled: 866000\n",
      "    num_agent_steps_trained: 866000\n",
      "    num_steps_sampled: 866000\n",
      "    num_steps_trained: 866000\n",
      "  iterations_since_restore: 866\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.75777777777778\n",
      "    ram_util_percent: 52.73777777777777\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0392032228182888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.571254235512946\n",
      "    mean_inference_ms: 1.9446183801161718\n",
      "    mean_raw_obs_processing_ms: 2.1839667654029373\n",
      "  time_since_restore: 23042.833404541016\n",
      "  time_this_iter_s: 31.558392763137817\n",
      "  time_total_s: 23042.833404541016\n",
      "  timers:\n",
      "    learn_throughput: 1317.644\n",
      "    learn_time_ms: 758.931\n",
      "    load_throughput: 43531.956\n",
      "    load_time_ms: 22.972\n",
      "    sample_throughput: 33.401\n",
      "    sample_time_ms: 29939.515\n",
      "    update_time_ms: 7.038\n",
      "  timestamp: 1635085925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 866000\n",
      "  training_iteration: 866\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   866</td><td style=\"text-align: right;\">         23042.8</td><td style=\"text-align: right;\">866000</td><td style=\"text-align: right;\"> -3.1827</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            320.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 867000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-32-36\n",
      "  done: false\n",
      "  episode_len_mean: 321.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.187599999999975\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2993\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9657305265150787e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1529809402094946\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01026103127367427\n",
      "          policy_loss: 0.05967237088415358\n",
      "          total_loss: 0.056988892952601115\n",
      "          vf_explained_var: 0.36193233728408813\n",
      "          vf_loss: 0.008846312211567743\n",
      "    num_agent_steps_sampled: 867000\n",
      "    num_agent_steps_trained: 867000\n",
      "    num_steps_sampled: 867000\n",
      "    num_steps_trained: 867000\n",
      "  iterations_since_restore: 867\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.54222222222221\n",
      "    ram_util_percent: 52.84\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03920799668697759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.577758551978814\n",
      "    mean_inference_ms: 1.9447496517448055\n",
      "    mean_raw_obs_processing_ms: 2.183636801806052\n",
      "  time_since_restore: 23074.34119105339\n",
      "  time_this_iter_s: 31.507786512374878\n",
      "  time_total_s: 23074.34119105339\n",
      "  timers:\n",
      "    learn_throughput: 1312.104\n",
      "    learn_time_ms: 762.135\n",
      "    load_throughput: 46275.082\n",
      "    load_time_ms: 21.61\n",
      "    sample_throughput: 33.391\n",
      "    sample_time_ms: 29948.548\n",
      "    update_time_ms: 6.517\n",
      "  timestamp: 1635085956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 867000\n",
      "  training_iteration: 867\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   867</td><td style=\"text-align: right;\">         23074.3</td><td style=\"text-align: right;\">867000</td><td style=\"text-align: right;\"> -3.1876</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            321.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 868000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 321.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.1946999999999752\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2996\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9657305265150787e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0353620972898272\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013620674676598298\n",
      "          policy_loss: 0.04242155986527602\n",
      "          total_loss: 0.04084860256148709\n",
      "          vf_explained_var: -0.01787044107913971\n",
      "          vf_loss: 0.00878064059264337\n",
      "    num_agent_steps_sampled: 868000\n",
      "    num_agent_steps_trained: 868000\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 868000\n",
      "  iterations_since_restore: 868\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.27045454545454\n",
      "    ram_util_percent: 52.972727272727276\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921247365498299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.58417327848539\n",
      "    mean_inference_ms: 1.9448816570346932\n",
      "    mean_raw_obs_processing_ms: 2.1833094293186766\n",
      "  time_since_restore: 23104.860631465912\n",
      "  time_this_iter_s: 30.519440412521362\n",
      "  time_total_s: 23104.860631465912\n",
      "  timers:\n",
      "    learn_throughput: 1311.429\n",
      "    learn_time_ms: 762.527\n",
      "    load_throughput: 46255.944\n",
      "    load_time_ms: 21.619\n",
      "    sample_throughput: 33.475\n",
      "    sample_time_ms: 29872.815\n",
      "    update_time_ms: 6.896\n",
      "  timestamp: 1635085987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 868\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   868</td><td style=\"text-align: right;\">         23104.9</td><td style=\"text-align: right;\">868000</td><td style=\"text-align: right;\"> -3.1947</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            321.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 869000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-33-39\n",
      "  done: false\n",
      "  episode_len_mean: 322.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2009999999999748\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2999\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9657305265150787e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.146434775988261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013622028809224791\n",
      "          policy_loss: 0.07041409744156732\n",
      "          total_loss: 0.06696950511799918\n",
      "          vf_explained_var: 0.3899853527545929\n",
      "          vf_loss: 0.008019733043491012\n",
      "    num_agent_steps_sampled: 869000\n",
      "    num_agent_steps_trained: 869000\n",
      "    num_steps_sampled: 869000\n",
      "    num_steps_trained: 869000\n",
      "  iterations_since_restore: 869\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.27333333333334\n",
      "    ram_util_percent: 52.87111111111112\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921692864803034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.590582569496846\n",
      "    mean_inference_ms: 1.945011853843887\n",
      "    mean_raw_obs_processing_ms: 2.1829835202684875\n",
      "  time_since_restore: 23136.557770252228\n",
      "  time_this_iter_s: 31.697138786315918\n",
      "  time_total_s: 23136.557770252228\n",
      "  timers:\n",
      "    learn_throughput: 1306.148\n",
      "    learn_time_ms: 765.61\n",
      "    load_throughput: 48329.047\n",
      "    load_time_ms: 20.691\n",
      "    sample_throughput: 35.424\n",
      "    sample_time_ms: 28229.204\n",
      "    update_time_ms: 7.007\n",
      "  timestamp: 1635086019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 869000\n",
      "  training_iteration: 869\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   869</td><td style=\"text-align: right;\">         23136.6</td><td style=\"text-align: right;\">869000</td><td style=\"text-align: right;\">  -3.201</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            322.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 870000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-34-28\n",
      "  done: false\n",
      "  episode_len_mean: 322.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2060999999999753\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3002\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9657305265150787e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0708776142862109\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013642881041234053\n",
      "          policy_loss: 0.028248040792014865\n",
      "          total_loss: 0.02614377070632246\n",
      "          vf_explained_var: 0.46467721462249756\n",
      "          vf_loss: 0.008604483341332524\n",
      "    num_agent_steps_sampled: 870000\n",
      "    num_agent_steps_trained: 870000\n",
      "    num_steps_sampled: 870000\n",
      "    num_steps_trained: 870000\n",
      "  iterations_since_restore: 870\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.89014084507042\n",
      "    ram_util_percent: 52.8225352112676\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039221388308895114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.59706865647662\n",
      "    mean_inference_ms: 1.9451432308581809\n",
      "    mean_raw_obs_processing_ms: 2.183224693446586\n",
      "  time_since_restore: 23186.23963022232\n",
      "  time_this_iter_s: 49.68185997009277\n",
      "  time_total_s: 23186.23963022232\n",
      "  timers:\n",
      "    learn_throughput: 1321.568\n",
      "    learn_time_ms: 756.677\n",
      "    load_throughput: 49964.31\n",
      "    load_time_ms: 20.014\n",
      "    sample_throughput: 32.942\n",
      "    sample_time_ms: 30356.582\n",
      "    update_time_ms: 7.262\n",
      "  timestamp: 1635086068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 870000\n",
      "  training_iteration: 870\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   870</td><td style=\"text-align: right;\">         23186.2</td><td style=\"text-align: right;\">870000</td><td style=\"text-align: right;\"> -3.2061</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            322.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 871000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-34-58\n",
      "  done: false\n",
      "  episode_len_mean: 323.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2136999999999745\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3005\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9657305265150787e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8873287353250715\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005182872058682619\n",
      "          policy_loss: -0.07933379825618532\n",
      "          total_loss: -0.07592994645237923\n",
      "          vf_explained_var: 0.3748216927051544\n",
      "          vf_loss: 0.012277131558706363\n",
      "    num_agent_steps_sampled: 871000\n",
      "    num_agent_steps_trained: 871000\n",
      "    num_steps_sampled: 871000\n",
      "    num_steps_trained: 871000\n",
      "  iterations_since_restore: 871\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.52857142857144\n",
      "    ram_util_percent: 52.94523809523809\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039225762641246345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.603468040363495\n",
      "    mean_inference_ms: 1.945273313996514\n",
      "    mean_raw_obs_processing_ms: 2.1834673982156434\n",
      "  time_since_restore: 23215.85520672798\n",
      "  time_this_iter_s: 29.61557650566101\n",
      "  time_total_s: 23215.85520672798\n",
      "  timers:\n",
      "    learn_throughput: 1327.722\n",
      "    learn_time_ms: 753.169\n",
      "    load_throughput: 47785.539\n",
      "    load_time_ms: 20.927\n",
      "    sample_throughput: 32.556\n",
      "    sample_time_ms: 30715.86\n",
      "    update_time_ms: 7.701\n",
      "  timestamp: 1635086098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 871000\n",
      "  training_iteration: 871\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   871</td><td style=\"text-align: right;\">         23215.9</td><td style=\"text-align: right;\">871000</td><td style=\"text-align: right;\"> -3.2137</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            323.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 872000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-35-29\n",
      "  done: false\n",
      "  episode_len_mean: 324.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.224299999999975\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3008\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9657305265150787e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.055175683233473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021075242529164404\n",
      "          policy_loss: -0.1075401777608527\n",
      "          total_loss: -0.10626623556017875\n",
      "          vf_explained_var: 0.4830555021762848\n",
      "          vf_loss: 0.011825664527714252\n",
      "    num_agent_steps_sampled: 872000\n",
      "    num_agent_steps_trained: 872000\n",
      "    num_steps_sampled: 872000\n",
      "    num_steps_trained: 872000\n",
      "  iterations_since_restore: 872\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.25333333333334\n",
      "    ram_util_percent: 53.00666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03923018515217515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.609908097628317\n",
      "    mean_inference_ms: 1.9454039766672853\n",
      "    mean_raw_obs_processing_ms: 2.183474786032024\n",
      "  time_since_restore: 23246.945877552032\n",
      "  time_this_iter_s: 31.090670824050903\n",
      "  time_total_s: 23246.945877552032\n",
      "  timers:\n",
      "    learn_throughput: 1327.027\n",
      "    learn_time_ms: 753.564\n",
      "    load_throughput: 50700.729\n",
      "    load_time_ms: 19.724\n",
      "    sample_throughput: 32.089\n",
      "    sample_time_ms: 31163.745\n",
      "    update_time_ms: 7.559\n",
      "  timestamp: 1635086129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 872000\n",
      "  training_iteration: 872\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   872</td><td style=\"text-align: right;\">         23246.9</td><td style=\"text-align: right;\">872000</td><td style=\"text-align: right;\"> -3.2243</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            324.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 873000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-36-00\n",
      "  done: false\n",
      "  episode_len_mean: 325.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2342999999999744\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3011\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.948595789772618e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0848696066273584\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01045293851750052\n",
      "          policy_loss: -0.13823228602608045\n",
      "          total_loss: -0.1392910318242179\n",
      "          vf_explained_var: 0.6469506025314331\n",
      "          vf_loss: 0.009789920867317253\n",
      "    num_agent_steps_sampled: 873000\n",
      "    num_agent_steps_trained: 873000\n",
      "    num_steps_sampled: 873000\n",
      "    num_steps_trained: 873000\n",
      "  iterations_since_restore: 873\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.49555555555555\n",
      "    ram_util_percent: 53.124444444444435\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03923460113660235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.61628378190692\n",
      "    mean_inference_ms: 1.945534271579769\n",
      "    mean_raw_obs_processing_ms: 2.18305597542782\n",
      "  time_since_restore: 23278.26574778557\n",
      "  time_this_iter_s: 31.319870233535767\n",
      "  time_total_s: 23278.26574778557\n",
      "  timers:\n",
      "    learn_throughput: 1326.293\n",
      "    learn_time_ms: 753.981\n",
      "    load_throughput: 49872.284\n",
      "    load_time_ms: 20.051\n",
      "    sample_throughput: 31.678\n",
      "    sample_time_ms: 31567.624\n",
      "    update_time_ms: 7.501\n",
      "  timestamp: 1635086160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 873000\n",
      "  training_iteration: 873\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   873</td><td style=\"text-align: right;\">         23278.3</td><td style=\"text-align: right;\">873000</td><td style=\"text-align: right;\"> -3.2343</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            325.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 874000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-36-32\n",
      "  done: false\n",
      "  episode_len_mean: 326.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.244499999999974\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3015\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.948595789772618e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9354338182343377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006416346769192\n",
      "          policy_loss: 0.0015701259175936381\n",
      "          total_loss: -0.00043461041318045723\n",
      "          vf_explained_var: 0.7452201843261719\n",
      "          vf_loss: 0.007349584011050562\n",
      "    num_agent_steps_sampled: 874000\n",
      "    num_agent_steps_trained: 874000\n",
      "    num_steps_sampled: 874000\n",
      "    num_steps_trained: 874000\n",
      "  iterations_since_restore: 874\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.29545454545452\n",
      "    ram_util_percent: 53.17954545454545\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03924050750712237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.624732719233098\n",
      "    mean_inference_ms: 1.9457083395720451\n",
      "    mean_raw_obs_processing_ms: 2.1824949007916654\n",
      "  time_since_restore: 23309.272508621216\n",
      "  time_this_iter_s: 31.006760835647583\n",
      "  time_total_s: 23309.272508621216\n",
      "  timers:\n",
      "    learn_throughput: 1334.154\n",
      "    learn_time_ms: 749.539\n",
      "    load_throughput: 50376.644\n",
      "    load_time_ms: 19.85\n",
      "    sample_throughput: 31.171\n",
      "    sample_time_ms: 32080.658\n",
      "    update_time_ms: 7.123\n",
      "  timestamp: 1635086192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 874000\n",
      "  training_iteration: 874\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   874</td><td style=\"text-align: right;\">         23309.3</td><td style=\"text-align: right;\">874000</td><td style=\"text-align: right;\"> -3.2445</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">             326.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 875000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-37-02\n",
      "  done: false\n",
      "  episode_len_mean: 327.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.252399999999975\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3018\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.948595789772618e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9417108992735544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006920384694684984\n",
      "          policy_loss: 0.02139736173881425\n",
      "          total_loss: 0.01880589135819011\n",
      "          vf_explained_var: 0.6618660092353821\n",
      "          vf_loss: 0.006825622191859616\n",
      "    num_agent_steps_sampled: 875000\n",
      "    num_agent_steps_trained: 875000\n",
      "    num_steps_sampled: 875000\n",
      "    num_steps_trained: 875000\n",
      "  iterations_since_restore: 875\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.28604651162792\n",
      "    ram_util_percent: 53.237209302325574\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03924490343184035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.630980537436944\n",
      "    mean_inference_ms: 1.9458378442848285\n",
      "    mean_raw_obs_processing_ms: 2.1820827574402806\n",
      "  time_since_restore: 23339.813717603683\n",
      "  time_this_iter_s: 30.54120898246765\n",
      "  time_total_s: 23339.813717603683\n",
      "  timers:\n",
      "    learn_throughput: 1332.781\n",
      "    learn_time_ms: 750.311\n",
      "    load_throughput: 51632.053\n",
      "    load_time_ms: 19.368\n",
      "    sample_throughput: 31.18\n",
      "    sample_time_ms: 32071.641\n",
      "    update_time_ms: 7.514\n",
      "  timestamp: 1635086222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 875000\n",
      "  training_iteration: 875\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   875</td><td style=\"text-align: right;\">         23339.8</td><td style=\"text-align: right;\">875000</td><td style=\"text-align: right;\"> -3.2524</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            327.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 876000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-37-32\n",
      "  done: false\n",
      "  episode_len_mean: 328.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2607999999999744\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3021\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.948595789772618e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8284119745095571\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01008492037091639\n",
      "          policy_loss: 0.04387798665298356\n",
      "          total_loss: 0.043505418300628665\n",
      "          vf_explained_var: 0.6651968359947205\n",
      "          vf_loss: 0.007911526872259047\n",
      "    num_agent_steps_sampled: 876000\n",
      "    num_agent_steps_trained: 876000\n",
      "    num_steps_sampled: 876000\n",
      "    num_steps_trained: 876000\n",
      "  iterations_since_restore: 876\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.87209302325581\n",
      "    ram_util_percent: 53.19767441860464\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03924923368177677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.637111522696305\n",
      "    mean_inference_ms: 1.945965477662237\n",
      "    mean_raw_obs_processing_ms: 2.181672219611911\n",
      "  time_since_restore: 23369.55308198929\n",
      "  time_this_iter_s: 29.73936438560486\n",
      "  time_total_s: 23369.55308198929\n",
      "  timers:\n",
      "    learn_throughput: 1331.918\n",
      "    learn_time_ms: 750.797\n",
      "    load_throughput: 52535.776\n",
      "    load_time_ms: 19.035\n",
      "    sample_throughput: 31.358\n",
      "    sample_time_ms: 31889.496\n",
      "    update_time_ms: 7.477\n",
      "  timestamp: 1635086252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 876000\n",
      "  training_iteration: 876\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   876</td><td style=\"text-align: right;\">         23369.6</td><td style=\"text-align: right;\">876000</td><td style=\"text-align: right;\"> -3.2608</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            328.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 877000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-38-02\n",
      "  done: false\n",
      "  episode_len_mean: 329.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.271799999999973\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3024\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.948595789772618e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7882462726698981\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005835164802327518\n",
      "          policy_loss: 0.05405792188313272\n",
      "          total_loss: 0.05545240417122841\n",
      "          vf_explained_var: 0.5529985427856445\n",
      "          vf_loss: 0.009276931453496218\n",
      "    num_agent_steps_sampled: 877000\n",
      "    num_agent_steps_trained: 877000\n",
      "    num_steps_sampled: 877000\n",
      "    num_steps_trained: 877000\n",
      "  iterations_since_restore: 877\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.12558139534885\n",
      "    ram_util_percent: 53.16976744186046\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03925347360209312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.64325528311898\n",
      "    mean_inference_ms: 1.9460924225944456\n",
      "    mean_raw_obs_processing_ms: 2.1812402618481044\n",
      "  time_since_restore: 23400.11341547966\n",
      "  time_this_iter_s: 30.560333490371704\n",
      "  time_total_s: 23400.11341547966\n",
      "  timers:\n",
      "    learn_throughput: 1338.393\n",
      "    learn_time_ms: 747.165\n",
      "    load_throughput: 52390.031\n",
      "    load_time_ms: 19.088\n",
      "    sample_throughput: 31.448\n",
      "    sample_time_ms: 31798.971\n",
      "    update_time_ms: 7.123\n",
      "  timestamp: 1635086282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 877000\n",
      "  training_iteration: 877\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   877</td><td style=\"text-align: right;\">         23400.1</td><td style=\"text-align: right;\">877000</td><td style=\"text-align: right;\"> -3.2718</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            329.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 878000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-38-35\n",
      "  done: false\n",
      "  episode_len_mean: 329.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2761999999999727\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3027\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.948595789772618e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6039283957746294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005138617926264525\n",
      "          policy_loss: 0.00437603493531545\n",
      "          total_loss: 0.007845055477486717\n",
      "          vf_explained_var: 0.5329447388648987\n",
      "          vf_loss: 0.009508291511641195\n",
      "    num_agent_steps_sampled: 878000\n",
      "    num_agent_steps_trained: 878000\n",
      "    num_steps_sampled: 878000\n",
      "    num_steps_trained: 878000\n",
      "  iterations_since_restore: 878\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.06382978723403\n",
      "    ram_util_percent: 53.20212765957446\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03925774275404988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.649399097429313\n",
      "    mean_inference_ms: 1.9462202388281944\n",
      "    mean_raw_obs_processing_ms: 2.180811133725236\n",
      "  time_since_restore: 23433.020598888397\n",
      "  time_this_iter_s: 32.90718340873718\n",
      "  time_total_s: 23433.020598888397\n",
      "  timers:\n",
      "    learn_throughput: 1340.212\n",
      "    learn_time_ms: 746.151\n",
      "    load_throughput: 52212.185\n",
      "    load_time_ms: 19.153\n",
      "    sample_throughput: 31.212\n",
      "    sample_time_ms: 32038.521\n",
      "    update_time_ms: 7.404\n",
      "  timestamp: 1635086315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 878000\n",
      "  training_iteration: 878\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   878</td><td style=\"text-align: right;\">           23433</td><td style=\"text-align: right;\">878000</td><td style=\"text-align: right;\"> -3.2762</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            329.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 879000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-39-24\n",
      "  done: false\n",
      "  episode_len_mean: 330.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.278399999999973\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3030\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.948595789772618e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6750765217675103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032046447512127448\n",
      "          policy_loss: -0.11988410246041085\n",
      "          total_loss: -0.11377069643802112\n",
      "          vf_explained_var: 0.46603816747665405\n",
      "          vf_loss: 0.012864161572522587\n",
      "    num_agent_steps_sampled: 879000\n",
      "    num_agent_steps_trained: 879000\n",
      "    num_steps_sampled: 879000\n",
      "    num_steps_trained: 879000\n",
      "  iterations_since_restore: 879\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.36478873239437\n",
      "    ram_util_percent: 53.26338028169014\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03926203980954739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.655596945247993\n",
      "    mean_inference_ms: 1.9463490226683928\n",
      "    mean_raw_obs_processing_ms: 2.1809480607031304\n",
      "  time_since_restore: 23482.08330798149\n",
      "  time_this_iter_s: 49.06270909309387\n",
      "  time_total_s: 23482.08330798149\n",
      "  timers:\n",
      "    learn_throughput: 1344.576\n",
      "    learn_time_ms: 743.729\n",
      "    load_throughput: 49522.977\n",
      "    load_time_ms: 20.193\n",
      "    sample_throughput: 29.606\n",
      "    sample_time_ms: 33777.236\n",
      "    update_time_ms: 6.678\n",
      "  timestamp: 1635086364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 879000\n",
      "  training_iteration: 879\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   879</td><td style=\"text-align: right;\">         23482.1</td><td style=\"text-align: right;\">879000</td><td style=\"text-align: right;\"> -3.2784</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            330.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 880000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-39-58\n",
      "  done: false\n",
      "  episode_len_mean: 330.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.283899999999974\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3034\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.474297894886309e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6175986064804925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004174677942155351\n",
      "          policy_loss: 0.0026214684877130722\n",
      "          total_loss: 0.00931916952961021\n",
      "          vf_explained_var: 0.409909188747406\n",
      "          vf_loss: 0.012873685328910749\n",
      "    num_agent_steps_sampled: 880000\n",
      "    num_agent_steps_trained: 880000\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "  iterations_since_restore: 880\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.49787234042556\n",
      "    ram_util_percent: 53.197872340425526\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039267926390528805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.663885989401052\n",
      "    mean_inference_ms: 1.9465228268803287\n",
      "    mean_raw_obs_processing_ms: 2.1811335760237087\n",
      "  time_since_restore: 23515.439903259277\n",
      "  time_this_iter_s: 33.356595277786255\n",
      "  time_total_s: 23515.439903259277\n",
      "  timers:\n",
      "    learn_throughput: 1341.94\n",
      "    learn_time_ms: 745.19\n",
      "    load_throughput: 50170.02\n",
      "    load_time_ms: 19.932\n",
      "    sample_throughput: 31.11\n",
      "    sample_time_ms: 32143.598\n",
      "    update_time_ms: 6.597\n",
      "  timestamp: 1635086398\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 880\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   880</td><td style=\"text-align: right;\">         23515.4</td><td style=\"text-align: right;\">880000</td><td style=\"text-align: right;\"> -3.2839</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            330.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 881000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-40-30\n",
      "  done: false\n",
      "  episode_len_mean: 330.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2840999999999734\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3037\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.371489474431545e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5413886825243632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004141632285964134\n",
      "          policy_loss: 0.07721472150749631\n",
      "          total_loss: 0.08085449751880434\n",
      "          vf_explained_var: 0.5026817321777344\n",
      "          vf_loss: 0.009053658954669825\n",
      "    num_agent_steps_sampled: 881000\n",
      "    num_agent_steps_trained: 881000\n",
      "    num_steps_sampled: 881000\n",
      "    num_steps_trained: 881000\n",
      "  iterations_since_restore: 881\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.15652173913043\n",
      "    ram_util_percent: 53.56956521739131\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039272287598711354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.670072867680204\n",
      "    mean_inference_ms: 1.946652334155193\n",
      "    mean_raw_obs_processing_ms: 2.1812743703167543\n",
      "  time_since_restore: 23547.6648042202\n",
      "  time_this_iter_s: 32.22490096092224\n",
      "  time_total_s: 23547.6648042202\n",
      "  timers:\n",
      "    learn_throughput: 1342.346\n",
      "    learn_time_ms: 744.964\n",
      "    load_throughput: 52359.881\n",
      "    load_time_ms: 19.099\n",
      "    sample_throughput: 30.858\n",
      "    sample_time_ms: 32406.255\n",
      "    update_time_ms: 5.896\n",
      "  timestamp: 1635086430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 881000\n",
      "  training_iteration: 881\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   881</td><td style=\"text-align: right;\">         23547.7</td><td style=\"text-align: right;\">881000</td><td style=\"text-align: right;\"> -3.2841</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            330.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 882000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-41-04\n",
      "  done: false\n",
      "  episode_len_mean: 330.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2831999999999737\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3040\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.6857447372157725e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5218933237923516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002986691899040592\n",
      "          policy_loss: -0.09753498054212995\n",
      "          total_loss: -0.08955834325816896\n",
      "          vf_explained_var: 0.28203240036964417\n",
      "          vf_loss: 0.013195568602532149\n",
      "    num_agent_steps_sampled: 882000\n",
      "    num_agent_steps_trained: 882000\n",
      "    num_steps_sampled: 882000\n",
      "    num_steps_trained: 882000\n",
      "  iterations_since_restore: 882\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.75208333333335\n",
      "    ram_util_percent: 53.541666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03927678915681547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.676299693780603\n",
      "    mean_inference_ms: 1.9467813105542318\n",
      "    mean_raw_obs_processing_ms: 2.1808208816746193\n",
      "  time_since_restore: 23581.068734169006\n",
      "  time_this_iter_s: 33.40392994880676\n",
      "  time_total_s: 23581.068734169006\n",
      "  timers:\n",
      "    learn_throughput: 1343.166\n",
      "    learn_time_ms: 744.509\n",
      "    load_throughput: 54677.48\n",
      "    load_time_ms: 18.289\n",
      "    sample_throughput: 30.638\n",
      "    sample_time_ms: 32638.979\n",
      "    update_time_ms: 5.778\n",
      "  timestamp: 1635086464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 882000\n",
      "  training_iteration: 882\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   882</td><td style=\"text-align: right;\">         23581.1</td><td style=\"text-align: right;\">882000</td><td style=\"text-align: right;\"> -3.2832</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            330.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 883000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-41-35\n",
      "  done: false\n",
      "  episode_len_mean: 329.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.269899999999973\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3044\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8428723686078863e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.55816860265202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005910972998613791\n",
      "          policy_loss: 0.0095374776257409\n",
      "          total_loss: 0.017115946859121323\n",
      "          vf_explained_var: 0.3397994041442871\n",
      "          vf_loss: 0.013160156251655685\n",
      "    num_agent_steps_sampled: 883000\n",
      "    num_agent_steps_trained: 883000\n",
      "    num_steps_sampled: 883000\n",
      "    num_steps_trained: 883000\n",
      "  iterations_since_restore: 883\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.90666666666667\n",
      "    ram_util_percent: 53.39777777777779\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039282712255971776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.68463677508581\n",
      "    mean_inference_ms: 1.946950188493435\n",
      "    mean_raw_obs_processing_ms: 2.1802431813834\n",
      "  time_since_restore: 23612.692354917526\n",
      "  time_this_iter_s: 31.623620748519897\n",
      "  time_total_s: 23612.692354917526\n",
      "  timers:\n",
      "    learn_throughput: 1346.479\n",
      "    learn_time_ms: 742.678\n",
      "    load_throughput: 55952.776\n",
      "    load_time_ms: 17.872\n",
      "    sample_throughput: 30.607\n",
      "    sample_time_ms: 32672.522\n",
      "    update_time_ms: 5.084\n",
      "  timestamp: 1635086495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 883000\n",
      "  training_iteration: 883\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   883</td><td style=\"text-align: right;\">         23612.7</td><td style=\"text-align: right;\">883000</td><td style=\"text-align: right;\"> -3.2699</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            329.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 884000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-42-08\n",
      "  done: false\n",
      "  episode_len_mean: 328.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.263099999999974\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3047\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8428723686078863e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5161735259824329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00446121526508168\n",
      "          policy_loss: 0.05420710345109304\n",
      "          total_loss: 0.057743267218271895\n",
      "          vf_explained_var: 0.4755716025829315\n",
      "          vf_loss: 0.008697898145894417\n",
      "    num_agent_steps_sampled: 884000\n",
      "    num_agent_steps_trained: 884000\n",
      "    num_steps_sampled: 884000\n",
      "    num_steps_trained: 884000\n",
      "  iterations_since_restore: 884\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.97446808510638\n",
      "    ram_util_percent: 53.39148936170214\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03928713901228911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.69101784174145\n",
      "    mean_inference_ms: 1.9470761352966295\n",
      "    mean_raw_obs_processing_ms: 2.1797942036122113\n",
      "  time_since_restore: 23645.34090423584\n",
      "  time_this_iter_s: 32.6485493183136\n",
      "  time_total_s: 23645.34090423584\n",
      "  timers:\n",
      "    learn_throughput: 1351.314\n",
      "    learn_time_ms: 740.021\n",
      "    load_throughput: 55808.864\n",
      "    load_time_ms: 17.918\n",
      "    sample_throughput: 30.451\n",
      "    sample_time_ms: 32839.288\n",
      "    update_time_ms: 5.216\n",
      "  timestamp: 1635086528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 884000\n",
      "  training_iteration: 884\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   884</td><td style=\"text-align: right;\">         23645.3</td><td style=\"text-align: right;\">884000</td><td style=\"text-align: right;\"> -3.2631</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            328.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 885000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 327.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2493999999999734\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3050\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.214361843039431e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5417231996854146\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005460502389990059\n",
      "          policy_loss: -0.09560434164272415\n",
      "          total_loss: -0.08806709655457073\n",
      "          vf_explained_var: 0.35335803031921387\n",
      "          vf_loss: 0.012954478731585874\n",
      "    num_agent_steps_sampled: 885000\n",
      "    num_agent_steps_trained: 885000\n",
      "    num_steps_sampled: 885000\n",
      "    num_steps_trained: 885000\n",
      "  iterations_since_restore: 885\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.25918367346938\n",
      "    ram_util_percent: 53.40816326530612\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03929150844528701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.69754955105218\n",
      "    mean_inference_ms: 1.9471997720759306\n",
      "    mean_raw_obs_processing_ms: 2.1793712579971625\n",
      "  time_since_restore: 23680.222488880157\n",
      "  time_this_iter_s: 34.88158464431763\n",
      "  time_total_s: 23680.222488880157\n",
      "  timers:\n",
      "    learn_throughput: 1351.78\n",
      "    learn_time_ms: 739.765\n",
      "    load_throughput: 57865.638\n",
      "    load_time_ms: 17.281\n",
      "    sample_throughput: 30.053\n",
      "    sample_time_ms: 33274.367\n",
      "    update_time_ms: 5.157\n",
      "  timestamp: 1635086563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 885000\n",
      "  training_iteration: 885\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   885</td><td style=\"text-align: right;\">         23680.2</td><td style=\"text-align: right;\">885000</td><td style=\"text-align: right;\"> -3.2494</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            327.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 886000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-43-15\n",
      "  done: false\n",
      "  episode_len_mean: 325.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2344999999999744\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3054\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.214361843039431e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7142310976982117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009703062591672622\n",
      "          policy_loss: 0.03513684769471486\n",
      "          total_loss: 0.039169737613863415\n",
      "          vf_explained_var: 0.4428485929965973\n",
      "          vf_loss: 0.011175200301739905\n",
      "    num_agent_steps_sampled: 886000\n",
      "    num_agent_steps_trained: 886000\n",
      "    num_steps_sampled: 886000\n",
      "    num_steps_trained: 886000\n",
      "  iterations_since_restore: 886\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1413043478261\n",
      "    ram_util_percent: 53.28478260869565\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039297306504375615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.706393425118144\n",
      "    mean_inference_ms: 1.9473645820300518\n",
      "    mean_raw_obs_processing_ms: 2.1788255421375022\n",
      "  time_since_restore: 23712.367270946503\n",
      "  time_this_iter_s: 32.144782066345215\n",
      "  time_total_s: 23712.367270946503\n",
      "  timers:\n",
      "    learn_throughput: 1353.291\n",
      "    learn_time_ms: 738.939\n",
      "    load_throughput: 57734.69\n",
      "    load_time_ms: 17.321\n",
      "    sample_throughput: 29.837\n",
      "    sample_time_ms: 33515.887\n",
      "    update_time_ms: 5.059\n",
      "  timestamp: 1635086595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 886000\n",
      "  training_iteration: 886\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   886</td><td style=\"text-align: right;\">         23712.4</td><td style=\"text-align: right;\">886000</td><td style=\"text-align: right;\"> -3.2345</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">             325.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 887000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-43-46\n",
      "  done: false\n",
      "  episode_len_mean: 325.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2278999999999747\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3057\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.214361843039431e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.660866622461213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06762405489971263\n",
      "          policy_loss: 0.11537427571084764\n",
      "          total_loss: 0.11432066849536365\n",
      "          vf_explained_var: 0.7117177248001099\n",
      "          vf_loss: 0.005555052059288654\n",
      "    num_agent_steps_sampled: 887000\n",
      "    num_agent_steps_trained: 887000\n",
      "    num_steps_sampled: 887000\n",
      "    num_steps_trained: 887000\n",
      "  iterations_since_restore: 887\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.09772727272728\n",
      "    ram_util_percent: 53.275000000000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03930163999212002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.713120403208478\n",
      "    mean_inference_ms: 1.9474892102247296\n",
      "    mean_raw_obs_processing_ms: 2.178405876597675\n",
      "  time_since_restore: 23743.260108709335\n",
      "  time_this_iter_s: 30.89283776283264\n",
      "  time_total_s: 23743.260108709335\n",
      "  timers:\n",
      "    learn_throughput: 1353.796\n",
      "    learn_time_ms: 738.664\n",
      "    load_throughput: 57097.522\n",
      "    load_time_ms: 17.514\n",
      "    sample_throughput: 29.807\n",
      "    sample_time_ms: 33549.362\n",
      "    update_time_ms: 4.94\n",
      "  timestamp: 1635086626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 887000\n",
      "  training_iteration: 887\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   887</td><td style=\"text-align: right;\">         23743.3</td><td style=\"text-align: right;\">887000</td><td style=\"text-align: right;\"> -3.2279</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            325.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 888000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-44-33\n",
      "  done: false\n",
      "  episode_len_mean: 325.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2276999999999743\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3060\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3821542764559145e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9804315712716845\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019251083354031827\n",
      "          policy_loss: -0.09943819642066956\n",
      "          total_loss: -0.10076255152622858\n",
      "          vf_explained_var: 0.6818151473999023\n",
      "          vf_loss: 0.00847995796551307\n",
      "    num_agent_steps_sampled: 888000\n",
      "    num_agent_steps_trained: 888000\n",
      "    num_steps_sampled: 888000\n",
      "    num_steps_trained: 888000\n",
      "  iterations_since_restore: 888\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.09705882352942\n",
      "    ram_util_percent: 53.34264705882354\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039306038522713166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.719788528134508\n",
      "    mean_inference_ms: 1.9476138590387069\n",
      "    mean_raw_obs_processing_ms: 2.178594526887927\n",
      "  time_since_restore: 23790.547701120377\n",
      "  time_this_iter_s: 47.28759241104126\n",
      "  time_total_s: 23790.547701120377\n",
      "  timers:\n",
      "    learn_throughput: 1348.695\n",
      "    learn_time_ms: 741.458\n",
      "    load_throughput: 54213.298\n",
      "    load_time_ms: 18.446\n",
      "    sample_throughput: 28.584\n",
      "    sample_time_ms: 34984.347\n",
      "    update_time_ms: 4.187\n",
      "  timestamp: 1635086673\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 888000\n",
      "  training_iteration: 888\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   888</td><td style=\"text-align: right;\">         23790.5</td><td style=\"text-align: right;\">888000</td><td style=\"text-align: right;\"> -3.2277</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            325.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 889000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-45-02\n",
      "  done: false\n",
      "  episode_len_mean: 325.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2368999999999746\n",
      "  episode_reward_min: -4.749999999999943\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3063\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3821542764559145e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0169642673598396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025242778553490552\n",
      "          policy_loss: -0.07823956981301308\n",
      "          total_loss: -0.0789436012506485\n",
      "          vf_explained_var: 0.5659016966819763\n",
      "          vf_loss: 0.009465611037901707\n",
      "    num_agent_steps_sampled: 889000\n",
      "    num_agent_steps_trained: 889000\n",
      "    num_steps_sampled: 889000\n",
      "    num_steps_trained: 889000\n",
      "  iterations_since_restore: 889\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17619047619047\n",
      "    ram_util_percent: 53.35476190476191\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03931035235855632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.726349578718608\n",
      "    mean_inference_ms: 1.947738210231012\n",
      "    mean_raw_obs_processing_ms: 2.178786059408963\n",
      "  time_since_restore: 23819.854771137238\n",
      "  time_this_iter_s: 29.307070016860962\n",
      "  time_total_s: 23819.854771137238\n",
      "  timers:\n",
      "    learn_throughput: 1349.533\n",
      "    learn_time_ms: 740.997\n",
      "    load_throughput: 54587.745\n",
      "    load_time_ms: 18.319\n",
      "    sample_throughput: 30.295\n",
      "    sample_time_ms: 33009.027\n",
      "    update_time_ms: 4.568\n",
      "  timestamp: 1635086702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 889000\n",
      "  training_iteration: 889\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   889</td><td style=\"text-align: right;\">         23819.9</td><td style=\"text-align: right;\">889000</td><td style=\"text-align: right;\"> -3.2369</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -4.75</td><td style=\"text-align: right;\">            325.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 890000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-45-28\n",
      "  done: false\n",
      "  episode_len_mean: 327.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.284199999999974\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3066\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0732314146838724e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2117137167188856\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021923642023888353\n",
      "          policy_loss: 0.05968795066906346\n",
      "          total_loss: 0.09074555283619298\n",
      "          vf_explained_var: -0.203145831823349\n",
      "          vf_loss: 0.043174737792772554\n",
      "    num_agent_steps_sampled: 890000\n",
      "    num_agent_steps_trained: 890000\n",
      "    num_steps_sampled: 890000\n",
      "    num_steps_trained: 890000\n",
      "  iterations_since_restore: 890\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96111111111111\n",
      "    ram_util_percent: 53.35833333333333\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03931464276180864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.732700872876844\n",
      "    mean_inference_ms: 1.9478617753921321\n",
      "    mean_raw_obs_processing_ms: 2.1789793318945256\n",
      "  time_since_restore: 23845.437034130096\n",
      "  time_this_iter_s: 25.582262992858887\n",
      "  time_total_s: 23845.437034130096\n",
      "  timers:\n",
      "    learn_throughput: 1349.163\n",
      "    learn_time_ms: 741.2\n",
      "    load_throughput: 54051.702\n",
      "    load_time_ms: 18.501\n",
      "    sample_throughput: 31.026\n",
      "    sample_time_ms: 32231.414\n",
      "    update_time_ms: 4.314\n",
      "  timestamp: 1635086728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 890000\n",
      "  training_iteration: 890\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   890</td><td style=\"text-align: right;\">         23845.4</td><td style=\"text-align: right;\">890000</td><td style=\"text-align: right;\"> -3.2842</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">             327.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 891000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-45-55\n",
      "  done: false\n",
      "  episode_len_mean: 329.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2991999999999746\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3069\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.109847122025808e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1367357068591648\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.11179304758125694\n",
      "          policy_loss: 0.010766646928257413\n",
      "          total_loss: 0.009913569026523166\n",
      "          vf_explained_var: 0.4826815724372864\n",
      "          vf_loss: 0.01051424883901038\n",
      "    num_agent_steps_sampled: 891000\n",
      "    num_agent_steps_trained: 891000\n",
      "    num_steps_sampled: 891000\n",
      "    num_steps_trained: 891000\n",
      "  iterations_since_restore: 891\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.04102564102564\n",
      "    ram_util_percent: 53.40000000000001\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03931897868673575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.738973167944582\n",
      "    mean_inference_ms: 1.9479862979205145\n",
      "    mean_raw_obs_processing_ms: 2.179151501495307\n",
      "  time_since_restore: 23872.53457713127\n",
      "  time_this_iter_s: 27.097543001174927\n",
      "  time_total_s: 23872.53457713127\n",
      "  timers:\n",
      "    learn_throughput: 1351.868\n",
      "    learn_time_ms: 739.717\n",
      "    load_throughput: 54357.26\n",
      "    load_time_ms: 18.397\n",
      "    sample_throughput: 31.525\n",
      "    sample_time_ms: 31720.677\n",
      "    update_time_ms: 3.94\n",
      "  timestamp: 1635086755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 891000\n",
      "  training_iteration: 891\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   891</td><td style=\"text-align: right;\">         23872.5</td><td style=\"text-align: right;\">891000</td><td style=\"text-align: right;\"> -3.2992</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">             329.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 892000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-46-23\n",
      "  done: false\n",
      "  episode_len_mean: 330.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.307799999999973\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3072\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6647706830387125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0290263540214963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01858621753685507\n",
      "          policy_loss: 0.09438242779837715\n",
      "          total_loss: 0.09427644792530272\n",
      "          vf_explained_var: 0.21951985359191895\n",
      "          vf_loss: 0.01018428162464665\n",
      "    num_agent_steps_sampled: 892000\n",
      "    num_agent_steps_trained: 892000\n",
      "    num_steps_sampled: 892000\n",
      "    num_steps_trained: 892000\n",
      "  iterations_since_restore: 892\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.87249999999999\n",
      "    ram_util_percent: 53.477500000000006\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03932338599876862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.745115203201305\n",
      "    mean_inference_ms: 1.9481094799689982\n",
      "    mean_raw_obs_processing_ms: 2.1787381188673423\n",
      "  time_since_restore: 23900.31592774391\n",
      "  time_this_iter_s: 27.78135061264038\n",
      "  time_total_s: 23900.31592774391\n",
      "  timers:\n",
      "    learn_throughput: 1355.321\n",
      "    learn_time_ms: 737.833\n",
      "    load_throughput: 54324.804\n",
      "    load_time_ms: 18.408\n",
      "    sample_throughput: 32.092\n",
      "    sample_time_ms: 31159.98\n",
      "    update_time_ms: 4.385\n",
      "  timestamp: 1635086783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 892000\n",
      "  training_iteration: 892\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   892</td><td style=\"text-align: right;\">         23900.3</td><td style=\"text-align: right;\">892000</td><td style=\"text-align: right;\"> -3.3078</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            330.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 893000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 329.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2974999999999732\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3075\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6647706830387125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6630650066667133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008623232514296102\n",
      "          policy_loss: 0.07263577878475189\n",
      "          total_loss: 0.07587586442629496\n",
      "          vf_explained_var: 0.47677260637283325\n",
      "          vf_loss: 0.009870733061365575\n",
      "    num_agent_steps_sampled: 893000\n",
      "    num_agent_steps_trained: 893000\n",
      "    num_steps_sampled: 893000\n",
      "    num_steps_trained: 893000\n",
      "  iterations_since_restore: 893\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.15\n",
      "    ram_util_percent: 53.46190476190476\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03932776044607138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.751281432721996\n",
      "    mean_inference_ms: 1.9482318530853966\n",
      "    mean_raw_obs_processing_ms: 2.1783275535158624\n",
      "  time_since_restore: 23930.027314901352\n",
      "  time_this_iter_s: 29.711387157440186\n",
      "  time_total_s: 23930.027314901352\n",
      "  timers:\n",
      "    learn_throughput: 1309.6\n",
      "    learn_time_ms: 763.592\n",
      "    load_throughput: 51722.401\n",
      "    load_time_ms: 19.334\n",
      "    sample_throughput: 32.319\n",
      "    sample_time_ms: 30941.818\n",
      "    update_time_ms: 4.589\n",
      "  timestamp: 1635086813\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 893000\n",
      "  training_iteration: 893\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   893</td><td style=\"text-align: right;\">           23930</td><td style=\"text-align: right;\">893000</td><td style=\"text-align: right;\"> -3.2975</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            329.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 894000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-47-23\n",
      "  done: false\n",
      "  episode_len_mean: 325.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.266099999999974\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3078\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6647706830387125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7843548029661178\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01936237569564838\n",
      "          policy_loss: 0.10926644305388132\n",
      "          total_loss: 0.10721593780650032\n",
      "          vf_explained_var: 0.3998672068119049\n",
      "          vf_loss: 0.005793038117311274\n",
      "    num_agent_steps_sampled: 894000\n",
      "    num_agent_steps_trained: 894000\n",
      "    num_steps_sampled: 894000\n",
      "    num_steps_trained: 894000\n",
      "  iterations_since_restore: 894\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.31136363636364\n",
      "    ram_util_percent: 53.44772727272728\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039331966223804395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.75757707868583\n",
      "    mean_inference_ms: 1.9483508639126845\n",
      "    mean_raw_obs_processing_ms: 2.177942469325467\n",
      "  time_since_restore: 23960.711641073227\n",
      "  time_this_iter_s: 30.684326171875\n",
      "  time_total_s: 23960.711641073227\n",
      "  timers:\n",
      "    learn_throughput: 1304.892\n",
      "    learn_time_ms: 766.347\n",
      "    load_throughput: 50137.456\n",
      "    load_time_ms: 19.945\n",
      "    sample_throughput: 32.529\n",
      "    sample_time_ms: 30741.972\n",
      "    update_time_ms: 4.468\n",
      "  timestamp: 1635086843\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 894000\n",
      "  training_iteration: 894\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   894</td><td style=\"text-align: right;\">         23960.7</td><td style=\"text-align: right;\">894000</td><td style=\"text-align: right;\"> -3.2661</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            325.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 895000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-47-55\n",
      "  done: false\n",
      "  episode_len_mean: 323.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999674\n",
      "  episode_reward_mean: -3.2399999999999745\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3081\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6647706830387125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6088803297943539\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010313113507604754\n",
      "          policy_loss: -0.05399209823873308\n",
      "          total_loss: -0.04699381697509024\n",
      "          vf_explained_var: 0.16597653925418854\n",
      "          vf_loss: 0.013087080031012496\n",
      "    num_agent_steps_sampled: 895000\n",
      "    num_agent_steps_trained: 895000\n",
      "    num_steps_sampled: 895000\n",
      "    num_steps_trained: 895000\n",
      "  iterations_since_restore: 895\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.50454545454544\n",
      "    ram_util_percent: 53.520454545454534\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03933602526066686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.763987303820755\n",
      "    mean_inference_ms: 1.948467468339664\n",
      "    mean_raw_obs_processing_ms: 2.177582052523787\n",
      "  time_since_restore: 23992.001089811325\n",
      "  time_this_iter_s: 31.289448738098145\n",
      "  time_total_s: 23992.001089811325\n",
      "  timers:\n",
      "    learn_throughput: 1310.25\n",
      "    learn_time_ms: 763.213\n",
      "    load_throughput: 47855.982\n",
      "    load_time_ms: 20.896\n",
      "    sample_throughput: 32.911\n",
      "    sample_time_ms: 30384.997\n",
      "    update_time_ms: 4.437\n",
      "  timestamp: 1635086875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 895000\n",
      "  training_iteration: 895\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   895</td><td style=\"text-align: right;\">           23992</td><td style=\"text-align: right;\">895000</td><td style=\"text-align: right;\">   -3.24</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            323.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 896000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-48-21\n",
      "  done: false\n",
      "  episode_len_mean: 319.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.228699999999975\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3084\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6647706830387125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1037249777052138\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01520721380911958\n",
      "          policy_loss: -0.10071531708041827\n",
      "          total_loss: -0.09711545391215218\n",
      "          vf_explained_var: 0.2990367114543915\n",
      "          vf_loss: 0.014637103951018717\n",
      "    num_agent_steps_sampled: 896000\n",
      "    num_agent_steps_trained: 896000\n",
      "    num_steps_sampled: 896000\n",
      "    num_steps_trained: 896000\n",
      "  iterations_since_restore: 896\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.41578947368423\n",
      "    ram_util_percent: 53.40526315789473\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03933997211389228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.770387749526417\n",
      "    mean_inference_ms: 1.948580094152652\n",
      "    mean_raw_obs_processing_ms: 2.1772236165167618\n",
      "  time_since_restore: 24018.11736035347\n",
      "  time_this_iter_s: 26.116270542144775\n",
      "  time_total_s: 24018.11736035347\n",
      "  timers:\n",
      "    learn_throughput: 1308.266\n",
      "    learn_time_ms: 764.371\n",
      "    load_throughput: 45475.337\n",
      "    load_time_ms: 21.99\n",
      "    sample_throughput: 33.58\n",
      "    sample_time_ms: 29779.628\n",
      "    update_time_ms: 4.483\n",
      "  timestamp: 1635086901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 896000\n",
      "  training_iteration: 896\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   896</td><td style=\"text-align: right;\">         24018.1</td><td style=\"text-align: right;\">896000</td><td style=\"text-align: right;\"> -3.2287</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">             319.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 897000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-48-48\n",
      "  done: false\n",
      "  episode_len_mean: 319.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.2259999999999747\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3087\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6647706830387125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1216315527757008\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01074902719328258\n",
      "          policy_loss: 0.056165639145506756\n",
      "          total_loss: 0.05592077399293582\n",
      "          vf_explained_var: 0.09068027883768082\n",
      "          vf_loss: 0.010971450567012653\n",
      "    num_agent_steps_sampled: 897000\n",
      "    num_agent_steps_trained: 897000\n",
      "    num_steps_sampled: 897000\n",
      "    num_steps_trained: 897000\n",
      "  iterations_since_restore: 897\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.74736842105263\n",
      "    ram_util_percent: 53.36578947368421\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03934387355926944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.776641520184562\n",
      "    mean_inference_ms: 1.948687109131145\n",
      "    mean_raw_obs_processing_ms: 2.1768890315195186\n",
      "  time_since_restore: 24044.94524550438\n",
      "  time_this_iter_s: 26.827885150909424\n",
      "  time_total_s: 24044.94524550438\n",
      "  timers:\n",
      "    learn_throughput: 1307.345\n",
      "    learn_time_ms: 764.909\n",
      "    load_throughput: 43975.592\n",
      "    load_time_ms: 22.74\n",
      "    sample_throughput: 34.047\n",
      "    sample_time_ms: 29371.501\n",
      "    update_time_ms: 4.626\n",
      "  timestamp: 1635086928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 897000\n",
      "  training_iteration: 897\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   897</td><td style=\"text-align: right;\">         24044.9</td><td style=\"text-align: right;\">897000</td><td style=\"text-align: right;\">  -3.226</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            319.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 898000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-49-31\n",
      "  done: false\n",
      "  episode_len_mean: 320.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.2312999999999756\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3090\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6647706830387125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3297556228107876\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013909675926491439\n",
      "          policy_loss: 0.005862179398536682\n",
      "          total_loss: 0.0027122407737705445\n",
      "          vf_explained_var: 0.1205274909734726\n",
      "          vf_loss: 0.010147611633712788\n",
      "    num_agent_steps_sampled: 898000\n",
      "    num_agent_steps_trained: 898000\n",
      "    num_steps_sampled: 898000\n",
      "    num_steps_trained: 898000\n",
      "  iterations_since_restore: 898\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.28709677419356\n",
      "    ram_util_percent: 53.36290322580645\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03934762142899647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.7826889396437\n",
      "    mean_inference_ms: 1.948792065327319\n",
      "    mean_raw_obs_processing_ms: 2.177134085782698\n",
      "  time_since_restore: 24088.171514749527\n",
      "  time_this_iter_s: 43.226269245147705\n",
      "  time_total_s: 24088.171514749527\n",
      "  timers:\n",
      "    learn_throughput: 1280.936\n",
      "    learn_time_ms: 780.679\n",
      "    load_throughput: 46174.57\n",
      "    load_time_ms: 21.657\n",
      "    sample_throughput: 34.542\n",
      "    sample_time_ms: 28950.452\n",
      "    update_time_ms: 4.732\n",
      "  timestamp: 1635086971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 898000\n",
      "  training_iteration: 898\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   898</td><td style=\"text-align: right;\">         24088.2</td><td style=\"text-align: right;\">898000</td><td style=\"text-align: right;\"> -3.2313</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            320.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 899000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-50-00\n",
      "  done: false\n",
      "  episode_len_mean: 320.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.238699999999976\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 3092\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6647706830387125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2164143708017137\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012357065131848958\n",
      "          policy_loss: -0.08996768825583988\n",
      "          total_loss: -0.08864288752277692\n",
      "          vf_explained_var: 0.07108862698078156\n",
      "          vf_loss: 0.013488942560636335\n",
      "    num_agent_steps_sampled: 899000\n",
      "    num_agent_steps_trained: 899000\n",
      "    num_steps_sampled: 899000\n",
      "    num_steps_trained: 899000\n",
      "  iterations_since_restore: 899\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.76829268292683\n",
      "    ram_util_percent: 53.321951219512194\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039350059549700896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.786656941776425\n",
      "    mean_inference_ms: 1.9488627652104873\n",
      "    mean_raw_obs_processing_ms: 2.177297938232579\n",
      "  time_since_restore: 24117.240129947662\n",
      "  time_this_iter_s: 29.068615198135376\n",
      "  time_total_s: 24117.240129947662\n",
      "  timers:\n",
      "    learn_throughput: 1279.55\n",
      "    learn_time_ms: 781.525\n",
      "    load_throughput: 46015.504\n",
      "    load_time_ms: 21.732\n",
      "    sample_throughput: 34.571\n",
      "    sample_time_ms: 28925.749\n",
      "    update_time_ms: 4.71\n",
      "  timestamp: 1635087000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 899000\n",
      "  training_iteration: 899\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   899</td><td style=\"text-align: right;\">         24117.2</td><td style=\"text-align: right;\">899000</td><td style=\"text-align: right;\"> -3.2387</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">             320.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 900000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-50-29\n",
      "  done: false\n",
      "  episode_len_mean: 321.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.2457999999999743\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3095\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6647706830387125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9200557854440478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02572379205942323\n",
      "          policy_loss: -0.10970156374904845\n",
      "          total_loss: -0.10289681947065724\n",
      "          vf_explained_var: 0.22168293595314026\n",
      "          vf_loss: 0.01600529464582602\n",
      "    num_agent_steps_sampled: 900000\n",
      "    num_agent_steps_trained: 900000\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "  iterations_since_restore: 900\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.92682926829268\n",
      "    ram_util_percent: 53.72926829268293\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03935373559545255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.792597923574803\n",
      "    mean_inference_ms: 1.9489697795774168\n",
      "    mean_raw_obs_processing_ms: 2.177523136088906\n",
      "  time_since_restore: 24145.87840795517\n",
      "  time_this_iter_s: 28.638278007507324\n",
      "  time_total_s: 24145.87840795517\n",
      "  timers:\n",
      "    learn_throughput: 1280.987\n",
      "    learn_time_ms: 780.648\n",
      "    load_throughput: 44404.233\n",
      "    load_time_ms: 22.52\n",
      "    sample_throughput: 34.21\n",
      "    sample_time_ms: 29231.435\n",
      "    update_time_ms: 4.598\n",
      "  timestamp: 1635087029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 900\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   900</td><td style=\"text-align: right;\">         24145.9</td><td style=\"text-align: right;\">900000</td><td style=\"text-align: right;\"> -3.2458</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 901000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 320.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.238799999999975\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3099\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.997156024558069e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.295022147231632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013253775914268375\n",
      "          policy_loss: -0.00043337717652320863\n",
      "          total_loss: -0.0005058219863308801\n",
      "          vf_explained_var: 0.4289683401584625\n",
      "          vf_loss: 0.012877772479421562\n",
      "    num_agent_steps_sampled: 901000\n",
      "    num_agent_steps_trained: 901000\n",
      "    num_steps_sampled: 901000\n",
      "    num_steps_trained: 901000\n",
      "  iterations_since_restore: 901\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.68863636363636\n",
      "    ram_util_percent: 53.84545454545455\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039358629188087906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.800455665264423\n",
      "    mean_inference_ms: 1.9491120977503151\n",
      "    mean_raw_obs_processing_ms: 2.1778349691878343\n",
      "  time_since_restore: 24176.80572628975\n",
      "  time_this_iter_s: 30.927318334579468\n",
      "  time_total_s: 24176.80572628975\n",
      "  timers:\n",
      "    learn_throughput: 1277.44\n",
      "    learn_time_ms: 782.816\n",
      "    load_throughput: 43242.699\n",
      "    load_time_ms: 23.125\n",
      "    sample_throughput: 33.771\n",
      "    sample_time_ms: 29611.499\n",
      "    update_time_ms: 4.723\n",
      "  timestamp: 1635087060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 901000\n",
      "  training_iteration: 901\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   901</td><td style=\"text-align: right;\">         24176.8</td><td style=\"text-align: right;\">901000</td><td style=\"text-align: right;\"> -3.2388</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            320.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 902000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-51-31\n",
      "  done: false\n",
      "  episode_len_mean: 320.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.238499999999974\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3102\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.997156024558069e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4386180996894837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010118474997729205\n",
      "          policy_loss: 0.027171524365743\n",
      "          total_loss: 0.022143538627359603\n",
      "          vf_explained_var: 0.46890321373939514\n",
      "          vf_loss: 0.009358188895405167\n",
      "    num_agent_steps_sampled: 902000\n",
      "    num_agent_steps_trained: 902000\n",
      "    num_steps_sampled: 902000\n",
      "    num_steps_trained: 902000\n",
      "  iterations_since_restore: 902\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.64\n",
      "    ram_util_percent: 53.89555555555556\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03936228117440453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.806224727631513\n",
      "    mean_inference_ms: 1.9492187555283522\n",
      "    mean_raw_obs_processing_ms: 2.177500979358912\n",
      "  time_since_restore: 24208.07120037079\n",
      "  time_this_iter_s: 31.26547408103943\n",
      "  time_total_s: 24208.07120037079\n",
      "  timers:\n",
      "    learn_throughput: 1267.174\n",
      "    learn_time_ms: 789.158\n",
      "    load_throughput: 43289.026\n",
      "    load_time_ms: 23.101\n",
      "    sample_throughput: 33.385\n",
      "    sample_time_ms: 29953.645\n",
      "    update_time_ms: 4.694\n",
      "  timestamp: 1635087091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 902000\n",
      "  training_iteration: 902\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   902</td><td style=\"text-align: right;\">         24208.1</td><td style=\"text-align: right;\">902000</td><td style=\"text-align: right;\"> -3.2385</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            320.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 903000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-52-02\n",
      "  done: false\n",
      "  episode_len_mean: 320.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.2389999999999746\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3105\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.997156024558069e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5388104557991027\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022523338881194614\n",
      "          policy_loss: 0.044537577198611365\n",
      "          total_loss: 0.0388840070201291\n",
      "          vf_explained_var: 0.28981006145477295\n",
      "          vf_loss: 0.009734523192875915\n",
      "    num_agent_steps_sampled: 903000\n",
      "    num_agent_steps_trained: 903000\n",
      "    num_steps_sampled: 903000\n",
      "    num_steps_trained: 903000\n",
      "  iterations_since_restore: 903\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.39090909090909\n",
      "    ram_util_percent: 53.88409090909091\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0393659972818166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.81200880956049\n",
      "    mean_inference_ms: 1.9493271331920392\n",
      "    mean_raw_obs_processing_ms: 2.177169782319546\n",
      "  time_since_restore: 24238.57531094551\n",
      "  time_this_iter_s: 30.50411057472229\n",
      "  time_total_s: 24238.57531094551\n",
      "  timers:\n",
      "    learn_throughput: 1304.549\n",
      "    learn_time_ms: 766.548\n",
      "    load_throughput: 44651.749\n",
      "    load_time_ms: 22.396\n",
      "    sample_throughput: 33.271\n",
      "    sample_time_ms: 30055.96\n",
      "    update_time_ms: 4.932\n",
      "  timestamp: 1635087122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 903000\n",
      "  training_iteration: 903\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   903</td><td style=\"text-align: right;\">         24238.6</td><td style=\"text-align: right;\">903000</td><td style=\"text-align: right;\">  -3.239</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            320.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 904000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-52-32\n",
      "  done: false\n",
      "  episode_len_mean: 320.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.2310999999999757\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3108\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.04957340368371e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1963421649403043\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01748130937384044\n",
      "          policy_loss: -0.11844316514001953\n",
      "          total_loss: -0.11940004138482942\n",
      "          vf_explained_var: 0.5431554913520813\n",
      "          vf_loss: 0.011006529091133012\n",
      "    num_agent_steps_sampled: 904000\n",
      "    num_agent_steps_trained: 904000\n",
      "    num_steps_sampled: 904000\n",
      "    num_steps_trained: 904000\n",
      "  iterations_since_restore: 904\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.83863636363637\n",
      "    ram_util_percent: 53.850000000000016\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03936974356036019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.81777139158394\n",
      "    mean_inference_ms: 1.9494347787204855\n",
      "    mean_raw_obs_processing_ms: 2.1768413683388523\n",
      "  time_since_restore: 24269.51280927658\n",
      "  time_this_iter_s: 30.937498331069946\n",
      "  time_total_s: 24269.51280927658\n",
      "  timers:\n",
      "    learn_throughput: 1295.572\n",
      "    learn_time_ms: 771.86\n",
      "    load_throughput: 45941.312\n",
      "    load_time_ms: 21.767\n",
      "    sample_throughput: 33.25\n",
      "    sample_time_ms: 30075.512\n",
      "    update_time_ms: 5.835\n",
      "  timestamp: 1635087152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 904000\n",
      "  training_iteration: 904\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   904</td><td style=\"text-align: right;\">         24269.5</td><td style=\"text-align: right;\">904000</td><td style=\"text-align: right;\"> -3.2311</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            320.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 905000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-53-04\n",
      "  done: false\n",
      "  episode_len_mean: 319.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.225399999999975\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3112\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.04957340368371e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0063809825314416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01697914477949267\n",
      "          policy_loss: -0.05517953129278289\n",
      "          total_loss: -0.054919251634014975\n",
      "          vf_explained_var: 0.5350525975227356\n",
      "          vf_loss: 0.010324077168479561\n",
      "    num_agent_steps_sampled: 905000\n",
      "    num_agent_steps_trained: 905000\n",
      "    num_steps_sampled: 905000\n",
      "    num_steps_trained: 905000\n",
      "  iterations_since_restore: 905\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.81555555555555\n",
      "    ram_util_percent: 53.728888888888896\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0393746774954318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.825376863928742\n",
      "    mean_inference_ms: 1.9495767889545843\n",
      "    mean_raw_obs_processing_ms: 2.176429897372378\n",
      "  time_since_restore: 24301.069214582443\n",
      "  time_this_iter_s: 31.556405305862427\n",
      "  time_total_s: 24301.069214582443\n",
      "  timers:\n",
      "    learn_throughput: 1291.043\n",
      "    learn_time_ms: 774.568\n",
      "    load_throughput: 47758.007\n",
      "    load_time_ms: 20.939\n",
      "    sample_throughput: 33.222\n",
      "    sample_time_ms: 30100.34\n",
      "    update_time_ms: 5.616\n",
      "  timestamp: 1635087184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 905000\n",
      "  training_iteration: 905\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   905</td><td style=\"text-align: right;\">         24301.1</td><td style=\"text-align: right;\">905000</td><td style=\"text-align: right;\"> -3.2254</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            319.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 906000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-53-31\n",
      "  done: false\n",
      "  episode_len_mean: 320.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.2384999999999757\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 3114\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.04957340368371e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9176500876744588\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011562201653525348\n",
      "          policy_loss: -0.0808603286743164\n",
      "          total_loss: -0.07979433875944879\n",
      "          vf_explained_var: 0.39512526988983154\n",
      "          vf_loss: 0.010242482973262667\n",
      "    num_agent_steps_sampled: 906000\n",
      "    num_agent_steps_trained: 906000\n",
      "    num_steps_sampled: 906000\n",
      "    num_steps_trained: 906000\n",
      "  iterations_since_restore: 906\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.46052631578948\n",
      "    ram_util_percent: 53.83684210526316\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03937718108801484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.829121623818022\n",
      "    mean_inference_ms: 1.9496480146296926\n",
      "    mean_raw_obs_processing_ms: 2.176213304828829\n",
      "  time_since_restore: 24328.179097414017\n",
      "  time_this_iter_s: 27.109882831573486\n",
      "  time_total_s: 24328.179097414017\n",
      "  timers:\n",
      "    learn_throughput: 1292.387\n",
      "    learn_time_ms: 773.762\n",
      "    load_throughput: 50281.105\n",
      "    load_time_ms: 19.888\n",
      "    sample_throughput: 33.111\n",
      "    sample_time_ms: 30201.439\n",
      "    update_time_ms: 5.873\n",
      "  timestamp: 1635087211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 906000\n",
      "  training_iteration: 906\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   906</td><td style=\"text-align: right;\">         24328.2</td><td style=\"text-align: right;\">906000</td><td style=\"text-align: right;\"> -3.2385</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            320.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 907000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-53-59\n",
      "  done: false\n",
      "  episode_len_mean: 321.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.2445999999999753\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3117\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.04957340368371e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9373373495207893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008484317940480126\n",
      "          policy_loss: -0.10507725261979634\n",
      "          total_loss: -0.10416703224182129\n",
      "          vf_explained_var: 0.5576027035713196\n",
      "          vf_loss: 0.010283584049385454\n",
      "    num_agent_steps_sampled: 907000\n",
      "    num_agent_steps_trained: 907000\n",
      "    num_steps_sampled: 907000\n",
      "    num_steps_trained: 907000\n",
      "  iterations_since_restore: 907\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.58\n",
      "    ram_util_percent: 53.80499999999999\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039381035996005356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.834695802203463\n",
      "    mean_inference_ms: 1.9497566093468175\n",
      "    mean_raw_obs_processing_ms: 2.1758692487225635\n",
      "  time_since_restore: 24355.93604874611\n",
      "  time_this_iter_s: 27.756951332092285\n",
      "  time_total_s: 24355.93604874611\n",
      "  timers:\n",
      "    learn_throughput: 1284.434\n",
      "    learn_time_ms: 778.553\n",
      "    load_throughput: 50877.235\n",
      "    load_time_ms: 19.655\n",
      "    sample_throughput: 33.015\n",
      "    sample_time_ms: 30289.143\n",
      "    update_time_ms: 6.553\n",
      "  timestamp: 1635087239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 907000\n",
      "  training_iteration: 907\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   907</td><td style=\"text-align: right;\">         24355.9</td><td style=\"text-align: right;\">907000</td><td style=\"text-align: right;\"> -3.2446</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 908000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-54-49\n",
      "  done: false\n",
      "  episode_len_mean: 320.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.2348999999999752\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3121\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.04957340368371e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7102284875180986\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005097581236892602\n",
      "          policy_loss: -0.046223192910353345\n",
      "          total_loss: -0.04532612851924366\n",
      "          vf_explained_var: 0.6851803660392761\n",
      "          vf_loss: 0.007999345339420769\n",
      "    num_agent_steps_sampled: 908000\n",
      "    num_agent_steps_trained: 908000\n",
      "    num_steps_sampled: 908000\n",
      "    num_steps_trained: 908000\n",
      "  iterations_since_restore: 908\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.59722222222223\n",
      "    ram_util_percent: 53.922222222222224\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03938618245322295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.842241942897964\n",
      "    mean_inference_ms: 1.9499039652567782\n",
      "    mean_raw_obs_processing_ms: 2.176153626108805\n",
      "  time_since_restore: 24406.021318674088\n",
      "  time_this_iter_s: 50.085269927978516\n",
      "  time_total_s: 24406.021318674088\n",
      "  timers:\n",
      "    learn_throughput: 1314.218\n",
      "    learn_time_ms: 760.909\n",
      "    load_throughput: 50824.893\n",
      "    load_time_ms: 19.675\n",
      "    sample_throughput: 32.266\n",
      "    sample_time_ms: 30992.853\n",
      "    update_time_ms: 6.449\n",
      "  timestamp: 1635087289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 908000\n",
      "  training_iteration: 908\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   908</td><td style=\"text-align: right;\">           24406</td><td style=\"text-align: right;\">908000</td><td style=\"text-align: right;\"> -3.2349</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            320.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 909000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-55-17\n",
      "  done: false\n",
      "  episode_len_mean: 321.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.2439999999999753\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 3123\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.04957340368371e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9945093704594506\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04953008357720845\n",
      "          policy_loss: -0.017760294261905882\n",
      "          total_loss: -0.018378120329644945\n",
      "          vf_explained_var: 0.6760273575782776\n",
      "          vf_loss: 0.009327214255204632\n",
      "    num_agent_steps_sampled: 909000\n",
      "    num_agent_steps_trained: 909000\n",
      "    num_steps_sampled: 909000\n",
      "    num_steps_trained: 909000\n",
      "  iterations_since_restore: 909\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.51538461538462\n",
      "    ram_util_percent: 54.02820512820512\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039388759599764726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.845916203822885\n",
      "    mean_inference_ms: 1.9499781574709403\n",
      "    mean_raw_obs_processing_ms: 2.176307539627838\n",
      "  time_since_restore: 24433.866973161697\n",
      "  time_this_iter_s: 27.845654487609863\n",
      "  time_total_s: 24433.866973161697\n",
      "  timers:\n",
      "    learn_throughput: 1299.285\n",
      "    learn_time_ms: 769.654\n",
      "    load_throughput: 53058.069\n",
      "    load_time_ms: 18.847\n",
      "    sample_throughput: 32.401\n",
      "    sample_time_ms: 30863.05\n",
      "    update_time_ms: 6.022\n",
      "  timestamp: 1635087317\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 909000\n",
      "  training_iteration: 909\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   909</td><td style=\"text-align: right;\">         24433.9</td><td style=\"text-align: right;\">909000</td><td style=\"text-align: right;\">  -3.244</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 910000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-55-48\n",
      "  done: false\n",
      "  episode_len_mean: 321.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.244499999999975\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3127\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.574360105525565e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7591665450069639\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.029594796174393326\n",
      "          policy_loss: -0.03205205574631691\n",
      "          total_loss: -0.031302675728996594\n",
      "          vf_explained_var: 0.5876702070236206\n",
      "          vf_loss: 0.008341002200419704\n",
      "    num_agent_steps_sampled: 910000\n",
      "    num_agent_steps_trained: 910000\n",
      "    num_steps_sampled: 910000\n",
      "    num_steps_trained: 910000\n",
      "  iterations_since_restore: 910\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.64444444444445\n",
      "    ram_util_percent: 54.024444444444455\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039393892223205954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.853269815939456\n",
      "    mean_inference_ms: 1.9501277030259723\n",
      "    mean_raw_obs_processing_ms: 2.1765985719061836\n",
      "  time_since_restore: 24465.376209020615\n",
      "  time_this_iter_s: 31.509235858917236\n",
      "  time_total_s: 24465.376209020615\n",
      "  timers:\n",
      "    learn_throughput: 1299.034\n",
      "    learn_time_ms: 769.803\n",
      "    load_throughput: 53049.881\n",
      "    load_time_ms: 18.85\n",
      "    sample_throughput: 32.103\n",
      "    sample_time_ms: 31149.428\n",
      "    update_time_ms: 6.803\n",
      "  timestamp: 1635087348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 910000\n",
      "  training_iteration: 910\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   910</td><td style=\"text-align: right;\">         24465.4</td><td style=\"text-align: right;\">910000</td><td style=\"text-align: right;\"> -3.2445</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 911000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-56-22\n",
      "  done: false\n",
      "  episode_len_mean: 321.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.2435999999999745\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3130\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.361540158288347e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5201512760586209\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027961108779652792\n",
      "          policy_loss: 0.06128378783663114\n",
      "          total_loss: 0.062231760720411936\n",
      "          vf_explained_var: 0.588551938533783\n",
      "          vf_loss: 0.006149421719601378\n",
      "    num_agent_steps_sampled: 911000\n",
      "    num_agent_steps_trained: 911000\n",
      "    num_steps_sampled: 911000\n",
      "    num_steps_trained: 911000\n",
      "  iterations_since_restore: 911\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.40208333333334\n",
      "    ram_util_percent: 54.06666666666667\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03939771829383113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.858727107056648\n",
      "    mean_inference_ms: 1.950239341548302\n",
      "    mean_raw_obs_processing_ms: 2.176272787183641\n",
      "  time_since_restore: 24498.775903224945\n",
      "  time_this_iter_s: 33.399694204330444\n",
      "  time_total_s: 24498.775903224945\n",
      "  timers:\n",
      "    learn_throughput: 1298.843\n",
      "    learn_time_ms: 769.916\n",
      "    load_throughput: 52132.946\n",
      "    load_time_ms: 19.182\n",
      "    sample_throughput: 31.852\n",
      "    sample_time_ms: 31395.454\n",
      "    update_time_ms: 7.502\n",
      "  timestamp: 1635087382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 911000\n",
      "  training_iteration: 911\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   911</td><td style=\"text-align: right;\">         24498.8</td><td style=\"text-align: right;\">911000</td><td style=\"text-align: right;\"> -3.2436</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 912000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-56-52\n",
      "  done: false\n",
      "  episode_len_mean: 321.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.2415999999999747\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3133\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5423102374325197e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6686349117093616\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009726903432150793\n",
      "          policy_loss: -0.09029314915339152\n",
      "          total_loss: -0.08762142302261458\n",
      "          vf_explained_var: 0.3914180397987366\n",
      "          vf_loss: 0.009358041345452268\n",
      "    num_agent_steps_sampled: 912000\n",
      "    num_agent_steps_trained: 912000\n",
      "    num_steps_sampled: 912000\n",
      "    num_steps_trained: 912000\n",
      "  iterations_since_restore: 912\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.85813953488372\n",
      "    ram_util_percent: 54.027906976744184\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039401360117091305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.864067424403018\n",
      "    mean_inference_ms: 1.9503480585737747\n",
      "    mean_raw_obs_processing_ms: 2.1759482627914757\n",
      "  time_since_restore: 24528.881909370422\n",
      "  time_this_iter_s: 30.106006145477295\n",
      "  time_total_s: 24528.881909370422\n",
      "  timers:\n",
      "    learn_throughput: 1310.653\n",
      "    learn_time_ms: 762.979\n",
      "    load_throughput: 50342.542\n",
      "    load_time_ms: 19.864\n",
      "    sample_throughput: 31.963\n",
      "    sample_time_ms: 31286.187\n",
      "    update_time_ms: 7.08\n",
      "  timestamp: 1635087412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 912000\n",
      "  training_iteration: 912\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   912</td><td style=\"text-align: right;\">         24528.9</td><td style=\"text-align: right;\">912000</td><td style=\"text-align: right;\"> -3.2416</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 913000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 321.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.2400999999999756\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3137\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5423102374325197e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5375603238741556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005116942659102443\n",
      "          policy_loss: 0.008928475942876603\n",
      "          total_loss: 0.01355018996530109\n",
      "          vf_explained_var: 0.3805585503578186\n",
      "          vf_loss: 0.009997301905726393\n",
      "    num_agent_steps_sampled: 913000\n",
      "    num_agent_steps_trained: 913000\n",
      "    num_steps_sampled: 913000\n",
      "    num_steps_trained: 913000\n",
      "  iterations_since_restore: 913\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.06666666666665\n",
      "    ram_util_percent: 54.041666666666664\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940635981109226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.871302515880064\n",
      "    mean_inference_ms: 1.9504979267061906\n",
      "    mean_raw_obs_processing_ms: 2.1754991592067396\n",
      "  time_since_restore: 24562.42168545723\n",
      "  time_this_iter_s: 33.53977608680725\n",
      "  time_total_s: 24562.42168545723\n",
      "  timers:\n",
      "    learn_throughput: 1316.56\n",
      "    learn_time_ms: 759.555\n",
      "    load_throughput: 50992.346\n",
      "    load_time_ms: 19.611\n",
      "    sample_throughput: 31.653\n",
      "    sample_time_ms: 31592.577\n",
      "    update_time_ms: 7.976\n",
      "  timestamp: 1635087446\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 913000\n",
      "  training_iteration: 913\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   913</td><td style=\"text-align: right;\">         24562.4</td><td style=\"text-align: right;\">913000</td><td style=\"text-align: right;\"> -3.2401</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 914000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-57-58\n",
      "  done: false\n",
      "  episode_len_mean: 321.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.2417999999999756\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3140\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5423102374325197e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6279765129089355\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00858683536039785\n",
      "          policy_loss: 0.053105664915508694\n",
      "          total_loss: 0.05403948956065708\n",
      "          vf_explained_var: 0.40896865725517273\n",
      "          vf_loss: 0.007213559575999776\n",
      "    num_agent_steps_sampled: 914000\n",
      "    num_agent_steps_trained: 914000\n",
      "    num_steps_sampled: 914000\n",
      "    num_steps_trained: 914000\n",
      "  iterations_since_restore: 914\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.56521739130434\n",
      "    ram_util_percent: 53.943478260869576\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940991263935317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.876616506654074\n",
      "    mean_inference_ms: 1.950610207220936\n",
      "    mean_raw_obs_processing_ms: 2.1751803183848115\n",
      "  time_since_restore: 24594.801161766052\n",
      "  time_this_iter_s: 32.37947630882263\n",
      "  time_total_s: 24594.801161766052\n",
      "  timers:\n",
      "    learn_throughput: 1326.529\n",
      "    learn_time_ms: 753.847\n",
      "    load_throughput: 49717.282\n",
      "    load_time_ms: 20.114\n",
      "    sample_throughput: 31.503\n",
      "    sample_time_ms: 31742.891\n",
      "    update_time_ms: 7.435\n",
      "  timestamp: 1635087478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 914000\n",
      "  training_iteration: 914\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   914</td><td style=\"text-align: right;\">         24594.8</td><td style=\"text-align: right;\">914000</td><td style=\"text-align: right;\"> -3.2418</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 915000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-58-30\n",
      "  done: false\n",
      "  episode_len_mean: 321.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.2420999999999753\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3143\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5423102374325197e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5652457803487778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019675183003994807\n",
      "          policy_loss: -0.06221788566973474\n",
      "          total_loss: -0.05690447207954195\n",
      "          vf_explained_var: 0.31694161891937256\n",
      "          vf_loss: 0.010965801227009958\n",
      "    num_agent_steps_sampled: 915000\n",
      "    num_agent_steps_trained: 915000\n",
      "    num_steps_sampled: 915000\n",
      "    num_steps_trained: 915000\n",
      "  iterations_since_restore: 915\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.62173913043479\n",
      "    ram_util_percent: 53.884782608695666\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039413579987387176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.881934641712604\n",
      "    mean_inference_ms: 1.950724229560646\n",
      "    mean_raw_obs_processing_ms: 2.174862908879099\n",
      "  time_since_restore: 24627.002984285355\n",
      "  time_this_iter_s: 32.20182251930237\n",
      "  time_total_s: 24627.002984285355\n",
      "  timers:\n",
      "    learn_throughput: 1328.454\n",
      "    learn_time_ms: 752.754\n",
      "    load_throughput: 47732.3\n",
      "    load_time_ms: 20.95\n",
      "    sample_throughput: 31.439\n",
      "    sample_time_ms: 31807.655\n",
      "    update_time_ms: 7.519\n",
      "  timestamp: 1635087510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 915000\n",
      "  training_iteration: 915\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   915</td><td style=\"text-align: right;\">           24627</td><td style=\"text-align: right;\">915000</td><td style=\"text-align: right;\"> -3.2421</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 916000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-59-03\n",
      "  done: false\n",
      "  episode_len_mean: 321.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.244799999999975\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3147\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5423102374325197e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.41287174622217815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003387428418052865\n",
      "          policy_loss: -0.016592579376366403\n",
      "          total_loss: -0.010188336008124882\n",
      "          vf_explained_var: 0.39796850085258484\n",
      "          vf_loss: 0.010532950320177608\n",
      "    num_agent_steps_sampled: 916000\n",
      "    num_agent_steps_trained: 916000\n",
      "    num_steps_sampled: 916000\n",
      "    num_steps_trained: 916000\n",
      "  iterations_since_restore: 916\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.66382978723404\n",
      "    ram_util_percent: 53.897872340425536\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03941853143849486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.889096385772728\n",
      "    mean_inference_ms: 1.9508790604598274\n",
      "    mean_raw_obs_processing_ms: 2.1744227971936936\n",
      "  time_since_restore: 24659.84572172165\n",
      "  time_this_iter_s: 32.842737436294556\n",
      "  time_total_s: 24659.84572172165\n",
      "  timers:\n",
      "    learn_throughput: 1326.504\n",
      "    learn_time_ms: 753.861\n",
      "    load_throughput: 47780.912\n",
      "    load_time_ms: 20.929\n",
      "    sample_throughput: 30.884\n",
      "    sample_time_ms: 32379.286\n",
      "    update_time_ms: 8.132\n",
      "  timestamp: 1635087543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 916000\n",
      "  training_iteration: 916\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   916</td><td style=\"text-align: right;\">         24659.8</td><td style=\"text-align: right;\">916000</td><td style=\"text-align: right;\"> -3.2448</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 917000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-59-52\n",
      "  done: false\n",
      "  episode_len_mean: 321.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.2415999999999747\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3150\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7711551187162599e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4628169920709398\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006641864378681264\n",
      "          policy_loss: 0.04594561027155982\n",
      "          total_loss: 0.04873049548930592\n",
      "          vf_explained_var: 0.48966625332832336\n",
      "          vf_loss: 0.007413046035475822\n",
      "    num_agent_steps_sampled: 917000\n",
      "    num_agent_steps_trained: 917000\n",
      "    num_steps_sampled: 917000\n",
      "    num_steps_trained: 917000\n",
      "  iterations_since_restore: 917\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.33142857142859\n",
      "    ram_util_percent: 53.99571428571429\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03942230094174983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.894329936792477\n",
      "    mean_inference_ms: 1.9509952137485111\n",
      "    mean_raw_obs_processing_ms: 2.174629567464009\n",
      "  time_since_restore: 24708.945887088776\n",
      "  time_this_iter_s: 49.100165367126465\n",
      "  time_total_s: 24708.945887088776\n",
      "  timers:\n",
      "    learn_throughput: 1329.239\n",
      "    learn_time_ms: 752.31\n",
      "    load_throughput: 48043.178\n",
      "    load_time_ms: 20.815\n",
      "    sample_throughput: 28.972\n",
      "    sample_time_ms: 34515.495\n",
      "    update_time_ms: 7.603\n",
      "  timestamp: 1635087592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 917000\n",
      "  training_iteration: 917\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   917</td><td style=\"text-align: right;\">         24708.9</td><td style=\"text-align: right;\">917000</td><td style=\"text-align: right;\"> -3.2416</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 918000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 321.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.2415999999999747\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3153\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7711551187162599e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.45649483501911164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007322492714669614\n",
      "          policy_loss: -0.11303567621443007\n",
      "          total_loss: -0.10643427032563421\n",
      "          vf_explained_var: 0.2533276379108429\n",
      "          vf_loss: 0.011166342492732738\n",
      "    num_agent_steps_sampled: 918000\n",
      "    num_agent_steps_trained: 918000\n",
      "    num_steps_sampled: 918000\n",
      "    num_steps_trained: 918000\n",
      "  iterations_since_restore: 918\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.67083333333333\n",
      "    ram_util_percent: 53.87083333333334\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039426055734515096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.899605621431377\n",
      "    mean_inference_ms: 1.9511107562852745\n",
      "    mean_raw_obs_processing_ms: 2.1748367716121524\n",
      "  time_since_restore: 24742.759464263916\n",
      "  time_this_iter_s: 33.81357717514038\n",
      "  time_total_s: 24742.759464263916\n",
      "  timers:\n",
      "    learn_throughput: 1330.334\n",
      "    learn_time_ms: 751.691\n",
      "    load_throughput: 46895.852\n",
      "    load_time_ms: 21.324\n",
      "    sample_throughput: 30.406\n",
      "    sample_time_ms: 32888.406\n",
      "    update_time_ms: 7.696\n",
      "  timestamp: 1635087626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 918000\n",
      "  training_iteration: 918\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   918</td><td style=\"text-align: right;\">         24742.8</td><td style=\"text-align: right;\">918000</td><td style=\"text-align: right;\"> -3.2416</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 919000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 321.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.2403999999999744\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3157\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7711551187162599e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.39558067984051176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0024259809766577845\n",
      "          policy_loss: 0.035525697304142845\n",
      "          total_loss: 0.042184004601505065\n",
      "          vf_explained_var: 0.25197574496269226\n",
      "          vf_loss: 0.01061410785963138\n",
      "    num_agent_steps_sampled: 919000\n",
      "    num_agent_steps_trained: 919000\n",
      "    num_steps_sampled: 919000\n",
      "    num_steps_trained: 919000\n",
      "  iterations_since_restore: 919\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.13695652173912\n",
      "    ram_util_percent: 53.954347826086966\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03943113091126207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.90672843925401\n",
      "    mean_inference_ms: 1.9512662528346896\n",
      "    mean_raw_obs_processing_ms: 2.1750956222367064\n",
      "  time_since_restore: 24774.641497135162\n",
      "  time_this_iter_s: 31.882032871246338\n",
      "  time_total_s: 24774.641497135162\n",
      "  timers:\n",
      "    learn_throughput: 1349.046\n",
      "    learn_time_ms: 741.265\n",
      "    load_throughput: 44173.632\n",
      "    load_time_ms: 22.638\n",
      "    sample_throughput: 30.03\n",
      "    sample_time_ms: 33300.424\n",
      "    update_time_ms: 8.533\n",
      "  timestamp: 1635087658\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 919000\n",
      "  training_iteration: 919\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   919</td><td style=\"text-align: right;\">         24774.6</td><td style=\"text-align: right;\">919000</td><td style=\"text-align: right;\"> -3.2404</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            321.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-01-29\n",
      "  done: false\n",
      "  episode_len_mean: 320.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.234199999999975\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3160\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.855775593581299e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3979245798455344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011705018504130749\n",
      "          policy_loss: 0.02555676665571001\n",
      "          total_loss: 0.02878173560731941\n",
      "          vf_explained_var: 0.2260773479938507\n",
      "          vf_loss: 0.0072042046554593574\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_agent_steps_trained: 920000\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "  iterations_since_restore: 920\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.84222222222222\n",
      "    ram_util_percent: 53.9711111111111\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039434762233485784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.91207942744047\n",
      "    mean_inference_ms: 1.9513800969940207\n",
      "    mean_raw_obs_processing_ms: 2.1747016648733295\n",
      "  time_since_restore: 24806.054095506668\n",
      "  time_this_iter_s: 31.412598371505737\n",
      "  time_total_s: 24806.054095506668\n",
      "  timers:\n",
      "    learn_throughput: 1346.164\n",
      "    learn_time_ms: 742.852\n",
      "    load_throughput: 43540.497\n",
      "    load_time_ms: 22.967\n",
      "    sample_throughput: 30.04\n",
      "    sample_time_ms: 33289.156\n",
      "    update_time_ms: 8.097\n",
      "  timestamp: 1635087689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 920\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   920</td><td style=\"text-align: right;\">         24806.1</td><td style=\"text-align: right;\">920000</td><td style=\"text-align: right;\"> -3.2342</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            320.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 921000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-02-02\n",
      "  done: false\n",
      "  episode_len_mean: 318.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.2190999999999756\n",
      "  episode_reward_min: -6.959999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3164\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.855775593581299e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.37607252995173135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006090749370499459\n",
      "          policy_loss: 0.007931280384461085\n",
      "          total_loss: 0.016337999577323594\n",
      "          vf_explained_var: 0.258786678314209\n",
      "          vf_loss: 0.01216744040656421\n",
      "    num_agent_steps_sampled: 921000\n",
      "    num_agent_steps_trained: 921000\n",
      "    num_steps_sampled: 921000\n",
      "    num_steps_trained: 921000\n",
      "  iterations_since_restore: 921\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.91521739130435\n",
      "    ram_util_percent: 54.00217391304348\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039439623814903166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.919330211492625\n",
      "    mean_inference_ms: 1.9515328864187507\n",
      "    mean_raw_obs_processing_ms: 2.174202433352428\n",
      "  time_since_restore: 24838.62252831459\n",
      "  time_this_iter_s: 32.56843280792236\n",
      "  time_total_s: 24838.62252831459\n",
      "  timers:\n",
      "    learn_throughput: 1339.726\n",
      "    learn_time_ms: 746.421\n",
      "    load_throughput: 45286.546\n",
      "    load_time_ms: 22.082\n",
      "    sample_throughput: 30.118\n",
      "    sample_time_ms: 33203.182\n",
      "    update_time_ms: 8.144\n",
      "  timestamp: 1635087722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 921000\n",
      "  training_iteration: 921\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   921</td><td style=\"text-align: right;\">         24838.6</td><td style=\"text-align: right;\">921000</td><td style=\"text-align: right;\"> -3.2191</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            318.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 922000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-02-36\n",
      "  done: false\n",
      "  episode_len_mean: 316.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.1696999999999758\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3167\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.855775593581299e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7087898562351863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04648224346801493\n",
      "          policy_loss: 0.06048825176225768\n",
      "          total_loss: 0.06073038122720188\n",
      "          vf_explained_var: 0.32039880752563477\n",
      "          vf_loss: 0.0073299904198696215\n",
      "    num_agent_steps_sampled: 922000\n",
      "    num_agent_steps_trained: 922000\n",
      "    num_steps_sampled: 922000\n",
      "    num_steps_trained: 922000\n",
      "  iterations_since_restore: 922\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.22448979591837\n",
      "    ram_util_percent: 54.11020408163265\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039443437784745974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.925006926094444\n",
      "    mean_inference_ms: 1.9516499186840275\n",
      "    mean_raw_obs_processing_ms: 2.173835789159299\n",
      "  time_since_restore: 24872.490587472916\n",
      "  time_this_iter_s: 33.868059158325195\n",
      "  time_total_s: 24872.490587472916\n",
      "  timers:\n",
      "    learn_throughput: 1321.027\n",
      "    learn_time_ms: 756.987\n",
      "    load_throughput: 45831.875\n",
      "    load_time_ms: 21.819\n",
      "    sample_throughput: 29.79\n",
      "    sample_time_ms: 33568.391\n",
      "    update_time_ms: 8.816\n",
      "  timestamp: 1635087756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 922000\n",
      "  training_iteration: 922\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   922</td><td style=\"text-align: right;\">         24872.5</td><td style=\"text-align: right;\">922000</td><td style=\"text-align: right;\"> -3.1697</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">            316.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 923000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 315.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.1560999999999764\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3170\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3283663390371952e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6546601802110672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07184579839288029\n",
      "          policy_loss: 0.03078805117143525\n",
      "          total_loss: 0.03214906288517846\n",
      "          vf_explained_var: 0.35113176703453064\n",
      "          vf_loss: 0.007907523100341981\n",
      "    num_agent_steps_sampled: 923000\n",
      "    num_agent_steps_trained: 923000\n",
      "    num_steps_sampled: 923000\n",
      "    num_steps_trained: 923000\n",
      "  iterations_since_restore: 923\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.53255813953488\n",
      "    ram_util_percent: 53.93255813953489\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039447203473757575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.930767638412544\n",
      "    mean_inference_ms: 1.951766989050615\n",
      "    mean_raw_obs_processing_ms: 2.173472130571599\n",
      "  time_since_restore: 24902.700314998627\n",
      "  time_this_iter_s: 30.20972752571106\n",
      "  time_total_s: 24902.700314998627\n",
      "  timers:\n",
      "    learn_throughput: 1321.295\n",
      "    learn_time_ms: 756.834\n",
      "    load_throughput: 45694.514\n",
      "    load_time_ms: 21.884\n",
      "    sample_throughput: 30.088\n",
      "    sample_time_ms: 33236.299\n",
      "    update_time_ms: 7.826\n",
      "  timestamp: 1635087786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 923000\n",
      "  training_iteration: 923\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   923</td><td style=\"text-align: right;\">         24902.7</td><td style=\"text-align: right;\">923000</td><td style=\"text-align: right;\"> -3.1561</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">            315.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 924000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-03-38\n",
      "  done: false\n",
      "  episode_len_mean: 315.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.1499999999999773\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3173\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9925495085557925e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6671566724777221\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006015766079576679\n",
      "          policy_loss: -0.11257549333903524\n",
      "          total_loss: -0.10928834651907285\n",
      "          vf_explained_var: 0.4066242277622223\n",
      "          vf_loss: 0.009958707003129853\n",
      "    num_agent_steps_sampled: 924000\n",
      "    num_agent_steps_trained: 924000\n",
      "    num_steps_sampled: 924000\n",
      "    num_steps_trained: 924000\n",
      "  iterations_since_restore: 924\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.02444444444443\n",
      "    ram_util_percent: 53.86222222222223\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03945089493161488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.93661928074105\n",
      "    mean_inference_ms: 1.951883971447002\n",
      "    mean_raw_obs_processing_ms: 2.1731115089393787\n",
      "  time_since_restore: 24934.1984705925\n",
      "  time_this_iter_s: 31.49815559387207\n",
      "  time_total_s: 24934.1984705925\n",
      "  timers:\n",
      "    learn_throughput: 1323.387\n",
      "    learn_time_ms: 755.637\n",
      "    load_throughput: 45606.173\n",
      "    load_time_ms: 21.927\n",
      "    sample_throughput: 30.166\n",
      "    sample_time_ms: 33149.944\n",
      "    update_time_ms: 7.183\n",
      "  timestamp: 1635087818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 924000\n",
      "  training_iteration: 924\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   924</td><td style=\"text-align: right;\">         24934.2</td><td style=\"text-align: right;\">924000</td><td style=\"text-align: right;\">   -3.15</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">               315</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 925000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-04-09\n",
      "  done: false\n",
      "  episode_len_mean: 314.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.140899999999977\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3177\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9925495085557925e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.615198979443974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03873530757489539\n",
      "          policy_loss: 0.02057269045876132\n",
      "          total_loss: 0.02541348296735022\n",
      "          vf_explained_var: 0.3196726143360138\n",
      "          vf_loss: 0.010992709950854381\n",
      "    num_agent_steps_sampled: 925000\n",
      "    num_agent_steps_trained: 925000\n",
      "    num_steps_sampled: 925000\n",
      "    num_steps_trained: 925000\n",
      "  iterations_since_restore: 925\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.02727272727273\n",
      "    ram_util_percent: 53.79090909090908\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039455849852603884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.944420330607585\n",
      "    mean_inference_ms: 1.9520392600429517\n",
      "    mean_raw_obs_processing_ms: 2.1726503487861817\n",
      "  time_since_restore: 24965.405212163925\n",
      "  time_this_iter_s: 31.20674157142639\n",
      "  time_total_s: 24965.405212163925\n",
      "  timers:\n",
      "    learn_throughput: 1322.269\n",
      "    learn_time_ms: 756.275\n",
      "    load_throughput: 45171.93\n",
      "    load_time_ms: 22.138\n",
      "    sample_throughput: 30.258\n",
      "    sample_time_ms: 33049.415\n",
      "    update_time_ms: 7.431\n",
      "  timestamp: 1635087849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 925000\n",
      "  training_iteration: 925\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   925</td><td style=\"text-align: right;\">         24965.4</td><td style=\"text-align: right;\">925000</td><td style=\"text-align: right;\"> -3.1409</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">            314.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=43203)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 926000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-04-59\n",
      "  done: false\n",
      "  episode_len_mean: 313.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.132999999999976\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3180\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.98882426283369e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.37737616366810267\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002099443002548658\n",
      "          policy_loss: 0.009551287525229983\n",
      "          total_loss: 0.014143656690915425\n",
      "          vf_explained_var: 0.24205784499645233\n",
      "          vf_loss: 0.00836612508735723\n",
      "    num_agent_steps_sampled: 926000\n",
      "    num_agent_steps_trained: 926000\n",
      "    num_steps_sampled: 926000\n",
      "    num_steps_trained: 926000\n",
      "  iterations_since_restore: 926\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.45774647887325\n",
      "    ram_util_percent: 53.79859154929576\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03945951204242679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.950242971050475\n",
      "    mean_inference_ms: 1.9521548531393031\n",
      "    mean_raw_obs_processing_ms: 2.172903767795039\n",
      "  time_since_restore: 25015.112815856934\n",
      "  time_this_iter_s: 49.70760369300842\n",
      "  time_total_s: 25015.112815856934\n",
      "  timers:\n",
      "    learn_throughput: 1322.469\n",
      "    learn_time_ms: 756.161\n",
      "    load_throughput: 42489.627\n",
      "    load_time_ms: 23.535\n",
      "    sample_throughput: 28.789\n",
      "    sample_time_ms: 34735.266\n",
      "    update_time_ms: 6.835\n",
      "  timestamp: 1635087899\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 926000\n",
      "  training_iteration: 926\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   926</td><td style=\"text-align: right;\">         25015.1</td><td style=\"text-align: right;\">926000</td><td style=\"text-align: right;\">  -3.133</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">             313.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 927000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-05-30\n",
      "  done: false\n",
      "  episode_len_mean: 311.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.1189999999999776\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3184\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.494412131416845e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.44730268319447836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005734949813342243\n",
      "          policy_loss: 0.006868832972314623\n",
      "          total_loss: 0.01458924917711152\n",
      "          vf_explained_var: 0.2271452397108078\n",
      "          vf_loss: 0.012193435099389818\n",
      "    num_agent_steps_sampled: 927000\n",
      "    num_agent_steps_trained: 927000\n",
      "    num_steps_sampled: 927000\n",
      "    num_steps_trained: 927000\n",
      "  iterations_since_restore: 927\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.04444444444445\n",
      "    ram_util_percent: 53.746666666666655\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03946440718055496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.95815026852577\n",
      "    mean_inference_ms: 1.9523105116870212\n",
      "    mean_raw_obs_processing_ms: 2.1732534113034285\n",
      "  time_since_restore: 25046.79162454605\n",
      "  time_this_iter_s: 31.67880868911743\n",
      "  time_total_s: 25046.79162454605\n",
      "  timers:\n",
      "    learn_throughput: 1329.353\n",
      "    learn_time_ms: 752.245\n",
      "    load_throughput: 41386.189\n",
      "    load_time_ms: 24.163\n",
      "    sample_throughput: 30.306\n",
      "    sample_time_ms: 32997.012\n",
      "    update_time_ms: 6.638\n",
      "  timestamp: 1635087930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 927000\n",
      "  training_iteration: 927\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   927</td><td style=\"text-align: right;\">         25046.8</td><td style=\"text-align: right;\">927000</td><td style=\"text-align: right;\">  -3.119</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">             311.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 928000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-06-00\n",
      "  done: false\n",
      "  episode_len_mean: 309.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.098399999999977\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3187\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.494412131416845e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.35393349942233826\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0058297054458217685\n",
      "          policy_loss: 0.06350647542211744\n",
      "          total_loss: 0.0700995233323839\n",
      "          vf_explained_var: 0.2143067866563797\n",
      "          vf_loss: 0.01013237618867101\n",
      "    num_agent_steps_sampled: 928000\n",
      "    num_agent_steps_trained: 928000\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 928000\n",
      "  iterations_since_restore: 928\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.40697674418605\n",
      "    ram_util_percent: 53.75116279069767\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03946792672626327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.964167224994252\n",
      "    mean_inference_ms: 1.9524242873655873\n",
      "    mean_raw_obs_processing_ms: 2.173533264171967\n",
      "  time_since_restore: 25076.79829478264\n",
      "  time_this_iter_s: 30.006670236587524\n",
      "  time_total_s: 25076.79829478264\n",
      "  timers:\n",
      "    learn_throughput: 1321.171\n",
      "    learn_time_ms: 756.904\n",
      "    load_throughput: 41168.779\n",
      "    load_time_ms: 24.29\n",
      "    sample_throughput: 30.664\n",
      "    sample_time_ms: 32611.244\n",
      "    update_time_ms: 6.898\n",
      "  timestamp: 1635087960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 928\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   928</td><td style=\"text-align: right;\">         25076.8</td><td style=\"text-align: right;\">928000</td><td style=\"text-align: right;\"> -3.0984</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">            309.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 929000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-06-34\n",
      "  done: false\n",
      "  episode_len_mean: 307.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.0730999999999784\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3191\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.494412131416845e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.300632132920954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002298599373173913\n",
      "          policy_loss: -0.012761207007699542\n",
      "          total_loss: -0.00344405182533794\n",
      "          vf_explained_var: 0.22180555760860443\n",
      "          vf_loss: 0.012323473321480883\n",
      "    num_agent_steps_sampled: 929000\n",
      "    num_agent_steps_trained: 929000\n",
      "    num_steps_sampled: 929000\n",
      "    num_steps_trained: 929000\n",
      "  iterations_since_restore: 929\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.97142857142858\n",
      "    ram_util_percent: 53.58979591836735\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039472681391954804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.972487081621512\n",
      "    mean_inference_ms: 1.952574819278456\n",
      "    mean_raw_obs_processing_ms: 2.173161794869565\n",
      "  time_since_restore: 25110.779439926147\n",
      "  time_this_iter_s: 33.98114514350891\n",
      "  time_total_s: 25110.779439926147\n",
      "  timers:\n",
      "    learn_throughput: 1319.813\n",
      "    learn_time_ms: 757.683\n",
      "    load_throughput: 43630.901\n",
      "    load_time_ms: 22.92\n",
      "    sample_throughput: 30.467\n",
      "    sample_time_ms: 32822.125\n",
      "    update_time_ms: 6.476\n",
      "  timestamp: 1635087994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 929000\n",
      "  training_iteration: 929\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   929</td><td style=\"text-align: right;\">         25110.8</td><td style=\"text-align: right;\">929000</td><td style=\"text-align: right;\"> -3.0731</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">            307.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 930000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-07-07\n",
      "  done: false\n",
      "  episode_len_mean: 304.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.0443999999999796\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3194\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.472060657084225e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2256522324350145\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0051811386708585875\n",
      "          policy_loss: 0.04941830899980333\n",
      "          total_loss: 0.055829444858762955\n",
      "          vf_explained_var: 0.36119750142097473\n",
      "          vf_loss: 0.008667657730661126\n",
      "    num_agent_steps_sampled: 930000\n",
      "    num_agent_steps_trained: 930000\n",
      "    num_steps_sampled: 930000\n",
      "    num_steps_trained: 930000\n",
      "  iterations_since_restore: 930\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.87872340425533\n",
      "    ram_util_percent: 53.71276595744682\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03947618117431802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.97879864082543\n",
      "    mean_inference_ms: 1.952684911654353\n",
      "    mean_raw_obs_processing_ms: 2.1729138515279924\n",
      "  time_since_restore: 25143.934431791306\n",
      "  time_this_iter_s: 33.15499186515808\n",
      "  time_total_s: 25143.934431791306\n",
      "  timers:\n",
      "    learn_throughput: 1308.545\n",
      "    learn_time_ms: 764.208\n",
      "    load_throughput: 44912.858\n",
      "    load_time_ms: 22.265\n",
      "    sample_throughput: 30.312\n",
      "    sample_time_ms: 32990.327\n",
      "    update_time_ms: 6.421\n",
      "  timestamp: 1635088027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 930000\n",
      "  training_iteration: 930\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   930</td><td style=\"text-align: right;\">         25143.9</td><td style=\"text-align: right;\">930000</td><td style=\"text-align: right;\"> -3.0444</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">            304.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 931000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 303.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.038499999999978\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3197\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.472060657084225e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5104250282049179\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.16178103157001383\n",
      "          policy_loss: -0.12399091728859478\n",
      "          total_loss: -0.11759431329038408\n",
      "          vf_explained_var: 0.3374548554420471\n",
      "          vf_loss: 0.011500735927580132\n",
      "    num_agent_steps_sampled: 931000\n",
      "    num_agent_steps_trained: 931000\n",
      "    num_steps_sampled: 931000\n",
      "    num_steps_trained: 931000\n",
      "  iterations_since_restore: 931\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.55106382978724\n",
      "    ram_util_percent: 53.73404255319148\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0394798592820112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.985188557845436\n",
      "    mean_inference_ms: 1.9527948069837928\n",
      "    mean_raw_obs_processing_ms: 2.1726678601338922\n",
      "  time_since_restore: 25176.913589954376\n",
      "  time_this_iter_s: 32.97915816307068\n",
      "  time_total_s: 25176.913589954376\n",
      "  timers:\n",
      "    learn_throughput: 1315.976\n",
      "    learn_time_ms: 759.892\n",
      "    load_throughput: 44892.332\n",
      "    load_time_ms: 22.276\n",
      "    sample_throughput: 30.271\n",
      "    sample_time_ms: 33035.37\n",
      "    update_time_ms: 7.015\n",
      "  timestamp: 1635088060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 931000\n",
      "  training_iteration: 931\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   931</td><td style=\"text-align: right;\">         25176.9</td><td style=\"text-align: right;\">931000</td><td style=\"text-align: right;\"> -3.0385</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">            303.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_722d3_00000:\n",
      "  agent_timesteps_total: 932000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-08-13\n",
      "  done: false\n",
      "  episode_len_mean: 303.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.031499999999978\n",
      "  episode_reward_min: -4.539999999999948\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 3201\n",
      "  experiment_id: e107d5add964470ebc331ff15020baaf\n",
      "  hostname: 7948e0475857\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1208090985626335e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.18397226201163397\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009603571202591846\n",
      "          policy_loss: 0.03925429392192099\n",
      "          total_loss: 0.04857671765817536\n",
      "          vf_explained_var: 0.2786206007003784\n",
      "          vf_loss: 0.011162138155971965\n",
      "    num_agent_steps_sampled: 932000\n",
      "    num_agent_steps_trained: 932000\n",
      "    num_steps_sampled: 932000\n",
      "    num_steps_trained: 932000\n",
      "  iterations_since_restore: 932\n",
      "  node_ip: 172.17.0.3\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.94042553191488\n",
      "    ram_util_percent: 53.68936170212766\n",
      "  pid: 43204\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039484788952620274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.99379978312002\n",
      "    mean_inference_ms: 1.9529403089823865\n",
      "    mean_raw_obs_processing_ms: 2.1723293357797013\n",
      "  time_since_restore: 25209.488146543503\n",
      "  time_this_iter_s: 32.57455658912659\n",
      "  time_total_s: 25209.488146543503\n",
      "  timers:\n",
      "    learn_throughput: 1330.318\n",
      "    learn_time_ms: 751.7\n",
      "    load_throughput: 45835.831\n",
      "    load_time_ms: 21.817\n",
      "    sample_throughput: 30.381\n",
      "    sample_time_ms: 32915.088\n",
      "    update_time_ms: 6.587\n",
      "  timestamp: 1635088093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 932000\n",
      "  training_iteration: 932\n",
      "  trial_id: 722d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.16 GiB heap, 0.0/13.58 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_08-07-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_722d3_00000</td><td>RUNNING </td><td>172.17.0.3:43204</td><td style=\"text-align: right;\">   932</td><td style=\"text-align: right;\">         25209.5</td><td style=\"text-align: right;\">932000</td><td style=\"text-align: right;\"> -3.0315</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.54</td><td style=\"text-align: right;\">            303.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO MultiTask (C3, C17) pretrained (AngelaCNN) (3 noops after placement) r: -0.01\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
