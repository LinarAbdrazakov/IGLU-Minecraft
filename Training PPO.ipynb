{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef090c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install ray torch torchvision tabulate tensorboard\n",
    "#!pip3 install 'ray[rllib]'\n",
    "#!pip3 install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder(features_dim)\n",
    "        #self.encoder.load_state_dict(\n",
    "        #    torch.load(\"Visual Autoencoder weights and models/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        #)\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C22']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 36.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 11:16:37,912\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 7c454_00000 but id 8f2a6_00000 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=16561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C22 not pretrained</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/8f2a6_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/8f2a6_00000</a><br/>\n",
       "                Run data is saved locally in <code>/src/wandb/run-20210916_111638-8f2a6_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16561)\u001b[0m 2021-09-16 11:16:43,997\tWARNING ppo.py:143 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1333.\n",
      "\u001b[2m\u001b[36m(pid=16561)\u001b[0m 2021-09-16 11:16:43,997\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=16561)\u001b[0m 2021-09-16 11:16:43,997\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16561)\u001b[0m 2021-09-16 11:16:53,758\tINFO trainable.py:109 -- Trainable.setup took 14.352 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=16561)\u001b[0m 2021-09-16 11:16:53,760\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 7998\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.16666666666666666\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 6\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.8881016077533843\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.002688916184766008\n",
      "          policy_loss: -0.005954116642955811\n",
      "          total_loss: -0.029144969095490993\n",
      "          vf_explained_var: -0.8339848518371582\n",
      "          vf_loss: 0.005152379287163043\n",
      "    num_agent_steps_sampled: 7998\n",
      "    num_agent_steps_trained: 7998\n",
      "    num_steps_sampled: 7998\n",
      "    num_steps_trained: 7998\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.8403041825095\n",
      "    ram_util_percent: 36.12623574144487\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08610847100304479\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 61.24097742806106\n",
      "    mean_inference_ms: 2.929338081644976\n",
      "    mean_raw_obs_processing_ms: 0.2351406737724374\n",
      "  time_since_restore: 184.43492460250854\n",
      "  time_this_iter_s: 184.43492460250854\n",
      "  time_total_s: 184.43492460250854\n",
      "  timers:\n",
      "    learn_throughput: 982.318\n",
      "    learn_time_ms: 8141.965\n",
      "    load_throughput: 151282.756\n",
      "    load_time_ms: 52.868\n",
      "    sample_throughput: 45.386\n",
      "    sample_time_ms: 176223.544\n",
      "    update_time_ms: 4.925\n",
      "  timestamp: 1631791198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7998\n",
      "  training_iteration: 1\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         184.435</td><td style=\"text-align: right;\">7998</td><td style=\"text-align: right;\">0.166667</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 15996\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 15\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.8810511558286604\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0069766820947797245\n",
      "          policy_loss: -0.018501796059921305\n",
      "          total_loss: 0.01548046025157135\n",
      "          vf_explained_var: -0.6140614748001099\n",
      "          vf_loss: 0.06209509945197218\n",
      "    num_agent_steps_sampled: 15996\n",
      "    num_agent_steps_trained: 15996\n",
      "    num_steps_sampled: 15996\n",
      "    num_steps_trained: 15996\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.82065217391306\n",
      "    ram_util_percent: 37.47173913043479\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08561299575575165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.89805045802474\n",
      "    mean_inference_ms: 2.8737457323357503\n",
      "    mean_raw_obs_processing_ms: 0.23775930602917134\n",
      "  time_since_restore: 248.36837530136108\n",
      "  time_this_iter_s: 63.93345069885254\n",
      "  time_total_s: 248.36837530136108\n",
      "  timers:\n",
      "    learn_throughput: 946.011\n",
      "    learn_time_ms: 8454.448\n",
      "    load_throughput: 158526.561\n",
      "    load_time_ms: 50.452\n",
      "    sample_throughput: 69.151\n",
      "    sample_time_ms: 115659.252\n",
      "    update_time_ms: 5.493\n",
      "  timestamp: 1631791262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15996\n",
      "  training_iteration: 2\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         248.368</td><td style=\"text-align: right;\">15996</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 23994\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-22-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.5238095238095238\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 21\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.866032196629432\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.007494281796975792\n",
      "          policy_loss: -0.01528315903038107\n",
      "          total_loss: 0.13063358450008014\n",
      "          vf_explained_var: -0.00601164810359478\n",
      "          vf_loss: 0.17382763715511593\n",
      "    num_agent_steps_sampled: 23994\n",
      "    num_agent_steps_trained: 23994\n",
      "    num_steps_sampled: 23994\n",
      "    num_steps_trained: 23994\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1891304347826\n",
      "    ram_util_percent: 37.59891304347827\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08497495309026494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.29402443471848\n",
      "    mean_inference_ms: 2.848722677221283\n",
      "    mean_raw_obs_processing_ms: 0.23673490140657\n",
      "  time_since_restore: 313.14793729782104\n",
      "  time_this_iter_s: 64.77956199645996\n",
      "  time_total_s: 313.14793729782104\n",
      "  timers:\n",
      "    learn_throughput: 945.157\n",
      "    learn_time_ms: 8462.085\n",
      "    load_throughput: 155425.204\n",
      "    load_time_ms: 51.459\n",
      "    sample_throughput: 83.446\n",
      "    sample_time_ms: 95846.971\n",
      "    update_time_ms: 5.47\n",
      "  timestamp: 1631791327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23994\n",
      "  training_iteration: 3\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         313.148</td><td style=\"text-align: right;\">23994</td><td style=\"text-align: right;\">-0.52381</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 31992\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-23-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -1.1333333333333333\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 30\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.8383094051832796\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.00997770332835937\n",
      "          policy_loss: -0.029522087469056088\n",
      "          total_loss: 0.28025808563275684\n",
      "          vf_explained_var: 0.2117086797952652\n",
      "          vf_loss: 0.3371654956130892\n",
      "    num_agent_steps_sampled: 31992\n",
      "    num_agent_steps_trained: 31992\n",
      "    num_steps_sampled: 31992\n",
      "    num_steps_trained: 31992\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.28265306122448\n",
      "    ram_util_percent: 37.75\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0845836312177878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.813545756970896\n",
      "    mean_inference_ms: 2.8323338636916278\n",
      "    mean_raw_obs_processing_ms: 0.23890680878594964\n",
      "  time_since_restore: 381.69353771209717\n",
      "  time_this_iter_s: 68.54560041427612\n",
      "  time_total_s: 381.69353771209717\n",
      "  timers:\n",
      "    learn_throughput: 922.475\n",
      "    learn_time_ms: 8670.153\n",
      "    load_throughput: 163942.53\n",
      "    load_time_ms: 48.785\n",
      "    sample_throughput: 92.268\n",
      "    sample_time_ms: 86682.615\n",
      "    update_time_ms: 4.937\n",
      "  timestamp: 1631791395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31992\n",
      "  training_iteration: 4\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         381.694</td><td style=\"text-align: right;\">31992</td><td style=\"text-align: right;\">-1.13333</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 39990\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-24-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -1.4102564102564104\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 39\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.826024647169216\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008174105763742681\n",
      "          policy_loss: -0.03430906725666856\n",
      "          total_loss: 0.16249907573945419\n",
      "          vf_explained_var: 0.27097663283348083\n",
      "          vf_loss: 0.2242509783955381\n",
      "    num_agent_steps_sampled: 39990\n",
      "    num_agent_steps_trained: 39990\n",
      "    num_steps_sampled: 39990\n",
      "    num_steps_trained: 39990\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.27319587628865\n",
      "    ram_util_percent: 37.87010309278351\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08426188985038455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.90860484306227\n",
      "    mean_inference_ms: 2.824735095074808\n",
      "    mean_raw_obs_processing_ms: 0.2404133927483693\n",
      "  time_since_restore: 449.4555640220642\n",
      "  time_this_iter_s: 67.76202630996704\n",
      "  time_total_s: 449.4555640220642\n",
      "  timers:\n",
      "    learn_throughput: 911.796\n",
      "    learn_time_ms: 8771.698\n",
      "    load_throughput: 171684.453\n",
      "    load_time_ms: 46.585\n",
      "    sample_throughput: 98.681\n",
      "    sample_time_ms: 81049.367\n",
      "    update_time_ms: 4.577\n",
      "  timestamp: 1631791463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39990\n",
      "  training_iteration: 5\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         449.456</td><td style=\"text-align: right;\">39990</td><td style=\"text-align: right;\">-1.41026</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 47988\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-25-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -1.1777777777777778\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 45\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.808658920052231\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008462480386543365\n",
      "          policy_loss: -0.025535488747540983\n",
      "          total_loss: 0.21682530496549862\n",
      "          vf_explained_var: 0.11320728808641434\n",
      "          vf_loss: 0.2696011338868649\n",
      "    num_agent_steps_sampled: 47988\n",
      "    num_agent_steps_trained: 47988\n",
      "    num_steps_sampled: 47988\n",
      "    num_steps_trained: 47988\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.04148936170212\n",
      "    ram_util_percent: 37.463829787234054\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08418267808200673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.417345913470974\n",
      "    mean_inference_ms: 2.821497296274125\n",
      "    mean_raw_obs_processing_ms: 0.240601518603667\n",
      "  time_since_restore: 515.566526889801\n",
      "  time_this_iter_s: 66.11096286773682\n",
      "  time_total_s: 515.566526889801\n",
      "  timers:\n",
      "    learn_throughput: 907.207\n",
      "    learn_time_ms: 8816.072\n",
      "    load_throughput: 172996.817\n",
      "    load_time_ms: 46.232\n",
      "    sample_throughput: 103.814\n",
      "    sample_time_ms: 77041.505\n",
      "    update_time_ms: 4.285\n",
      "  timestamp: 1631791529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47988\n",
      "  training_iteration: 6\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 39.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         515.567</td><td style=\"text-align: right;\">47988</td><td style=\"text-align: right;\">-1.17778</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 55986\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-26-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.8703703703703703\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 54\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.786359139155316\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.009055078653998658\n",
      "          policy_loss: -0.03028115157879168\n",
      "          total_loss: 0.13331852041495343\n",
      "          vf_explained_var: 0.2560194134712219\n",
      "          vf_loss: 0.1905577555293798\n",
      "    num_agent_steps_sampled: 55986\n",
      "    num_agent_steps_trained: 55986\n",
      "    num_steps_sampled: 55986\n",
      "    num_steps_trained: 55986\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.64545454545454\n",
      "    ram_util_percent: 37.54636363636364\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08403263105052493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.61985548473106\n",
      "    mean_inference_ms: 2.8170478716450313\n",
      "    mean_raw_obs_processing_ms: 0.2415870150299855\n",
      "  time_since_restore: 592.8983232975006\n",
      "  time_this_iter_s: 77.33179640769958\n",
      "  time_total_s: 592.8983232975006\n",
      "  timers:\n",
      "    learn_throughput: 766.671\n",
      "    learn_time_ms: 10432.12\n",
      "    load_throughput: 176311.684\n",
      "    load_time_ms: 45.363\n",
      "    sample_throughput: 107.793\n",
      "    sample_time_ms: 74197.916\n",
      "    update_time_ms: 4.164\n",
      "  timestamp: 1631791606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55986\n",
      "  training_iteration: 7\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         592.898</td><td style=\"text-align: right;\">55986</td><td style=\"text-align: right;\">-0.87037</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 63984\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.6190476190476191\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 63\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.776591469651909\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.009038241556711277\n",
      "          policy_loss: -0.02004421950488161\n",
      "          total_loss: 0.15451362164771204\n",
      "          vf_explained_var: 0.08677450567483902\n",
      "          vf_loss: 0.2014199320151801\n",
      "    num_agent_steps_sampled: 63984\n",
      "    num_agent_steps_trained: 63984\n",
      "    num_steps_sampled: 63984\n",
      "    num_steps_trained: 63984\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.81827956989247\n",
      "    ram_util_percent: 37.80860215053763\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08395627227427006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.222318393087317\n",
      "    mean_inference_ms: 2.813567543047135\n",
      "    mean_raw_obs_processing_ms: 0.24227229270521378\n",
      "  time_since_restore: 658.0495052337646\n",
      "  time_this_iter_s: 65.15118193626404\n",
      "  time_total_s: 658.0495052337646\n",
      "  timers:\n",
      "    learn_throughput: 782.31\n",
      "    learn_time_ms: 10223.568\n",
      "    load_throughput: 183383.488\n",
      "    load_time_ms: 43.614\n",
      "    sample_throughput: 111.138\n",
      "    sample_time_ms: 71964.277\n",
      "    update_time_ms: 4.076\n",
      "  timestamp: 1631791672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63984\n",
      "  training_iteration: 8\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.6/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">          658.05</td><td style=\"text-align: right;\">63984</td><td style=\"text-align: right;\">-0.619048</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 71982\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-28-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.5217391304347826\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 69\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.738396813023475\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008848986293529993\n",
      "          policy_loss: -0.019896211192732858\n",
      "          total_loss: 0.12755512626721494\n",
      "          vf_explained_var: 0.08280733972787857\n",
      "          vf_loss: 0.17395040721254504\n",
      "    num_agent_steps_sampled: 71982\n",
      "    num_agent_steps_trained: 71982\n",
      "    num_steps_sampled: 71982\n",
      "    num_steps_trained: 71982\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.53736263736263\n",
      "    ram_util_percent: 38.48461538461538\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08386734803927685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.428587586103585\n",
      "    mean_inference_ms: 2.8106045968508866\n",
      "    mean_raw_obs_processing_ms: 0.24257244041060422\n",
      "  time_since_restore: 721.5567154884338\n",
      "  time_this_iter_s: 63.50721025466919\n",
      "  time_total_s: 721.5567154884338\n",
      "  timers:\n",
      "    learn_throughput: 791.945\n",
      "    learn_time_ms: 10099.183\n",
      "    load_throughput: 184694.623\n",
      "    load_time_ms: 43.304\n",
      "    sample_throughput: 114.248\n",
      "    sample_time_ms: 70005.579\n",
      "    update_time_ms: 4.002\n",
      "  timestamp: 1631791735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71982\n",
      "  training_iteration: 9\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 45.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         721.557</td><td style=\"text-align: right;\">71982</td><td style=\"text-align: right;\">-0.521739</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 79980\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.44871794871794873\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 78\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.722803311194143\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008820954381142056\n",
      "          policy_loss: -0.032651735706034524\n",
      "          total_loss: 0.13398446832553193\n",
      "          vf_explained_var: 0.243268221616745\n",
      "          vf_loss: 0.19298214290456306\n",
      "    num_agent_steps_sampled: 79980\n",
      "    num_agent_steps_trained: 79980\n",
      "    num_steps_sampled: 79980\n",
      "    num_steps_trained: 79980\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.4696629213483\n",
      "    ram_util_percent: 38.406741573033706\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08371552466487901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.40203290268329\n",
      "    mean_inference_ms: 2.806549242871052\n",
      "    mean_raw_obs_processing_ms: 0.24287579139823448\n",
      "  time_since_restore: 783.9799389839172\n",
      "  time_this_iter_s: 62.4232234954834\n",
      "  time_total_s: 783.9799389839172\n",
      "  timers:\n",
      "    learn_throughput: 804.391\n",
      "    learn_time_ms: 9942.923\n",
      "    load_throughput: 184910.532\n",
      "    load_time_ms: 43.253\n",
      "    sample_throughput: 116.953\n",
      "    sample_time_ms: 68386.378\n",
      "    update_time_ms: 3.993\n",
      "  timestamp: 1631791798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79980\n",
      "  training_iteration: 10\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          783.98</td><td style=\"text-align: right;\">79980</td><td style=\"text-align: right;\">-0.448718</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 87978\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 87\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7351321648525935\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008737843671759003\n",
      "          policy_loss: -0.033650567164013705\n",
      "          total_loss: 0.06209255929120005\n",
      "          vf_explained_var: 0.2675935626029968\n",
      "          vf_loss: 0.12222066335880788\n",
      "    num_agent_steps_sampled: 87978\n",
      "    num_agent_steps_trained: 87978\n",
      "    num_steps_sampled: 87978\n",
      "    num_steps_trained: 87978\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.50722891566265\n",
      "    ram_util_percent: 37.79397590361445\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08353880314055652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.527128953035838\n",
      "    mean_inference_ms: 2.801648420581768\n",
      "    mean_raw_obs_processing_ms: 0.24300088577966705\n",
      "  time_since_restore: 842.4943270683289\n",
      "  time_this_iter_s: 58.51438808441162\n",
      "  time_total_s: 842.4943270683289\n",
      "  timers:\n",
      "    learn_throughput: 806.206\n",
      "    learn_time_ms: 9920.541\n",
      "    load_throughput: 191451.979\n",
      "    load_time_ms: 41.775\n",
      "    sample_throughput: 143.288\n",
      "    sample_time_ms: 55817.524\n",
      "    update_time_ms: 3.867\n",
      "  timestamp: 1631791856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87978\n",
      "  training_iteration: 11\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         842.494</td><td style=\"text-align: right;\">87978</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 95976\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-32-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.3125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: -0.17708333333333334\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 96\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7290013469675536\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.00932654607253701\n",
      "          policy_loss: -0.025135908913748558\n",
      "          total_loss: 0.11684737002416964\n",
      "          vf_explained_var: 0.18681135773658752\n",
      "          vf_loss: 0.16834063702262939\n",
      "    num_agent_steps_sampled: 95976\n",
      "    num_agent_steps_trained: 95976\n",
      "    num_steps_sampled: 95976\n",
      "    num_steps_trained: 95976\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.60347222222222\n",
      "    ram_util_percent: 37.83680555555556\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08338202866726607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.79114987494883\n",
      "    mean_inference_ms: 2.796571452316742\n",
      "    mean_raw_obs_processing_ms: 0.3025713589636134\n",
      "  time_since_restore: 942.9783570766449\n",
      "  time_this_iter_s: 100.48403000831604\n",
      "  time_total_s: 942.9783570766449\n",
      "  timers:\n",
      "    learn_throughput: 714.02\n",
      "    learn_time_ms: 11201.367\n",
      "    load_throughput: 188833.599\n",
      "    load_time_ms: 42.355\n",
      "    sample_throughput: 137.444\n",
      "    sample_time_ms: 58191.136\n",
      "    update_time_ms: 3.646\n",
      "  timestamp: 1631791957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95976\n",
      "  training_iteration: 12\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         942.978</td><td style=\"text-align: right;\">95976</td><td style=\"text-align: right;\">-0.177083</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">           996.312</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 103974\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-33-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 102\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7226495319797146\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.009664144447838358\n",
      "          policy_loss: -0.025963372660059762\n",
      "          total_loss: 0.16033497497299184\n",
      "          vf_explained_var: 0.15665513277053833\n",
      "          vf_loss: 0.21255842908010167\n",
      "    num_agent_steps_sampled: 103974\n",
      "    num_agent_steps_trained: 103974\n",
      "    num_steps_sampled: 103974\n",
      "    num_steps_trained: 103974\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.91607142857143\n",
      "    ram_util_percent: 38.42053571428572\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08337228314532506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.660769897474747\n",
      "    mean_inference_ms: 2.795143360399386\n",
      "    mean_raw_obs_processing_ms: 0.3356908480799945\n",
      "  time_since_restore: 1021.9593932628632\n",
      "  time_this_iter_s: 78.98103618621826\n",
      "  time_total_s: 1021.9593932628632\n",
      "  timers:\n",
      "    learn_throughput: 636.509\n",
      "    learn_time_ms: 12565.425\n",
      "    load_throughput: 192369.697\n",
      "    load_time_ms: 41.576\n",
      "    sample_throughput: 137.307\n",
      "    sample_time_ms: 58248.829\n",
      "    update_time_ms: 3.434\n",
      "  timestamp: 1631792036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103974\n",
      "  training_iteration: 13\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         1021.96</td><td style=\"text-align: right;\">103974</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 111972\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 111\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.710610889619397\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.009550321380593096\n",
      "          policy_loss: -0.035924607719124486\n",
      "          total_loss: 0.06735979847369655\n",
      "          vf_explained_var: 0.09175171703100204\n",
      "          vf_loss: 0.12943548189428564\n",
      "    num_agent_steps_sampled: 111972\n",
      "    num_agent_steps_trained: 111972\n",
      "    num_steps_sampled: 111972\n",
      "    num_steps_trained: 111972\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.2725663716814\n",
      "    ram_util_percent: 38.56725663716814\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08299435641431929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.094404817058752\n",
      "    mean_inference_ms: 2.783263114569298\n",
      "    mean_raw_obs_processing_ms: 0.3849781703369503\n",
      "  time_since_restore: 1100.6529433727264\n",
      "  time_this_iter_s: 78.69355010986328\n",
      "  time_total_s: 1100.6529433727264\n",
      "  timers:\n",
      "    learn_throughput: 580.202\n",
      "    learn_time_ms: 13784.86\n",
      "    load_throughput: 187769.862\n",
      "    load_time_ms: 42.595\n",
      "    sample_throughput: 137.796\n",
      "    sample_time_ms: 58042.249\n",
      "    update_time_ms: 3.505\n",
      "  timestamp: 1631792114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111972\n",
      "  training_iteration: 14\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.6/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         1100.65</td><td style=\"text-align: right;\">111972</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 119970\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-36-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 120\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6952090247984857\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.010876209339983057\n",
      "          policy_loss: -0.04096266748383641\n",
      "          total_loss: 0.12682537371192568\n",
      "          vf_explained_var: 0.18071217834949493\n",
      "          vf_loss: 0.1936525094042462\n",
      "    num_agent_steps_sampled: 119970\n",
      "    num_agent_steps_trained: 119970\n",
      "    num_steps_sampled: 119970\n",
      "    num_steps_trained: 119970\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.64479166666666\n",
      "    ram_util_percent: 38.67291666666667\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08258912491660428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.75361403891907\n",
      "    mean_inference_ms: 2.7722343286247964\n",
      "    mean_raw_obs_processing_ms: 0.4309923848998085\n",
      "  time_since_restore: 1168.5370781421661\n",
      "  time_this_iter_s: 67.8841347694397\n",
      "  time_total_s: 1168.5370781421661\n",
      "  timers:\n",
      "    learn_throughput: 584.266\n",
      "    learn_time_ms: 13688.962\n",
      "    load_throughput: 194987.863\n",
      "    load_time_ms: 41.018\n",
      "    sample_throughput: 137.536\n",
      "    sample_time_ms: 58152.069\n",
      "    update_time_ms: 3.552\n",
      "  timestamp: 1631792182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119970\n",
      "  training_iteration: 15\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         1168.54</td><td style=\"text-align: right;\">119970</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 127968\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-37-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.28\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 126\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6792934102396813\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.011083304086060893\n",
      "          policy_loss: -0.03504054083298611\n",
      "          total_loss: 0.07663222109217958\n",
      "          vf_explained_var: 0.24995848536491394\n",
      "          vf_loss: 0.13735736470697046\n",
      "    num_agent_steps_sampled: 127968\n",
      "    num_agent_steps_trained: 127968\n",
      "    num_steps_sampled: 127968\n",
      "    num_steps_trained: 127968\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.78387096774193\n",
      "    ram_util_percent: 38.78172043010753\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08248034373161749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.20656852197887\n",
      "    mean_inference_ms: 2.770811573252997\n",
      "    mean_raw_obs_processing_ms: 0.45981032704362484\n",
      "  time_since_restore: 1233.3575127124786\n",
      "  time_this_iter_s: 64.8204345703125\n",
      "  time_total_s: 1233.3575127124786\n",
      "  timers:\n",
      "    learn_throughput: 586.269\n",
      "    learn_time_ms: 13642.208\n",
      "    load_throughput: 194490.224\n",
      "    load_time_ms: 41.123\n",
      "    sample_throughput: 137.73\n",
      "    sample_time_ms: 58070.06\n",
      "    update_time_ms: 3.594\n",
      "  timestamp: 1631792247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127968\n",
      "  training_iteration: 16\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         1233.36</td><td style=\"text-align: right;\">127968</td><td style=\"text-align: right;\">    0.28</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 135966\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.65\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 135\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6749989978728754\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.010698700096124182\n",
      "          policy_loss: -0.034340334133875945\n",
      "          total_loss: 0.19364940883511658\n",
      "          vf_explained_var: 0.23379212617874146\n",
      "          vf_loss: 0.25366986276161285\n",
      "    num_agent_steps_sampled: 135966\n",
      "    num_agent_steps_trained: 135966\n",
      "    num_steps_sampled: 135966\n",
      "    num_steps_trained: 135966\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.29887640449438\n",
      "    ram_util_percent: 38.657303370786515\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0821925496561631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.528380486335415\n",
      "    mean_inference_ms: 2.76260741560867\n",
      "    mean_raw_obs_processing_ms: 0.4996866525849416\n",
      "  time_since_restore: 1295.5395469665527\n",
      "  time_this_iter_s: 62.1820342540741\n",
      "  time_total_s: 1295.5395469665527\n",
      "  timers:\n",
      "    learn_throughput: 641.128\n",
      "    learn_time_ms: 12474.889\n",
      "    load_throughput: 196227.331\n",
      "    load_time_ms: 40.759\n",
      "    sample_throughput: 138.558\n",
      "    sample_time_ms: 57722.987\n",
      "    update_time_ms: 3.619\n",
      "  timestamp: 1631792310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135966\n",
      "  training_iteration: 17\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         1295.54</td><td style=\"text-align: right;\">135966</td><td style=\"text-align: right;\">    0.65</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 143964\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-39-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.72\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 144\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6620880526881066\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.011162646149588108\n",
      "          policy_loss: -0.039232920260438994\n",
      "          total_loss: 0.09086696411913602\n",
      "          vf_explained_var: 0.1298505812883377\n",
      "          vf_loss: 0.15560449967749157\n",
      "    num_agent_steps_sampled: 143964\n",
      "    num_agent_steps_trained: 143964\n",
      "    num_steps_sampled: 143964\n",
      "    num_steps_trained: 143964\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.89425287356323\n",
      "    ram_util_percent: 38.48850574712644\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08185610300343173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.99482831644865\n",
      "    mean_inference_ms: 2.7520988694421824\n",
      "    mean_raw_obs_processing_ms: 0.537528741536875\n",
      "  time_since_restore: 1357.1198580265045\n",
      "  time_this_iter_s: 61.58031105995178\n",
      "  time_total_s: 1357.1198580265045\n",
      "  timers:\n",
      "    learn_throughput: 642.831\n",
      "    learn_time_ms: 12441.834\n",
      "    load_throughput: 193015.543\n",
      "    load_time_ms: 41.437\n",
      "    sample_throughput: 139.342\n",
      "    sample_time_ms: 57398.372\n",
      "    update_time_ms: 3.585\n",
      "  timestamp: 1631792371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143964\n",
      "  training_iteration: 18\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         1357.12</td><td style=\"text-align: right;\">143964</td><td style=\"text-align: right;\">    0.72</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 151962\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-40-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.76\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 150\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6688474650024085\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.010640450947400695\n",
      "          policy_loss: -0.03395673263197144\n",
      "          total_loss: 0.15609483885108144\n",
      "          vf_explained_var: 0.15117712318897247\n",
      "          vf_loss: 0.215676001300134\n",
      "    num_agent_steps_sampled: 151962\n",
      "    num_agent_steps_trained: 151962\n",
      "    num_steps_sampled: 151962\n",
      "    num_steps_trained: 151962\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.46483516483518\n",
      "    ram_util_percent: 38.49560439560439\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08170412045861991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.719470649463165\n",
      "    mean_inference_ms: 2.747923010499774\n",
      "    mean_raw_obs_processing_ms: 0.5612724795859713\n",
      "  time_since_restore: 1420.5556366443634\n",
      "  time_this_iter_s: 63.43577861785889\n",
      "  time_total_s: 1420.5556366443634\n",
      "  timers:\n",
      "    learn_throughput: 645.297\n",
      "    learn_time_ms: 12394.3\n",
      "    load_throughput: 188240.845\n",
      "    load_time_ms: 42.488\n",
      "    sample_throughput: 139.248\n",
      "    sample_time_ms: 57437.268\n",
      "    update_time_ms: 3.644\n",
      "  timestamp: 1631792435\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151962\n",
      "  training_iteration: 19\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         1420.56</td><td style=\"text-align: right;\">151962</td><td style=\"text-align: right;\">    0.76</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 159960\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-41-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.79\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 159\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.646208614431402\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.011744973155672047\n",
      "          policy_loss: -0.031779992810740905\n",
      "          total_loss: 0.2207532058780392\n",
      "          vf_explained_var: 0.2052232176065445\n",
      "          vf_loss: 0.2778207877821099\n",
      "    num_agent_steps_sampled: 159960\n",
      "    num_agent_steps_trained: 159960\n",
      "    num_steps_sampled: 159960\n",
      "    num_steps_trained: 159960\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96521739130435\n",
      "    ram_util_percent: 38.49130434782608\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08139939034120339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.35912978764842\n",
      "    mean_inference_ms: 2.7393912730132763\n",
      "    mean_raw_obs_processing_ms: 0.5949348319638649\n",
      "  time_since_restore: 1485.3464937210083\n",
      "  time_this_iter_s: 64.7908570766449\n",
      "  time_total_s: 1485.3464937210083\n",
      "  timers:\n",
      "    learn_throughput: 645.658\n",
      "    learn_time_ms: 12387.372\n",
      "    load_throughput: 189842.717\n",
      "    load_time_ms: 42.13\n",
      "    sample_throughput: 138.657\n",
      "    sample_time_ms: 57681.913\n",
      "    update_time_ms: 3.593\n",
      "  timestamp: 1631792499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159960\n",
      "  training_iteration: 20\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         1485.35</td><td style=\"text-align: right;\">159960</td><td style=\"text-align: right;\">    0.79</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 167958\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.84\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 168\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.66153879832196\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01070887426503301\n",
      "          policy_loss: -0.03491573014687146\n",
      "          total_loss: 0.0739468068595455\n",
      "          vf_explained_var: 0.10461995750665665\n",
      "          vf_loss: 0.1344070377807203\n",
      "    num_agent_steps_sampled: 167958\n",
      "    num_agent_steps_trained: 167958\n",
      "    num_steps_sampled: 167958\n",
      "    num_steps_trained: 167958\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.99780219780219\n",
      "    ram_util_percent: 38.45164835164835\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08109256437217878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.059126857308485\n",
      "    mean_inference_ms: 2.730522121197587\n",
      "    mean_raw_obs_processing_ms: 0.6270371607444393\n",
      "  time_since_restore: 1548.5594985485077\n",
      "  time_this_iter_s: 63.21300482749939\n",
      "  time_total_s: 1548.5594985485077\n",
      "  timers:\n",
      "    learn_throughput: 642.742\n",
      "    learn_time_ms: 12443.555\n",
      "    load_throughput: 190458.269\n",
      "    load_time_ms: 41.993\n",
      "    sample_throughput: 137.67\n",
      "    sample_time_ms: 58095.501\n",
      "    update_time_ms: 3.543\n",
      "  timestamp: 1631792563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167958\n",
      "  training_iteration: 21\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         1548.56</td><td style=\"text-align: right;\">167958</td><td style=\"text-align: right;\">    0.84</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 175956\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-43-46\n",
      "  done: false\n",
      "  episode_len_mean: 996.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.91\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 174\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.640846380623438\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0112574302182743\n",
      "          policy_loss: -0.03175792544279047\n",
      "          total_loss: 0.02230186160673858\n",
      "          vf_explained_var: 0.15102416276931763\n",
      "          vf_loss: 0.07934250728989531\n",
      "    num_agent_steps_sampled: 175956\n",
      "    num_agent_steps_trained: 175956\n",
      "    num_steps_sampled: 175956\n",
      "    num_steps_trained: 175956\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.75777777777778\n",
      "    ram_util_percent: 38.44111111111111\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08096054323597966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.894952873849558\n",
      "    mean_inference_ms: 2.727076257316112\n",
      "    mean_raw_obs_processing_ms: 0.6475048002287517\n",
      "  time_since_restore: 1612.03710603714\n",
      "  time_this_iter_s: 63.4776074886322\n",
      "  time_total_s: 1612.03710603714\n",
      "  timers:\n",
      "    learn_throughput: 719.371\n",
      "    learn_time_ms: 11118.053\n",
      "    load_throughput: 197295.895\n",
      "    load_time_ms: 40.538\n",
      "    sample_throughput: 143.536\n",
      "    sample_time_ms: 55721.277\n",
      "    update_time_ms: 3.519\n",
      "  timestamp: 1631792626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175956\n",
      "  training_iteration: 22\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         1612.04</td><td style=\"text-align: right;\">175956</td><td style=\"text-align: right;\">    0.91</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 183954\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-45-13\n",
      "  done: false\n",
      "  episode_len_mean: 991.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.99\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 183\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5951980365219938\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.012253395637303693\n",
      "          policy_loss: -0.03897907147384299\n",
      "          total_loss: 0.018147120102539018\n",
      "          vf_explained_var: 0.2999287545681\n",
      "          vf_loss: 0.08185283208531277\n",
      "    num_agent_steps_sampled: 183954\n",
      "    num_agent_steps_trained: 183954\n",
      "    num_steps_sampled: 183954\n",
      "    num_steps_trained: 183954\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.84471544715447\n",
      "    ram_util_percent: 38.552845528455286\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08076533459195645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.689088338225112\n",
      "    mean_inference_ms: 2.720499381668108\n",
      "    mean_raw_obs_processing_ms: 0.7030886821321558\n",
      "  time_since_restore: 1698.332267522812\n",
      "  time_this_iter_s: 86.295161485672\n",
      "  time_total_s: 1698.332267522812\n",
      "  timers:\n",
      "    learn_throughput: 822.556\n",
      "    learn_time_ms: 9723.352\n",
      "    load_throughput: 201378.923\n",
      "    load_time_ms: 39.716\n",
      "    sample_throughput: 138.262\n",
      "    sample_time_ms: 57846.754\n",
      "    update_time_ms: 3.611\n",
      "  timestamp: 1631792713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183954\n",
      "  training_iteration: 23\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         1698.33</td><td style=\"text-align: right;\">183954</td><td style=\"text-align: right;\">    0.99</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            991.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 191952\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-46-16\n",
      "  done: false\n",
      "  episode_len_mean: 994.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 192\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.59551025846953\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.012242155842591966\n",
      "          policy_loss: -0.04673371776858325\n",
      "          total_loss: 0.04530597931845615\n",
      "          vf_explained_var: 0.2936638295650482\n",
      "          vf_loss: 0.1167705833933176\n",
      "    num_agent_steps_sampled: 191952\n",
      "    num_agent_steps_trained: 191952\n",
      "    num_steps_sampled: 191952\n",
      "    num_steps_trained: 191952\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.74505494505495\n",
      "    ram_util_percent: 38.88351648351647\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08058663725236338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.509427863152162\n",
      "    mean_inference_ms: 2.7151516741822865\n",
      "    mean_raw_obs_processing_ms: 0.7262145501119827\n",
      "  time_since_restore: 1761.67999958992\n",
      "  time_this_iter_s: 63.347732067108154\n",
      "  time_total_s: 1761.67999958992\n",
      "  timers:\n",
      "    learn_throughput: 951.12\n",
      "    learn_time_ms: 8409.031\n",
      "    load_throughput: 207056.704\n",
      "    load_time_ms: 38.627\n",
      "    sample_throughput: 138.789\n",
      "    sample_time_ms: 57626.838\n",
      "    update_time_ms: 3.791\n",
      "  timestamp: 1631792776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191952\n",
      "  training_iteration: 24\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         1761.68</td><td style=\"text-align: right;\">191952</td><td style=\"text-align: right;\">    1.05</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 199950\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-47-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 198\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.621447112739727\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.012827699700947902\n",
      "          policy_loss: -0.040775396425517335\n",
      "          total_loss: 0.032160328888905146\n",
      "          vf_explained_var: 0.19416747987270355\n",
      "          vf_loss: 0.09786742551913184\n",
      "    num_agent_steps_sampled: 199950\n",
      "    num_agent_steps_trained: 199950\n",
      "    num_steps_sampled: 199950\n",
      "    num_steps_trained: 199950\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.70681818181816\n",
      "    ram_util_percent: 39.03409090909091\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08045823145239905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.384550305169167\n",
      "    mean_inference_ms: 2.709321904549156\n",
      "    mean_raw_obs_processing_ms: 0.7226527938580102\n",
      "  time_since_restore: 1823.408900976181\n",
      "  time_this_iter_s: 61.728901386260986\n",
      "  time_total_s: 1823.408900976181\n",
      "  timers:\n",
      "    learn_throughput: 952.079\n",
      "    learn_time_ms: 8400.559\n",
      "    load_throughput: 197642.64\n",
      "    load_time_ms: 40.467\n",
      "    sample_throughput: 140.272\n",
      "    sample_time_ms: 57017.734\n",
      "    update_time_ms: 3.766\n",
      "  timestamp: 1631792838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199950\n",
      "  training_iteration: 25\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         1823.41</td><td style=\"text-align: right;\">199950</td><td style=\"text-align: right;\">    1.03</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 207948\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 207\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.595196802641756\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.013876440128516864\n",
      "          policy_loss: -0.04460880148414803\n",
      "          total_loss: 0.019621311918261553\n",
      "          vf_explained_var: 0.21758291125297546\n",
      "          vf_loss: 0.08879443628166461\n",
      "    num_agent_steps_sampled: 207948\n",
      "    num_agent_steps_trained: 207948\n",
      "    num_steps_sampled: 207948\n",
      "    num_steps_trained: 207948\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.32954545454545\n",
      "    ram_util_percent: 39.08863636363637\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08031326992443617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.245225922437285\n",
      "    mean_inference_ms: 2.7076002069494725\n",
      "    mean_raw_obs_processing_ms: 0.7220987186124307\n",
      "  time_since_restore: 1885.2017073631287\n",
      "  time_this_iter_s: 61.79280638694763\n",
      "  time_total_s: 1885.2017073631287\n",
      "  timers:\n",
      "    learn_throughput: 957.699\n",
      "    learn_time_ms: 8351.268\n",
      "    load_throughput: 192150.422\n",
      "    load_time_ms: 41.624\n",
      "    sample_throughput: 140.904\n",
      "    sample_time_ms: 56762.137\n",
      "    update_time_ms: 3.81\n",
      "  timestamp: 1631792900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207948\n",
      "  training_iteration: 26\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">          1885.2</td><td style=\"text-align: right;\">207948</td><td style=\"text-align: right;\">    1.03</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 215946\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-49-22\n",
      "  done: false\n",
      "  episode_len_mean: 995.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.1\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 216\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.574108123266569\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01414900815840626\n",
      "          policy_loss: -0.03546152598955619\n",
      "          total_loss: 0.0385273184376939\n",
      "          vf_explained_var: 0.12288613617420197\n",
      "          vf_loss: 0.09831502440320428\n",
      "    num_agent_steps_sampled: 215946\n",
      "    num_agent_steps_trained: 215946\n",
      "    num_steps_sampled: 215946\n",
      "    num_steps_trained: 215946\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.28876404494382\n",
      "    ram_util_percent: 39.155056179775286\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08017155223744338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.102306659824336\n",
      "    mean_inference_ms: 2.7030115063935245\n",
      "    mean_raw_obs_processing_ms: 0.7227740500226649\n",
      "  time_since_restore: 1947.3275334835052\n",
      "  time_this_iter_s: 62.12582612037659\n",
      "  time_total_s: 1947.3275334835052\n",
      "  timers:\n",
      "    learn_throughput: 963.129\n",
      "    learn_time_ms: 8304.187\n",
      "    load_throughput: 183763.993\n",
      "    load_time_ms: 43.523\n",
      "    sample_throughput: 140.809\n",
      "    sample_time_ms: 56800.291\n",
      "    update_time_ms: 3.768\n",
      "  timestamp: 1631792962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215946\n",
      "  training_iteration: 27\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         1947.33</td><td style=\"text-align: right;\">215946</td><td style=\"text-align: right;\">     1.1</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 223944\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-50-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.12\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 222\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.557285665953031\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.011574813922962923\n",
      "          policy_loss: -0.03287142490797866\n",
      "          total_loss: 0.016256824955945052\n",
      "          vf_explained_var: 0.13122741878032684\n",
      "          vf_loss: 0.07354362520128889\n",
      "    num_agent_steps_sampled: 223944\n",
      "    num_agent_steps_trained: 223944\n",
      "    num_steps_sampled: 223944\n",
      "    num_steps_trained: 223944\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.40689655172415\n",
      "    ram_util_percent: 39.19195402298851\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08006839686508144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.997857557091653\n",
      "    mean_inference_ms: 2.697235201412436\n",
      "    mean_raw_obs_processing_ms: 0.7230516672330276\n",
      "  time_since_restore: 2008.621554851532\n",
      "  time_this_iter_s: 61.29402136802673\n",
      "  time_total_s: 2008.621554851532\n",
      "  timers:\n",
      "    learn_throughput: 966.752\n",
      "    learn_time_ms: 8273.06\n",
      "    load_throughput: 183159.78\n",
      "    load_time_ms: 43.667\n",
      "    sample_throughput: 140.805\n",
      "    sample_time_ms: 56802.048\n",
      "    update_time_ms: 3.756\n",
      "  timestamp: 1631793023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223944\n",
      "  training_iteration: 28\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         2008.62</td><td style=\"text-align: right;\">223944</td><td style=\"text-align: right;\">    1.12</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 231942\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-51-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.13\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 231\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5339522018227525\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014663932680889514\n",
      "          policy_loss: -0.04794177598450133\n",
      "          total_loss: -0.016420470237211195\n",
      "          vf_explained_var: 0.35908690094947815\n",
      "          vf_loss: 0.055394434041508865\n",
      "    num_agent_steps_sampled: 231942\n",
      "    num_agent_steps_trained: 231942\n",
      "    num_steps_sampled: 231942\n",
      "    num_steps_trained: 231942\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.9465909090909\n",
      "    ram_util_percent: 39.17159090909092\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07995879886902121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.88420379170947\n",
      "    mean_inference_ms: 2.6955553539193047\n",
      "    mean_raw_obs_processing_ms: 0.7266457861181536\n",
      "  time_since_restore: 2070.1660466194153\n",
      "  time_this_iter_s: 61.5444917678833\n",
      "  time_total_s: 2070.1660466194153\n",
      "  timers:\n",
      "    learn_throughput: 971.572\n",
      "    learn_time_ms: 8232.022\n",
      "    load_throughput: 189267.858\n",
      "    load_time_ms: 42.258\n",
      "    sample_throughput: 141.17\n",
      "    sample_time_ms: 56655.022\n",
      "    update_time_ms: 3.67\n",
      "  timestamp: 1631793085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231942\n",
      "  training_iteration: 29\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         2070.17</td><td style=\"text-align: right;\">231942</td><td style=\"text-align: right;\">    1.13</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 239940\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-52-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 240\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.586463124521317\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01366295135804194\n",
      "          policy_loss: -0.04687954148337726\n",
      "          total_loss: -0.003363801510904425\n",
      "          vf_explained_var: 0.16269417107105255\n",
      "          vf_loss: 0.0680140752517443\n",
      "    num_agent_steps_sampled: 239940\n",
      "    num_agent_steps_trained: 239940\n",
      "    num_steps_sampled: 239940\n",
      "    num_steps_trained: 239940\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.42528735632185\n",
      "    ram_util_percent: 39.12758620689657\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07986294734608602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.770311963663936\n",
      "    mean_inference_ms: 2.691814782831064\n",
      "    mean_raw_obs_processing_ms: 0.730601799550696\n",
      "  time_since_restore: 2131.0684735774994\n",
      "  time_this_iter_s: 60.902426958084106\n",
      "  time_total_s: 2131.0684735774994\n",
      "  timers:\n",
      "    learn_throughput: 975.492\n",
      "    learn_time_ms: 8198.939\n",
      "    load_throughput: 187215.096\n",
      "    load_time_ms: 42.721\n",
      "    sample_throughput: 142.066\n",
      "    sample_time_ms: 56297.877\n",
      "    update_time_ms: 3.646\n",
      "  timestamp: 1631793146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239940\n",
      "  training_iteration: 30\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         2131.07</td><td style=\"text-align: right;\">239940</td><td style=\"text-align: right;\">    1.24</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 247938\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-53-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 246\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5571753712110623\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.013447373668381349\n",
      "          policy_loss: -0.041440258107037956\n",
      "          total_loss: -0.0074527545271301145\n",
      "          vf_explained_var: 0.13404233753681183\n",
      "          vf_loss: 0.058214519909014724\n",
      "    num_agent_steps_sampled: 247938\n",
      "    num_agent_steps_trained: 247938\n",
      "    num_steps_sampled: 247938\n",
      "    num_steps_trained: 247938\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.49883720930232\n",
      "    ram_util_percent: 39.088372093023246\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07979009835499279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.685601582325305\n",
      "    mean_inference_ms: 2.686894938863794\n",
      "    mean_raw_obs_processing_ms: 0.732958668797381\n",
      "  time_since_restore: 2191.4570960998535\n",
      "  time_this_iter_s: 60.388622522354126\n",
      "  time_total_s: 2191.4570960998535\n",
      "  timers:\n",
      "    learn_throughput: 978.936\n",
      "    learn_time_ms: 8170.098\n",
      "    load_throughput: 185661.805\n",
      "    load_time_ms: 43.078\n",
      "    sample_throughput: 142.711\n",
      "    sample_time_ms: 56043.307\n",
      "    update_time_ms: 3.673\n",
      "  timestamp: 1631793206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247938\n",
      "  training_iteration: 31\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         2191.46</td><td style=\"text-align: right;\">247938</td><td style=\"text-align: right;\">     1.3</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 255936\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-54-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 255\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5551416215076244\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014135348108204689\n",
      "          policy_loss: -0.04820424225181341\n",
      "          total_loss: -0.00684582963786138\n",
      "          vf_explained_var: 0.32279059290885925\n",
      "          vf_loss: 0.06549629360088147\n",
      "    num_agent_steps_sampled: 255936\n",
      "    num_agent_steps_trained: 255936\n",
      "    num_steps_sampled: 255936\n",
      "    num_steps_trained: 255936\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.05176470588236\n",
      "    ram_util_percent: 39.0435294117647\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07970309469411928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.593404083665675\n",
      "    mean_inference_ms: 2.6859205723604096\n",
      "    mean_raw_obs_processing_ms: 0.7389714726539398\n",
      "  time_since_restore: 2251.397445201874\n",
      "  time_this_iter_s: 59.940349102020264\n",
      "  time_total_s: 2251.397445201874\n",
      "  timers:\n",
      "    learn_throughput: 981.226\n",
      "    learn_time_ms: 8151.025\n",
      "    load_throughput: 185439.197\n",
      "    load_time_ms: 43.13\n",
      "    sample_throughput: 143.571\n",
      "    sample_time_ms: 55707.451\n",
      "    update_time_ms: 3.621\n",
      "  timestamp: 1631793266\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255936\n",
      "  training_iteration: 32\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">          2251.4</td><td style=\"text-align: right;\">255936</td><td style=\"text-align: right;\">     1.4</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 263934\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-55-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 264\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.556210273568348\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014527659315608286\n",
      "          policy_loss: -0.036297927643623086\n",
      "          total_loss: -0.00791272993364762\n",
      "          vf_explained_var: 0.1879570186138153\n",
      "          vf_loss: 0.052494532938781244\n",
      "    num_agent_steps_sampled: 263934\n",
      "    num_agent_steps_trained: 263934\n",
      "    num_steps_sampled: 263934\n",
      "    num_steps_trained: 263934\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.77176470588235\n",
      "    ram_util_percent: 39.019999999999996\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07961395326508958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.492884885577865\n",
      "    mean_inference_ms: 2.682293101197424\n",
      "    mean_raw_obs_processing_ms: 0.7449431062707188\n",
      "  time_since_restore: 2310.9676461219788\n",
      "  time_this_iter_s: 59.57020092010498\n",
      "  time_total_s: 2310.9676461219788\n",
      "  timers:\n",
      "    learn_throughput: 978.795\n",
      "    learn_time_ms: 8171.27\n",
      "    load_throughput: 183433.902\n",
      "    load_time_ms: 43.602\n",
      "    sample_throughput: 150.868\n",
      "    sample_time_ms: 53013.356\n",
      "    update_time_ms: 3.621\n",
      "  timestamp: 1631793326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263934\n",
      "  training_iteration: 33\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 43.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         2310.97</td><td style=\"text-align: right;\">263934</td><td style=\"text-align: right;\">    1.38</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 271932\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 990.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 273\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.57078369176516\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.013929636927755258\n",
      "          policy_loss: -0.04249770745700125\n",
      "          total_loss: -0.00852261502996728\n",
      "          vf_explained_var: 0.2412363737821579\n",
      "          vf_loss: 0.0582899660440988\n",
      "    num_agent_steps_sampled: 271932\n",
      "    num_agent_steps_trained: 271932\n",
      "    num_steps_sampled: 271932\n",
      "    num_steps_trained: 271932\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.32767857142858\n",
      "    ram_util_percent: 38.73392857142857\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07952534596908958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.40133991299892\n",
      "    mean_inference_ms: 2.677963851632458\n",
      "    mean_raw_obs_processing_ms: 0.7695572499891118\n",
      "  time_since_restore: 2389.41277885437\n",
      "  time_this_iter_s: 78.44513273239136\n",
      "  time_total_s: 2389.41277885437\n",
      "  timers:\n",
      "    learn_throughput: 978.816\n",
      "    learn_time_ms: 8171.094\n",
      "    load_throughput: 183686.111\n",
      "    load_time_ms: 43.542\n",
      "    sample_throughput: 146.687\n",
      "    sample_time_ms: 54524.154\n",
      "    update_time_ms: 3.359\n",
      "  timestamp: 1631793404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271932\n",
      "  training_iteration: 34\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         2389.41</td><td style=\"text-align: right;\">271932</td><td style=\"text-align: right;\">    1.37</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            990.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 279930\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-57-44\n",
      "  done: false\n",
      "  episode_len_mean: 993.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 279\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5655304588297363\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01430842027414017\n",
      "          policy_loss: -0.04571282756865345\n",
      "          total_loss: -0.02984971114764771\n",
      "          vf_explained_var: 0.03619702532887459\n",
      "          vf_loss: 0.04008757855852074\n",
      "    num_agent_steps_sampled: 279930\n",
      "    num_agent_steps_trained: 279930\n",
      "    num_steps_sampled: 279930\n",
      "    num_steps_trained: 279930\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.68235294117648\n",
      "    ram_util_percent: 38.59058823529413\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0794439446597754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.333139332758826\n",
      "    mean_inference_ms: 2.675901212048533\n",
      "    mean_raw_obs_processing_ms: 0.7727123706253939\n",
      "  time_since_restore: 2448.812593460083\n",
      "  time_this_iter_s: 59.39981460571289\n",
      "  time_total_s: 2448.812593460083\n",
      "  timers:\n",
      "    learn_throughput: 977.338\n",
      "    learn_time_ms: 8183.453\n",
      "    load_throughput: 185763.486\n",
      "    load_time_ms: 43.055\n",
      "    sample_throughput: 147.351\n",
      "    sample_time_ms: 54278.725\n",
      "    update_time_ms: 3.319\n",
      "  timestamp: 1631793464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279930\n",
      "  training_iteration: 35\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.6/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         2448.81</td><td style=\"text-align: right;\">279930</td><td style=\"text-align: right;\">     1.4</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            993.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 287928\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-58-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 288\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5964004716565534\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.012853221862939142\n",
      "          policy_loss: -0.03980504870615018\n",
      "          total_loss: -0.02366230064622497\n",
      "          vf_explained_var: -0.09410770982503891\n",
      "          vf_loss: 0.040821431010119084\n",
      "    num_agent_steps_sampled: 287928\n",
      "    num_agent_steps_trained: 287928\n",
      "    num_steps_sampled: 287928\n",
      "    num_steps_trained: 287928\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.12558139534885\n",
      "    ram_util_percent: 38.674418604651166\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0793560081518273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.237256978607583\n",
      "    mean_inference_ms: 2.6720354452395547\n",
      "    mean_raw_obs_processing_ms: 0.7714982934747833\n",
      "  time_since_restore: 2509.2748680114746\n",
      "  time_this_iter_s: 60.4622745513916\n",
      "  time_total_s: 2509.2748680114746\n",
      "  timers:\n",
      "    learn_throughput: 974.957\n",
      "    learn_time_ms: 8203.436\n",
      "    load_throughput: 195807.437\n",
      "    load_time_ms: 40.846\n",
      "    sample_throughput: 147.762\n",
      "    sample_time_ms: 54127.648\n",
      "    update_time_ms: 3.306\n",
      "  timestamp: 1631793524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287928\n",
      "  training_iteration: 36\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.6/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         2509.27</td><td style=\"text-align: right;\">287928</td><td style=\"text-align: right;\">     1.4</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 295926\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_11-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 297\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5679651652612994\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01624336108746099\n",
      "          policy_loss: -0.03983656619224817\n",
      "          total_loss: -0.02991190986627693\n",
      "          vf_explained_var: 0.09360939264297485\n",
      "          vf_loss: 0.033979971643902804\n",
      "    num_agent_steps_sampled: 295926\n",
      "    num_agent_steps_trained: 295926\n",
      "    num_steps_sampled: 295926\n",
      "    num_steps_trained: 295926\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.68255813953488\n",
      "    ram_util_percent: 38.7453488372093\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07929303910123761\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.149188650364422\n",
      "    mean_inference_ms: 2.667839101444165\n",
      "    mean_raw_obs_processing_ms: 0.7705799916881731\n",
      "  time_since_restore: 2568.941365003586\n",
      "  time_this_iter_s: 59.666496992111206\n",
      "  time_total_s: 2568.941365003586\n",
      "  timers:\n",
      "    learn_throughput: 970.828\n",
      "    learn_time_ms: 8238.332\n",
      "    load_throughput: 202560.739\n",
      "    load_time_ms: 39.484\n",
      "    sample_throughput: 148.529\n",
      "    sample_time_ms: 53848.222\n",
      "    update_time_ms: 3.726\n",
      "  timestamp: 1631793584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295926\n",
      "  training_iteration: 37\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         2568.94</td><td style=\"text-align: right;\">295926</td><td style=\"text-align: right;\">    1.41</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 303924\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 303\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5498534576867216\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015431217330787875\n",
      "          policy_loss: -0.04313082046085789\n",
      "          total_loss: -0.029662701826021876\n",
      "          vf_explained_var: 0.07323203235864639\n",
      "          vf_loss: 0.03742352997983045\n",
      "    num_agent_steps_sampled: 303924\n",
      "    num_agent_steps_trained: 303924\n",
      "    num_steps_sampled: 303924\n",
      "    num_steps_trained: 303924\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.944578313253\n",
      "    ram_util_percent: 38.86506024096385\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0792378257399314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.087275657053674\n",
      "    mean_inference_ms: 2.6658281603242084\n",
      "    mean_raw_obs_processing_ms: 0.7716682422240743\n",
      "  time_since_restore: 2627.5788764953613\n",
      "  time_this_iter_s: 58.63751149177551\n",
      "  time_total_s: 2627.5788764953613\n",
      "  timers:\n",
      "    learn_throughput: 968.735\n",
      "    learn_time_ms: 8256.13\n",
      "    load_throughput: 202798.057\n",
      "    load_time_ms: 39.438\n",
      "    sample_throughput: 149.314\n",
      "    sample_time_ms: 53564.961\n",
      "    update_time_ms: 3.77\n",
      "  timestamp: 1631793642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303924\n",
      "  training_iteration: 38\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         2627.58</td><td style=\"text-align: right;\">303924</td><td style=\"text-align: right;\">    1.38</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 311922\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-01-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 312\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.535438863436381\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015114705404629974\n",
      "          policy_loss: -0.04348464724157126\n",
      "          total_loss: -0.02977239047527634\n",
      "          vf_explained_var: 0.18281039595603943\n",
      "          vf_loss: 0.03755517336859127\n",
      "    num_agent_steps_sampled: 311922\n",
      "    num_agent_steps_trained: 311922\n",
      "    num_steps_sampled: 311922\n",
      "    num_steps_trained: 311922\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.95476190476191\n",
      "    ram_util_percent: 38.842857142857135\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07916374924361361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.001204146977155\n",
      "    mean_inference_ms: 2.662064153695634\n",
      "    mean_raw_obs_processing_ms: 0.7723607636774007\n",
      "  time_since_restore: 2685.9965360164642\n",
      "  time_this_iter_s: 58.417659521102905\n",
      "  time_total_s: 2685.9965360164642\n",
      "  timers:\n",
      "    learn_throughput: 965.51\n",
      "    learn_time_ms: 8283.703\n",
      "    load_throughput: 204842.194\n",
      "    load_time_ms: 39.045\n",
      "    sample_throughput: 150.268\n",
      "    sample_time_ms: 53224.79\n",
      "    update_time_ms: 3.974\n",
      "  timestamp: 1631793701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311922\n",
      "  training_iteration: 39\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">            2686</td><td style=\"text-align: right;\">311922</td><td style=\"text-align: right;\">    1.39</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 319920\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-02-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 321\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.469206952792342\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015194439552444459\n",
      "          policy_loss: -0.049896796860842293\n",
      "          total_loss: -0.020444601712127527\n",
      "          vf_explained_var: 0.1486934870481491\n",
      "          vf_loss: 0.05262482107212637\n",
      "    num_agent_steps_sampled: 319920\n",
      "    num_agent_steps_trained: 319920\n",
      "    num_steps_sampled: 319920\n",
      "    num_steps_trained: 319920\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.73882352941176\n",
      "    ram_util_percent: 38.81647058823529\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07909898258383094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.922948722811395\n",
      "    mean_inference_ms: 2.657996211950889\n",
      "    mean_raw_obs_processing_ms: 0.7731424989793407\n",
      "  time_since_restore: 2745.7627568244934\n",
      "  time_this_iter_s: 59.766220808029175\n",
      "  time_total_s: 2745.7627568244934\n",
      "  timers:\n",
      "    learn_throughput: 964.349\n",
      "    learn_time_ms: 8293.676\n",
      "    load_throughput: 210932.983\n",
      "    load_time_ms: 37.917\n",
      "    sample_throughput: 150.618\n",
      "    sample_time_ms: 53101.208\n",
      "    update_time_ms: 3.998\n",
      "  timestamp: 1631793761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319920\n",
      "  training_iteration: 40\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         2745.76</td><td style=\"text-align: right;\">319920</td><td style=\"text-align: right;\">    1.45</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 327918\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-03-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.47\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 327\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.478336580850745\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015472642185064984\n",
      "          policy_loss: -0.045682557535568075\n",
      "          total_loss: -0.018550577046229474\n",
      "          vf_explained_var: 0.03487660363316536\n",
      "          vf_loss: 0.05036808071296252\n",
      "    num_agent_steps_sampled: 327918\n",
      "    num_agent_steps_trained: 327918\n",
      "    num_steps_sampled: 327918\n",
      "    num_steps_trained: 327918\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.15119047619048\n",
      "    ram_util_percent: 38.813095238095244\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07903959975993274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.867479673125366\n",
      "    mean_inference_ms: 2.655876865770648\n",
      "    mean_raw_obs_processing_ms: 0.7750513818330669\n",
      "  time_since_restore: 2804.8249146938324\n",
      "  time_this_iter_s: 59.06215786933899\n",
      "  time_total_s: 2804.8249146938324\n",
      "  timers:\n",
      "    learn_throughput: 959.003\n",
      "    learn_time_ms: 8339.911\n",
      "    load_throughput: 207824.048\n",
      "    load_time_ms: 38.484\n",
      "    sample_throughput: 151.127\n",
      "    sample_time_ms: 52922.348\n",
      "    update_time_ms: 3.972\n",
      "  timestamp: 1631793820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327918\n",
      "  training_iteration: 41\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         2804.82</td><td style=\"text-align: right;\">327918</td><td style=\"text-align: right;\">    1.47</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 335916\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-04-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.5\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 336\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4028151227581884\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.018059452780608082\n",
      "          policy_loss: -0.04442179935073019\n",
      "          total_loss: -0.007923677916167885\n",
      "          vf_explained_var: 0.011908459477126598\n",
      "          vf_loss: 0.05872032697693074\n",
      "    num_agent_steps_sampled: 335916\n",
      "    num_agent_steps_trained: 335916\n",
      "    num_steps_sampled: 335916\n",
      "    num_steps_trained: 335916\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.90581395348836\n",
      "    ram_util_percent: 38.79302325581396\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07896270803367622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.79207505041771\n",
      "    mean_inference_ms: 2.6522819566050773\n",
      "    mean_raw_obs_processing_ms: 0.7772274325326692\n",
      "  time_since_restore: 2865.0485360622406\n",
      "  time_this_iter_s: 60.2236213684082\n",
      "  time_total_s: 2865.0485360622406\n",
      "  timers:\n",
      "    learn_throughput: 956.343\n",
      "    learn_time_ms: 8363.107\n",
      "    load_throughput: 197289.977\n",
      "    load_time_ms: 40.539\n",
      "    sample_throughput: 151.117\n",
      "    sample_time_ms: 52925.886\n",
      "    update_time_ms: 3.948\n",
      "  timestamp: 1631793880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335916\n",
      "  training_iteration: 42\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         2865.05</td><td style=\"text-align: right;\">335916</td><td style=\"text-align: right;\">     1.5</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 343914\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-05-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.5\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 345\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4054943987118302\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017019757657668454\n",
      "          policy_loss: -0.04306054337410837\n",
      "          total_loss: -0.0306440551534936\n",
      "          vf_explained_var: 0.08500026911497116\n",
      "          vf_loss: 0.03476945524446927\n",
      "    num_agent_steps_sampled: 343914\n",
      "    num_agent_steps_trained: 343914\n",
      "    num_steps_sampled: 343914\n",
      "    num_steps_trained: 343914\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.55833333333332\n",
      "    ram_util_percent: 38.74285714285715\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07890401435269165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.723279499807237\n",
      "    mean_inference_ms: 2.648720556211901\n",
      "    mean_raw_obs_processing_ms: 0.7793971826452011\n",
      "  time_since_restore: 2923.797034740448\n",
      "  time_this_iter_s: 58.7484986782074\n",
      "  time_total_s: 2923.797034740448\n",
      "  timers:\n",
      "    learn_throughput: 961.464\n",
      "    learn_time_ms: 8318.561\n",
      "    load_throughput: 200975.601\n",
      "    load_time_ms: 39.796\n",
      "    sample_throughput: 151.221\n",
      "    sample_time_ms: 52889.65\n",
      "    update_time_ms: 3.855\n",
      "  timestamp: 1631793939\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343914\n",
      "  training_iteration: 43\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">          2923.8</td><td style=\"text-align: right;\">343914</td><td style=\"text-align: right;\">     1.5</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 351912\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-06-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 351\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3985872307131366\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015898659137850876\n",
      "          policy_loss: -0.046927300263797087\n",
      "          total_loss: 0.0056824869776685395\n",
      "          vf_explained_var: 0.028007186949253082\n",
      "          vf_loss: 0.07500579303014092\n",
      "    num_agent_steps_sampled: 351912\n",
      "    num_agent_steps_trained: 351912\n",
      "    num_steps_sampled: 351912\n",
      "    num_steps_trained: 351912\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.4329411764706\n",
      "    ram_util_percent: 38.74823529411766\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07884972360624497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.674378010086\n",
      "    mean_inference_ms: 2.6471560980164304\n",
      "    mean_raw_obs_processing_ms: 0.7818367604152645\n",
      "  time_since_restore: 2983.512350320816\n",
      "  time_this_iter_s: 59.71531558036804\n",
      "  time_total_s: 2983.512350320816\n",
      "  timers:\n",
      "    learn_throughput: 958.661\n",
      "    learn_time_ms: 8342.886\n",
      "    load_throughput: 201257.262\n",
      "    load_time_ms: 39.74\n",
      "    sample_throughput: 156.849\n",
      "    sample_time_ms: 50991.711\n",
      "    update_time_ms: 3.864\n",
      "  timestamp: 1631793999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351912\n",
      "  training_iteration: 44\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.6/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         2983.51</td><td style=\"text-align: right;\">351912</td><td style=\"text-align: right;\">    1.45</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 359910\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-07-56\n",
      "  done: false\n",
      "  episode_len_mean: 991.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.39\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 360\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.317290501056179\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.020316794588724547\n",
      "          policy_loss: -0.047727202393755476\n",
      "          total_loss: -0.008768871803117055\n",
      "          vf_explained_var: 0.09213171154260635\n",
      "          vf_loss: 0.060099555632182125\n",
      "    num_agent_steps_sampled: 359910\n",
      "    num_agent_steps_trained: 359910\n",
      "    num_steps_sampled: 359910\n",
      "    num_steps_trained: 359910\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.93513513513514\n",
      "    ram_util_percent: 38.63243243243244\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07878681740751674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.608519489210693\n",
      "    mean_inference_ms: 2.6445366072120864\n",
      "    mean_raw_obs_processing_ms: 0.798275805511399\n",
      "  time_since_restore: 3061.2335152626038\n",
      "  time_this_iter_s: 77.72116494178772\n",
      "  time_total_s: 3061.2335152626038\n",
      "  timers:\n",
      "    learn_throughput: 960.381\n",
      "    learn_time_ms: 8327.942\n",
      "    load_throughput: 200415.236\n",
      "    load_time_ms: 39.907\n",
      "    sample_throughput: 151.364\n",
      "    sample_time_ms: 52839.506\n",
      "    update_time_ms: 3.849\n",
      "  timestamp: 1631794076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359910\n",
      "  training_iteration: 45\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         3061.23</td><td style=\"text-align: right;\">359910</td><td style=\"text-align: right;\">    1.39</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            991.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 367908\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-08-57\n",
      "  done: false\n",
      "  episode_len_mean: 993.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 369\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2200308074233353\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0166361897862409\n",
      "          policy_loss: -0.039967240031147676\n",
      "          total_loss: 0.0035969251456360023\n",
      "          vf_explained_var: 0.21184197068214417\n",
      "          vf_loss: 0.06326904366699157\n",
      "    num_agent_steps_sampled: 367908\n",
      "    num_agent_steps_trained: 367908\n",
      "    num_steps_sampled: 367908\n",
      "    num_steps_trained: 367908\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.04482758620689\n",
      "    ram_util_percent: 38.531034482758635\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07872103705627222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.54612301980665\n",
      "    mean_inference_ms: 2.6420218503408455\n",
      "    mean_raw_obs_processing_ms: 0.8043419237619662\n",
      "  time_since_restore: 3122.3193855285645\n",
      "  time_this_iter_s: 61.08587026596069\n",
      "  time_total_s: 3122.3193855285645\n",
      "  timers:\n",
      "    learn_throughput: 963.968\n",
      "    learn_time_ms: 8296.952\n",
      "    load_throughput: 198141.464\n",
      "    load_time_ms: 40.365\n",
      "    sample_throughput: 151.097\n",
      "    sample_time_ms: 52932.972\n",
      "    update_time_ms: 3.78\n",
      "  timestamp: 1631794137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367908\n",
      "  training_iteration: 46\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         3122.32</td><td style=\"text-align: right;\">367908</td><td style=\"text-align: right;\">    1.37</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            993.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 375906\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-09-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 375\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3515758587468056\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015002638220077068\n",
      "          policy_loss: -0.04610261258829425\n",
      "          total_loss: -0.01582119806559496\n",
      "          vf_explained_var: 0.10789804905653\n",
      "          vf_loss: 0.05154677653479921\n",
      "    num_agent_steps_sampled: 375906\n",
      "    num_agent_steps_trained: 375906\n",
      "    num_steps_sampled: 375906\n",
      "    num_steps_trained: 375906\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.25243902439026\n",
      "    ram_util_percent: 38.60243902439024\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07865409571577325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.49594658319561\n",
      "    mean_inference_ms: 2.637611242888732\n",
      "    mean_raw_obs_processing_ms: 0.8019540866354916\n",
      "  time_since_restore: 3179.5401771068573\n",
      "  time_this_iter_s: 57.22079157829285\n",
      "  time_total_s: 3179.5401771068573\n",
      "  timers:\n",
      "    learn_throughput: 968.26\n",
      "    learn_time_ms: 8260.178\n",
      "    load_throughput: 195008.152\n",
      "    load_time_ms: 41.014\n",
      "    sample_throughput: 151.691\n",
      "    sample_time_ms: 52725.563\n",
      "    update_time_ms: 3.411\n",
      "  timestamp: 1631794195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375906\n",
      "  training_iteration: 47\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 42.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         3179.54</td><td style=\"text-align: right;\">375906</td><td style=\"text-align: right;\">    1.38</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 383904\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-10-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 384\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4728973716817877\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014947996793216452\n",
      "          policy_loss: -0.030306653223020494\n",
      "          total_loss: 0.013687725159870361\n",
      "          vf_explained_var: -0.06941216439008713\n",
      "          vf_loss: 0.06648115263187818\n",
      "    num_agent_steps_sampled: 383904\n",
      "    num_agent_steps_trained: 383904\n",
      "    num_steps_sampled: 383904\n",
      "    num_steps_trained: 383904\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.25764705882351\n",
      "    ram_util_percent: 39.542352941176475\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07860328736761207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.44598633162949\n",
      "    mean_inference_ms: 2.6378811629322843\n",
      "    mean_raw_obs_processing_ms: 0.8011724970337135\n",
      "  time_since_restore: 3239.4045734405518\n",
      "  time_this_iter_s: 59.86439633369446\n",
      "  time_total_s: 3239.4045734405518\n",
      "  timers:\n",
      "    learn_throughput: 961.166\n",
      "    learn_time_ms: 8321.139\n",
      "    load_throughput: 194523.381\n",
      "    load_time_ms: 41.116\n",
      "    sample_throughput: 151.518\n",
      "    sample_time_ms: 52785.783\n",
      "    update_time_ms: 3.376\n",
      "  timestamp: 1631794255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383904\n",
      "  training_iteration: 48\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">          3239.4</td><td style=\"text-align: right;\">383904</td><td style=\"text-align: right;\">    1.37</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 391902\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-12-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.4\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 393\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.483915621747253\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01447686321821552\n",
      "          policy_loss: -0.04623801046982408\n",
      "          total_loss: -0.0266540356421022\n",
      "          vf_explained_var: 0.31743085384368896\n",
      "          vf_loss: 0.042251601633590756\n",
      "    num_agent_steps_sampled: 391902\n",
      "    num_agent_steps_trained: 391902\n",
      "    num_steps_sampled: 391902\n",
      "    num_steps_trained: 391902\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.67128712871286\n",
      "    ram_util_percent: 43.67524752475247\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07852951561821359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.395874070278115\n",
      "    mean_inference_ms: 2.635651664408529\n",
      "    mean_raw_obs_processing_ms: 0.8000299757562204\n",
      "  time_since_restore: 3309.787294149399\n",
      "  time_this_iter_s: 70.38272070884705\n",
      "  time_total_s: 3309.787294149399\n",
      "  timers:\n",
      "    learn_throughput: 952.181\n",
      "    learn_time_ms: 8399.662\n",
      "    load_throughput: 191128.557\n",
      "    load_time_ms: 41.846\n",
      "    sample_throughput: 148.375\n",
      "    sample_time_ms: 53903.822\n",
      "    update_time_ms: 3.166\n",
      "  timestamp: 1631794325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391902\n",
      "  training_iteration: 49\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         3309.79</td><td style=\"text-align: right;\">391902</td><td style=\"text-align: right;\">     1.4</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 399900\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-13-16\n",
      "  done: false\n",
      "  episode_len_mean: 995.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 399\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4389907370331465\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01508856158915252\n",
      "          policy_loss: -0.05813720378704289\n",
      "          total_loss: -0.03912457913881348\n",
      "          vf_explained_var: 0.12385588884353638\n",
      "          vf_loss: 0.04113924685779268\n",
      "    num_agent_steps_sampled: 399900\n",
      "    num_agent_steps_trained: 399900\n",
      "    num_steps_sampled: 399900\n",
      "    num_steps_trained: 399900\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.63168316831683\n",
      "    ram_util_percent: 43.74653465346535\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07846336210999758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.358919660549745\n",
      "    mean_inference_ms: 2.631794749127566\n",
      "    mean_raw_obs_processing_ms: 0.7985922984039574\n",
      "  time_since_restore: 3380.3943123817444\n",
      "  time_this_iter_s: 70.60701823234558\n",
      "  time_total_s: 3380.3943123817444\n",
      "  timers:\n",
      "    learn_throughput: 944.001\n",
      "    learn_time_ms: 8472.45\n",
      "    load_throughput: 188988.067\n",
      "    load_time_ms: 42.32\n",
      "    sample_throughput: 145.641\n",
      "    sample_time_ms: 54915.804\n",
      "    update_time_ms: 3.147\n",
      "  timestamp: 1631794396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399900\n",
      "  training_iteration: 50\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         3380.39</td><td style=\"text-align: right;\">399900</td><td style=\"text-align: right;\">    1.41</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 407898\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-14-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 408\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3423865784880937\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014810716685475898\n",
      "          policy_loss: -0.040200014506536784\n",
      "          total_loss: 0.019543579844657773\n",
      "          vf_explained_var: 0.16518180072307587\n",
      "          vf_loss: 0.08094585226511994\n",
      "    num_agent_steps_sampled: 407898\n",
      "    num_agent_steps_trained: 407898\n",
      "    num_steps_sampled: 407898\n",
      "    num_steps_trained: 407898\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.43823529411766\n",
      "    ram_util_percent: 43.74901960784313\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07844181212773121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.33461814944085\n",
      "    mean_inference_ms: 2.6334925275947905\n",
      "    mean_raw_obs_processing_ms: 0.7990879170347652\n",
      "  time_since_restore: 3452.0665423870087\n",
      "  time_this_iter_s: 71.67223000526428\n",
      "  time_total_s: 3452.0665423870087\n",
      "  timers:\n",
      "    learn_throughput: 942.09\n",
      "    learn_time_ms: 8489.638\n",
      "    load_throughput: 192464.835\n",
      "    load_time_ms: 41.556\n",
      "    sample_throughput: 142.415\n",
      "    sample_time_ms: 56159.865\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1631794467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407898\n",
      "  training_iteration: 51\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         3452.07</td><td style=\"text-align: right;\">407898</td><td style=\"text-align: right;\">    1.44</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 415896\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-15-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.36\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 417\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3381462324050166\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017254841999020464\n",
      "          policy_loss: -0.04963656614524543\n",
      "          total_loss: -0.011000218552847702\n",
      "          vf_explained_var: 0.1370316445827484\n",
      "          vf_loss: 0.05942958266416725\n",
      "    num_agent_steps_sampled: 415896\n",
      "    num_agent_steps_trained: 415896\n",
      "    num_steps_sampled: 415896\n",
      "    num_steps_trained: 415896\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.70098039215688\n",
      "    ram_util_percent: 43.76078431372549\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07840406183432495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.309542180603092\n",
      "    mean_inference_ms: 2.6327483699400873\n",
      "    mean_raw_obs_processing_ms: 0.7991979998628815\n",
      "  time_since_restore: 3523.402509212494\n",
      "  time_this_iter_s: 71.33596682548523\n",
      "  time_total_s: 3523.402509212494\n",
      "  timers:\n",
      "    learn_throughput: 936.229\n",
      "    learn_time_ms: 8542.784\n",
      "    load_throughput: 202697.943\n",
      "    load_time_ms: 39.458\n",
      "    sample_throughput: 139.777\n",
      "    sample_time_ms: 57219.566\n",
      "    update_time_ms: 3.926\n",
      "  timestamp: 1631794539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415896\n",
      "  training_iteration: 52\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">          3523.4</td><td style=\"text-align: right;\">415896</td><td style=\"text-align: right;\">    1.36</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 423894\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-16-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.33\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 423\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3226546661828156\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016866094742635274\n",
      "          policy_loss: -0.048464063356720634\n",
      "          total_loss: -0.0043752826840406465\n",
      "          vf_explained_var: -0.013932964764535427\n",
      "          vf_loss: 0.06478541356721712\n",
      "    num_agent_steps_sampled: 423894\n",
      "    num_agent_steps_trained: 423894\n",
      "    num_steps_sampled: 423894\n",
      "    num_steps_trained: 423894\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.13786407766989\n",
      "    ram_util_percent: 43.77961165048542\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.078363728399641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.289850836582367\n",
      "    mean_inference_ms: 2.629734631646589\n",
      "    mean_raw_obs_processing_ms: 0.7985451624000015\n",
      "  time_since_restore: 3596.049439430237\n",
      "  time_this_iter_s: 72.64693021774292\n",
      "  time_total_s: 3596.049439430237\n",
      "  timers:\n",
      "    learn_throughput: 925.513\n",
      "    learn_time_ms: 8641.697\n",
      "    load_throughput: 195313.913\n",
      "    load_time_ms: 40.949\n",
      "    sample_throughput: 136.697\n",
      "    sample_time_ms: 58508.923\n",
      "    update_time_ms: 4.005\n",
      "  timestamp: 1631794611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423894\n",
      "  training_iteration: 53\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         3596.05</td><td style=\"text-align: right;\">423894</td><td style=\"text-align: right;\">    1.33</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 431892\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-18-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 432\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3651265184084576\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01601845098784543\n",
      "          policy_loss: -0.04137111582883423\n",
      "          total_loss: -0.0150731208448809\n",
      "          vf_explained_var: 0.16249212622642517\n",
      "          vf_loss: 0.04754649207674427\n",
      "    num_agent_steps_sampled: 431892\n",
      "    num_agent_steps_trained: 431892\n",
      "    num_steps_sampled: 431892\n",
      "    num_steps_trained: 431892\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.82647058823531\n",
      "    ram_util_percent: 43.780392156862746\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07837380800951668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.289189411758358\n",
      "    mean_inference_ms: 2.6324763402504785\n",
      "    mean_raw_obs_processing_ms: 0.7998584717785895\n",
      "  time_since_restore: 3667.298687696457\n",
      "  time_this_iter_s: 71.24924826622009\n",
      "  time_total_s: 3667.298687696457\n",
      "  timers:\n",
      "    learn_throughput: 922.493\n",
      "    learn_time_ms: 8669.988\n",
      "    load_throughput: 194332.488\n",
      "    load_time_ms: 41.156\n",
      "    sample_throughput: 134.119\n",
      "    sample_time_ms: 59633.435\n",
      "    update_time_ms: 4.05\n",
      "  timestamp: 1631794683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431892\n",
      "  training_iteration: 54\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">          3667.3</td><td style=\"text-align: right;\">431892</td><td style=\"text-align: right;\">    1.37</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 439890\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-19-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.37\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 441\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3275750525536076\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015783882830079497\n",
      "          policy_loss: -0.043611852996932564\n",
      "          total_loss: 0.005765580336853701\n",
      "          vf_explained_var: -0.05139797553420067\n",
      "          vf_loss: 0.07028560081236465\n",
      "    num_agent_steps_sampled: 439890\n",
      "    num_agent_steps_trained: 439890\n",
      "    num_steps_sampled: 439890\n",
      "    num_steps_trained: 439890\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.75392156862745\n",
      "    ram_util_percent: 43.77156862745097\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0783700021634388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.287868322327263\n",
      "    mean_inference_ms: 2.632598024766024\n",
      "    mean_raw_obs_processing_ms: 0.8007433076883445\n",
      "  time_since_restore: 3738.7275021076202\n",
      "  time_this_iter_s: 71.42881441116333\n",
      "  time_total_s: 3738.7275021076202\n",
      "  timers:\n",
      "    learn_throughput: 916.321\n",
      "    learn_time_ms: 8728.383\n",
      "    load_throughput: 187680.462\n",
      "    load_time_ms: 42.615\n",
      "    sample_throughput: 135.69\n",
      "    sample_time_ms: 58942.972\n",
      "    update_time_ms: 4.139\n",
      "  timestamp: 1631794754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439890\n",
      "  training_iteration: 55\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         3738.73</td><td style=\"text-align: right;\">439890</td><td style=\"text-align: right;\">    1.37</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 447888\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-20-43\n",
      "  done: false\n",
      "  episode_len_mean: 992.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 449\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.300777542462913\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017960692248036154\n",
      "          policy_loss: -0.0423527045643598\n",
      "          total_loss: 0.01538607582089401\n",
      "          vf_explained_var: 0.11563196778297424\n",
      "          vf_loss: 0.07805245231564147\n",
      "    num_agent_steps_sampled: 447888\n",
      "    num_agent_steps_trained: 447888\n",
      "    num_steps_sampled: 447888\n",
      "    num_steps_trained: 447888\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.36929133858268\n",
      "    ram_util_percent: 43.76299212598425\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07840144889506108\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.304962489393\n",
      "    mean_inference_ms: 2.63501828419966\n",
      "    mean_raw_obs_processing_ms: 0.8091150696432952\n",
      "  time_since_restore: 3827.693211555481\n",
      "  time_this_iter_s: 88.96570944786072\n",
      "  time_total_s: 3827.693211555481\n",
      "  timers:\n",
      "    learn_throughput: 902.101\n",
      "    learn_time_ms: 8865.969\n",
      "    load_throughput: 173127.823\n",
      "    load_time_ms: 46.197\n",
      "    sample_throughput: 129.861\n",
      "    sample_time_ms: 61588.723\n",
      "    update_time_ms: 4.555\n",
      "  timestamp: 1631794843\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447888\n",
      "  training_iteration: 56\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         3827.69</td><td style=\"text-align: right;\">447888</td><td style=\"text-align: right;\">    1.38</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            992.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 455886\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-22-13\n",
      "  done: false\n",
      "  episode_len_mean: 992.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.46\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 456\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.395124021140478\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015322126012734945\n",
      "          policy_loss: -0.04059576728851885\n",
      "          total_loss: -0.006573879791884333\n",
      "          vf_explained_var: -0.12497514486312866\n",
      "          vf_loss: 0.055674807234887554\n",
      "    num_agent_steps_sampled: 455886\n",
      "    num_agent_steps_trained: 455886\n",
      "    num_steps_sampled: 455886\n",
      "    num_steps_trained: 455886\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.69921259842522\n",
      "    ram_util_percent: 43.58267716535432\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0783893978767622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.304219331520326\n",
      "    mean_inference_ms: 2.6333817272580986\n",
      "    mean_raw_obs_processing_ms: 0.8101410020522638\n",
      "  time_since_restore: 3917.277002811432\n",
      "  time_this_iter_s: 89.58379125595093\n",
      "  time_total_s: 3917.277002811432\n",
      "  timers:\n",
      "    learn_throughput: 888.621\n",
      "    learn_time_ms: 9000.466\n",
      "    load_throughput: 175316.971\n",
      "    load_time_ms: 45.62\n",
      "    sample_throughput: 123.637\n",
      "    sample_time_ms: 64689.546\n",
      "    update_time_ms: 4.723\n",
      "  timestamp: 1631794933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455886\n",
      "  training_iteration: 57\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         3917.28</td><td style=\"text-align: right;\">455886</td><td style=\"text-align: right;\">    1.46</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             992.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 463884\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-23-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.5\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 465\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.429405577080224\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014935927896333216\n",
      "          policy_loss: -0.03980515303005094\n",
      "          total_loss: -0.0014811469677595362\n",
      "          vf_explained_var: -0.07066130638122559\n",
      "          vf_loss: 0.060377672573323715\n",
      "    num_agent_steps_sampled: 463884\n",
      "    num_agent_steps_trained: 463884\n",
      "    num_steps_sampled: 463884\n",
      "    num_steps_trained: 463884\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.96960784313721\n",
      "    ram_util_percent: 43.64509803921569\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07842593818817378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.325645596746003\n",
      "    mean_inference_ms: 2.6345817758157675\n",
      "    mean_raw_obs_processing_ms: 0.8087674258931672\n",
      "  time_since_restore: 3988.666426897049\n",
      "  time_this_iter_s: 71.38942408561707\n",
      "  time_total_s: 3988.666426897049\n",
      "  timers:\n",
      "    learn_throughput: 886.225\n",
      "    learn_time_ms: 9024.796\n",
      "    load_throughput: 176169.253\n",
      "    load_time_ms: 45.4\n",
      "    sample_throughput: 121.516\n",
      "    sample_time_ms: 65818.258\n",
      "    update_time_ms: 4.796\n",
      "  timestamp: 1631795004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463884\n",
      "  training_iteration: 58\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         3988.67</td><td style=\"text-align: right;\">463884</td><td style=\"text-align: right;\">     1.5</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 471882\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-24-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.56\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 472\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5071597596650483\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.012621764841228348\n",
      "          policy_loss: -0.04837222546063644\n",
      "          total_loss: -0.046377431991339854\n",
      "          vf_explained_var: -0.366502583026886\n",
      "          vf_loss: 0.025173126083488265\n",
      "    num_agent_steps_sampled: 471882\n",
      "    num_agent_steps_trained: 471882\n",
      "    num_steps_sampled: 471882\n",
      "    num_steps_trained: 471882\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.6822429906542\n",
      "    ram_util_percent: 43.79813084112149\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0784715884212272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.348629988054583\n",
      "    mean_inference_ms: 2.636147726381907\n",
      "    mean_raw_obs_processing_ms: 0.807756112005091\n",
      "  time_since_restore: 4063.6908934116364\n",
      "  time_this_iter_s: 75.0244665145874\n",
      "  time_total_s: 4063.6908934116364\n",
      "  timers:\n",
      "    learn_throughput: 885.91\n",
      "    learn_time_ms: 9028.004\n",
      "    load_throughput: 175634.563\n",
      "    load_time_ms: 45.538\n",
      "    sample_throughput: 120.674\n",
      "    sample_time_ms: 66277.62\n",
      "    update_time_ms: 5.364\n",
      "  timestamp: 1631795079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471882\n",
      "  training_iteration: 59\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         4063.69</td><td style=\"text-align: right;\">471882</td><td style=\"text-align: right;\">    1.56</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 479880\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-25-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.55\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 480\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.487966963809024\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014439600226899996\n",
      "          policy_loss: -0.038984028814781094\n",
      "          total_loss: -0.014582512888216203\n",
      "          vf_explained_var: -0.39268097281455994\n",
      "          vf_loss: 0.04711524445437751\n",
      "    num_agent_steps_sampled: 479880\n",
      "    num_agent_steps_trained: 479880\n",
      "    num_steps_sampled: 479880\n",
      "    num_steps_trained: 479880\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.08888888888889\n",
      "    ram_util_percent: 43.88055555555557\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07852831218965477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.382554015732772\n",
      "    mean_inference_ms: 2.638143361469969\n",
      "    mean_raw_obs_processing_ms: 0.8071763371096123\n",
      "  time_since_restore: 4139.481308937073\n",
      "  time_this_iter_s: 75.7904155254364\n",
      "  time_total_s: 4139.481308937073\n",
      "  timers:\n",
      "    learn_throughput: 876.933\n",
      "    learn_time_ms: 9120.428\n",
      "    load_throughput: 173175.817\n",
      "    load_time_ms: 46.184\n",
      "    sample_throughput: 119.909\n",
      "    sample_time_ms: 66700.365\n",
      "    update_time_ms: 6.004\n",
      "  timestamp: 1631795155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479880\n",
      "  training_iteration: 60\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         4139.48</td><td style=\"text-align: right;\">479880</td><td style=\"text-align: right;\">    1.55</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 487878\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-27-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.58\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 489\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.326210919118697\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015902394768551455\n",
      "          policy_loss: -0.03935419794332276\n",
      "          total_loss: 0.0035247101638746516\n",
      "          vf_explained_var: 0.018999241292476654\n",
      "          vf_loss: 0.06375565777100267\n",
      "    num_agent_steps_sampled: 487878\n",
      "    num_agent_steps_trained: 487878\n",
      "    num_steps_sampled: 487878\n",
      "    num_steps_trained: 487878\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.48392857142858\n",
      "    ram_util_percent: 43.872321428571425\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0786032209524879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.426908109690928\n",
      "    mean_inference_ms: 2.640949262406289\n",
      "    mean_raw_obs_processing_ms: 0.8066789915452888\n",
      "  time_since_restore: 4217.394871234894\n",
      "  time_this_iter_s: 77.91356229782104\n",
      "  time_total_s: 4217.394871234894\n",
      "  timers:\n",
      "    learn_throughput: 870.795\n",
      "    learn_time_ms: 9184.715\n",
      "    load_throughput: 173802.71\n",
      "    load_time_ms: 46.018\n",
      "    sample_throughput: 118.912\n",
      "    sample_time_ms: 67260.063\n",
      "    update_time_ms: 6.094\n",
      "  timestamp: 1631795233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487878\n",
      "  training_iteration: 61\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         4217.39</td><td style=\"text-align: right;\">487878</td><td style=\"text-align: right;\">    1.58</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 495876\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-28-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.6\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 496\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2660551858204667\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016928274823445565\n",
      "          policy_loss: -0.042780560857906776\n",
      "          total_loss: -0.010450509761369997\n",
      "          vf_explained_var: 0.011205574497580528\n",
      "          vf_loss: 0.05245136146570408\n",
      "    num_agent_steps_sampled: 495876\n",
      "    num_agent_steps_trained: 495876\n",
      "    num_steps_sampled: 495876\n",
      "    num_steps_trained: 495876\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.27619047619048\n",
      "    ram_util_percent: 43.86095238095238\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07866069301121416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.462161208976546\n",
      "    mean_inference_ms: 2.6431607761244873\n",
      "    mean_raw_obs_processing_ms: 0.8063451200632228\n",
      "  time_since_restore: 4291.259514093399\n",
      "  time_this_iter_s: 73.86464285850525\n",
      "  time_total_s: 4291.259514093399\n",
      "  timers:\n",
      "    learn_throughput: 866.183\n",
      "    learn_time_ms: 9233.616\n",
      "    load_throughput: 173258.193\n",
      "    load_time_ms: 46.162\n",
      "    sample_throughput: 118.552\n",
      "    sample_time_ms: 67464.166\n",
      "    update_time_ms: 5.407\n",
      "  timestamp: 1631795307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495876\n",
      "  training_iteration: 62\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         4291.26</td><td style=\"text-align: right;\">495876</td><td style=\"text-align: right;\">     1.6</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 503874\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-29-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.56\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 504\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.34460996197116\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014290694592042694\n",
      "          policy_loss: -0.039172701331316145\n",
      "          total_loss: -0.012599768513633359\n",
      "          vf_explained_var: -0.08215177059173584\n",
      "          vf_loss: 0.04787542767086071\n",
      "    num_agent_steps_sampled: 503874\n",
      "    num_agent_steps_trained: 503874\n",
      "    num_steps_sampled: 503874\n",
      "    num_steps_trained: 503874\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.79181818181819\n",
      "    ram_util_percent: 43.86000000000001\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07873836987509235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.503227477292835\n",
      "    mean_inference_ms: 2.645735307427024\n",
      "    mean_raw_obs_processing_ms: 0.8064755455968406\n",
      "  time_since_restore: 4368.3581528663635\n",
      "  time_this_iter_s: 77.09863877296448\n",
      "  time_total_s: 4368.3581528663635\n",
      "  timers:\n",
      "    learn_throughput: 859.952\n",
      "    learn_time_ms: 9300.52\n",
      "    load_throughput: 175523.918\n",
      "    load_time_ms: 45.566\n",
      "    sample_throughput: 117.893\n",
      "    sample_time_ms: 67841.051\n",
      "    update_time_ms: 6.067\n",
      "  timestamp: 1631795384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503874\n",
      "  training_iteration: 63\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         4368.36</td><td style=\"text-align: right;\">503874</td><td style=\"text-align: right;\">    1.56</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 511872\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-31-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.57\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 513\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3353955312441754\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014673658516890006\n",
      "          policy_loss: -0.04010880892165005\n",
      "          total_loss: -0.005891389270583468\n",
      "          vf_explained_var: -0.03649448975920677\n",
      "          vf_loss: 0.05537032527686034\n",
      "    num_agent_steps_sampled: 511872\n",
      "    num_agent_steps_trained: 511872\n",
      "    num_steps_sampled: 511872\n",
      "    num_steps_trained: 511872\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.93577981651376\n",
      "    ram_util_percent: 43.82018348623855\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07883401372131646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.550143288877152\n",
      "    mean_inference_ms: 2.648743374856362\n",
      "    mean_raw_obs_processing_ms: 0.806787364653998\n",
      "  time_since_restore: 4444.514217376709\n",
      "  time_this_iter_s: 76.15606451034546\n",
      "  time_total_s: 4444.514217376709\n",
      "  timers:\n",
      "    learn_throughput: 851.963\n",
      "    learn_time_ms: 9387.733\n",
      "    load_throughput: 174938.195\n",
      "    load_time_ms: 45.719\n",
      "    sample_throughput: 117.195\n",
      "    sample_time_ms: 68245.043\n",
      "    update_time_ms: 6.016\n",
      "  timestamp: 1631795460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511872\n",
      "  training_iteration: 64\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         4444.51</td><td style=\"text-align: right;\">511872</td><td style=\"text-align: right;\">    1.57</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 519870\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-32-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.62\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 520\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.31117315202631\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01641306806161008\n",
      "          policy_loss: -0.05402066140605878\n",
      "          total_loss: -0.035695375994809216\n",
      "          vf_explained_var: 0.07651188224554062\n",
      "          vf_loss: 0.03897505545317726\n",
      "    num_agent_steps_sampled: 519870\n",
      "    num_agent_steps_trained: 519870\n",
      "    num_steps_sampled: 519870\n",
      "    num_steps_trained: 519870\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.29528301886793\n",
      "    ram_util_percent: 43.8188679245283\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0789059073147757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.587288194307718\n",
      "    mean_inference_ms: 2.6510364801096755\n",
      "    mean_raw_obs_processing_ms: 0.8070161726446591\n",
      "  time_since_restore: 4519.415175676346\n",
      "  time_this_iter_s: 74.90095829963684\n",
      "  time_total_s: 4519.415175676346\n",
      "  timers:\n",
      "    learn_throughput: 844.988\n",
      "    learn_time_ms: 9465.228\n",
      "    load_throughput: 180843.642\n",
      "    load_time_ms: 44.226\n",
      "    sample_throughput: 116.73\n",
      "    sample_time_ms: 68516.9\n",
      "    update_time_ms: 6.147\n",
      "  timestamp: 1631795535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519870\n",
      "  training_iteration: 65\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         4519.42</td><td style=\"text-align: right;\">519870</td><td style=\"text-align: right;\">    1.62</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 527868\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-33-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.68\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 528\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4035957923499485\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015579389551546296\n",
      "          policy_loss: -0.04530260538762455\n",
      "          total_loss: -0.034916875339163246\n",
      "          vf_explained_var: -0.09261863678693771\n",
      "          vf_loss: 0.032084779109885934\n",
      "    num_agent_steps_sampled: 527868\n",
      "    num_agent_steps_trained: 527868\n",
      "    num_steps_sampled: 527868\n",
      "    num_steps_trained: 527868\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.52999999999999\n",
      "    ram_util_percent: 43.79909090909091\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07901116410266623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.630268323305685\n",
      "    mean_inference_ms: 2.654111845993606\n",
      "    mean_raw_obs_processing_ms: 0.8077700368739074\n",
      "  time_since_restore: 4596.481709241867\n",
      "  time_this_iter_s: 77.06653356552124\n",
      "  time_total_s: 4596.481709241867\n",
      "  timers:\n",
      "    learn_throughput: 841.578\n",
      "    learn_time_ms: 9503.577\n",
      "    load_throughput: 192381.943\n",
      "    load_time_ms: 41.574\n",
      "    sample_throughput: 118.857\n",
      "    sample_time_ms: 67290.907\n",
      "    update_time_ms: 5.749\n",
      "  timestamp: 1631795613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527868\n",
      "  training_iteration: 66\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         4596.48</td><td style=\"text-align: right;\">527868</td><td style=\"text-align: right;\">    1.68</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 535866\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-34-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.64\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 537\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.306110998122923\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01627420611698573\n",
      "          policy_loss: -0.04129185476590709\n",
      "          total_loss: -0.02735153546216347\n",
      "          vf_explained_var: -0.014066220261156559\n",
      "          vf_loss: 0.034560298568366336\n",
      "    num_agent_steps_sampled: 535866\n",
      "    num_agent_steps_trained: 535866\n",
      "    num_steps_sampled: 535866\n",
      "    num_steps_trained: 535866\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.60462962962963\n",
      "    ram_util_percent: 43.7611111111111\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0791191096735027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.679392314530514\n",
      "    mean_inference_ms: 2.6574166752832435\n",
      "    mean_raw_obs_processing_ms: 0.8086199122837673\n",
      "  time_since_restore: 4672.135364294052\n",
      "  time_this_iter_s: 75.65365505218506\n",
      "  time_total_s: 4672.135364294052\n",
      "  timers:\n",
      "    learn_throughput: 841.198\n",
      "    learn_time_ms: 9507.874\n",
      "    load_throughput: 191773.427\n",
      "    load_time_ms: 41.705\n",
      "    sample_throughput: 121.377\n",
      "    sample_time_ms: 65893.797\n",
      "    update_time_ms: 5.944\n",
      "  timestamp: 1631795688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535866\n",
      "  training_iteration: 67\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         4672.14</td><td style=\"text-align: right;\">535866</td><td style=\"text-align: right;\">    1.64</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 543864\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-36-25\n",
      "  done: false\n",
      "  episode_len_mean: 993.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.61\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 546\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2815889543102634\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016683876023871436\n",
      "          policy_loss: -0.041727524000390245\n",
      "          total_loss: -0.011318292878868598\n",
      "          vf_explained_var: -0.06511492282152176\n",
      "          vf_loss: 0.05072253915958197\n",
      "    num_agent_steps_sampled: 543864\n",
      "    num_agent_steps_trained: 543864\n",
      "    num_steps_sampled: 543864\n",
      "    num_steps_trained: 543864\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.62971014492753\n",
      "    ram_util_percent: 43.585507246376814\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07922797067158666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.730611511655255\n",
      "    mean_inference_ms: 2.6607237135202695\n",
      "    mean_raw_obs_processing_ms: 0.8149748059384071\n",
      "  time_since_restore: 4768.7237112522125\n",
      "  time_this_iter_s: 96.5883469581604\n",
      "  time_total_s: 4768.7237112522125\n",
      "  timers:\n",
      "    learn_throughput: 837.377\n",
      "    learn_time_ms: 9551.258\n",
      "    load_throughput: 185016.085\n",
      "    load_time_ms: 43.229\n",
      "    sample_throughput: 116.987\n",
      "    sample_time_ms: 68366.285\n",
      "    update_time_ms: 6.843\n",
      "  timestamp: 1631795785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543864\n",
      "  training_iteration: 68\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 47.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         4768.72</td><td style=\"text-align: right;\">543864</td><td style=\"text-align: right;\">    1.61</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            993.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 551862\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-37-40\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.67\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 552\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.293670138364197\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016214503537539147\n",
      "          policy_loss: -0.05065026009375209\n",
      "          total_loss: -0.028070664415836975\n",
      "          vf_explained_var: 0.021726546809077263\n",
      "          vf_loss: 0.043084121206575024\n",
      "    num_agent_steps_sampled: 551862\n",
      "    num_agent_steps_trained: 551862\n",
      "    num_steps_sampled: 551862\n",
      "    num_steps_trained: 551862\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.76574074074072\n",
      "    ram_util_percent: 43.708333333333336\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07926636144493034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.755292505384705\n",
      "    mean_inference_ms: 2.6599321544646237\n",
      "    mean_raw_obs_processing_ms: 0.8139377619184793\n",
      "  time_since_restore: 4844.297963857651\n",
      "  time_this_iter_s: 75.57425260543823\n",
      "  time_total_s: 4844.297963857651\n",
      "  timers:\n",
      "    learn_throughput: 836.017\n",
      "    learn_time_ms: 9566.791\n",
      "    load_throughput: 175167.752\n",
      "    load_time_ms: 45.659\n",
      "    sample_throughput: 116.926\n",
      "    sample_time_ms: 68401.95\n",
      "    update_time_ms: 6.482\n",
      "  timestamp: 1631795860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551862\n",
      "  training_iteration: 69\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">          4844.3</td><td style=\"text-align: right;\">551862</td><td style=\"text-align: right;\">    1.67</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 559860\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-38-55\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.62\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 561\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.31554532781724\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016754134380315353\n",
      "          policy_loss: -0.04662451316397236\n",
      "          total_loss: -0.033577044081363465\n",
      "          vf_explained_var: -0.0757332593202591\n",
      "          vf_loss: 0.03368980137841596\n",
      "    num_agent_steps_sampled: 559860\n",
      "    num_agent_steps_trained: 559860\n",
      "    num_steps_sampled: 559860\n",
      "    num_steps_trained: 559860\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.02358490566037\n",
      "    ram_util_percent: 43.798113207547175\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07939895223777606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.814610468041458\n",
      "    mean_inference_ms: 2.666146182288368\n",
      "    mean_raw_obs_processing_ms: 0.8140319054813916\n",
      "  time_since_restore: 4918.9504935741425\n",
      "  time_this_iter_s: 74.6525297164917\n",
      "  time_total_s: 4918.9504935741425\n",
      "  timers:\n",
      "    learn_throughput: 839.565\n",
      "    learn_time_ms: 9526.361\n",
      "    load_throughput: 176178.227\n",
      "    load_time_ms: 45.397\n",
      "    sample_throughput: 117.048\n",
      "    sample_time_ms: 68330.939\n",
      "    update_time_ms: 5.931\n",
      "  timestamp: 1631795935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559860\n",
      "  training_iteration: 70\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         4918.95</td><td style=\"text-align: right;\">559860</td><td style=\"text-align: right;\">    1.62</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 567858\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-40-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.55\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 570\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.278489553415647\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01857181873451146\n",
      "          policy_loss: -0.041231416103490175\n",
      "          total_loss: -0.0037599456131017658\n",
      "          vf_explained_var: 0.08645354211330414\n",
      "          vf_loss: 0.057470593051684477\n",
      "    num_agent_steps_sampled: 567858\n",
      "    num_agent_steps_trained: 567858\n",
      "    num_steps_sampled: 567858\n",
      "    num_steps_trained: 567858\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.19908256880734\n",
      "    ram_util_percent: 43.86880733944953\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07949241506903827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.866666152432085\n",
      "    mean_inference_ms: 2.6681965487585937\n",
      "    mean_raw_obs_processing_ms: 0.8137950604886425\n",
      "  time_since_restore: 4995.063417196274\n",
      "  time_this_iter_s: 76.11292362213135\n",
      "  time_total_s: 4995.063417196274\n",
      "  timers:\n",
      "    learn_throughput: 837.624\n",
      "    learn_time_ms: 9548.438\n",
      "    load_throughput: 176135.121\n",
      "    load_time_ms: 45.408\n",
      "    sample_throughput: 117.397\n",
      "    sample_time_ms: 68127.54\n",
      "    update_time_ms: 5.889\n",
      "  timestamp: 1631796011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 567858\n",
      "  training_iteration: 71\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         4995.06</td><td style=\"text-align: right;\">567858</td><td style=\"text-align: right;\">    1.55</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 575856\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-41-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.58\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 576\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.388044209249558\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.019018071566591778\n",
      "          policy_loss: -0.04947592248033572\n",
      "          total_loss: -0.04856390590899654\n",
      "          vf_explained_var: 0.010240960866212845\n",
      "          vf_loss: 0.021939747742726466\n",
      "    num_agent_steps_sampled: 575856\n",
      "    num_agent_steps_trained: 575856\n",
      "    num_steps_sampled: 575856\n",
      "    num_steps_trained: 575856\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.18411214953268\n",
      "    ram_util_percent: 44.00747663551402\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07953611367181129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.88517378469563\n",
      "    mean_inference_ms: 2.6682760834826436\n",
      "    mean_raw_obs_processing_ms: 0.8131864429339088\n",
      "  time_since_restore: 5070.386972904205\n",
      "  time_this_iter_s: 75.32355570793152\n",
      "  time_total_s: 5070.386972904205\n",
      "  timers:\n",
      "    learn_throughput: 832.028\n",
      "    learn_time_ms: 9612.662\n",
      "    load_throughput: 175068.84\n",
      "    load_time_ms: 45.685\n",
      "    sample_throughput: 117.257\n",
      "    sample_time_ms: 68209.131\n",
      "    update_time_ms: 5.878\n",
      "  timestamp: 1631796087\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 575856\n",
      "  training_iteration: 72\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.6/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         5070.39</td><td style=\"text-align: right;\">575856</td><td style=\"text-align: right;\">    1.58</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 583854\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-42-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.54\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 585\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3345918797677565\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01555725421462913\n",
      "          policy_loss: -0.03627482792587891\n",
      "          total_loss: -0.014948173335462969\n",
      "          vf_explained_var: -0.20600496232509613\n",
      "          vf_loss: 0.04233898513090436\n",
      "    num_agent_steps_sampled: 583854\n",
      "    num_agent_steps_trained: 583854\n",
      "    num_steps_sampled: 583854\n",
      "    num_steps_trained: 583854\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.33508771929823\n",
      "    ram_util_percent: 43.43684210526316\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07966080575384737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.94067691621734\n",
      "    mean_inference_ms: 2.6740177356558665\n",
      "    mean_raw_obs_processing_ms: 0.8136327557754854\n",
      "  time_since_restore: 5150.145705699921\n",
      "  time_this_iter_s: 79.75873279571533\n",
      "  time_total_s: 5150.145705699921\n",
      "  timers:\n",
      "    learn_throughput: 831.939\n",
      "    learn_time_ms: 9613.681\n",
      "    load_throughput: 176131.607\n",
      "    load_time_ms: 45.409\n",
      "    sample_throughput: 116.802\n",
      "    sample_time_ms: 68475.085\n",
      "    update_time_ms: 5.479\n",
      "  timestamp: 1631796167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 583854\n",
      "  training_iteration: 73\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         5150.15</td><td style=\"text-align: right;\">583854</td><td style=\"text-align: right;\">    1.54</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 591852\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-44-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.48\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 594\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3975882171302714\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01757978198438478\n",
      "          policy_loss: -0.045865760879811424\n",
      "          total_loss: -0.04541776729046657\n",
      "          vf_explained_var: -0.2776126563549042\n",
      "          vf_loss: 0.02178690730445763\n",
      "    num_agent_steps_sampled: 591852\n",
      "    num_agent_steps_trained: 591852\n",
      "    num_steps_sampled: 591852\n",
      "    num_steps_trained: 591852\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.94205607476633\n",
      "    ram_util_percent: 43.521495327102805\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07975100009304495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.98833571732377\n",
      "    mean_inference_ms: 2.675846878607098\n",
      "    mean_raw_obs_processing_ms: 0.8137883485144508\n",
      "  time_since_restore: 5225.058756351471\n",
      "  time_this_iter_s: 74.9130506515503\n",
      "  time_total_s: 5225.058756351471\n",
      "  timers:\n",
      "    learn_throughput: 833.548\n",
      "    learn_time_ms: 9595.123\n",
      "    load_throughput: 178950.503\n",
      "    load_time_ms: 44.694\n",
      "    sample_throughput: 116.982\n",
      "    sample_time_ms: 68369.483\n",
      "    update_time_ms: 5.463\n",
      "  timestamp: 1631796241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 591852\n",
      "  training_iteration: 74\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 47.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         5225.06</td><td style=\"text-align: right;\">591852</td><td style=\"text-align: right;\">    1.48</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 599850\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-45-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.54\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 600\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.263950816790263\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017540802025044842\n",
      "          policy_loss: -0.041925129147186393\n",
      "          total_loss: -0.029405374651754735\n",
      "          vf_explained_var: -0.11528264731168747\n",
      "          vf_loss: 0.03252814282054177\n",
      "    num_agent_steps_sampled: 599850\n",
      "    num_agent_steps_trained: 599850\n",
      "    num_steps_sampled: 599850\n",
      "    num_steps_trained: 599850\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.43942307692308\n",
      "    ram_util_percent: 43.48846153846154\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07978751270920675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.003834424297636\n",
      "    mean_inference_ms: 2.6757042986889688\n",
      "    mean_raw_obs_processing_ms: 0.8134126645542902\n",
      "  time_since_restore: 5297.742406845093\n",
      "  time_this_iter_s: 72.68365049362183\n",
      "  time_total_s: 5297.742406845093\n",
      "  timers:\n",
      "    learn_throughput: 832.076\n",
      "    learn_time_ms: 9612.103\n",
      "    load_throughput: 173566.926\n",
      "    load_time_ms: 46.08\n",
      "    sample_throughput: 117.396\n",
      "    sample_time_ms: 68128.634\n",
      "    update_time_ms: 5.455\n",
      "  timestamp: 1631796314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599850\n",
      "  training_iteration: 75\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 47.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         5297.74</td><td style=\"text-align: right;\">599850</td><td style=\"text-align: right;\">    1.54</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 607848\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-46-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.54\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 609\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2289034885744896\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.018680712937771442\n",
      "          policy_loss: -0.04317213136812932\n",
      "          total_loss: -0.018747635584597487\n",
      "          vf_explained_var: 0.0183955617249012\n",
      "          vf_loss: 0.04391142286989908\n",
      "    num_agent_steps_sampled: 607848\n",
      "    num_agent_steps_trained: 607848\n",
      "    num_steps_sampled: 607848\n",
      "    num_steps_trained: 607848\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.70091743119266\n",
      "    ram_util_percent: 43.37339449541283\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07988360132482339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.054060021010656\n",
      "    mean_inference_ms: 2.68090477131315\n",
      "    mean_raw_obs_processing_ms: 0.8140759110740318\n",
      "  time_since_restore: 5374.815693616867\n",
      "  time_this_iter_s: 77.07328677177429\n",
      "  time_total_s: 5374.815693616867\n",
      "  timers:\n",
      "    learn_throughput: 835.225\n",
      "    learn_time_ms: 9575.863\n",
      "    load_throughput: 178403.662\n",
      "    load_time_ms: 44.831\n",
      "    sample_throughput: 117.33\n",
      "    sample_time_ms: 68166.671\n",
      "    update_time_ms: 5.893\n",
      "  timestamp: 1631796391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 607848\n",
      "  training_iteration: 76\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 47.6/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         5374.82</td><td style=\"text-align: right;\">607848</td><td style=\"text-align: right;\">    1.54</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 615846\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-47-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.5\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 618\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2219432873110616\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017981485972689244\n",
      "          policy_loss: -0.044762362285407\n",
      "          total_loss: -0.008062835891921354\n",
      "          vf_explained_var: 0.04168079420924187\n",
      "          vf_loss: 0.05622173513176911\n",
      "    num_agent_steps_sampled: 615846\n",
      "    num_agent_steps_trained: 615846\n",
      "    num_steps_sampled: 615846\n",
      "    num_steps_trained: 615846\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.88468468468467\n",
      "    ram_util_percent: 43.26666666666668\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07994994134710837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.09717387845582\n",
      "    mean_inference_ms: 2.6824200092506114\n",
      "    mean_raw_obs_processing_ms: 0.8144706203366493\n",
      "  time_since_restore: 5452.071633577347\n",
      "  time_this_iter_s: 77.25593996047974\n",
      "  time_total_s: 5452.071633577347\n",
      "  timers:\n",
      "    learn_throughput: 835.361\n",
      "    learn_time_ms: 9574.301\n",
      "    load_throughput: 180400.463\n",
      "    load_time_ms: 44.335\n",
      "    sample_throughput: 117.052\n",
      "    sample_time_ms: 68328.637\n",
      "    update_time_ms: 5.742\n",
      "  timestamp: 1631796469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615846\n",
      "  training_iteration: 77\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 47.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         5452.07</td><td style=\"text-align: right;\">615846</td><td style=\"text-align: right;\">     1.5</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 623844\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-49-09\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 624\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.328553036464158\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.019976390425434466\n",
      "          policy_loss: -0.05591437352560861\n",
      "          total_loss: -0.051167637960965275\n",
      "          vf_explained_var: -0.04735914617776871\n",
      "          vf_loss: 0.025035805979108948\n",
      "    num_agent_steps_sampled: 623844\n",
      "    num_agent_steps_trained: 623844\n",
      "    num_steps_sampled: 623844\n",
      "    num_steps_trained: 623844\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.46052631578945\n",
      "    ram_util_percent: 43.51842105263158\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07997093237244272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.111166777301936\n",
      "    mean_inference_ms: 2.6819842473344178\n",
      "    mean_raw_obs_processing_ms: 0.8142639760189746\n",
      "  time_since_restore: 5532.426569223404\n",
      "  time_this_iter_s: 80.35493564605713\n",
      "  time_total_s: 5532.426569223404\n",
      "  timers:\n",
      "    learn_throughput: 830.75\n",
      "    learn_time_ms: 9627.443\n",
      "    load_throughput: 184023.976\n",
      "    load_time_ms: 43.462\n",
      "    sample_throughput: 119.991\n",
      "    sample_time_ms: 66655.132\n",
      "    update_time_ms: 5.167\n",
      "  timestamp: 1631796549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 623844\n",
      "  training_iteration: 78\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         5532.43</td><td style=\"text-align: right;\">623844</td><td style=\"text-align: right;\">    1.41</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16563)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16560)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16555)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 631842\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-50-53\n",
      "  done: false\n",
      "  episode_len_mean: 992.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.45\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 633\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.278118989031802\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01930915653621572\n",
      "          policy_loss: -0.053118612011393875\n",
      "          total_loss: -0.0545981671230527\n",
      "          vf_explained_var: -0.09623102843761444\n",
      "          vf_loss: 0.018405259733735482\n",
      "    num_agent_steps_sampled: 631842\n",
      "    num_agent_steps_trained: 631842\n",
      "    num_steps_sampled: 631842\n",
      "    num_steps_trained: 631842\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.08513513513515\n",
      "    ram_util_percent: 43.76486486486487\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08004634832025634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.160397357871865\n",
      "    mean_inference_ms: 2.68665731422336\n",
      "    mean_raw_obs_processing_ms: 0.8229405301643211\n",
      "  time_since_restore: 5636.180264949799\n",
      "  time_this_iter_s: 103.75369572639465\n",
      "  time_total_s: 5636.180264949799\n",
      "  timers:\n",
      "    learn_throughput: 827.368\n",
      "    learn_time_ms: 9666.8\n",
      "    load_throughput: 180750.001\n",
      "    load_time_ms: 44.249\n",
      "    sample_throughput: 115.19\n",
      "    sample_time_ms: 69433.055\n",
      "    update_time_ms: 5.546\n",
      "  timestamp: 1631796653\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 631842\n",
      "  training_iteration: 79\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 48.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         5636.18</td><td style=\"text-align: right;\">631842</td><td style=\"text-align: right;\">    1.45</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            992.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 639840\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-52-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.46\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 642\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3093689540381073\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01739836994555741\n",
      "          policy_loss: -0.041263773735182016\n",
      "          total_loss: -0.03878351455524323\n",
      "          vf_explained_var: -0.005081591196358204\n",
      "          vf_loss: 0.022964194063059686\n",
      "    num_agent_steps_sampled: 639840\n",
      "    num_agent_steps_trained: 639840\n",
      "    num_steps_sampled: 639840\n",
      "    num_steps_trained: 639840\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.67606837606839\n",
      "    ram_util_percent: 44.35042735042736\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08010680823354491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.201170217520406\n",
      "    mean_inference_ms: 2.688709704632911\n",
      "    mean_raw_obs_processing_ms: 0.8262842767282921\n",
      "  time_since_restore: 5718.304565906525\n",
      "  time_this_iter_s: 82.12430095672607\n",
      "  time_total_s: 5718.304565906525\n",
      "  timers:\n",
      "    learn_throughput: 822.323\n",
      "    learn_time_ms: 9726.102\n",
      "    load_throughput: 172229.718\n",
      "    load_time_ms: 46.438\n",
      "    sample_throughput: 114.067\n",
      "    sample_time_ms: 70116.678\n",
      "    update_time_ms: 5.825\n",
      "  timestamp: 1631796735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 639840\n",
      "  training_iteration: 80\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 49.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">          5718.3</td><td style=\"text-align: right;\">639840</td><td style=\"text-align: right;\">    1.46</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 647838\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-53-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 648\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.394855607709577\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017199367281859595\n",
      "          policy_loss: -0.041253210868566266\n",
      "          total_loss: -0.048533000033949655\n",
      "          vf_explained_var: -0.1285753697156906\n",
      "          vf_loss: 0.014088859463827834\n",
      "    num_agent_steps_sampled: 647838\n",
      "    num_agent_steps_trained: 647838\n",
      "    num_steps_sampled: 647838\n",
      "    num_steps_trained: 647838\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.32499999999999\n",
      "    ram_util_percent: 45.49354838709679\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.080131820029271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.221360713196084\n",
      "    mean_inference_ms: 2.6876300340591297\n",
      "    mean_raw_obs_processing_ms: 0.8255882957344555\n",
      "  time_since_restore: 5804.700944662094\n",
      "  time_this_iter_s: 86.39637875556946\n",
      "  time_total_s: 5804.700944662094\n",
      "  timers:\n",
      "    learn_throughput: 823.635\n",
      "    learn_time_ms: 9710.612\n",
      "    load_throughput: 177189.477\n",
      "    load_time_ms: 45.138\n",
      "    sample_throughput: 112.399\n",
      "    sample_time_ms: 71157.432\n",
      "    update_time_ms: 7.78\n",
      "  timestamp: 1631796821\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647838\n",
      "  training_iteration: 81\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 53.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">          5804.7</td><td style=\"text-align: right;\">647838</td><td style=\"text-align: right;\">    1.42</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 655836\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-55-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 657\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.343746301435655\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.020624471214702254\n",
      "          policy_loss: -0.055678876775807594\n",
      "          total_loss: -0.04447417800785393\n",
      "          vf_explained_var: 0.06630068272352219\n",
      "          vf_loss: 0.0315484900678125\n",
      "    num_agent_steps_sampled: 655836\n",
      "    num_agent_steps_trained: 655836\n",
      "    num_steps_sampled: 655836\n",
      "    num_steps_trained: 655836\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.39298245614034\n",
      "    ram_util_percent: 46.403508771929815\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08020773826545813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.274637080649395\n",
      "    mean_inference_ms: 2.6922336573620487\n",
      "    mean_raw_obs_processing_ms: 0.8256834256793114\n",
      "  time_since_restore: 5885.227603435516\n",
      "  time_this_iter_s: 80.52665877342224\n",
      "  time_total_s: 5885.227603435516\n",
      "  timers:\n",
      "    learn_throughput: 827.926\n",
      "    learn_time_ms: 9660.284\n",
      "    load_throughput: 180547.559\n",
      "    load_time_ms: 44.299\n",
      "    sample_throughput: 111.504\n",
      "    sample_time_ms: 71728.13\n",
      "    update_time_ms: 8.555\n",
      "  timestamp: 1631796902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 655836\n",
      "  training_iteration: 82\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 49.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         5885.23</td><td style=\"text-align: right;\">655836</td><td style=\"text-align: right;\">    1.38</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            996.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 663834\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-56-18\n",
      "  done: false\n",
      "  episode_len_mean: 996.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.34\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 666\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22500000000000006\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3456068351704586\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016780760691118713\n",
      "          policy_loss: -0.05063666874763145\n",
      "          total_loss: -0.0455327956236258\n",
      "          vf_explained_var: -0.0216665081679821\n",
      "          vf_loss: 0.024784270639316007\n",
      "    num_agent_steps_sampled: 663834\n",
      "    num_agent_steps_trained: 663834\n",
      "    num_steps_sampled: 663834\n",
      "    num_steps_trained: 663834\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.57129629629628\n",
      "    ram_util_percent: 45.1324074074074\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026076611730666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.318638386162114\n",
      "    mean_inference_ms: 2.6942752376262664\n",
      "    mean_raw_obs_processing_ms: 0.8254968460130033\n",
      "  time_since_restore: 5960.827925443649\n",
      "  time_this_iter_s: 75.60032200813293\n",
      "  time_total_s: 5960.827925443649\n",
      "  timers:\n",
      "    learn_throughput: 828.322\n",
      "    learn_time_ms: 9655.669\n",
      "    load_throughput: 174830.066\n",
      "    load_time_ms: 45.747\n",
      "    sample_throughput: 112.147\n",
      "    sample_time_ms: 71316.797\n",
      "    update_time_ms: 8.893\n",
      "  timestamp: 1631796978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 663834\n",
      "  training_iteration: 83\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 49.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         5960.83</td><td style=\"text-align: right;\">663834</td><td style=\"text-align: right;\">    1.34</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            996.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 671832\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 996.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 672\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22500000000000006\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4194439667527394\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015528270042876994\n",
      "          policy_loss: -0.041746577913422254\n",
      "          total_loss: -0.03560349635118919\n",
      "          vf_explained_var: -0.20198941230773926\n",
      "          vf_loss: 0.02684366079965814\n",
      "    num_agent_steps_sampled: 671832\n",
      "    num_agent_steps_trained: 671832\n",
      "    num_steps_sampled: 671832\n",
      "    num_steps_trained: 671832\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.93027522935779\n",
      "    ram_util_percent: 45.06422018348624\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08028185663954415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.339245130260586\n",
      "    mean_inference_ms: 2.6932243387477555\n",
      "    mean_raw_obs_processing_ms: 0.8249847070972015\n",
      "  time_since_restore: 6037.373907566071\n",
      "  time_this_iter_s: 76.54598212242126\n",
      "  time_total_s: 6037.373907566071\n",
      "  timers:\n",
      "    learn_throughput: 828.425\n",
      "    learn_time_ms: 9654.465\n",
      "    load_throughput: 173649.314\n",
      "    load_time_ms: 46.058\n",
      "    sample_throughput: 111.887\n",
      "    sample_time_ms: 71482.812\n",
      "    update_time_ms: 8.895\n",
      "  timestamp: 1631797054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 671832\n",
      "  training_iteration: 84\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 49.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         6037.37</td><td style=\"text-align: right;\">671832</td><td style=\"text-align: right;\">    1.41</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            996.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 679830\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_12-58-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.38\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 681\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22500000000000006\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3099025872445877\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015479124373707272\n",
      "          policy_loss: -0.04808092906630488\n",
      "          total_loss: -0.04497974758907672\n",
      "          vf_explained_var: 0.023173486813902855\n",
      "          vf_loss: 0.0227174040930432\n",
      "    num_agent_steps_sampled: 679830\n",
      "    num_agent_steps_trained: 679830\n",
      "    num_steps_sampled: 679830\n",
      "    num_steps_trained: 679830\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.84545454545454\n",
      "    ram_util_percent: 45.00272727272728\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08034248018190006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.39106313998128\n",
      "    mean_inference_ms: 2.697477871951644\n",
      "    mean_raw_obs_processing_ms: 0.82537243696856\n",
      "  time_since_restore: 6114.020923376083\n",
      "  time_this_iter_s: 76.64701581001282\n",
      "  time_total_s: 6114.020923376083\n",
      "  timers:\n",
      "    learn_throughput: 830.327\n",
      "    learn_time_ms: 9632.349\n",
      "    load_throughput: 175866.321\n",
      "    load_time_ms: 45.478\n",
      "    sample_throughput: 111.234\n",
      "    sample_time_ms: 71902.577\n",
      "    update_time_ms: 8.918\n",
      "  timestamp: 1631797131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 679830\n",
      "  training_iteration: 85\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 49.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         6114.02</td><td style=\"text-align: right;\">679830</td><td style=\"text-align: right;\">    1.38</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            996.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 687828\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_13-00-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 690\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22500000000000006\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3273062923903107\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016470402905133446\n",
      "          policy_loss: -0.04300591170597541\n",
      "          total_loss: -0.03127206518835518\n",
      "          vf_explained_var: -0.08418731391429901\n",
      "          vf_loss: 0.0313010694485681\n",
      "    num_agent_steps_sampled: 687828\n",
      "    num_agent_steps_trained: 687828\n",
      "    num_steps_sampled: 687828\n",
      "    num_steps_trained: 687828\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.33805309734514\n",
      "    ram_util_percent: 45.83451327433629\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08039094481280942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.43437404706603\n",
      "    mean_inference_ms: 2.699310687599521\n",
      "    mean_raw_obs_processing_ms: 0.8255350538149618\n",
      "  time_since_restore: 6193.364999771118\n",
      "  time_this_iter_s: 79.34407639503479\n",
      "  time_total_s: 6193.364999771118\n",
      "  timers:\n",
      "    learn_throughput: 832.359\n",
      "    learn_time_ms: 9608.834\n",
      "    load_throughput: 177141.104\n",
      "    load_time_ms: 45.15\n",
      "    sample_throughput: 110.846\n",
      "    sample_time_ms: 72154.376\n",
      "    update_time_ms: 8.462\n",
      "  timestamp: 1631797210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 687828\n",
      "  training_iteration: 86\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 49.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         6193.36</td><td style=\"text-align: right;\">687828</td><td style=\"text-align: right;\">    1.44</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            996.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_8f2a6_00000:\n",
      "  agent_timesteps_total: 695826\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-16_13-01-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 1.42\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 696\n",
      "  experiment_id: 43f4be814be443d08f2f6e093d3d4a7f\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22500000000000006\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.37692408023342\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015367304387735058\n",
      "          policy_loss: -0.04452828146487234\n",
      "          total_loss: -0.04383521518239411\n",
      "          vf_explained_var: -0.2191319763660431\n",
      "          vf_loss: 0.021004663283641884\n",
      "    num_agent_steps_sampled: 695826\n",
      "    num_agent_steps_trained: 695826\n",
      "    num_steps_sampled: 695826\n",
      "    num_steps_trained: 695826\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.78055555555555\n",
      "    ram_util_percent: 45.604629629629635\n",
      "  pid: 16561\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08041337471935098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.454584630634965\n",
      "    mean_inference_ms: 2.6982721266068985\n",
      "    mean_raw_obs_processing_ms: 0.82533995698897\n",
      "  time_since_restore: 6269.009223937988\n",
      "  time_this_iter_s: 75.64422416687012\n",
      "  time_total_s: 6269.009223937988\n",
      "  timers:\n",
      "    learn_throughput: 830.709\n",
      "    learn_time_ms: 9627.923\n",
      "    load_throughput: 176998.477\n",
      "    load_time_ms: 45.187\n",
      "    sample_throughput: 111.122\n",
      "    sample_time_ms: 71974.647\n",
      "    update_time_ms: 8.333\n",
      "  timestamp: 1631797286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 695826\n",
      "  training_iteration: 87\n",
      "  trial_id: 8f2a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 49.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/60.08 GiB heap, 0.0/29.74 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-16_11-16-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8f2a6_00000</td><td>RUNNING </td><td>192.168.1.96:16561</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         6269.01</td><td style=\"text-align: right;\">695826</td><td style=\"text-align: right;\">    1.42</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            996.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C22 not pretrained\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])#callbacks=[\n",
    "        #    CustomLoggerCallback(),\n",
    "        #])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c1f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
