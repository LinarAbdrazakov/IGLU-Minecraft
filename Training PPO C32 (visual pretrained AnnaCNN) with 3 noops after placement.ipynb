{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C32']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 14:10:16,231\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-06 14:10:16,240\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=175)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=175)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C32 pretrained (AnnaCNN) (3 noops after placement)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/21374_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/21374_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211006_141016-21374_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=175)\u001b[0m 2021-10-06 14:10:19,757\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=175)\u001b[0m 2021-10-06 14:10:19,757\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=175)\u001b[0m 2021-10-06 14:10:28,630\tINFO trainable.py:109 -- Trainable.setup took 11.389 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=175)\u001b[0m 2021-10-06 14:10:28,631\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-11-27\n",
      "  done: false\n",
      "  episode_len_mean: 966.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5071485141913096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02036470083990674\n",
      "          policy_loss: 0.017980292439460754\n",
      "          total_loss: 0.0417120463318295\n",
      "          vf_explained_var: 0.6013163924217224\n",
      "          vf_loss: 0.024730300173784296\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.421176470588232\n",
      "    ram_util_percent: 50.48941176470588\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039277376828493776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 56.791792859087934\n",
      "    mean_inference_ms: 1.4206312753103831\n",
      "    mean_raw_obs_processing_ms: 0.12659716915774655\n",
      "  time_since_restore: 59.195371866226196\n",
      "  time_this_iter_s: 59.195371866226196\n",
      "  time_total_s: 59.195371866226196\n",
      "  timers:\n",
      "    learn_throughput: 1439.307\n",
      "    learn_time_ms: 694.779\n",
      "    load_throughput: 62813.43\n",
      "    load_time_ms: 15.92\n",
      "    sample_throughput: 17.1\n",
      "    sample_time_ms: 58479.94\n",
      "    update_time_ms: 2.077\n",
      "  timestamp: 1633529487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         59.1954</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               966</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-11-39\n",
      "  done: false\n",
      "  episode_len_mean: 873.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7572787642478943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019120520803810513\n",
      "          policy_loss: -0.14099260816971462\n",
      "          total_loss: -0.09946962280405892\n",
      "          vf_explained_var: 0.4845787584781647\n",
      "          vf_loss: 0.04335961723700166\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.94117647058823\n",
      "    ram_util_percent: 58.699999999999996\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877169002486126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.98674343825245\n",
      "    mean_inference_ms: 1.3980647101251822\n",
      "    mean_raw_obs_processing_ms: 0.12225565065490508\n",
      "  time_since_restore: 70.90327334403992\n",
      "  time_this_iter_s: 11.70790147781372\n",
      "  time_total_s: 70.90327334403992\n",
      "  timers:\n",
      "    learn_throughput: 1496.632\n",
      "    learn_time_ms: 668.167\n",
      "    load_throughput: 105055.893\n",
      "    load_time_ms: 9.519\n",
      "    sample_throughput: 28.761\n",
      "    sample_time_ms: 34769.282\n",
      "    update_time_ms: 2.09\n",
      "  timestamp: 1633529499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         70.9033</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               873</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-11-52\n",
      "  done: false\n",
      "  episode_len_mean: 841.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9105160368813409\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014182283091645351\n",
      "          policy_loss: 0.04532004056705369\n",
      "          total_loss: 0.05650502944158183\n",
      "          vf_explained_var: 0.7249862551689148\n",
      "          vf_loss: 0.01603546527007388\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.67777777777778\n",
      "    ram_util_percent: 58.82222222222222\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844931293954191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.536945180526324\n",
      "    mean_inference_ms: 1.3840035188978843\n",
      "    mean_raw_obs_processing_ms: 0.11977396109113836\n",
      "  time_since_restore: 83.58182454109192\n",
      "  time_this_iter_s: 12.678551197052002\n",
      "  time_total_s: 83.58182454109192\n",
      "  timers:\n",
      "    learn_throughput: 1516.762\n",
      "    learn_time_ms: 659.299\n",
      "    load_throughput: 134943.183\n",
      "    load_time_ms: 7.411\n",
      "    sample_throughput: 36.779\n",
      "    sample_time_ms: 27189.407\n",
      "    update_time_ms: 1.954\n",
      "  timestamp: 1633529512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         83.5818</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               841</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-12-05\n",
      "  done: false\n",
      "  episode_len_mean: 758.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 5\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.237285042471356\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018882099107416374\n",
      "          policy_loss: 0.030106200464069845\n",
      "          total_loss: 0.03896068860259321\n",
      "          vf_explained_var: 0.6835134029388428\n",
      "          vf_loss: 0.0155627084016386\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.56842105263158\n",
      "    ram_util_percent: 58.91052631578947\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03808896905731776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.947047503779366\n",
      "    mean_inference_ms: 1.3672154206380536\n",
      "    mean_raw_obs_processing_ms: 0.12024517333936195\n",
      "  time_since_restore: 97.05998110771179\n",
      "  time_this_iter_s: 13.478156566619873\n",
      "  time_total_s: 97.05998110771179\n",
      "  timers:\n",
      "    learn_throughput: 1531.042\n",
      "    learn_time_ms: 653.15\n",
      "    load_throughput: 158139.861\n",
      "    load_time_ms: 6.324\n",
      "    sample_throughput: 42.371\n",
      "    sample_time_ms: 23601.098\n",
      "    update_time_ms: 1.879\n",
      "  timestamp: 1633529525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">           97.06</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             758.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-12-19\n",
      "  done: false\n",
      "  episode_len_mean: 738.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3801479564772712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006479052537750837\n",
      "          policy_loss: -0.20445111791292828\n",
      "          total_loss: -0.20725220864017804\n",
      "          vf_explained_var: 0.5822589993476868\n",
      "          vf_loss: 0.009056675348741312\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.27894736842104\n",
      "    ram_util_percent: 59.01052631578948\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03798654637861598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.936200374985507\n",
      "    mean_inference_ms: 1.3622690411703129\n",
      "    mean_raw_obs_processing_ms: 0.11993724260702592\n",
      "  time_since_restore: 110.40587568283081\n",
      "  time_this_iter_s: 13.345894575119019\n",
      "  time_total_s: 110.40587568283081\n",
      "  timers:\n",
      "    learn_throughput: 1514.39\n",
      "    learn_time_ms: 660.332\n",
      "    load_throughput: 176324.609\n",
      "    load_time_ms: 5.671\n",
      "    sample_throughput: 46.706\n",
      "    sample_time_ms: 21410.752\n",
      "    update_time_ms: 1.852\n",
      "  timestamp: 1633529539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         110.406</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             738.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-12-32\n",
      "  done: false\n",
      "  episode_len_mean: 721.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 8\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2484274956915113\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0348012464928598\n",
      "          policy_loss: 0.15753751479917102\n",
      "          total_loss: 0.15988288621107738\n",
      "          vf_explained_var: 0.3884495198726654\n",
      "          vf_loss: 0.004389277400655879\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.43157894736842\n",
      "    ram_util_percent: 59.53684210526315\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03785228875221845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.04286365913275\n",
      "    mean_inference_ms: 1.3557445653704194\n",
      "    mean_raw_obs_processing_ms: 0.12053882548549609\n",
      "  time_since_restore: 123.35051131248474\n",
      "  time_this_iter_s: 12.94463562965393\n",
      "  time_total_s: 123.35051131248474\n",
      "  timers:\n",
      "    learn_throughput: 1523.506\n",
      "    learn_time_ms: 656.381\n",
      "    load_throughput: 189840.483\n",
      "    load_time_ms: 5.268\n",
      "    sample_throughput: 50.271\n",
      "    sample_time_ms: 19892.37\n",
      "    update_time_ms: 1.833\n",
      "  timestamp: 1633529552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         123.351</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            721.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-12-46\n",
      "  done: false\n",
      "  episode_len_mean: 699.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 10\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3315726942486232\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014072222721292436\n",
      "          policy_loss: -0.24840349265270764\n",
      "          total_loss: -0.2501309122476313\n",
      "          vf_explained_var: 0.9145650267601013\n",
      "          vf_loss: 0.005255805984294663\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.115\n",
      "    ram_util_percent: 59.44000000000001\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03773437146003659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.12042992074912\n",
      "    mean_inference_ms: 1.350759663135276\n",
      "    mean_raw_obs_processing_ms: 0.12146584213440233\n",
      "  time_since_restore: 137.3300747871399\n",
      "  time_this_iter_s: 13.979563474655151\n",
      "  time_total_s: 137.3300747871399\n",
      "  timers:\n",
      "    learn_throughput: 1531.901\n",
      "    learn_time_ms: 652.784\n",
      "    load_throughput: 201783.661\n",
      "    load_time_ms: 4.956\n",
      "    sample_throughput: 52.752\n",
      "    sample_time_ms: 18956.487\n",
      "    update_time_ms: 1.794\n",
      "  timestamp: 1633529566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">          137.33</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             699.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-12-59\n",
      "  done: false\n",
      "  episode_len_mean: 687.4545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5393257021903992\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013605327564017718\n",
      "          policy_loss: 0.04146591822306315\n",
      "          total_loss: 0.03747817675272624\n",
      "          vf_explained_var: 0.5638198852539062\n",
      "          vf_loss: 0.005283113080076873\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.327777777777776\n",
      "    ram_util_percent: 59.43333333333333\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037685868362308465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.348782363031493\n",
      "    mean_inference_ms: 1.348562074098854\n",
      "    mean_raw_obs_processing_ms: 0.12161013018131417\n",
      "  time_since_restore: 150.4939568042755\n",
      "  time_this_iter_s: 13.16388201713562\n",
      "  time_total_s: 150.4939568042755\n",
      "  timers:\n",
      "    learn_throughput: 1533.916\n",
      "    learn_time_ms: 651.926\n",
      "    load_throughput: 212227.442\n",
      "    load_time_ms: 4.712\n",
      "    sample_throughput: 55.094\n",
      "    sample_time_ms: 18150.767\n",
      "    update_time_ms: 1.769\n",
      "  timestamp: 1633529579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         150.494</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           687.455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-13-13\n",
      "  done: false\n",
      "  episode_len_mean: 673.1538461538462\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 13\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6309255401293437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01687483100464541\n",
      "          policy_loss: 0.047973526186413235\n",
      "          total_loss: 0.043344876170158385\n",
      "          vf_explained_var: 0.3448779284954071\n",
      "          vf_loss: 0.004086927717111798\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.576190476190476\n",
      "    ram_util_percent: 59.480952380952395\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037606453388848014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.08569979294328\n",
      "    mean_inference_ms: 1.3447766929719738\n",
      "    mean_raw_obs_processing_ms: 0.12217658740681515\n",
      "  time_since_restore: 164.84367084503174\n",
      "  time_this_iter_s: 14.349714040756226\n",
      "  time_total_s: 164.84367084503174\n",
      "  timers:\n",
      "    learn_throughput: 1535.98\n",
      "    learn_time_ms: 651.05\n",
      "    load_throughput: 220953.121\n",
      "    load_time_ms: 4.526\n",
      "    sample_throughput: 56.638\n",
      "    sample_time_ms: 17656.043\n",
      "    update_time_ms: 1.768\n",
      "  timestamp: 1633529593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         164.844</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           673.154</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-13-29\n",
      "  done: false\n",
      "  episode_len_mean: 661.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 15\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6917937080065408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01106552777327337\n",
      "          policy_loss: -0.06258109625842836\n",
      "          total_loss: -0.07094924946626027\n",
      "          vf_explained_var: 0.6892577409744263\n",
      "          vf_loss: 0.0035702959259247616\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.809090909090905\n",
      "    ram_util_percent: 59.536363636363646\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03754783144964124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.119269863116248\n",
      "    mean_inference_ms: 1.341921556698804\n",
      "    mean_raw_obs_processing_ms: 0.12292581141166835\n",
      "  time_since_restore: 180.18283343315125\n",
      "  time_this_iter_s: 15.339162588119507\n",
      "  time_total_s: 180.18283343315125\n",
      "  timers:\n",
      "    learn_throughput: 1530.677\n",
      "    learn_time_ms: 653.306\n",
      "    load_throughput: 227774.284\n",
      "    load_time_ms: 4.39\n",
      "    sample_throughput: 57.616\n",
      "    sample_time_ms: 17356.228\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1633529609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         180.183</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             661.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-13-44\n",
      "  done: false\n",
      "  episode_len_mean: 646.9411764705883\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 17\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.573128412829505\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006799255701171268\n",
      "          policy_loss: -0.16901106304592556\n",
      "          total_loss: -0.17974253661102718\n",
      "          vf_explained_var: 0.8314281702041626\n",
      "          vf_loss: 0.0019401410355284396\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.345454545454544\n",
      "    ram_util_percent: 59.76818181818181\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037508972236099725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.353418952339396\n",
      "    mean_inference_ms: 1.339805132935056\n",
      "    mean_raw_obs_processing_ms: 0.12371761286232197\n",
      "  time_since_restore: 195.71282172203064\n",
      "  time_this_iter_s: 15.529988288879395\n",
      "  time_total_s: 195.71282172203064\n",
      "  timers:\n",
      "    learn_throughput: 1541.047\n",
      "    learn_time_ms: 648.909\n",
      "    load_throughput: 321851.471\n",
      "    load_time_ms: 3.107\n",
      "    sample_throughput: 76.95\n",
      "    sample_time_ms: 12995.41\n",
      "    update_time_ms: 1.732\n",
      "  timestamp: 1633529624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         195.713</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           646.941</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-13-57\n",
      "  done: false\n",
      "  episode_len_mean: 640.6111111111111\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7682346635394626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015775486535120897\n",
      "          policy_loss: -0.029562983330753113\n",
      "          total_loss: -0.03450597839223014\n",
      "          vf_explained_var: 0.6856840252876282\n",
      "          vf_loss: 0.005640376653496383\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.984210526315785\n",
      "    ram_util_percent: 59.62631578947368\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03749001213102779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.01383900189152\n",
      "    mean_inference_ms: 1.3387939384290113\n",
      "    mean_raw_obs_processing_ms: 0.12398150671541244\n",
      "  time_since_restore: 209.0138282775879\n",
      "  time_this_iter_s: 13.301006555557251\n",
      "  time_total_s: 209.0138282775879\n",
      "  timers:\n",
      "    learn_throughput: 1543.075\n",
      "    learn_time_ms: 648.056\n",
      "    load_throughput: 322695.862\n",
      "    load_time_ms: 3.099\n",
      "    sample_throughput: 76.013\n",
      "    sample_time_ms: 13155.621\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1633529637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         209.014</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           640.611</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-14-11\n",
      "  done: false\n",
      "  episode_len_mean: 637.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 20\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.033910017543369\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015038769193013174\n",
      "          policy_loss: 0.04858702673680253\n",
      "          total_loss: 0.038440572429034446\n",
      "          vf_explained_var: 0.5137497186660767\n",
      "          vf_loss: 0.0034252033504243527\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.463157894736845\n",
      "    ram_util_percent: 59.526315789473685\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037456188920026556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.404976156868322\n",
      "    mean_inference_ms: 1.336883477144652\n",
      "    mean_raw_obs_processing_ms: 0.1245967486000534\n",
      "  time_since_restore: 222.3134617805481\n",
      "  time_this_iter_s: 13.299633502960205\n",
      "  time_total_s: 222.3134617805481\n",
      "  timers:\n",
      "    learn_throughput: 1541.969\n",
      "    learn_time_ms: 648.522\n",
      "    load_throughput: 323242.985\n",
      "    load_time_ms: 3.094\n",
      "    sample_throughput: 75.659\n",
      "    sample_time_ms: 13217.241\n",
      "    update_time_ms: 1.697\n",
      "  timestamp: 1633529651\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         222.313</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            637.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 632.0454545454545\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 22\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0082857065730626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010726978662933343\n",
      "          policy_loss: 0.025617263134982853\n",
      "          total_loss: 0.016292735520336363\n",
      "          vf_explained_var: 0.5362178683280945\n",
      "          vf_loss: 0.005931187763861898\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.21\n",
      "    ram_util_percent: 59.33\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037424015384899596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.88631239672334\n",
      "    mean_inference_ms: 1.3351731963175308\n",
      "    mean_raw_obs_processing_ms: 0.12527179718642711\n",
      "  time_since_restore: 236.23878598213196\n",
      "  time_this_iter_s: 13.925324201583862\n",
      "  time_total_s: 236.23878598213196\n",
      "  timers:\n",
      "    learn_throughput: 1542.167\n",
      "    learn_time_ms: 648.438\n",
      "    load_throughput: 317449.688\n",
      "    load_time_ms: 3.15\n",
      "    sample_throughput: 75.404\n",
      "    sample_time_ms: 13261.974\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1633529665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         236.239</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           632.045</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-14-38\n",
      "  done: false\n",
      "  episode_len_mean: 629.9130434782609\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7029116961691115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007257166093498629\n",
      "          policy_loss: 0.03668283820152283\n",
      "          total_loss: 0.02722620674305492\n",
      "          vf_explained_var: 0.2606094777584076\n",
      "          vf_loss: 0.004306760321681698\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.136842105263156\n",
      "    ram_util_percent: 58.99473684210527\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03740895840400648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.65030785025289\n",
      "    mean_inference_ms: 1.334351864312515\n",
      "    mean_raw_obs_processing_ms: 0.12551706280924285\n",
      "  time_since_restore: 249.4398488998413\n",
      "  time_this_iter_s: 13.20106291770935\n",
      "  time_total_s: 249.4398488998413\n",
      "  timers:\n",
      "    learn_throughput: 1552.198\n",
      "    learn_time_ms: 644.248\n",
      "    load_throughput: 314215.38\n",
      "    load_time_ms: 3.183\n",
      "    sample_throughput: 75.462\n",
      "    sample_time_ms: 13251.669\n",
      "    update_time_ms: 1.691\n",
      "  timestamp: 1633529678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">          249.44</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           629.913</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-14-51\n",
      "  done: false\n",
      "  episode_len_mean: 627.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 25\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8636412964926825\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008762467419924865\n",
      "          policy_loss: 0.014001313596963882\n",
      "          total_loss: 0.0022644942419396507\n",
      "          vf_explained_var: -0.14676326513290405\n",
      "          vf_loss: 0.0029564852852167357\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29473684210526\n",
      "    ram_util_percent: 58.331578947368406\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03737867206974955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.218841257500106\n",
      "    mean_inference_ms: 1.3327902886659715\n",
      "    mean_raw_obs_processing_ms: 0.12609019007560412\n",
      "  time_since_restore: 262.8488335609436\n",
      "  time_this_iter_s: 13.408984661102295\n",
      "  time_total_s: 262.8488335609436\n",
      "  timers:\n",
      "    learn_throughput: 1549.386\n",
      "    learn_time_ms: 645.417\n",
      "    load_throughput: 314491.032\n",
      "    load_time_ms: 3.18\n",
      "    sample_throughput: 75.205\n",
      "    sample_time_ms: 13296.949\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1633529691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         262.849</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            627.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-15-05\n",
      "  done: false\n",
      "  episode_len_mean: 625.2962962962963\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 27\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9211766110526192\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01160630989442579\n",
      "          policy_loss: -0.04057098492566082\n",
      "          total_loss: -0.05114153027534485\n",
      "          vf_explained_var: 0.33848413825035095\n",
      "          vf_loss: 0.003418383117241319\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.51\n",
      "    ram_util_percent: 57.71500000000001\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03735048682185891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.839788039388004\n",
      "    mean_inference_ms: 1.3314050844561314\n",
      "    mean_raw_obs_processing_ms: 0.12670584946185168\n",
      "  time_since_restore: 276.6787419319153\n",
      "  time_this_iter_s: 13.82990837097168\n",
      "  time_total_s: 276.6787419319153\n",
      "  timers:\n",
      "    learn_throughput: 1546.214\n",
      "    learn_time_ms: 646.741\n",
      "    load_throughput: 314429.734\n",
      "    load_time_ms: 3.18\n",
      "    sample_throughput: 75.298\n",
      "    sample_time_ms: 13280.641\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1633529705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         276.679</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           625.296</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 622.8571428571429\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7786972721417744\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01038574451884641\n",
      "          policy_loss: -0.09402968933184942\n",
      "          total_loss: -0.09938083932631546\n",
      "          vf_explained_var: 0.6572713851928711\n",
      "          vf_loss: 0.007762235470969851\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.76470588235294\n",
      "    ram_util_percent: 57.2235294117647\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0373367127680904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.662222619707386\n",
      "    mean_inference_ms: 1.3307433543104195\n",
      "    mean_raw_obs_processing_ms: 0.12715508043423396\n",
      "  time_since_restore: 288.79337215423584\n",
      "  time_this_iter_s: 12.114630222320557\n",
      "  time_total_s: 288.79337215423584\n",
      "  timers:\n",
      "    learn_throughput: 1548.823\n",
      "    learn_time_ms: 645.652\n",
      "    load_throughput: 312285.31\n",
      "    load_time_ms: 3.202\n",
      "    sample_throughput: 75.891\n",
      "    sample_time_ms: 13176.771\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1633529717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         288.793</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           622.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-15-45\n",
      "  done: false\n",
      "  episode_len_mean: 628.5333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 30\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0220434374279446\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002352285615261233\n",
      "          policy_loss: 0.12548725762301022\n",
      "          total_loss: 0.10655476500590642\n",
      "          vf_explained_var: 0.005254317540675402\n",
      "          vf_loss: 0.00022941265985233864\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.6575\n",
      "    ram_util_percent: 56.345000000000006\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03730987196974779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.32366484222461\n",
      "    mean_inference_ms: 1.3294668348565102\n",
      "    mean_raw_obs_processing_ms: 0.1894136649036966\n",
      "  time_since_restore: 316.93917536735535\n",
      "  time_this_iter_s: 28.145803213119507\n",
      "  time_total_s: 316.93917536735535\n",
      "  timers:\n",
      "    learn_throughput: 1547.11\n",
      "    learn_time_ms: 646.366\n",
      "    load_throughput: 220884.21\n",
      "    load_time_ms: 4.527\n",
      "    sample_throughput: 68.708\n",
      "    sample_time_ms: 14554.357\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1633529745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         316.939</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           628.533</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-16-04\n",
      "  done: false\n",
      "  episode_len_mean: 619.0625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 32\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.415374745262994\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010805874109853262\n",
      "          policy_loss: 0.07235568546586567\n",
      "          total_loss: 0.05318358954456118\n",
      "          vf_explained_var: -0.18650773167610168\n",
      "          vf_loss: 0.0025503262166037327\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.651851851851866\n",
      "    ram_util_percent: 59.43333333333334\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037294242629842704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.036847306563605\n",
      "    mean_inference_ms: 1.328457529497946\n",
      "    mean_raw_obs_processing_ms: 0.24110392434521538\n",
      "  time_since_restore: 335.7838213443756\n",
      "  time_this_iter_s: 18.844645977020264\n",
      "  time_total_s: 335.7838213443756\n",
      "  timers:\n",
      "    learn_throughput: 1554.599\n",
      "    learn_time_ms: 643.253\n",
      "    load_throughput: 167735.929\n",
      "    load_time_ms: 5.962\n",
      "    sample_throughput: 67.084\n",
      "    sample_time_ms: 14906.618\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1633529764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         335.784</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           619.062</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-16-23\n",
      "  done: false\n",
      "  episode_len_mean: 608.6764705882352\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 34\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3879616234037613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018679893227057844\n",
      "          policy_loss: -0.028022249870830112\n",
      "          total_loss: -0.044779184460639956\n",
      "          vf_explained_var: 0.22471220791339874\n",
      "          vf_loss: 0.002919705363132784\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.22222222222222\n",
      "    ram_util_percent: 59.38148148148148\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037280446641263565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.79101233681053\n",
      "    mean_inference_ms: 1.3275766439926218\n",
      "    mean_raw_obs_processing_ms: 0.28428188415819833\n",
      "  time_since_restore: 354.3125259876251\n",
      "  time_this_iter_s: 18.52870464324951\n",
      "  time_total_s: 354.3125259876251\n",
      "  timers:\n",
      "    learn_throughput: 1552.629\n",
      "    learn_time_ms: 644.069\n",
      "    load_throughput: 137032.485\n",
      "    load_time_ms: 7.298\n",
      "    sample_throughput: 65.771\n",
      "    sample_time_ms: 15204.337\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1633529783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         354.313</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           608.676</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 599.0555555555555\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 36\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3926669862535266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01506703068843272\n",
      "          policy_loss: -0.008186367319689856\n",
      "          total_loss: -0.026808388779560724\n",
      "          vf_explained_var: 0.312719464302063\n",
      "          vf_loss: 0.0019145689102717572\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.685714285714276\n",
      "    ram_util_percent: 59.114285714285714\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03726925716180156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.58183506905282\n",
      "    mean_inference_ms: 1.3268214881425768\n",
      "    mean_raw_obs_processing_ms: 0.32058997362980707\n",
      "  time_since_restore: 374.15628457069397\n",
      "  time_this_iter_s: 19.843758583068848\n",
      "  time_total_s: 374.15628457069397\n",
      "  timers:\n",
      "    learn_throughput: 1530.781\n",
      "    learn_time_ms: 653.261\n",
      "    load_throughput: 121235.273\n",
      "    load_time_ms: 8.248\n",
      "    sample_throughput: 63.098\n",
      "    sample_time_ms: 15848.345\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1633529803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         374.156</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           599.056</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-17-01\n",
      "  done: false\n",
      "  episode_len_mean: 588.8974358974359\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 39\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4023157119750977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01362160603421055\n",
      "          policy_loss: 0.010513169649574492\n",
      "          total_loss: -0.009322771181662878\n",
      "          vf_explained_var: -0.27893152832984924\n",
      "          vf_loss: 0.0011223516582200925\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.88846153846154\n",
      "    ram_util_percent: 59.34615384615385\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037258369330122575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.31458767506488\n",
      "    mean_inference_ms: 1.3259418354958394\n",
      "    mean_raw_obs_processing_ms: 0.3655986510181884\n",
      "  time_since_restore: 392.3394684791565\n",
      "  time_this_iter_s: 18.183183908462524\n",
      "  time_total_s: 392.3394684791565\n",
      "  timers:\n",
      "    learn_throughput: 1531.501\n",
      "    learn_time_ms: 652.954\n",
      "    load_throughput: 103736.981\n",
      "    load_time_ms: 9.64\n",
      "    sample_throughput: 61.216\n",
      "    sample_time_ms: 16335.625\n",
      "    update_time_ms: 1.702\n",
      "  timestamp: 1633529821\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         392.339</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           588.897</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-17-19\n",
      "  done: false\n",
      "  episode_len_mean: 582.8048780487804\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 41\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4834727419747247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011574483860775824\n",
      "          policy_loss: -0.14559343732479546\n",
      "          total_loss: -0.16671252734959124\n",
      "          vf_explained_var: -0.4205456078052521\n",
      "          vf_loss: 0.001111378644903501\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.13461538461539\n",
      "    ram_util_percent: 59.10769230769232\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0372518569298687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.16179052275483\n",
      "    mean_inference_ms: 1.3254463385637472\n",
      "    mean_raw_obs_processing_ms: 0.39041673386470577\n",
      "  time_since_restore: 410.45883417129517\n",
      "  time_this_iter_s: 18.119365692138672\n",
      "  time_total_s: 410.45883417129517\n",
      "  timers:\n",
      "    learn_throughput: 1525.887\n",
      "    learn_time_ms: 655.356\n",
      "    load_throughput: 91372.205\n",
      "    load_time_ms: 10.944\n",
      "    sample_throughput: 59.697\n",
      "    sample_time_ms: 16751.341\n",
      "    update_time_ms: 1.705\n",
      "  timestamp: 1633529839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         410.459</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           582.805</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-17-37\n",
      "  done: false\n",
      "  episode_len_mean: 577.4883720930233\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 43\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4518007066514755\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012524637337005571\n",
      "          policy_loss: -0.11472606667213969\n",
      "          total_loss: -0.1340815024657382\n",
      "          vf_explained_var: -0.9340353012084961\n",
      "          vf_loss: 0.0023445265367627146\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.071999999999996\n",
      "    ram_util_percent: 59.507999999999996\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724764205367835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.025594706589555\n",
      "    mean_inference_ms: 1.3250474772122336\n",
      "    mean_raw_obs_processing_ms: 0.4115925735663329\n",
      "  time_since_restore: 428.2072193622589\n",
      "  time_this_iter_s: 17.748385190963745\n",
      "  time_total_s: 428.2072193622589\n",
      "  timers:\n",
      "    learn_throughput: 1514.624\n",
      "    learn_time_ms: 660.23\n",
      "    load_throughput: 81715.074\n",
      "    load_time_ms: 12.238\n",
      "    sample_throughput: 58.14\n",
      "    sample_time_ms: 17199.854\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1633529857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         428.207</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           577.488</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-17-53\n",
      "  done: false\n",
      "  episode_len_mean: 575.8222222222222\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 45\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1440978712505765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018279019161902457\n",
      "          policy_loss: 0.014767130878236558\n",
      "          total_loss: 0.0004511662241485384\n",
      "          vf_explained_var: -0.042492207139730453\n",
      "          vf_loss: 0.0030122344201016756\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.595652173913045\n",
      "    ram_util_percent: 60.24782608695654\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724667394300051\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.900490977905225\n",
      "    mean_inference_ms: 1.3247695077622872\n",
      "    mean_raw_obs_processing_ms: 0.429738227868575\n",
      "  time_since_restore: 444.2110860347748\n",
      "  time_this_iter_s: 16.00386667251587\n",
      "  time_total_s: 444.2110860347748\n",
      "  timers:\n",
      "    learn_throughput: 1496.007\n",
      "    learn_time_ms: 668.446\n",
      "    load_throughput: 81763.978\n",
      "    load_time_ms: 12.23\n",
      "    sample_throughput: 57.309\n",
      "    sample_time_ms: 17449.366\n",
      "    update_time_ms: 3.03\n",
      "  timestamp: 1633529873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         444.211</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           575.822</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 573.7234042553191\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 47\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2811380439334448\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010784412147657128\n",
      "          policy_loss: -0.05398653737372822\n",
      "          total_loss: -0.07198821165495449\n",
      "          vf_explained_var: 0.019241265952587128\n",
      "          vf_loss: 0.0023832121123430424\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.270833333333336\n",
      "    ram_util_percent: 60.6\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724735073040207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.786466798531077\n",
      "    mean_inference_ms: 1.3245549169265805\n",
      "    mean_raw_obs_processing_ms: 0.4453025875175454\n",
      "  time_since_restore: 460.9313955307007\n",
      "  time_this_iter_s: 16.720309495925903\n",
      "  time_total_s: 460.9313955307007\n",
      "  timers:\n",
      "    learn_throughput: 1493.97\n",
      "    learn_time_ms: 669.357\n",
      "    load_throughput: 79218.076\n",
      "    load_time_ms: 12.623\n",
      "    sample_throughput: 56.379\n",
      "    sample_time_ms: 17737.095\n",
      "    update_time_ms: 3.033\n",
      "  timestamp: 1633529890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         460.931</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           573.723</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-18-26\n",
      "  done: false\n",
      "  episode_len_mean: 570.9591836734694\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 49\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.321840148501926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012037205824069997\n",
      "          policy_loss: -0.09495010715391901\n",
      "          total_loss: -0.11391078556577365\n",
      "          vf_explained_var: -0.7240142226219177\n",
      "          vf_loss: 0.0015493477526534762\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.86086956521739\n",
      "    ram_util_percent: 60.230434782608675\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724717203853679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.68103717488278\n",
      "    mean_inference_ms: 1.3243420006821516\n",
      "    mean_raw_obs_processing_ms: 0.4586675416943813\n",
      "  time_since_restore: 476.84364891052246\n",
      "  time_this_iter_s: 15.912253379821777\n",
      "  time_total_s: 476.84364891052246\n",
      "  timers:\n",
      "    learn_throughput: 1493.275\n",
      "    learn_time_ms: 669.669\n",
      "    load_throughput: 79248.909\n",
      "    load_time_ms: 12.618\n",
      "    sample_throughput: 55.198\n",
      "    sample_time_ms: 18116.562\n",
      "    update_time_ms: 3.029\n",
      "  timestamp: 1633529906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         476.844</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           570.959</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-18-42\n",
      "  done: false\n",
      "  episode_len_mean: 570.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 50\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1659639199574787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008767390166521673\n",
      "          policy_loss: -0.15171853767500984\n",
      "          total_loss: -0.17039725076821116\n",
      "          vf_explained_var: -0.5867584943771362\n",
      "          vf_loss: 0.0010082620923640207\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.439130434782605\n",
      "    ram_util_percent: 59.93913043478261\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724684563940126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.63153211691749\n",
      "    mean_inference_ms: 1.3242316112165815\n",
      "    mean_raw_obs_processing_ms: 0.46450762639762866\n",
      "  time_since_restore: 493.2653925418854\n",
      "  time_this_iter_s: 16.421743631362915\n",
      "  time_total_s: 493.2653925418854\n",
      "  timers:\n",
      "    learn_throughput: 1496.247\n",
      "    learn_time_ms: 668.339\n",
      "    load_throughput: 85040.591\n",
      "    load_time_ms: 11.759\n",
      "    sample_throughput: 59.01\n",
      "    sample_time_ms: 16946.339\n",
      "    update_time_ms: 3.03\n",
      "  timestamp: 1633529922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         493.265</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               570</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-18-59\n",
      "  done: false\n",
      "  episode_len_mean: 565.377358490566\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 53\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.030163052346971\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013700035027653396\n",
      "          policy_loss: 0.038079239428043365\n",
      "          total_loss: 0.02203517576886548\n",
      "          vf_explained_var: -0.5337970852851868\n",
      "          vf_loss: 0.0011750592099916603\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.536\n",
      "    ram_util_percent: 59.60800000000001\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037245264916554284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.495747830231803\n",
      "    mean_inference_ms: 1.323912108786179\n",
      "    mean_raw_obs_processing_ms: 0.47968943185024193\n",
      "  time_since_restore: 510.52127385139465\n",
      "  time_this_iter_s: 17.255881309509277\n",
      "  time_total_s: 510.52127385139465\n",
      "  timers:\n",
      "    learn_throughput: 1497.698\n",
      "    learn_time_ms: 667.691\n",
      "    load_throughput: 87847.29\n",
      "    load_time_ms: 11.383\n",
      "    sample_throughput: 59.565\n",
      "    sample_time_ms: 16788.474\n",
      "    update_time_ms: 3.029\n",
      "  timestamp: 1633529939\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         510.521</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           565.377</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-19-16\n",
      "  done: false\n",
      "  episode_len_mean: 563.290909090909\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 55\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.306532796223958\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012281638065878024\n",
      "          policy_loss: -0.036201330295039545\n",
      "          total_loss: -0.05241681196623378\n",
      "          vf_explained_var: -0.5271282196044922\n",
      "          vf_loss: 0.004086479420463244\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.30833333333333\n",
      "    ram_util_percent: 59.15416666666666\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724348065500227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.414194318527677\n",
      "    mean_inference_ms: 1.323704704079963\n",
      "    mean_raw_obs_processing_ms: 0.48820175758756085\n",
      "  time_since_restore: 527.5553679466248\n",
      "  time_this_iter_s: 17.034094095230103\n",
      "  time_total_s: 527.5553679466248\n",
      "  timers:\n",
      "    learn_throughput: 1503.402\n",
      "    learn_time_ms: 665.158\n",
      "    load_throughput: 87734.465\n",
      "    load_time_ms: 11.398\n",
      "    sample_throughput: 60.091\n",
      "    sample_time_ms: 16641.527\n",
      "    update_time_ms: 3.035\n",
      "  timestamp: 1633529956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         527.555</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           563.291</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-19-33\n",
      "  done: false\n",
      "  episode_len_mean: 562.5535714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 56\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.082824687163035\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011409892622198055\n",
      "          policy_loss: -0.0010770115587446425\n",
      "          total_loss: -0.018179722792572444\n",
      "          vf_explained_var: -0.048923835158348083\n",
      "          vf_loss: 0.001158305869547702\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.76666666666667\n",
      "    ram_util_percent: 58.762499999999996\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724273368222898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.375924268054757\n",
      "    mean_inference_ms: 1.3236014859049618\n",
      "    mean_raw_obs_processing_ms: 0.49191063259123713\n",
      "  time_since_restore: 544.554080247879\n",
      "  time_this_iter_s: 16.998712301254272\n",
      "  time_total_s: 544.554080247879\n",
      "  timers:\n",
      "    learn_throughput: 1522.285\n",
      "    learn_time_ms: 656.907\n",
      "    load_throughput: 92152.936\n",
      "    load_time_ms: 10.852\n",
      "    sample_throughput: 61.103\n",
      "    sample_time_ms: 16365.925\n",
      "    update_time_ms: 2.977\n",
      "  timestamp: 1633529973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         544.554</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           562.554</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-19-53\n",
      "  done: false\n",
      "  episode_len_mean: 558.4067796610169\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 59\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.276282024383545\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010962610247752795\n",
      "          policy_loss: -0.005573253871666061\n",
      "          total_loss: -0.02327241376042366\n",
      "          vf_explained_var: -0.10978514701128006\n",
      "          vf_loss: 0.002597075110275505\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.510714285714286\n",
      "    ram_util_percent: 58.18571428571429\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03723998528291582\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.27303017791854\n",
      "    mean_inference_ms: 1.3233032293296352\n",
      "    mean_raw_obs_processing_ms: 0.501537318143759\n",
      "  time_since_restore: 563.7649209499359\n",
      "  time_this_iter_s: 19.210840702056885\n",
      "  time_total_s: 563.7649209499359\n",
      "  timers:\n",
      "    learn_throughput: 1521.214\n",
      "    learn_time_ms: 657.37\n",
      "    load_throughput: 91679.377\n",
      "    load_time_ms: 10.908\n",
      "    sample_throughput: 60.723\n",
      "    sample_time_ms: 16468.163\n",
      "    update_time_ms: 2.97\n",
      "  timestamp: 1633529993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         563.765</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           558.407</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-20-29\n",
      "  done: false\n",
      "  episode_len_mean: 554.0983606557377\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 61\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.364213373925951\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014953473117437882\n",
      "          policy_loss: -0.04507027682330873\n",
      "          total_loss: -0.06397131317191654\n",
      "          vf_explained_var: -0.9040290713310242\n",
      "          vf_loss: 0.0013765647962120258\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.81923076923077\n",
      "    ram_util_percent: 58.47115384615386\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03723815361772232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.212334986372312\n",
      "    mean_inference_ms: 1.3231171381149607\n",
      "    mean_raw_obs_processing_ms: 0.5235414670701978\n",
      "  time_since_restore: 599.9352796077728\n",
      "  time_this_iter_s: 36.170358657836914\n",
      "  time_total_s: 599.9352796077728\n",
      "  timers:\n",
      "    learn_throughput: 1519.255\n",
      "    learn_time_ms: 658.217\n",
      "    load_throughput: 91800.977\n",
      "    load_time_ms: 10.893\n",
      "    sample_throughput: 54.727\n",
      "    sample_time_ms: 18272.405\n",
      "    update_time_ms: 2.964\n",
      "  timestamp: 1633530029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         599.935</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           554.098</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 549.8571428571429\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 63\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.41789038711124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013813483555789654\n",
      "          policy_loss: -0.016767474760611852\n",
      "          total_loss: -0.03649651611016856\n",
      "          vf_explained_var: -0.7549787163734436\n",
      "          vf_loss: 0.0013418266666121782\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.55555555555555\n",
      "    ram_util_percent: 59.05925925925926\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03723641370450888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.15790427757606\n",
      "    mean_inference_ms: 1.3229359920484578\n",
      "    mean_raw_obs_processing_ms: 0.543223958401535\n",
      "  time_since_restore: 619.2135667800903\n",
      "  time_this_iter_s: 19.278287172317505\n",
      "  time_total_s: 619.2135667800903\n",
      "  timers:\n",
      "    learn_throughput: 1529.785\n",
      "    learn_time_ms: 653.687\n",
      "    load_throughput: 91432.159\n",
      "    load_time_ms: 10.937\n",
      "    sample_throughput: 54.26\n",
      "    sample_time_ms: 18429.924\n",
      "    update_time_ms: 2.942\n",
      "  timestamp: 1633530048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         619.214</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           549.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-21-06\n",
      "  done: false\n",
      "  episode_len_mean: 545.030303030303\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 66\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2955459541744654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012959420608604036\n",
      "          policy_loss: 0.002609030447072453\n",
      "          total_loss: -0.015099832870894008\n",
      "          vf_explained_var: -0.5157346725463867\n",
      "          vf_loss: 0.00233072799940904\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.33846153846154\n",
      "    ram_util_percent: 58.95384615384616\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037233783532090725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.08420220565984\n",
      "    mean_inference_ms: 1.3226748500312349\n",
      "    mean_raw_obs_processing_ms: 0.5692912502561601\n",
      "  time_since_restore: 637.3147299289703\n",
      "  time_this_iter_s: 18.101163148880005\n",
      "  time_total_s: 637.3147299289703\n",
      "  timers:\n",
      "    learn_throughput: 1551.513\n",
      "    learn_time_ms: 644.532\n",
      "    load_throughput: 81388.748\n",
      "    load_time_ms: 12.287\n",
      "    sample_throughput: 53.622\n",
      "    sample_time_ms: 18649.21\n",
      "    update_time_ms: 1.625\n",
      "  timestamp: 1633530066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         637.315</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            545.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-21-25\n",
      "  done: false\n",
      "  episode_len_mean: 541.6323529411765\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 68\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3326867540677387\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009617861664608679\n",
      "          policy_loss: -0.004300108386410607\n",
      "          total_loss: -0.02447380936808056\n",
      "          vf_explained_var: -0.6420633792877197\n",
      "          vf_loss: 0.0009891453102075806\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.303703703703704\n",
      "    ram_util_percent: 59.16296296296296\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037232193289726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.040405418778693\n",
      "    mean_inference_ms: 1.322509375895459\n",
      "    mean_raw_obs_processing_ms: 0.5846272742753825\n",
      "  time_since_restore: 656.2340714931488\n",
      "  time_this_iter_s: 18.919341564178467\n",
      "  time_total_s: 656.2340714931488\n",
      "  timers:\n",
      "    learn_throughput: 1549.985\n",
      "    learn_time_ms: 645.168\n",
      "    load_throughput: 75384.787\n",
      "    load_time_ms: 13.265\n",
      "    sample_throughput: 53.001\n",
      "    sample_time_ms: 18867.507\n",
      "    update_time_ms: 1.615\n",
      "  timestamp: 1633530085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         656.234</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           541.632</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-21-43\n",
      "  done: false\n",
      "  episode_len_mean: 538.6142857142858\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 70\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.357290572590298\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014586195217466033\n",
      "          policy_loss: -0.03261737231579092\n",
      "          total_loss: -0.05126663521967\n",
      "          vf_explained_var: -0.37463459372520447\n",
      "          vf_loss: 0.0016417478961456152\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.25000000000001\n",
      "    ram_util_percent: 59.449999999999996\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03723034221026837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.999997076458275\n",
      "    mean_inference_ms: 1.3223481428717263\n",
      "    mean_raw_obs_processing_ms: 0.5983732684855628\n",
      "  time_since_restore: 674.1788456439972\n",
      "  time_this_iter_s: 17.94477415084839\n",
      "  time_total_s: 674.1788456439972\n",
      "  timers:\n",
      "    learn_throughput: 1550.327\n",
      "    learn_time_ms: 645.025\n",
      "    load_throughput: 69011.824\n",
      "    load_time_ms: 14.49\n",
      "    sample_throughput: 52.439\n",
      "    sample_time_ms: 19069.662\n",
      "    update_time_ms: 1.613\n",
      "  timestamp: 1633530103\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         674.179</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           538.614</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-22-00\n",
      "  done: false\n",
      "  episode_len_mean: 536.8194444444445\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 72\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.302868376837836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016770324673748328\n",
      "          policy_loss: -0.06713909726175997\n",
      "          total_loss: -0.08446305936409368\n",
      "          vf_explained_var: -0.4178003668785095\n",
      "          vf_loss: 0.0019313968478753749\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.420833333333334\n",
      "    ram_util_percent: 59.675000000000004\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037228512052888754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.961883866568956\n",
      "    mean_inference_ms: 1.3221866660700445\n",
      "    mean_raw_obs_processing_ms: 0.6107009340140497\n",
      "  time_since_restore: 691.0380434989929\n",
      "  time_this_iter_s: 16.859197854995728\n",
      "  time_total_s: 691.0380434989929\n",
      "  timers:\n",
      "    learn_throughput: 1550.743\n",
      "    learn_time_ms: 644.852\n",
      "    load_throughput: 69216.135\n",
      "    load_time_ms: 14.447\n",
      "    sample_throughput: 52.319\n",
      "    sample_time_ms: 19113.603\n",
      "    update_time_ms: 1.617\n",
      "  timestamp: 1633530120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         691.038</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           536.819</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 532.6533333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 75\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2388808621300593\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012639134931509312\n",
      "          policy_loss: -0.04549748144216008\n",
      "          total_loss: -0.06336293009420237\n",
      "          vf_explained_var: 0.20213334262371063\n",
      "          vf_loss: 0.0016795565031417128\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.684000000000005\n",
      "    ram_util_percent: 59.896\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03722586661970069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.909649896921525\n",
      "    mean_inference_ms: 1.3219535451283844\n",
      "    mean_raw_obs_processing_ms: 0.627119313828477\n",
      "  time_since_restore: 709.0337426662445\n",
      "  time_this_iter_s: 17.995699167251587\n",
      "  time_total_s: 709.0337426662445\n",
      "  timers:\n",
      "    learn_throughput: 1548.319\n",
      "    learn_time_ms: 645.862\n",
      "    load_throughput: 67443.925\n",
      "    load_time_ms: 14.827\n",
      "    sample_throughput: 52.121\n",
      "    sample_time_ms: 19186.183\n",
      "    update_time_ms: 1.619\n",
      "  timestamp: 1633530138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         709.034</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           532.653</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 530.8571428571429\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 77\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2847841554217867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013260609883498375\n",
      "          policy_loss: 0.01716281846165657\n",
      "          total_loss: -0.0015166004498799641\n",
      "          vf_explained_var: 0.017092270776629448\n",
      "          vf_loss: 0.0011847848793776292\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.37307692307692\n",
      "    ram_util_percent: 60.18076923076922\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0372241350061922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.877855199366673\n",
      "    mean_inference_ms: 1.3218022249316617\n",
      "    mean_raw_obs_processing_ms: 0.6368160492157086\n",
      "  time_since_restore: 727.0975179672241\n",
      "  time_this_iter_s: 18.063775300979614\n",
      "  time_total_s: 727.0975179672241\n",
      "  timers:\n",
      "    learn_throughput: 1548.272\n",
      "    learn_time_ms: 645.881\n",
      "    load_throughput: 67274.844\n",
      "    load_time_ms: 14.864\n",
      "    sample_throughput: 51.843\n",
      "    sample_time_ms: 19289.077\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1633530156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         727.098</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           530.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-22-56\n",
      "  done: false\n",
      "  episode_len_mean: 527.746835443038\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 79\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3261839866638185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01624224440879733\n",
      "          policy_loss: -0.08537489490376579\n",
      "          total_loss: -0.10366679843929079\n",
      "          vf_explained_var: -0.18625831604003906\n",
      "          vf_loss: 0.0013154288113582878\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.246428571428574\n",
      "    ram_util_percent: 60.267857142857146\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03722245744446116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.849372942323466\n",
      "    mean_inference_ms: 1.321653425083132\n",
      "    mean_raw_obs_processing_ms: 0.6455195583546577\n",
      "  time_since_restore: 746.8250620365143\n",
      "  time_this_iter_s: 19.72754406929016\n",
      "  time_total_s: 746.8250620365143\n",
      "  timers:\n",
      "    learn_throughput: 1544.786\n",
      "    learn_time_ms: 647.339\n",
      "    load_throughput: 63192.163\n",
      "    load_time_ms: 15.825\n",
      "    sample_throughput: 51.126\n",
      "    sample_time_ms: 19559.551\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1633530176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         746.825</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           527.747</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-23-14\n",
      "  done: false\n",
      "  episode_len_mean: 525.5432098765432\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 81\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.364904114935133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014838121770841036\n",
      "          policy_loss: -0.045731910939017933\n",
      "          total_loss: -0.06443202466600471\n",
      "          vf_explained_var: -0.5770139694213867\n",
      "          vf_loss: 0.0016103483833527813\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.55555555555556\n",
      "    ram_util_percent: 59.955555555555556\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037220723304203375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.823166256657844\n",
      "    mean_inference_ms: 1.3215117012239308\n",
      "    mean_raw_obs_processing_ms: 0.6533189316007877\n",
      "  time_since_restore: 765.299684047699\n",
      "  time_this_iter_s: 18.474622011184692\n",
      "  time_total_s: 765.299684047699\n",
      "  timers:\n",
      "    learn_throughput: 1544.049\n",
      "    learn_time_ms: 647.648\n",
      "    load_throughput: 62686.038\n",
      "    load_time_ms: 15.953\n",
      "    sample_throughput: 51.32\n",
      "    sample_time_ms: 19485.466\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1633530194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">           765.3</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           525.543</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 522.4404761904761\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 84\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1176261782646177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012655571290237136\n",
      "          policy_loss: -0.07568255298667484\n",
      "          total_loss: -0.09316671085026529\n",
      "          vf_explained_var: -0.15387170016765594\n",
      "          vf_loss: 0.000844599854058793\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.78\n",
      "    ram_util_percent: 59.6\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721837315903413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.78659800947227\n",
      "    mean_inference_ms: 1.321306325574999\n",
      "    mean_raw_obs_processing_ms: 0.6637000063860337\n",
      "  time_since_restore: 782.8324213027954\n",
      "  time_this_iter_s: 17.532737255096436\n",
      "  time_total_s: 782.8324213027954\n",
      "  timers:\n",
      "    learn_throughput: 1549.976\n",
      "    learn_time_ms: 645.171\n",
      "    load_throughput: 61943.015\n",
      "    load_time_ms: 16.144\n",
      "    sample_throughput: 56.741\n",
      "    sample_time_ms: 17623.983\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1633530212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         782.832</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            522.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-23-51\n",
      "  done: false\n",
      "  episode_len_mean: 520.7325581395348\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 86\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2112664116753473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012004902103658467\n",
      "          policy_loss: -0.07485327091481951\n",
      "          total_loss: -0.09126981182230844\n",
      "          vf_explained_var: -0.8568789958953857\n",
      "          vf_loss: 0.0029950198772389237\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.34814814814814\n",
      "    ram_util_percent: 59.38518518518518\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721678511480487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.7646273257195\n",
      "    mean_inference_ms: 1.321173552879137\n",
      "    mean_raw_obs_processing_ms: 0.6698103584741751\n",
      "  time_since_restore: 801.7495777606964\n",
      "  time_this_iter_s: 18.917156457901\n",
      "  time_total_s: 801.7495777606964\n",
      "  timers:\n",
      "    learn_throughput: 1549.742\n",
      "    learn_time_ms: 645.269\n",
      "    load_throughput: 61385.568\n",
      "    load_time_ms: 16.29\n",
      "    sample_throughput: 56.858\n",
      "    sample_time_ms: 17587.623\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1633530231\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">          801.75</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           520.733</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-24-10\n",
      "  done: false\n",
      "  episode_len_mean: 518.5568181818181\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 88\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.173849850230747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02085878316454885\n",
      "          policy_loss: -0.04586854964080784\n",
      "          total_loss: -0.06180076205896007\n",
      "          vf_explained_var: 0.03841510787606239\n",
      "          vf_loss: 0.0011130587657033984\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.38148148148149\n",
      "    ram_util_percent: 59.12222222222223\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721522982200839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.744687604910744\n",
      "    mean_inference_ms: 1.3210432572241169\n",
      "    mean_raw_obs_processing_ms: 0.6752613432284706\n",
      "  time_since_restore: 820.8601622581482\n",
      "  time_this_iter_s: 19.110584497451782\n",
      "  time_total_s: 820.8601622581482\n",
      "  timers:\n",
      "    learn_throughput: 1551.665\n",
      "    learn_time_ms: 644.469\n",
      "    load_throughput: 60877.272\n",
      "    load_time_ms: 16.426\n",
      "    sample_throughput: 56.532\n",
      "    sample_time_ms: 17689.211\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1633530250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">          820.86</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           518.557</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-24-47\n",
      "  done: false\n",
      "  episode_len_mean: 514.5164835164835\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 91\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2923230780495536\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013768339420960944\n",
      "          policy_loss: -0.08760065183871323\n",
      "          total_loss: -0.10472276426023908\n",
      "          vf_explained_var: -0.8233001828193665\n",
      "          vf_loss: 0.0011543036490264866\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.467924528301886\n",
      "    ram_util_percent: 58.832075471698126\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721308936897405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.718841515568155\n",
      "    mean_inference_ms: 1.3208568621164543\n",
      "    mean_raw_obs_processing_ms: 0.6941094329972491\n",
      "  time_since_restore: 858.0224659442902\n",
      "  time_this_iter_s: 37.16230368614197\n",
      "  time_total_s: 858.0224659442902\n",
      "  timers:\n",
      "    learn_throughput: 1551.686\n",
      "    learn_time_ms: 644.46\n",
      "    load_throughput: 60293.915\n",
      "    load_time_ms: 16.585\n",
      "    sample_throughput: 51.247\n",
      "    sample_time_ms: 19513.335\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1633530287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         858.022</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           514.516</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-25-05\n",
      "  done: false\n",
      "  episode_len_mean: 512.989247311828\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 93\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1808781147003176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011784553128113107\n",
      "          policy_loss: 0.027052655402157042\n",
      "          total_loss: 0.011412174627184867\n",
      "          vf_explained_var: -0.8177082538604736\n",
      "          vf_loss: 0.0021910164518178336\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.04615384615384\n",
      "    ram_util_percent: 59.31153846153847\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721172430754165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.702812348022581\n",
      "    mean_inference_ms: 1.3207366387791528\n",
      "    mean_raw_obs_processing_ms: 0.7055128211253382\n",
      "  time_since_restore: 875.7955331802368\n",
      "  time_this_iter_s: 17.773067235946655\n",
      "  time_total_s: 875.7955331802368\n",
      "  timers:\n",
      "    learn_throughput: 1550.696\n",
      "    learn_time_ms: 644.872\n",
      "    load_throughput: 59427.25\n",
      "    load_time_ms: 16.827\n",
      "    sample_throughput: 51.294\n",
      "    sample_time_ms: 19495.494\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1633530305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         875.796</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           512.989</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-25-23\n",
      "  done: false\n",
      "  episode_len_mean: 511.4947368421053\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 95\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.204087773958842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011901261408288056\n",
      "          policy_loss: -0.060460909828543664\n",
      "          total_loss: -0.07451910629040664\n",
      "          vf_explained_var: -0.7502480149269104\n",
      "          vf_loss: 0.0039660032618687386\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.46538461538462\n",
      "    ram_util_percent: 59.46923076923077\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721041828196423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.687883031157229\n",
      "    mean_inference_ms: 1.3206188561610652\n",
      "    mean_raw_obs_processing_ms: 0.7159807952174476\n",
      "  time_since_restore: 893.9623494148254\n",
      "  time_this_iter_s: 18.166816234588623\n",
      "  time_total_s: 893.9623494148254\n",
      "  timers:\n",
      "    learn_throughput: 1548.942\n",
      "    learn_time_ms: 645.602\n",
      "    load_throughput: 55780.511\n",
      "    load_time_ms: 17.927\n",
      "    sample_throughput: 50.957\n",
      "    sample_time_ms: 19624.422\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1633530323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         893.962</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           511.495</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-25-40\n",
      "  done: false\n",
      "  episode_len_mean: 509.7244897959184\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 98\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1857634411917792\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013878724560874092\n",
      "          policy_loss: -0.03707688897848129\n",
      "          total_loss: -0.05208363106681241\n",
      "          vf_explained_var: -0.60608971118927\n",
      "          vf_loss: 0.002166821953142062\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.520833333333336\n",
      "    ram_util_percent: 59.604166666666664\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037208590565116995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.666584996224737\n",
      "    mean_inference_ms: 1.3204452341321058\n",
      "    mean_raw_obs_processing_ms: 0.7302546615461302\n",
      "  time_since_restore: 911.0878312587738\n",
      "  time_this_iter_s: 17.125481843948364\n",
      "  time_total_s: 911.0878312587738\n",
      "  timers:\n",
      "    learn_throughput: 1548.487\n",
      "    learn_time_ms: 645.792\n",
      "    load_throughput: 59132.504\n",
      "    load_time_ms: 16.911\n",
      "    sample_throughput: 51.182\n",
      "    sample_time_ms: 19538.208\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633530340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         911.088</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           509.724</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-25-58\n",
      "  done: false\n",
      "  episode_len_mean: 508.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 100\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.164301233821445\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00854582910428873\n",
      "          policy_loss: 0.029392323518792788\n",
      "          total_loss: 0.012245290923035806\n",
      "          vf_explained_var: -0.5702173709869385\n",
      "          vf_loss: 0.0016117609668678293\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.303999999999995\n",
      "    ram_util_percent: 59.824\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03720727637003973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.65317532873046\n",
      "    mean_inference_ms: 1.3203305439134976\n",
      "    mean_raw_obs_processing_ms: 0.7388892881621112\n",
      "  time_since_restore: 928.4520847797394\n",
      "  time_this_iter_s: 17.364253520965576\n",
      "  time_total_s: 928.4520847797394\n",
      "  timers:\n",
      "    learn_throughput: 1548.656\n",
      "    learn_time_ms: 645.721\n",
      "    load_throughput: 58848.923\n",
      "    load_time_ms: 16.993\n",
      "    sample_throughput: 51.366\n",
      "    sample_time_ms: 19468.249\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633530358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         928.452</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            508.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-26-14\n",
      "  done: false\n",
      "  episode_len_mean: 500.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 102\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2526859203974405\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012062967186520761\n",
      "          policy_loss: 0.00011251237657335069\n",
      "          total_loss: -0.01698901831275887\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0013540760200056764\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.35217391304349\n",
      "    ram_util_percent: 60.03913043478259\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037174647594065115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.053048609275402\n",
      "    mean_inference_ms: 1.318658776731685\n",
      "    mean_raw_obs_processing_ms: 0.7592953962291696\n",
      "  time_since_restore: 944.8078043460846\n",
      "  time_this_iter_s: 16.355719566345215\n",
      "  time_total_s: 944.8078043460846\n",
      "  timers:\n",
      "    learn_throughput: 1551.323\n",
      "    learn_time_ms: 644.611\n",
      "    load_throughput: 62437.072\n",
      "    load_time_ms: 16.016\n",
      "    sample_throughput: 52.265\n",
      "    sample_time_ms: 19133.149\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1633530374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         944.808</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            500.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-26-30\n",
      "  done: false\n",
      "  episode_len_mean: 495.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 104\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.298818850517273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012481657720367719\n",
      "          policy_loss: -0.06959351143903203\n",
      "          total_loss: -0.08752974429064327\n",
      "          vf_explained_var: -0.5838901400566101\n",
      "          vf_loss: 0.0008393950959240707\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.53913043478261\n",
      "    ram_util_percent: 60.24347826086956\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716376204099922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.875338843085006\n",
      "    mean_inference_ms: 1.3179648469321381\n",
      "    mean_raw_obs_processing_ms: 0.7794162417376339\n",
      "  time_since_restore: 961.0989100933075\n",
      "  time_this_iter_s: 16.2911057472229\n",
      "  time_total_s: 961.0989100933075\n",
      "  timers:\n",
      "    learn_throughput: 1555.593\n",
      "    learn_time_ms: 642.842\n",
      "    load_throughput: 67459.003\n",
      "    load_time_ms: 14.824\n",
      "    sample_throughput: 52.86\n",
      "    sample_time_ms: 18917.781\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633530390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         961.099</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            495.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-26-47\n",
      "  done: false\n",
      "  episode_len_mean: 493.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 106\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1999814907709756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013399046313698312\n",
      "          policy_loss: -0.029844710230827333\n",
      "          total_loss: -0.045759806202517614\n",
      "          vf_explained_var: -0.560759425163269\n",
      "          vf_loss: 0.0015625397926972559\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.362500000000004\n",
      "    ram_util_percent: 60.44166666666666\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03715596194970385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.75500140342631\n",
      "    mean_inference_ms: 1.3174497858668173\n",
      "    mean_raw_obs_processing_ms: 0.7991417059158326\n",
      "  time_since_restore: 977.7356629371643\n",
      "  time_this_iter_s: 16.63675284385681\n",
      "  time_total_s: 977.7356629371643\n",
      "  timers:\n",
      "    learn_throughput: 1557.784\n",
      "    learn_time_ms: 641.938\n",
      "    load_throughput: 72997.61\n",
      "    load_time_ms: 13.699\n",
      "    sample_throughput: 53.106\n",
      "    sample_time_ms: 18830.203\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633530407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         977.736</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             493.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-27-03\n",
      "  done: false\n",
      "  episode_len_mean: 489.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 108\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.179515798886617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010415570427012449\n",
      "          policy_loss: 0.02132337594197856\n",
      "          total_loss: 0.007981575632260905\n",
      "          vf_explained_var: -0.80272376537323\n",
      "          vf_loss: 0.004938100235611071\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.46086956521738\n",
      "    ram_util_percent: 60.582608695652155\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037149227999454894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.686429104582043\n",
      "    mean_inference_ms: 1.3170026486658881\n",
      "    mean_raw_obs_processing_ms: 0.8184687075621184\n",
      "  time_since_restore: 993.7072041034698\n",
      "  time_this_iter_s: 15.971541166305542\n",
      "  time_total_s: 993.7072041034698\n",
      "  timers:\n",
      "    learn_throughput: 1560.699\n",
      "    learn_time_ms: 640.739\n",
      "    load_throughput: 82022.849\n",
      "    load_time_ms: 12.192\n",
      "    sample_throughput: 53.942\n",
      "    sample_time_ms: 18538.32\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1633530423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         993.707</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            489.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-27-20\n",
      "  done: false\n",
      "  episode_len_mean: 486.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 110\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2539958053165012\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012417594525764346\n",
      "          policy_loss: -0.0014058492249912685\n",
      "          total_loss: -0.01820049877795908\n",
      "          vf_explained_var: -0.6187347769737244\n",
      "          vf_loss: 0.0015543701683378053\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.332\n",
      "    ram_util_percent: 60.52\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714612120861002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.636610333116623\n",
      "    mean_inference_ms: 1.3166598809956247\n",
      "    mean_raw_obs_processing_ms: 0.8374007805744113\n",
      "  time_since_restore: 1011.1210060119629\n",
      "  time_this_iter_s: 17.413801908493042\n",
      "  time_total_s: 1011.1210060119629\n",
      "  timers:\n",
      "    learn_throughput: 1557.464\n",
      "    learn_time_ms: 642.069\n",
      "    load_throughput: 87461.324\n",
      "    load_time_ms: 11.434\n",
      "    sample_throughput: 54.442\n",
      "    sample_time_ms: 18368.06\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1633530440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         1011.12</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            486.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-27-37\n",
      "  done: false\n",
      "  episode_len_mean: 484.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 112\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2351653046078153\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010411954960599651\n",
      "          policy_loss: -0.0555434246857961\n",
      "          total_loss: -0.07273421006070244\n",
      "          vf_explained_var: -0.41190120577812195\n",
      "          vf_loss: 0.001646830111147008\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.416666666666664\n",
      "    ram_util_percent: 60.45416666666667\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037144648659803575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.607551035822924\n",
      "    mean_inference_ms: 1.3164258078597837\n",
      "    mean_raw_obs_processing_ms: 0.8560305352591095\n",
      "  time_since_restore: 1027.819905281067\n",
      "  time_this_iter_s: 16.698899269104004\n",
      "  time_total_s: 1027.819905281067\n",
      "  timers:\n",
      "    learn_throughput: 1563.862\n",
      "    learn_time_ms: 639.443\n",
      "    load_throughput: 97026.36\n",
      "    load_time_ms: 10.306\n",
      "    sample_throughput: 61.254\n",
      "    sample_time_ms: 16325.459\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633530457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1027.82</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            484.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-27-55\n",
      "  done: false\n",
      "  episode_len_mean: 482.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 114\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.245934674474928\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012523311945593734\n",
      "          policy_loss: -0.0708002725823058\n",
      "          total_loss: -0.0867017411109474\n",
      "          vf_explained_var: -0.4623943269252777\n",
      "          vf_loss: 0.0023312577039986434\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.272\n",
      "    ram_util_percent: 60.47200000000001\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037143406642556324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.58664109168828\n",
      "    mean_inference_ms: 1.3162215542805824\n",
      "    mean_raw_obs_processing_ms: 0.8742980192020106\n",
      "  time_since_restore: 1045.522872686386\n",
      "  time_this_iter_s: 17.702967405319214\n",
      "  time_total_s: 1045.522872686386\n",
      "  timers:\n",
      "    learn_throughput: 1561.294\n",
      "    learn_time_ms: 640.495\n",
      "    load_throughput: 95811.771\n",
      "    load_time_ms: 10.437\n",
      "    sample_throughput: 61.285\n",
      "    sample_time_ms: 16317.29\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1633530475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1045.52</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            482.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-28-12\n",
      "  done: false\n",
      "  episode_len_mean: 480.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 116\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.297883150312636\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010132855197880571\n",
      "          policy_loss: -0.05304202079359028\n",
      "          total_loss: -0.07136777024716139\n",
      "          vf_explained_var: -0.21610872447490692\n",
      "          vf_loss: 0.0012332441886731733\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.56666666666667\n",
      "    ram_util_percent: 60.375\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714162379291173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.571026739816578\n",
      "    mean_inference_ms: 1.3160139342086048\n",
      "    mean_raw_obs_processing_ms: 0.8922212165567669\n",
      "  time_since_restore: 1062.6455240249634\n",
      "  time_this_iter_s: 17.12265133857727\n",
      "  time_total_s: 1062.6455240249634\n",
      "  timers:\n",
      "    learn_throughput: 1562.599\n",
      "    learn_time_ms: 639.96\n",
      "    load_throughput: 106606.75\n",
      "    load_time_ms: 9.38\n",
      "    sample_throughput: 61.673\n",
      "    sample_time_ms: 16214.456\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633530492\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1062.65</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            480.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 478.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 119\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2065973732206556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009205779578744928\n",
      "          policy_loss: 0.040876177160276306\n",
      "          total_loss: 0.025552483234140607\n",
      "          vf_explained_var: -0.8208518028259277\n",
      "          vf_loss: 0.003635329944599006\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.28\n",
      "    ram_util_percent: 60.232\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713918355753327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.561687121262535\n",
      "    mean_inference_ms: 1.3157573631409172\n",
      "    mean_raw_obs_processing_ms: 0.9186588908546665\n",
      "  time_since_restore: 1080.055151462555\n",
      "  time_this_iter_s: 17.409627437591553\n",
      "  time_total_s: 1080.055151462555\n",
      "  timers:\n",
      "    learn_throughput: 1565.359\n",
      "    learn_time_ms: 638.831\n",
      "    load_throughput: 100805.23\n",
      "    load_time_ms: 9.92\n",
      "    sample_throughput: 61.563\n",
      "    sample_time_ms: 16243.468\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633530509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1080.06</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            478.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-29-05\n",
      "  done: false\n",
      "  episode_len_mean: 474.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 121\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.210782119962904\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013914061018620909\n",
      "          policy_loss: -0.0525794956419203\n",
      "          total_loss: -0.0675370781785912\n",
      "          vf_explained_var: -0.4350879192352295\n",
      "          vf_loss: 0.0024542418431438917\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.60588235294118\n",
      "    ram_util_percent: 60.07647058823529\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713862834012899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.564526841685197\n",
      "    mean_inference_ms: 1.3156446059480191\n",
      "    mean_raw_obs_processing_ms: 0.941790606339728\n",
      "  time_since_restore: 1115.3906979560852\n",
      "  time_this_iter_s: 35.33554649353027\n",
      "  time_total_s: 1115.3906979560852\n",
      "  timers:\n",
      "    learn_throughput: 1563.516\n",
      "    learn_time_ms: 639.584\n",
      "    load_throughput: 100540.154\n",
      "    load_time_ms: 9.946\n",
      "    sample_throughput: 55.433\n",
      "    sample_time_ms: 18039.826\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1633530545\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1115.39</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            474.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-29-23\n",
      "  done: false\n",
      "  episode_len_mean: 472.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 123\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.260226527849833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012784510392571056\n",
      "          policy_loss: -0.08002541483276421\n",
      "          total_loss: -0.09591560941189528\n",
      "          vf_explained_var: 0.04735171049833298\n",
      "          vf_loss: 0.0023972962433213576\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.55\n",
      "    ram_util_percent: 59.42692307692309\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713873791937004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.572394063083458\n",
      "    mean_inference_ms: 1.3155621852059358\n",
      "    mean_raw_obs_processing_ms: 0.96454416258983\n",
      "  time_since_restore: 1133.6156208515167\n",
      "  time_this_iter_s: 18.22492289543152\n",
      "  time_total_s: 1133.6156208515167\n",
      "  timers:\n",
      "    learn_throughput: 1566.052\n",
      "    learn_time_ms: 638.548\n",
      "    load_throughput: 90539.268\n",
      "    load_time_ms: 11.045\n",
      "    sample_throughput: 54.865\n",
      "    sample_time_ms: 18226.669\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633530563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1133.62</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            472.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-29-42\n",
      "  done: false\n",
      "  episode_len_mean: 468.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 125\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3036494493484496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009072005559401535\n",
      "          policy_loss: 0.0010133017475406328\n",
      "          total_loss: -0.01779507233036889\n",
      "          vf_explained_var: -0.9826317429542542\n",
      "          vf_loss: 0.0011663195599491397\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.37777777777778\n",
      "    ram_util_percent: 59.45925925925926\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713999301187732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.58724724049087\n",
      "    mean_inference_ms: 1.315525935579247\n",
      "    mean_raw_obs_processing_ms: 0.9869176233711326\n",
      "  time_since_restore: 1152.5103025436401\n",
      "  time_this_iter_s: 18.894681692123413\n",
      "  time_total_s: 1152.5103025436401\n",
      "  timers:\n",
      "    learn_throughput: 1561.186\n",
      "    learn_time_ms: 640.539\n",
      "    load_throughput: 81344.709\n",
      "    load_time_ms: 12.293\n",
      "    sample_throughput: 54.101\n",
      "    sample_time_ms: 18483.79\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633530582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1152.51</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            468.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-30-00\n",
      "  done: false\n",
      "  episode_len_mean: 464.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 128\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2633219639460247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010870558291378203\n",
      "          policy_loss: -0.025533304487665495\n",
      "          total_loss: -0.04283280972805288\n",
      "          vf_explained_var: -0.9067294597625732\n",
      "          vf_loss: 0.0016649022465571762\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.18846153846153\n",
      "    ram_util_percent: 59.54615384615386\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714298806680371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.617093671498653\n",
      "    mean_inference_ms: 1.315503496850068\n",
      "    mean_raw_obs_processing_ms: 1.0198695363293253\n",
      "  time_since_restore: 1170.9443113803864\n",
      "  time_this_iter_s: 18.434008836746216\n",
      "  time_total_s: 1170.9443113803864\n",
      "  timers:\n",
      "    learn_throughput: 1555.197\n",
      "    learn_time_ms: 643.005\n",
      "    load_throughput: 74400.335\n",
      "    load_time_ms: 13.441\n",
      "    sample_throughput: 53.591\n",
      "    sample_time_ms: 18659.914\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1633530600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1170.94</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             464.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-30-17\n",
      "  done: false\n",
      "  episode_len_mean: 460.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 130\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.262018500434028\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01256987745735024\n",
      "          policy_loss: -0.017163740636573898\n",
      "          total_loss: -0.033854976751738125\n",
      "          vf_explained_var: 0.00913392473012209\n",
      "          vf_loss: 0.001686617081415736\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.39166666666667\n",
      "    ram_util_percent: 59.79583333333334\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714595241159769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.645683470272257\n",
      "    mean_inference_ms: 1.315527112716139\n",
      "    mean_raw_obs_processing_ms: 1.0229998917262615\n",
      "  time_since_restore: 1187.7881648540497\n",
      "  time_this_iter_s: 16.84385347366333\n",
      "  time_total_s: 1187.7881648540497\n",
      "  timers:\n",
      "    learn_throughput: 1552.232\n",
      "    learn_time_ms: 644.234\n",
      "    load_throughput: 71502.309\n",
      "    load_time_ms: 13.986\n",
      "    sample_throughput: 53.346\n",
      "    sample_time_ms: 18745.387\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1633530617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1187.79</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            460.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-30-37\n",
      "  done: false\n",
      "  episode_len_mean: 458.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 132\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2744289610120987\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014970757955841347\n",
      "          policy_loss: -0.02121506511337227\n",
      "          total_loss: -0.0377461899485853\n",
      "          vf_explained_var: -0.42829030752182007\n",
      "          vf_loss: 0.0011605346594781925\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.148275862068964\n",
      "    ram_util_percent: 59.98620689655171\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714627538528591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.672096915690718\n",
      "    mean_inference_ms: 1.3155154004397176\n",
      "    mean_raw_obs_processing_ms: 1.0266956645840972\n",
      "  time_since_restore: 1207.7465822696686\n",
      "  time_this_iter_s: 19.958417415618896\n",
      "  time_total_s: 1207.7465822696686\n",
      "  timers:\n",
      "    learn_throughput: 1552.738\n",
      "    learn_time_ms: 644.024\n",
      "    load_throughput: 67975.47\n",
      "    load_time_ms: 14.711\n",
      "    sample_throughput: 52.633\n",
      "    sample_time_ms: 18999.36\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1633530637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1207.75</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             458.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 458.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 134\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3086379554536607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013461410523445691\n",
      "          policy_loss: -0.04299076713828577\n",
      "          total_loss: -0.05870185926970509\n",
      "          vf_explained_var: -0.36026525497436523\n",
      "          vf_loss: 0.0028320579065216913\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.2\n",
      "    ram_util_percent: 60.19230769230769\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371466102413883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.696371918736304\n",
      "    mean_inference_ms: 1.3154992156950451\n",
      "    mean_raw_obs_processing_ms: 1.0309005207768243\n",
      "  time_since_restore: 1226.070407152176\n",
      "  time_this_iter_s: 18.323824882507324\n",
      "  time_total_s: 1226.070407152176\n",
      "  timers:\n",
      "    learn_throughput: 1547.79\n",
      "    learn_time_ms: 646.083\n",
      "    load_throughput: 63230.459\n",
      "    load_time_ms: 15.815\n",
      "    sample_throughput: 52.196\n",
      "    sample_time_ms: 19158.704\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1633530656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1226.07</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            458.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-31-15\n",
      "  done: false\n",
      "  episode_len_mean: 458.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 137\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.309504606988695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013302460613304213\n",
      "          policy_loss: -0.04964480337997278\n",
      "          total_loss: -0.06717836211125056\n",
      "          vf_explained_var: -0.6082040071487427\n",
      "          vf_loss: 0.001071908403860612\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.233333333333334\n",
      "    ram_util_percent: 60.32592592592593\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371459021998675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.727785240695216\n",
      "    mean_inference_ms: 1.3154428055788465\n",
      "    mean_raw_obs_processing_ms: 1.038201444202624\n",
      "  time_since_restore: 1245.313051700592\n",
      "  time_this_iter_s: 19.242644548416138\n",
      "  time_total_s: 1245.313051700592\n",
      "  timers:\n",
      "    learn_throughput: 1548.696\n",
      "    learn_time_ms: 645.704\n",
      "    load_throughput: 63645.819\n",
      "    load_time_ms: 15.712\n",
      "    sample_throughput: 51.778\n",
      "    sample_time_ms: 19313.127\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1633530675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1245.31</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            458.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-31-34\n",
      "  done: false\n",
      "  episode_len_mean: 457.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 139\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.174455165863037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014611806583692497\n",
      "          policy_loss: -0.09045104434092839\n",
      "          total_loss: -0.10503003522753715\n",
      "          vf_explained_var: -0.005307760555297136\n",
      "          vf_loss: 0.002234076606368439\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.28928571428571\n",
      "    ram_util_percent: 60.517857142857125\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037144708911430714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.748182456737373\n",
      "    mean_inference_ms: 1.31538523705158\n",
      "    mean_raw_obs_processing_ms: 1.0432013964511555\n",
      "  time_since_restore: 1264.492985010147\n",
      "  time_this_iter_s: 19.179933309555054\n",
      "  time_total_s: 1264.492985010147\n",
      "  timers:\n",
      "    learn_throughput: 1545.424\n",
      "    learn_time_ms: 647.072\n",
      "    load_throughput: 59705.82\n",
      "    load_time_ms: 16.749\n",
      "    sample_throughput: 51.239\n",
      "    sample_time_ms: 19516.451\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633530694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1264.49</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            457.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-31-51\n",
      "  done: false\n",
      "  episode_len_mean: 456.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 141\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3049838410483465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011999864805120926\n",
      "          policy_loss: 0.015151694065166844\n",
      "          total_loss: -0.002031138249569469\n",
      "          vf_explained_var: -0.5345748662948608\n",
      "          vf_loss: 0.0018170525022368465\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.28333333333333\n",
      "    ram_util_percent: 60.52916666666667\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714349158852378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.76705090526716\n",
      "    mean_inference_ms: 1.3153187148607768\n",
      "    mean_raw_obs_processing_ms: 1.048537474645645\n",
      "  time_since_restore: 1281.6480424404144\n",
      "  time_this_iter_s: 17.155057430267334\n",
      "  time_total_s: 1281.6480424404144\n",
      "  timers:\n",
      "    learn_throughput: 1546.604\n",
      "    learn_time_ms: 646.578\n",
      "    load_throughput: 61846.289\n",
      "    load_time_ms: 16.169\n",
      "    sample_throughput: 51.303\n",
      "    sample_time_ms: 19492.061\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1633530711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1281.65</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             456.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-32-09\n",
      "  done: false\n",
      "  episode_len_mean: 457.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 143\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1558111270268756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01535775453611878\n",
      "          policy_loss: -0.02641830180461208\n",
      "          total_loss: -0.040415118200083575\n",
      "          vf_explained_var: -0.18624000251293182\n",
      "          vf_loss: 0.002378051785571087\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.18\n",
      "    ram_util_percent: 60.47200000000001\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371415142084464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.7849073545585\n",
      "    mean_inference_ms: 1.3152293292936323\n",
      "    mean_raw_obs_processing_ms: 1.0541650001436427\n",
      "  time_since_restore: 1299.0095765590668\n",
      "  time_this_iter_s: 17.361534118652344\n",
      "  time_total_s: 1299.0095765590668\n",
      "  timers:\n",
      "    learn_throughput: 1544.745\n",
      "    learn_time_ms: 647.356\n",
      "    load_throughput: 61789.345\n",
      "    load_time_ms: 16.184\n",
      "    sample_throughput: 56.517\n",
      "    sample_time_ms: 17693.858\n",
      "    update_time_ms: 1.697\n",
      "  timestamp: 1633530729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1299.01</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            457.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 454.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 146\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3218122826682195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01256766959601272\n",
      "          policy_loss: 0.048260841394464175\n",
      "          total_loss: 0.03064615548484855\n",
      "          vf_explained_var: -0.7175397872924805\n",
      "          vf_loss: 0.0013618484993154804\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.3962962962963\n",
      "    ram_util_percent: 60.3962962962963\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713628451073748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.812845828197332\n",
      "    mean_inference_ms: 1.3150292828101553\n",
      "    mean_raw_obs_processing_ms: 1.0632287912708303\n",
      "  time_since_restore: 1317.7167644500732\n",
      "  time_this_iter_s: 18.70718789100647\n",
      "  time_total_s: 1317.7167644500732\n",
      "  timers:\n",
      "    learn_throughput: 1545.618\n",
      "    learn_time_ms: 646.99\n",
      "    load_throughput: 61531.095\n",
      "    load_time_ms: 16.252\n",
      "    sample_throughput: 56.362\n",
      "    sample_time_ms: 17742.364\n",
      "    update_time_ms: 1.697\n",
      "  timestamp: 1633530747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1317.72</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-32-46\n",
      "  done: false\n",
      "  episode_len_mean: 453.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 148\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.279278302192688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012370526666550526\n",
      "          policy_loss: -0.10574529940883319\n",
      "          total_loss: -0.12265685742927922\n",
      "          vf_explained_var: -0.5146021246910095\n",
      "          vf_loss: 0.0017061691582461612\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.08461538461539\n",
      "    ram_util_percent: 60.30769230769231\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713250388070862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.831833078333194\n",
      "    mean_inference_ms: 1.3148863514618347\n",
      "    mean_raw_obs_processing_ms: 1.0695591362365462\n",
      "  time_since_restore: 1336.2329668998718\n",
      "  time_this_iter_s: 18.516202449798584\n",
      "  time_total_s: 1336.2329668998718\n",
      "  timers:\n",
      "    learn_throughput: 1545.908\n",
      "    learn_time_ms: 646.869\n",
      "    load_throughput: 62114.096\n",
      "    load_time_ms: 16.099\n",
      "    sample_throughput: 56.482\n",
      "    sample_time_ms: 17704.805\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1633530766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1336.23</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            453.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-33-22\n",
      "  done: false\n",
      "  episode_len_mean: 450.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 150\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2125047657224868\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010364889193530314\n",
      "          policy_loss: -0.03274722312473589\n",
      "          total_loss: -0.050445713868571655\n",
      "          vf_explained_var: -0.5462191104888916\n",
      "          vf_loss: 0.0009284086719465752\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.73207547169811\n",
      "    ram_util_percent: 60.094339622641506\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371290438901523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.851485062490237\n",
      "    mean_inference_ms: 1.3147543797498937\n",
      "    mean_raw_obs_processing_ms: 1.0807697546282093\n",
      "  time_since_restore: 1372.8431570529938\n",
      "  time_this_iter_s: 36.61019015312195\n",
      "  time_total_s: 1372.8431570529938\n",
      "  timers:\n",
      "    learn_throughput: 1549.084\n",
      "    learn_time_ms: 645.543\n",
      "    load_throughput: 62231.322\n",
      "    load_time_ms: 16.069\n",
      "    sample_throughput: 51.22\n",
      "    sample_time_ms: 19523.776\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1633530802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1372.84</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            450.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-33-41\n",
      "  done: false\n",
      "  episode_len_mean: 449.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 153\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3414266029993693\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011211919712488763\n",
      "          policy_loss: -0.025538456646932495\n",
      "          total_loss: -0.043779262113902304\n",
      "          vf_explained_var: -0.2473910003900528\n",
      "          vf_loss: 0.001389438354332621\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.714814814814815\n",
      "    ram_util_percent: 59.77777777777777\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03712438736665985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.880740681780134\n",
      "    mean_inference_ms: 1.314569820322232\n",
      "    mean_raw_obs_processing_ms: 1.0979925021408372\n",
      "  time_since_restore: 1391.7799036502838\n",
      "  time_this_iter_s: 18.93674659729004\n",
      "  time_total_s: 1391.7799036502838\n",
      "  timers:\n",
      "    learn_throughput: 1551.764\n",
      "    learn_time_ms: 644.428\n",
      "    load_throughput: 58699.277\n",
      "    load_time_ms: 17.036\n",
      "    sample_throughput: 50.676\n",
      "    sample_time_ms: 19733.206\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1633530821\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1391.78</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            449.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-33-58\n",
      "  done: false\n",
      "  episode_len_mean: 448.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 155\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1882214360766943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01217207338010697\n",
      "          policy_loss: 0.0009710455934206645\n",
      "          total_loss: -0.015173502018054326\n",
      "          vf_explained_var: -0.9320875406265259\n",
      "          vf_loss: 0.0016295925300154421\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.25833333333333\n",
      "    ram_util_percent: 59.79999999999999\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03712162141874384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.899743559233054\n",
      "    mean_inference_ms: 1.3144515992394288\n",
      "    mean_raw_obs_processing_ms: 1.1095458347662737\n",
      "  time_since_restore: 1408.792043685913\n",
      "  time_this_iter_s: 17.012140035629272\n",
      "  time_total_s: 1408.792043685913\n",
      "  timers:\n",
      "    learn_throughput: 1550.916\n",
      "    learn_time_ms: 644.78\n",
      "    load_throughput: 62600.151\n",
      "    load_time_ms: 15.974\n",
      "    sample_throughput: 51.442\n",
      "    sample_time_ms: 19439.284\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1633530838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1408.79</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            448.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-34-15\n",
      "  done: false\n",
      "  episode_len_mean: 448.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 157\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1291398829884\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011636820810970998\n",
      "          policy_loss: -0.012898222470862999\n",
      "          total_loss: -0.02858266444462869\n",
      "          vf_explained_var: -0.5012930631637573\n",
      "          vf_loss: 0.0016795295743375189\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.44347826086956\n",
      "    ram_util_percent: 59.79999999999998\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037118884892557276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.917338809607502\n",
      "    mean_inference_ms: 1.3143414680934293\n",
      "    mean_raw_obs_processing_ms: 1.1213038680960052\n",
      "  time_since_restore: 1425.2685573101044\n",
      "  time_this_iter_s: 16.476513624191284\n",
      "  time_total_s: 1425.2685573101044\n",
      "  timers:\n",
      "    learn_throughput: 1553.114\n",
      "    learn_time_ms: 643.868\n",
      "    load_throughput: 67233.326\n",
      "    load_time_ms: 14.874\n",
      "    sample_throughput: 51.93\n",
      "    sample_time_ms: 19256.579\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1633530855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1425.27</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            448.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-34-34\n",
      "  done: false\n",
      "  episode_len_mean: 448.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 159\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.218181037902832\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01341784562254548\n",
      "          policy_loss: -0.08980967932277256\n",
      "          total_loss: -0.10558321424242523\n",
      "          vf_explained_var: -0.850307047367096\n",
      "          vf_loss: 0.0018797515775077045\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.33703703703704\n",
      "    ram_util_percent: 60.0074074074074\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711629663758153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.934461179671361\n",
      "    mean_inference_ms: 1.3142354283202957\n",
      "    mean_raw_obs_processing_ms: 1.1329176714289328\n",
      "  time_since_restore: 1443.9913370609283\n",
      "  time_this_iter_s: 18.722779750823975\n",
      "  time_total_s: 1443.9913370609283\n",
      "  timers:\n",
      "    learn_throughput: 1550.681\n",
      "    learn_time_ms: 644.878\n",
      "    load_throughput: 67311.12\n",
      "    load_time_ms: 14.856\n",
      "    sample_throughput: 52.074\n",
      "    sample_time_ms: 19203.613\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1633530874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1443.99</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            448.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-34-52\n",
      "  done: false\n",
      "  episode_len_mean: 448.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 161\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.391434375445048\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013128162112786316\n",
      "          policy_loss: -0.022643980818490186\n",
      "          total_loss: -0.04095953293144703\n",
      "          vf_explained_var: -0.5883063673973083\n",
      "          vf_loss: 0.0011680376243829312\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.315384615384616\n",
      "    ram_util_percent: 60.157692307692315\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711371543899557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.950445513623428\n",
      "    mean_inference_ms: 1.3141337915692504\n",
      "    mean_raw_obs_processing_ms: 1.1344187833998507\n",
      "  time_since_restore: 1462.3836205005646\n",
      "  time_this_iter_s: 18.39228343963623\n",
      "  time_total_s: 1462.3836205005646\n",
      "  timers:\n",
      "    learn_throughput: 1552.806\n",
      "    learn_time_ms: 643.996\n",
      "    load_throughput: 67274.197\n",
      "    load_time_ms: 14.865\n",
      "    sample_throughput: 52.286\n",
      "    sample_time_ms: 19125.717\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1633530892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1462.38</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            448.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-35-10\n",
      "  done: false\n",
      "  episode_len_mean: 449.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 163\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3750210205713906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010508286609777226\n",
      "          policy_loss: -0.003525549504492018\n",
      "          total_loss: -0.02274986079169644\n",
      "          vf_explained_var: -0.9996708631515503\n",
      "          vf_loss: 0.000979351446342965\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.34615384615385\n",
      "    ram_util_percent: 60.29615384615386\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037111066187206235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.965070854862654\n",
      "    mean_inference_ms: 1.314038802789276\n",
      "    mean_raw_obs_processing_ms: 1.136225532495667\n",
      "  time_since_restore: 1480.4497706890106\n",
      "  time_this_iter_s: 18.066150188446045\n",
      "  time_total_s: 1480.4497706890106\n",
      "  timers:\n",
      "    learn_throughput: 1546.679\n",
      "    learn_time_ms: 646.547\n",
      "    load_throughput: 62584.645\n",
      "    load_time_ms: 15.978\n",
      "    sample_throughput: 52.048\n",
      "    sample_time_ms: 19213.151\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1633530910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1480.45</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            449.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-35-27\n",
      "  done: false\n",
      "  episode_len_mean: 450.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 165\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1773183080885143\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01501757690437964\n",
      "          policy_loss: -0.1617034246524175\n",
      "          total_loss: -0.1765613739689191\n",
      "          vf_explained_var: 0.13014277815818787\n",
      "          vf_loss: 0.001846800649461026\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.420833333333334\n",
      "    ram_util_percent: 60.50416666666666\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710843770913879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.97870008527961\n",
      "    mean_inference_ms: 1.3139468418386875\n",
      "    mean_raw_obs_processing_ms: 1.1382986443967724\n",
      "  time_since_restore: 1496.9447026252747\n",
      "  time_this_iter_s: 16.494931936264038\n",
      "  time_total_s: 1496.9447026252747\n",
      "  timers:\n",
      "    learn_throughput: 1547.406\n",
      "    learn_time_ms: 646.243\n",
      "    load_throughput: 67216.841\n",
      "    load_time_ms: 14.877\n",
      "    sample_throughput: 52.28\n",
      "    sample_time_ms: 19127.884\n",
      "    update_time_ms: 1.691\n",
      "  timestamp: 1633530927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1496.94</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            450.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-35-45\n",
      "  done: false\n",
      "  episode_len_mean: 451.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 168\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2433457295099895\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010948023276296166\n",
      "          policy_loss: -0.09057746984892422\n",
      "          total_loss: -0.10598767151435216\n",
      "          vf_explained_var: -0.5489946603775024\n",
      "          vf_loss: 0.0033282958921821166\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.565384615384616\n",
      "    ram_util_percent: 60.70769230769231\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710439912098022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.998170047071245\n",
      "    mean_inference_ms: 1.3138111166986308\n",
      "    mean_raw_obs_processing_ms: 1.1415477251588786\n",
      "  time_since_restore: 1514.9381892681122\n",
      "  time_this_iter_s: 17.993486642837524\n",
      "  time_total_s: 1514.9381892681122\n",
      "  timers:\n",
      "    learn_throughput: 1542.961\n",
      "    learn_time_ms: 648.104\n",
      "    load_throughput: 66864.941\n",
      "    load_time_ms: 14.956\n",
      "    sample_throughput: 52.481\n",
      "    sample_time_ms: 19054.598\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1633530945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1514.94</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            451.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-36-01\n",
      "  done: false\n",
      "  episode_len_mean: 452.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 169\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1493202884991964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015346623217862346\n",
      "          policy_loss: -0.005156778295834859\n",
      "          total_loss: -0.01972512797349029\n",
      "          vf_explained_var: -0.4760540723800659\n",
      "          vf_loss: 0.0017453676193124718\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.22727272727272\n",
      "    ram_util_percent: 60.69545454545455\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037103120698855616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.003966789434871\n",
      "    mean_inference_ms: 1.31376719436565\n",
      "    mean_raw_obs_processing_ms: 1.142835513155032\n",
      "  time_since_restore: 1530.7478685379028\n",
      "  time_this_iter_s: 15.80967926979065\n",
      "  time_total_s: 1530.7478685379028\n",
      "  timers:\n",
      "    learn_throughput: 1542.763\n",
      "    learn_time_ms: 648.188\n",
      "    load_throughput: 74222.201\n",
      "    load_time_ms: 13.473\n",
      "    sample_throughput: 53.233\n",
      "    sample_time_ms: 18785.322\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1633530961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1530.75</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            452.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-36-18\n",
      "  done: false\n",
      "  episode_len_mean: 453.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 172\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0935402592023213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012793774082070384\n",
      "          policy_loss: 0.03730097450315952\n",
      "          total_loss: 0.022322470405035547\n",
      "          vf_explained_var: -0.2819552421569824\n",
      "          vf_loss: 0.0016389997941183133\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.275999999999996\n",
      "    ram_util_percent: 60.576\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037099202368051365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.021243149808459\n",
      "    mean_inference_ms: 1.3136410640688285\n",
      "    mean_raw_obs_processing_ms: 1.1468083462046692\n",
      "  time_since_restore: 1547.9658257961273\n",
      "  time_this_iter_s: 17.217957258224487\n",
      "  time_total_s: 1547.9658257961273\n",
      "  timers:\n",
      "    learn_throughput: 1540.605\n",
      "    learn_time_ms: 649.095\n",
      "    load_throughput: 74458.185\n",
      "    load_time_ms: 13.43\n",
      "    sample_throughput: 59.364\n",
      "    sample_time_ms: 16845.218\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1633530978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1547.97</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            453.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 455.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 173\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.220693924691942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012937244788909898\n",
      "          policy_loss: -0.11842137525478999\n",
      "          total_loss: -0.1337782039410538\n",
      "          vf_explained_var: -0.32547527551651\n",
      "          vf_loss: 0.0024837897003938754\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.413636363636364\n",
      "    ram_util_percent: 60.436363636363666\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037097927274324934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.026531774358432\n",
      "    mean_inference_ms: 1.3136011726390686\n",
      "    mean_raw_obs_processing_ms: 1.148295111956639\n",
      "  time_since_restore: 1563.7388007640839\n",
      "  time_this_iter_s: 15.772974967956543\n",
      "  time_total_s: 1563.7388007640839\n",
      "  timers:\n",
      "    learn_throughput: 1541.255\n",
      "    learn_time_ms: 648.822\n",
      "    load_throughput: 83860.922\n",
      "    load_time_ms: 11.925\n",
      "    sample_throughput: 60.494\n",
      "    sample_time_ms: 16530.613\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1633530994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1563.74</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-36-51\n",
      "  done: false\n",
      "  episode_len_mean: 455.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 176\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2617243236965603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013986885505918966\n",
      "          policy_loss: -0.11271899744040437\n",
      "          total_loss: -0.12897504303190443\n",
      "          vf_explained_var: -0.4634079337120056\n",
      "          vf_loss: 0.001640620860012455\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.403999999999996\n",
      "    ram_util_percent: 60.32\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037094086812690746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.04214072910605\n",
      "    mean_inference_ms: 1.3134828803027512\n",
      "    mean_raw_obs_processing_ms: 1.152613713066749\n",
      "  time_since_restore: 1581.2530426979065\n",
      "  time_this_iter_s: 17.514241933822632\n",
      "  time_total_s: 1581.2530426979065\n",
      "  timers:\n",
      "    learn_throughput: 1539.267\n",
      "    learn_time_ms: 649.66\n",
      "    load_throughput: 76668.226\n",
      "    load_time_ms: 13.043\n",
      "    sample_throughput: 60.318\n",
      "    sample_time_ms: 16578.84\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1633531011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1581.25</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-37-09\n",
      "  done: false\n",
      "  episode_len_mean: 456.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 178\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.138553094863892\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013886788812780597\n",
      "          policy_loss: -0.0345614335189263\n",
      "          total_loss: -0.04983204570081499\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0014281248589718921\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.24230769230769\n",
      "    ram_util_percent: 60.169230769230765\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709157776244123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.051691648939459\n",
      "    mean_inference_ms: 1.3134100390859416\n",
      "    mean_raw_obs_processing_ms: 1.1557331659450751\n",
      "  time_since_restore: 1599.5337591171265\n",
      "  time_this_iter_s: 18.28071641921997\n",
      "  time_total_s: 1599.5337591171265\n",
      "  timers:\n",
      "    learn_throughput: 1541.524\n",
      "    learn_time_ms: 648.709\n",
      "    load_throughput: 70767.009\n",
      "    load_time_ms: 14.131\n",
      "    sample_throughput: 59.669\n",
      "    sample_time_ms: 16759.118\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1633531029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1599.53</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            456.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 455.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 180\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2965694506963095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012813300758249666\n",
      "          policy_loss: -0.06463656131592062\n",
      "          total_loss: -0.08176885823615723\n",
      "          vf_explained_var: -0.6290180087089539\n",
      "          vf_loss: 0.0015089103473453886\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.94705882352941\n",
      "    ram_util_percent: 59.954901960784326\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037089102046523645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.060492157670389\n",
      "    mean_inference_ms: 1.3133389537560853\n",
      "    mean_raw_obs_processing_ms: 1.1627972889189895\n",
      "  time_since_restore: 1634.764371395111\n",
      "  time_this_iter_s: 35.23061227798462\n",
      "  time_total_s: 1634.764371395111\n",
      "  timers:\n",
      "    learn_throughput: 1544.312\n",
      "    learn_time_ms: 647.537\n",
      "    load_throughput: 70249.406\n",
      "    load_time_ms: 14.235\n",
      "    sample_throughput: 54.316\n",
      "    sample_time_ms: 18410.95\n",
      "    update_time_ms: 1.697\n",
      "  timestamp: 1633531065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1634.76</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-38-03\n",
      "  done: false\n",
      "  episode_len_mean: 455.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 182\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.23397888607449\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015943360009293566\n",
      "          policy_loss: -0.051412911257810064\n",
      "          total_loss: -0.06646213755011558\n",
      "          vf_explained_var: -0.8304919600486755\n",
      "          vf_loss: 0.0019096770453163319\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.504000000000005\n",
      "    ram_util_percent: 59.972\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708668117322842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.068966651598416\n",
      "    mean_inference_ms: 1.313268641866945\n",
      "    mean_raw_obs_processing_ms: 1.1699616575365852\n",
      "  time_since_restore: 1652.7817845344543\n",
      "  time_this_iter_s: 18.01741313934326\n",
      "  time_total_s: 1652.7817845344543\n",
      "  timers:\n",
      "    learn_throughput: 1541.752\n",
      "    learn_time_ms: 648.613\n",
      "    load_throughput: 69966.987\n",
      "    load_time_ms: 14.292\n",
      "    sample_throughput: 54.43\n",
      "    sample_time_ms: 18372.353\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1633531083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1652.78</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-38-20\n",
      "  done: false\n",
      "  episode_len_mean: 456.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 185\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2407259040408665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011932234647159411\n",
      "          policy_loss: -0.05839931014925241\n",
      "          total_loss: -0.07543425396498707\n",
      "          vf_explained_var: -0.9110748171806335\n",
      "          vf_loss: 0.0013451884112631281\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.38076923076923\n",
      "    ram_util_percent: 60.29999999999999\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708309474573919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.081154294335606\n",
      "    mean_inference_ms: 1.3131643784176397\n",
      "    mean_raw_obs_processing_ms: 1.1807640190758892\n",
      "  time_since_restore: 1670.4113159179688\n",
      "  time_this_iter_s: 17.629531383514404\n",
      "  time_total_s: 1670.4113159179688\n",
      "  timers:\n",
      "    learn_throughput: 1544.67\n",
      "    learn_time_ms: 647.388\n",
      "    load_throughput: 72148.773\n",
      "    load_time_ms: 13.86\n",
      "    sample_throughput: 54.554\n",
      "    sample_time_ms: 18330.374\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1633531100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1670.41</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            456.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-38-39\n",
      "  done: false\n",
      "  episode_len_mean: 455.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 187\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2763402541478475\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011605149491409625\n",
      "          policy_loss: -0.059434059510628384\n",
      "          total_loss: -0.0770717040532165\n",
      "          vf_explained_var: -0.7551144957542419\n",
      "          vf_loss: 0.0012090178374718462\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.38461538461539\n",
      "    ram_util_percent: 60.184615384615384\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370807601649565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.088482813180313\n",
      "    mean_inference_ms: 1.3130983142652701\n",
      "    mean_raw_obs_processing_ms: 1.1881124654113486\n",
      "  time_since_restore: 1688.8424971103668\n",
      "  time_this_iter_s: 18.43118119239807\n",
      "  time_total_s: 1688.8424971103668\n",
      "  timers:\n",
      "    learn_throughput: 1543.846\n",
      "    learn_time_ms: 647.733\n",
      "    load_throughput: 66878.268\n",
      "    load_time_ms: 14.953\n",
      "    sample_throughput: 53.988\n",
      "    sample_time_ms: 18522.569\n",
      "    update_time_ms: 1.668\n",
      "  timestamp: 1633531119\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1688.84</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-38-56\n",
      "  done: false\n",
      "  episode_len_mean: 455.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 189\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.248442437913683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011735029692419217\n",
      "          policy_loss: 0.022656976824833287\n",
      "          total_loss: 0.0059732073297103245\n",
      "          vf_explained_var: -0.9996225833892822\n",
      "          vf_loss: 0.001840081545459624\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.144\n",
      "    ram_util_percent: 60.16\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707837582258756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.09459209452274\n",
      "    mean_inference_ms: 1.3130336171709072\n",
      "    mean_raw_obs_processing_ms: 1.1920067308120796\n",
      "  time_since_restore: 1706.0999054908752\n",
      "  time_this_iter_s: 17.257408380508423\n",
      "  time_total_s: 1706.0999054908752\n",
      "  timers:\n",
      "    learn_throughput: 1545.982\n",
      "    learn_time_ms: 646.838\n",
      "    load_throughput: 72611.415\n",
      "    load_time_ms: 13.772\n",
      "    sample_throughput: 54.198\n",
      "    sample_time_ms: 18451.011\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1633531136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">          1706.1</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-39-12\n",
      "  done: false\n",
      "  episode_len_mean: 457.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 191\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.177664271990458\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010118302807141117\n",
      "          policy_loss: -0.09747256934642792\n",
      "          total_loss: -0.11452404004004267\n",
      "          vf_explained_var: -0.3371618986129761\n",
      "          vf_loss: 0.0013102450834897657\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.268181818181816\n",
      "    ram_util_percent: 60.32727272727274\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370759311346603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.099625558706178\n",
      "    mean_inference_ms: 1.312968037365938\n",
      "    mean_raw_obs_processing_ms: 1.1922835596819248\n",
      "  time_since_restore: 1721.9012489318848\n",
      "  time_this_iter_s: 15.801343441009521\n",
      "  time_total_s: 1721.9012489318848\n",
      "  timers:\n",
      "    learn_throughput: 1547.592\n",
      "    learn_time_ms: 646.165\n",
      "    load_throughput: 72612.044\n",
      "    load_time_ms: 13.772\n",
      "    sample_throughput: 54.198\n",
      "    sample_time_ms: 18450.859\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1633531152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">          1721.9</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            457.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-39-28\n",
      "  done: false\n",
      "  episode_len_mean: 459.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 193\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2130037705103556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015577218766152433\n",
      "          policy_loss: -0.023505234842499098\n",
      "          total_loss: -0.03848656684988075\n",
      "          vf_explained_var: 0.1344163715839386\n",
      "          vf_loss: 0.0018913917243480682\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.225\n",
      "    ram_util_percent: 60.5375\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707352720934369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.104205157958898\n",
      "    mean_inference_ms: 1.3129043745766658\n",
      "    mean_raw_obs_processing_ms: 1.1927755288029491\n",
      "  time_since_restore: 1738.3613421916962\n",
      "  time_this_iter_s: 16.4600932598114\n",
      "  time_total_s: 1738.3613421916962\n",
      "  timers:\n",
      "    learn_throughput: 1549.806\n",
      "    learn_time_ms: 645.242\n",
      "    load_throughput: 78816.152\n",
      "    load_time_ms: 12.688\n",
      "    sample_throughput: 54.417\n",
      "    sample_time_ms: 18376.716\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1633531168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1738.36</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             459.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 460.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 195\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.172675559255812\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012660730609803843\n",
      "          policy_loss: -0.029406818913088904\n",
      "          total_loss: -0.04521373990509245\n",
      "          vf_explained_var: -0.9895788431167603\n",
      "          vf_loss: 0.0016468360127570728\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.486956521739124\n",
      "    ram_util_percent: 60.71304347826087\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707103789828748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.10811571573176\n",
      "    mean_inference_ms: 1.3128424357738717\n",
      "    mean_raw_obs_processing_ms: 1.1934666173952817\n",
      "  time_since_restore: 1754.474149942398\n",
      "  time_this_iter_s: 16.112807750701904\n",
      "  time_total_s: 1754.474149942398\n",
      "  timers:\n",
      "    learn_throughput: 1547.54\n",
      "    learn_time_ms: 646.187\n",
      "    load_throughput: 76403.016\n",
      "    load_time_ms: 13.088\n",
      "    sample_throughput: 54.32\n",
      "    sample_time_ms: 18409.346\n",
      "    update_time_ms: 1.778\n",
      "  timestamp: 1633531184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1754.47</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             460.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-40-00\n",
      "  done: false\n",
      "  episode_len_mean: 462.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 197\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.168949267599318\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009105063643593227\n",
      "          policy_loss: 0.0008460729072491328\n",
      "          total_loss: -0.016488761641085148\n",
      "          vf_explained_var: -0.8362541198730469\n",
      "          vf_loss: 0.001281697592154766\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.25\n",
      "    ram_util_percent: 60.81818181818183\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706852396723571\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.111669598449353\n",
      "    mean_inference_ms: 1.3127827127180165\n",
      "    mean_raw_obs_processing_ms: 1.1943385387996686\n",
      "  time_since_restore: 1769.9670622348785\n",
      "  time_this_iter_s: 15.492912292480469\n",
      "  time_total_s: 1769.9670622348785\n",
      "  timers:\n",
      "    learn_throughput: 1549.112\n",
      "    learn_time_ms: 645.531\n",
      "    load_throughput: 86113.138\n",
      "    load_time_ms: 11.613\n",
      "    sample_throughput: 54.917\n",
      "    sample_time_ms: 18209.33\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1633531200\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1769.97</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            462.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-40-16\n",
      "  done: false\n",
      "  episode_len_mean: 463.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 199\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2301124572753905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013519666971414778\n",
      "          policy_loss: -0.03482242917848958\n",
      "          total_loss: -0.05076894907073842\n",
      "          vf_explained_var: -0.9688199162483215\n",
      "          vf_loss: 0.0017917163592452805\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.334782608695654\n",
      "    ram_util_percent: 60.96521739130434\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706610643829695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.114902079617123\n",
      "    mean_inference_ms: 1.3127241528705258\n",
      "    mean_raw_obs_processing_ms: 1.1951894753792767\n",
      "  time_since_restore: 1785.9416782855988\n",
      "  time_this_iter_s: 15.974616050720215\n",
      "  time_total_s: 1785.9416782855988\n",
      "  timers:\n",
      "    learn_throughput: 1544.569\n",
      "    learn_time_ms: 647.43\n",
      "    load_throughput: 98832.522\n",
      "    load_time_ms: 10.118\n",
      "    sample_throughput: 55.623\n",
      "    sample_time_ms: 17978.329\n",
      "    update_time_ms: 1.791\n",
      "  timestamp: 1633531216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1785.94</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-40-32\n",
      "  done: false\n",
      "  episode_len_mean: 463.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 201\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.214715846379598\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014436478659876626\n",
      "          policy_loss: -0.032895842318733534\n",
      "          total_loss: -0.048679857949415845\n",
      "          vf_explained_var: -0.8472967147827148\n",
      "          vf_loss: 0.001490830888117974\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.30869565217391\n",
      "    ram_util_percent: 60.93913043478263\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370638062753661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.118080212252686\n",
      "    mean_inference_ms: 1.312668851269661\n",
      "    mean_raw_obs_processing_ms: 1.1962182232993834\n",
      "  time_since_restore: 1802.4236943721771\n",
      "  time_this_iter_s: 16.48201608657837\n",
      "  time_total_s: 1802.4236943721771\n",
      "  timers:\n",
      "    learn_throughput: 1545.114\n",
      "    learn_time_ms: 647.201\n",
      "    load_throughput: 107048.891\n",
      "    load_time_ms: 9.342\n",
      "    sample_throughput: 62.094\n",
      "    sample_time_ms: 16104.488\n",
      "    update_time_ms: 1.784\n",
      "  timestamp: 1633531232\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1802.42</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-40-50\n",
      "  done: false\n",
      "  episode_len_mean: 464.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 203\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2004836055967543\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011110009985713414\n",
      "          policy_loss: -0.04417083896696568\n",
      "          total_loss: -0.06119958547254403\n",
      "          vf_explained_var: -0.619084358215332\n",
      "          vf_loss: 0.0012264606135431676\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.104\n",
      "    ram_util_percent: 60.876000000000005\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037061561984958906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.121550181156547\n",
      "    mean_inference_ms: 1.312616749331844\n",
      "    mean_raw_obs_processing_ms: 1.1974128658022754\n",
      "  time_since_restore: 1819.6126911640167\n",
      "  time_this_iter_s: 17.1889967918396\n",
      "  time_total_s: 1819.6126911640167\n",
      "  timers:\n",
      "    learn_throughput: 1545.928\n",
      "    learn_time_ms: 646.861\n",
      "    load_throughput: 121263.314\n",
      "    load_time_ms: 8.247\n",
      "    sample_throughput: 62.41\n",
      "    sample_time_ms: 16023.075\n",
      "    update_time_ms: 1.778\n",
      "  timestamp: 1633531250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1819.61</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            464.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-41-07\n",
      "  done: false\n",
      "  episode_len_mean: 463.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 205\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1462151765823365\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013774605113417987\n",
      "          policy_loss: 0.028213853078583876\n",
      "          total_loss: 0.012718661046690411\n",
      "          vf_explained_var: -0.7859753370285034\n",
      "          vf_loss: 0.0013180306006688625\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.076\n",
      "    ram_util_percent: 60.856\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037059462571843886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.125360399837447\n",
      "    mean_inference_ms: 1.3125686294169001\n",
      "    mean_raw_obs_processing_ms: 1.198763220136936\n",
      "  time_since_restore: 1837.3916125297546\n",
      "  time_this_iter_s: 17.778921365737915\n",
      "  time_total_s: 1837.3916125297546\n",
      "  timers:\n",
      "    learn_throughput: 1543.014\n",
      "    learn_time_ms: 648.082\n",
      "    load_throughput: 117819.845\n",
      "    load_time_ms: 8.488\n",
      "    sample_throughput: 62.358\n",
      "    sample_time_ms: 16036.539\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1633531267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1837.39</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-41-27\n",
      "  done: false\n",
      "  episode_len_mean: 461.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 208\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1164684189690486\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015144936443746322\n",
      "          policy_loss: -0.010661742008394665\n",
      "          total_loss: -0.025531425658199523\n",
      "          vf_explained_var: -0.9418252110481262\n",
      "          vf_loss: 0.0011835848899661666\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.19642857142857\n",
      "    ram_util_percent: 60.79285714285714\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705656442620814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.132246935769878\n",
      "    mean_inference_ms: 1.312504258630301\n",
      "    mean_raw_obs_processing_ms: 1.2011030844702133\n",
      "  time_since_restore: 1856.8335723876953\n",
      "  time_this_iter_s: 19.441959857940674\n",
      "  time_total_s: 1856.8335723876953\n",
      "  timers:\n",
      "    learn_throughput: 1543.266\n",
      "    learn_time_ms: 647.976\n",
      "    load_throughput: 117924.853\n",
      "    load_time_ms: 8.48\n",
      "    sample_throughput: 61.967\n",
      "    sample_time_ms: 16137.722\n",
      "    update_time_ms: 1.779\n",
      "  timestamp: 1633531287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         1856.83</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            461.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-42-00\n",
      "  done: false\n",
      "  episode_len_mean: 461.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 210\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.023369548055861\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012599653125545146\n",
      "          policy_loss: -0.04191623373577992\n",
      "          total_loss: -0.05676527033663458\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0011322746764765017\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.083333333333336\n",
      "    ram_util_percent: 60.69166666666666\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037054775707870496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.13670923077339\n",
      "    mean_inference_ms: 1.3124638929767132\n",
      "    mean_raw_obs_processing_ms: 1.2062368478945527\n",
      "  time_since_restore: 1890.333841085434\n",
      "  time_this_iter_s: 33.50026869773865\n",
      "  time_total_s: 1890.333841085434\n",
      "  timers:\n",
      "    learn_throughput: 1539.462\n",
      "    learn_time_ms: 649.577\n",
      "    load_throughput: 104399.027\n",
      "    load_time_ms: 9.579\n",
      "    sample_throughput: 56.308\n",
      "    sample_time_ms: 17759.314\n",
      "    update_time_ms: 1.782\n",
      "  timestamp: 1633531320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         1890.33</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            461.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-42-17\n",
      "  done: false\n",
      "  episode_len_mean: 463.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 212\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1037011437945896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014356402763997798\n",
      "          policy_loss: -0.07432575180298752\n",
      "          total_loss: -0.08941909625298447\n",
      "          vf_explained_var: -0.4512487053871155\n",
      "          vf_loss: 0.0010983810727743224\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.07826086956521\n",
      "    ram_util_percent: 59.82608695652174\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705286196820562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.141119924203137\n",
      "    mean_inference_ms: 1.312423393063474\n",
      "    mean_raw_obs_processing_ms: 1.2114617311036542\n",
      "  time_since_restore: 1906.639254808426\n",
      "  time_this_iter_s: 16.305413722991943\n",
      "  time_total_s: 1906.639254808426\n",
      "  timers:\n",
      "    learn_throughput: 1541.563\n",
      "    learn_time_ms: 648.692\n",
      "    load_throughput: 100281.025\n",
      "    load_time_ms: 9.972\n",
      "    sample_throughput: 56.148\n",
      "    sample_time_ms: 17810.195\n",
      "    update_time_ms: 1.789\n",
      "  timestamp: 1633531337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1906.64</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-42-33\n",
      "  done: false\n",
      "  episode_len_mean: 463.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 214\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1146015326182046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015018884662280721\n",
      "          policy_loss: -0.03651810301881697\n",
      "          total_loss: -0.050631789780325356\n",
      "          vf_explained_var: -0.8525304794311523\n",
      "          vf_loss: 0.0019634534617782467\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.516666666666666\n",
      "    ram_util_percent: 60.1875\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705099898617323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.145203443156511\n",
      "    mean_inference_ms: 1.3123841399292895\n",
      "    mean_raw_obs_processing_ms: 1.2167748238884486\n",
      "  time_since_restore: 1923.2930722236633\n",
      "  time_this_iter_s: 16.653817415237427\n",
      "  time_total_s: 1923.2930722236633\n",
      "  timers:\n",
      "    learn_throughput: 1542.818\n",
      "    learn_time_ms: 648.165\n",
      "    load_throughput: 100393.599\n",
      "    load_time_ms: 9.961\n",
      "    sample_throughput: 56.084\n",
      "    sample_time_ms: 17830.489\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1633531353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1923.29</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-42-54\n",
      "  done: false\n",
      "  episode_len_mean: 461.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 216\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.030696678161621\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016428791813042923\n",
      "          policy_loss: -0.008682504461871254\n",
      "          total_loss: -0.020621332050197654\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.002823420918624227\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.22068965517241\n",
      "    ram_util_percent: 60.1896551724138\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037049169215537164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.149884840515023\n",
      "    mean_inference_ms: 1.3123482772396429\n",
      "    mean_raw_obs_processing_ms: 1.2221663745726814\n",
      "  time_since_restore: 1943.7850978374481\n",
      "  time_this_iter_s: 20.49202561378479\n",
      "  time_total_s: 1943.7850978374481\n",
      "  timers:\n",
      "    learn_throughput: 1541.883\n",
      "    learn_time_ms: 648.558\n",
      "    load_throughput: 90337.438\n",
      "    load_time_ms: 11.07\n",
      "    sample_throughput: 54.744\n",
      "    sample_time_ms: 18266.919\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1633531374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         1943.79</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            461.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-43-09\n",
      "  done: false\n",
      "  episode_len_mean: 461.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 218\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1084663775232104\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012353388392143156\n",
      "          policy_loss: -0.034215447430809336\n",
      "          total_loss: -0.049308751482102606\n",
      "          vf_explained_var: -0.647167980670929\n",
      "          vf_loss: 0.0018220888922870573\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.095454545454544\n",
      "    ram_util_percent: 60.17272727272728\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704744729149178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.154068673798086\n",
      "    mean_inference_ms: 1.3123123585570553\n",
      "    mean_raw_obs_processing_ms: 1.2276238972094322\n",
      "  time_since_restore: 1958.942393064499\n",
      "  time_this_iter_s: 15.157295227050781\n",
      "  time_total_s: 1958.942393064499\n",
      "  timers:\n",
      "    learn_throughput: 1545.573\n",
      "    learn_time_ms: 647.009\n",
      "    load_throughput: 90571.331\n",
      "    load_time_ms: 11.041\n",
      "    sample_throughput: 54.84\n",
      "    sample_time_ms: 18234.968\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1633531389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1958.94</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            461.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-43-26\n",
      "  done: false\n",
      "  episode_len_mean: 462.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 220\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2147815386454264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01159210754501721\n",
      "          policy_loss: -0.06568854157295492\n",
      "          total_loss: -0.08142132156838973\n",
      "          vf_explained_var: -0.91539067029953\n",
      "          vf_loss: 0.0025026990618142817\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.21304347826087\n",
      "    ram_util_percent: 60.33043478260871\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370457229961864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.158001118500492\n",
      "    mean_inference_ms: 1.3122743231163503\n",
      "    mean_raw_obs_processing_ms: 1.2300973050414756\n",
      "  time_since_restore: 1975.3027184009552\n",
      "  time_this_iter_s: 16.3603253364563\n",
      "  time_total_s: 1975.3027184009552\n",
      "  timers:\n",
      "    learn_throughput: 1546.762\n",
      "    learn_time_ms: 646.512\n",
      "    load_throughput: 87403.913\n",
      "    load_time_ms: 11.441\n",
      "    sample_throughput: 54.724\n",
      "    sample_time_ms: 18273.599\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1633531406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">          1975.3</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            462.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-43-41\n",
      "  done: false\n",
      "  episode_len_mean: 463.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 222\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.230794514550103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010767944803126555\n",
      "          policy_loss: -0.02272097049281001\n",
      "          total_loss: -0.03997972996181084\n",
      "          vf_explained_var: -0.8024856448173523\n",
      "          vf_loss: 0.0014150042752994019\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.304545454545455\n",
      "    ram_util_percent: 60.50454545454544\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037044017981863714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.16128053258669\n",
      "    mean_inference_ms: 1.3122352971489153\n",
      "    mean_raw_obs_processing_ms: 1.2297761327835388\n",
      "  time_since_restore: 1990.4177701473236\n",
      "  time_this_iter_s: 15.115051746368408\n",
      "  time_total_s: 1990.4177701473236\n",
      "  timers:\n",
      "    learn_throughput: 1549.494\n",
      "    learn_time_ms: 645.372\n",
      "    load_throughput: 94073.275\n",
      "    load_time_ms: 10.63\n",
      "    sample_throughput: 55.13\n",
      "    sample_time_ms: 18138.853\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1633531421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         1990.42</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 465.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 224\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.045667682753669\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010294158130852556\n",
      "          policy_loss: -0.08829964784284433\n",
      "          total_loss: -0.10415485238449441\n",
      "          vf_explained_var: -0.9894804954528809\n",
      "          vf_loss: 0.0011271928922117998\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.23181818181818\n",
      "    ram_util_percent: 60.65909090909092\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037042357999746915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.16375526151193\n",
      "    mean_inference_ms: 1.3121957944617122\n",
      "    mean_raw_obs_processing_ms: 1.229611905116966\n",
      "  time_since_restore: 2005.76828789711\n",
      "  time_this_iter_s: 15.350517749786377\n",
      "  time_total_s: 2005.76828789711\n",
      "  timers:\n",
      "    learn_throughput: 1552.225\n",
      "    learn_time_ms: 644.237\n",
      "    load_throughput: 98427.804\n",
      "    load_time_ms: 10.16\n",
      "    sample_throughput: 55.69\n",
      "    sample_time_ms: 17956.619\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1633531436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         2005.77</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            465.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 466.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 226\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1493347353405423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012050547453964612\n",
      "          policy_loss: -0.09249902203058204\n",
      "          total_loss: -0.10772469863295556\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0022006089398120014\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.17083333333333\n",
      "    ram_util_percent: 60.78333333333333\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704075730290508\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.165758635747101\n",
      "    mean_inference_ms: 1.3121564119322204\n",
      "    mean_raw_obs_processing_ms: 1.2295921613089855\n",
      "  time_since_restore: 2022.9895408153534\n",
      "  time_this_iter_s: 17.221252918243408\n",
      "  time_total_s: 2022.9895408153534\n",
      "  timers:\n",
      "    learn_throughput: 1556.301\n",
      "    learn_time_ms: 642.549\n",
      "    load_throughput: 108160.553\n",
      "    load_time_ms: 9.246\n",
      "    sample_throughput: 55.855\n",
      "    sample_time_ms: 17903.474\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1633531453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         2022.99</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            466.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-44-29\n",
      "  done: false\n",
      "  episode_len_mean: 467.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 228\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1765519645478992\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01127484576661397\n",
      "          policy_loss: -0.06601722261144055\n",
      "          total_loss: -0.08213262909816371\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0018448510517676672\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.256521739130434\n",
      "    ram_util_percent: 60.95217391304348\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703916739153598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.1673346027438\n",
      "    mean_inference_ms: 1.3121167467351378\n",
      "    mean_raw_obs_processing_ms: 1.229542636242573\n",
      "  time_since_restore: 2038.9470014572144\n",
      "  time_this_iter_s: 15.957460641860962\n",
      "  time_total_s: 2038.9470014572144\n",
      "  timers:\n",
      "    learn_throughput: 1554.729\n",
      "    learn_time_ms: 643.199\n",
      "    load_throughput: 128761.14\n",
      "    load_time_ms: 7.766\n",
      "    sample_throughput: 56.961\n",
      "    sample_time_ms: 17555.874\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633531469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         2038.95</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            467.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-44-45\n",
      "  done: false\n",
      "  episode_len_mean: 468.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 230\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1137907160653007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012085161146310636\n",
      "          policy_loss: -0.05797718343221479\n",
      "          total_loss: -0.07338635846972466\n",
      "          vf_explained_var: -0.8581089377403259\n",
      "          vf_loss: 0.0016499908472825257\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.186956521739134\n",
      "    ram_util_percent: 61.04347826086955\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703760758604888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.168819521148594\n",
      "    mean_inference_ms: 1.3120790201140056\n",
      "    mean_raw_obs_processing_ms: 1.22963528648957\n",
      "  time_since_restore: 2055.1012892723083\n",
      "  time_this_iter_s: 16.154287815093994\n",
      "  time_total_s: 2055.1012892723083\n",
      "  timers:\n",
      "    learn_throughput: 1560.479\n",
      "    learn_time_ms: 640.829\n",
      "    load_throughput: 138743.198\n",
      "    load_time_ms: 7.208\n",
      "    sample_throughput: 63.194\n",
      "    sample_time_ms: 15824.215\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1633531485\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">          2055.1</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            468.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-45-01\n",
      "  done: false\n",
      "  episode_len_mean: 470.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 232\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0785913308461508\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012038686228544983\n",
      "          policy_loss: -0.07258203716741668\n",
      "          total_loss: -0.0876965146097872\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0016083799483668474\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.25909090909091\n",
      "    ram_util_percent: 61.03636363636363\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703627979228342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.169108018846941\n",
      "    mean_inference_ms: 1.3120409302014169\n",
      "    mean_raw_obs_processing_ms: 1.2298619775150137\n",
      "  time_since_restore: 2070.3106145858765\n",
      "  time_this_iter_s: 15.209325313568115\n",
      "  time_total_s: 2070.3106145858765\n",
      "  timers:\n",
      "    learn_throughput: 1561.296\n",
      "    learn_time_ms: 640.494\n",
      "    load_throughput: 146852.699\n",
      "    load_time_ms: 6.81\n",
      "    sample_throughput: 63.632\n",
      "    sample_time_ms: 15715.37\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633531501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         2070.31</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            470.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-45-17\n",
      "  done: false\n",
      "  episode_len_mean: 471.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 234\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.072985009352366\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011381302727191825\n",
      "          policy_loss: -0.0638427403031124\n",
      "          total_loss: -0.0789082329099377\n",
      "          vf_explained_var: -0.7495878338813782\n",
      "          vf_loss: 0.001823168739469515\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.24347826086956\n",
      "    ram_util_percent: 61.039130434782614\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703492453530702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.168920285043022\n",
      "    mean_inference_ms: 1.3120024512887571\n",
      "    mean_raw_obs_processing_ms: 1.2302178133907216\n",
      "  time_since_restore: 2086.67138171196\n",
      "  time_this_iter_s: 16.360767126083374\n",
      "  time_total_s: 2086.67138171196\n",
      "  timers:\n",
      "    learn_throughput: 1559.909\n",
      "    learn_time_ms: 641.063\n",
      "    load_throughput: 137499.23\n",
      "    load_time_ms: 7.273\n",
      "    sample_throughput: 63.755\n",
      "    sample_time_ms: 15685.021\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1633531517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         2086.67</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            471.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-45-33\n",
      "  done: false\n",
      "  episode_len_mean: 472.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 236\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1714448478486803\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011807922156435606\n",
      "          policy_loss: -0.06624793853196832\n",
      "          total_loss: -0.0823870016882817\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0015902118084745276\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.59130434782609\n",
      "    ram_util_percent: 60.986956521739124\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703368997867653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.167921097361736\n",
      "    mean_inference_ms: 1.3119662091323372\n",
      "    mean_raw_obs_processing_ms: 1.230686418591044\n",
      "  time_since_restore: 2102.541814804077\n",
      "  time_this_iter_s: 15.87043309211731\n",
      "  time_total_s: 2102.541814804077\n",
      "  timers:\n",
      "    learn_throughput: 1562.738\n",
      "    learn_time_ms: 639.903\n",
      "    load_throughput: 173354.881\n",
      "    load_time_ms: 5.769\n",
      "    sample_throughput: 65.679\n",
      "    sample_time_ms: 15225.535\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1633531533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         2102.54</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            472.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-45-50\n",
      "  done: false\n",
      "  episode_len_mean: 473.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 238\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0205370201004875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013717436601165861\n",
      "          policy_loss: -0.05815521101984713\n",
      "          total_loss: -0.07205952944027053\n",
      "          vf_explained_var: -0.7761900424957275\n",
      "          vf_loss: 0.0016714170497531692\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.6\n",
      "    ram_util_percent: 60.992\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703252663193212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.166670244094853\n",
      "    mean_inference_ms: 1.3119323539499286\n",
      "    mean_raw_obs_processing_ms: 1.2311209391898095\n",
      "  time_since_restore: 2119.903606891632\n",
      "  time_this_iter_s: 17.36179208755493\n",
      "  time_total_s: 2119.903606891632\n",
      "  timers:\n",
      "    learn_throughput: 1557.937\n",
      "    learn_time_ms: 641.875\n",
      "    load_throughput: 159153.67\n",
      "    load_time_ms: 6.283\n",
      "    sample_throughput: 64.752\n",
      "    sample_time_ms: 15443.489\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1633531550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">          2119.9</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            473.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-46-24\n",
      "  done: false\n",
      "  episode_len_mean: 474.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 240\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0046339604589676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016265575673774145\n",
      "          policy_loss: -0.058983074418372576\n",
      "          total_loss: -0.07154465586774879\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0019951257324363624\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.648936170212764\n",
      "    ram_util_percent: 60.81702127659572\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703147861218171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.16501016507565\n",
      "    mean_inference_ms: 1.3119003520794976\n",
      "    mean_raw_obs_processing_ms: 1.234592153210808\n",
      "  time_since_restore: 2153.3420102596283\n",
      "  time_this_iter_s: 33.438403367996216\n",
      "  time_total_s: 2153.3420102596283\n",
      "  timers:\n",
      "    learn_throughput: 1556.615\n",
      "    learn_time_ms: 642.419\n",
      "    load_throughput: 135836.464\n",
      "    load_time_ms: 7.362\n",
      "    sample_throughput: 58.31\n",
      "    sample_time_ms: 17149.693\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1633531584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         2153.34</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            474.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-46-41\n",
      "  done: false\n",
      "  episode_len_mean: 474.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 242\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1038234260347153\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013872002866591534\n",
      "          policy_loss: -0.08110035091845526\n",
      "          total_loss: -0.09608647383542525\n",
      "          vf_explained_var: -0.9992802739143372\n",
      "          vf_loss: 0.0013703092583455146\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.58\n",
      "    ram_util_percent: 60.06799999999999\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703046703859971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.163399198868497\n",
      "    mean_inference_ms: 1.3118694801722264\n",
      "    mean_raw_obs_processing_ms: 1.2381490851972317\n",
      "  time_since_restore: 2170.795848608017\n",
      "  time_this_iter_s: 17.453838348388672\n",
      "  time_total_s: 2170.795848608017\n",
      "  timers:\n",
      "    learn_throughput: 1551.598\n",
      "    learn_time_ms: 644.497\n",
      "    load_throughput: 121543.379\n",
      "    load_time_ms: 8.228\n",
      "    sample_throughput: 57.535\n",
      "    sample_time_ms: 17380.626\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1633531601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">          2170.8</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            474.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-46-59\n",
      "  done: false\n",
      "  episode_len_mean: 475.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 245\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0383077250586616\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013826126016902633\n",
      "          policy_loss: -0.025374649630652533\n",
      "          total_loss: -0.03976141061219904\n",
      "          vf_explained_var: -0.9480652809143066\n",
      "          vf_loss: 0.001329998072469607\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.11600000000001\n",
      "    ram_util_percent: 60.008\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702887036555206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.160655636228697\n",
      "    mean_inference_ms: 1.3118231174720012\n",
      "    mean_raw_obs_processing_ms: 1.243671799558812\n",
      "  time_since_restore: 2188.300058364868\n",
      "  time_this_iter_s: 17.504209756851196\n",
      "  time_total_s: 2188.300058364868\n",
      "  timers:\n",
      "    learn_throughput: 1552.675\n",
      "    learn_time_ms: 644.05\n",
      "    load_throughput: 100101.287\n",
      "    load_time_ms: 9.99\n",
      "    sample_throughput: 56.835\n",
      "    sample_time_ms: 17594.675\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1633531619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">          2188.3</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             475.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-47-14\n",
      "  done: false\n",
      "  episode_len_mean: 476.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 247\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0574903938505384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011986341859278148\n",
      "          policy_loss: -0.06517844498157502\n",
      "          total_loss: -0.07984317830867238\n",
      "          vf_explained_var: -0.705316424369812\n",
      "          vf_loss: 0.0018647794003805352\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.017391304347825\n",
      "    ram_util_percent: 60.039130434782585\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037027763052124046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.158256415303908\n",
      "    mean_inference_ms: 1.3117913888885957\n",
      "    mean_raw_obs_processing_ms: 1.2473802784076045\n",
      "  time_since_restore: 2203.8262252807617\n",
      "  time_this_iter_s: 15.526166915893555\n",
      "  time_total_s: 2203.8262252807617\n",
      "  timers:\n",
      "    learn_throughput: 1548.996\n",
      "    learn_time_ms: 645.579\n",
      "    load_throughput: 104097.17\n",
      "    load_time_ms: 9.606\n",
      "    sample_throughput: 57.392\n",
      "    sample_time_ms: 17423.997\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1633531634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         2203.83</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            476.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-47-34\n",
      "  done: false\n",
      "  episode_len_mean: 476.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 249\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0215391569667394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013571061993774603\n",
      "          policy_loss: -0.06309663511605726\n",
      "          total_loss: -0.07741633014132579\n",
      "          vf_explained_var: -0.9968013167381287\n",
      "          vf_loss: 0.0013154641304734267\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.25555555555556\n",
      "    ram_util_percent: 60.185185185185176\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702662369182808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.155813103504201\n",
      "    mean_inference_ms: 1.311760029206313\n",
      "    mean_raw_obs_processing_ms: 1.2488145663462364\n",
      "  time_since_restore: 2223.3079335689545\n",
      "  time_this_iter_s: 19.48170828819275\n",
      "  time_total_s: 2223.3079335689545\n",
      "  timers:\n",
      "    learn_throughput: 1551.306\n",
      "    learn_time_ms: 644.618\n",
      "    load_throughput: 90374.032\n",
      "    load_time_ms: 11.065\n",
      "    sample_throughput: 56.256\n",
      "    sample_time_ms: 17775.911\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1633531654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         2223.31</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             476.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-47-54\n",
      "  done: false\n",
      "  episode_len_mean: 475.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 252\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9869281159506904\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007596515365932848\n",
      "          policy_loss: -0.12651936933398247\n",
      "          total_loss: -0.1423713580601745\n",
      "          vf_explained_var: -0.5487685203552246\n",
      "          vf_loss: 0.0014534680036983143\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.13333333333334\n",
      "    ram_util_percent: 60.40333333333333\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037024918445081885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.15217838295179\n",
      "    mean_inference_ms: 1.3117131573818694\n",
      "    mean_raw_obs_processing_ms: 1.2476764944114154\n",
      "  time_since_restore: 2243.9453728199005\n",
      "  time_this_iter_s: 20.637439250946045\n",
      "  time_total_s: 2243.9453728199005\n",
      "  timers:\n",
      "    learn_throughput: 1548.131\n",
      "    learn_time_ms: 645.94\n",
      "    load_throughput: 85730.486\n",
      "    load_time_ms: 11.664\n",
      "    sample_throughput: 54.878\n",
      "    sample_time_ms: 18222.315\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633531674\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         2243.95</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            475.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-48-14\n",
      "  done: false\n",
      "  episode_len_mean: 474.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 254\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.866599726676941\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01146309807988241\n",
      "          policy_loss: -0.03532946478161547\n",
      "          total_loss: -0.04844040796160698\n",
      "          vf_explained_var: -0.9422371983528137\n",
      "          vf_loss: 0.001686254082273485\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.067857142857136\n",
      "    ram_util_percent: 60.58571428571428\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037023863897110605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.150106361222155\n",
      "    mean_inference_ms: 1.311683932845153\n",
      "    mean_raw_obs_processing_ms: 1.2469896166460288\n",
      "  time_since_restore: 2263.9351432323456\n",
      "  time_this_iter_s: 19.98977041244507\n",
      "  time_total_s: 2263.9351432323456\n",
      "  timers:\n",
      "    learn_throughput: 1543.207\n",
      "    learn_time_ms: 648.001\n",
      "    load_throughput: 75934.658\n",
      "    load_time_ms: 13.169\n",
      "    sample_throughput: 53.485\n",
      "    sample_time_ms: 18696.764\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1633531694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         2263.94</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            474.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-48-36\n",
      "  done: false\n",
      "  episode_len_mean: 470.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 257\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6172933922873602\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016022174401140375\n",
      "          policy_loss: -0.10977666088276439\n",
      "          total_loss: -0.11598870919810401\n",
      "          vf_explained_var: -0.3366422653198242\n",
      "          vf_loss: 0.004553403966646228\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.08709677419354\n",
      "    ram_util_percent: 60.80322580645163\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702235269377126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.148365095882825\n",
      "    mean_inference_ms: 1.3116430648234851\n",
      "    mean_raw_obs_processing_ms: 1.2462242631657847\n",
      "  time_since_restore: 2285.637377977371\n",
      "  time_this_iter_s: 21.702234745025635\n",
      "  time_total_s: 2285.637377977371\n",
      "  timers:\n",
      "    learn_throughput: 1544.931\n",
      "    learn_time_ms: 647.278\n",
      "    load_throughput: 72283.433\n",
      "    load_time_ms: 13.834\n",
      "    sample_throughput: 51.999\n",
      "    sample_time_ms: 19230.983\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633531716\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         2285.64</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            470.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 468.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 260\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6332072218259175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014375702326088004\n",
      "          policy_loss: -0.005763823456234402\n",
      "          total_loss: -0.01582788568403986\n",
      "          vf_explained_var: -0.10792546719312668\n",
      "          vf_loss: 0.0014162184670567513\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.10625\n",
      "    ram_util_percent: 60.98125\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702084375464953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.14724719141993\n",
      "    mean_inference_ms: 1.3115997252031846\n",
      "    mean_raw_obs_processing_ms: 1.245925964473586\n",
      "  time_since_restore: 2307.7537231445312\n",
      "  time_this_iter_s: 22.116345167160034\n",
      "  time_total_s: 2307.7537231445312\n",
      "  timers:\n",
      "    learn_throughput: 1543.257\n",
      "    learn_time_ms: 647.98\n",
      "    load_throughput: 65271.142\n",
      "    load_time_ms: 15.321\n",
      "    sample_throughput: 50.369\n",
      "    sample_time_ms: 19853.368\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1633531738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         2307.75</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            468.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-49-19\n",
      "  done: false\n",
      "  episode_len_mean: 466.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 262\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.911447775363922\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011452408421139202\n",
      "          policy_loss: -0.039003903542955715\n",
      "          total_loss: -0.05295069159732924\n",
      "          vf_explained_var: -0.9986703395843506\n",
      "          vf_loss: 0.001302498187093685\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.1448275862069\n",
      "    ram_util_percent: 61.0\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701993537775274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.146761753931067\n",
      "    mean_inference_ms: 1.3115688439119482\n",
      "    mean_raw_obs_processing_ms: 1.2458791078829374\n",
      "  time_since_restore: 2328.305418729782\n",
      "  time_this_iter_s: 20.551695585250854\n",
      "  time_total_s: 2328.305418729782\n",
      "  timers:\n",
      "    learn_throughput: 1546.452\n",
      "    learn_time_ms: 646.642\n",
      "    load_throughput: 61380.987\n",
      "    load_time_ms: 16.292\n",
      "    sample_throughput: 49.572\n",
      "    sample_time_ms: 20172.726\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1633531759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         2328.31</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             466.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 463.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 265\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3782996204164293\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013008882018731575\n",
      "          policy_loss: 0.07596377581357956\n",
      "          total_loss: 0.06804253955682119\n",
      "          vf_explained_var: -0.23255503177642822\n",
      "          vf_loss: 0.0014712595503725526\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.43448275862069\n",
      "    ram_util_percent: 61.13448275862068\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037018828990999036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.146706244571279\n",
      "    mean_inference_ms: 1.3115260559751716\n",
      "    mean_raw_obs_processing_ms: 1.2460343467399075\n",
      "  time_since_restore: 2348.2302417755127\n",
      "  time_this_iter_s: 19.92482304573059\n",
      "  time_total_s: 2348.2302417755127\n",
      "  timers:\n",
      "    learn_throughput: 1536.117\n",
      "    learn_time_ms: 650.992\n",
      "    load_throughput: 61851.396\n",
      "    load_time_ms: 16.168\n",
      "    sample_throughput: 53.143\n",
      "    sample_time_ms: 18817.082\n",
      "    update_time_ms: 1.711\n",
      "  timestamp: 1633531779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         2348.23</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-49-55\n",
      "  done: false\n",
      "  episode_len_mean: 464.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 266\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.837842082977295\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012461721754999171\n",
      "          policy_loss: -0.06399207313855489\n",
      "          total_loss: -0.07614485323429107\n",
      "          vf_explained_var: -0.4911079406738281\n",
      "          vf_loss: 0.0020198110542777514\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.28695652173913\n",
      "    ram_util_percent: 61.40869565217392\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701858496534103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.146568252481565\n",
      "    mean_inference_ms: 1.3115156508461339\n",
      "    mean_raw_obs_processing_ms: 1.246170786951999\n",
      "  time_since_restore: 2364.5037932395935\n",
      "  time_this_iter_s: 16.27355146408081\n",
      "  time_total_s: 2364.5037932395935\n",
      "  timers:\n",
      "    learn_throughput: 1535.023\n",
      "    learn_time_ms: 651.456\n",
      "    load_throughput: 61936.703\n",
      "    load_time_ms: 16.146\n",
      "    sample_throughput: 53.48\n",
      "    sample_time_ms: 18698.614\n",
      "    update_time_ms: 1.704\n",
      "  timestamp: 1633531795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">          2364.5</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            464.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-50-17\n",
      "  done: false\n",
      "  episode_len_mean: 462.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 269\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9129462242126465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014798905718050313\n",
      "          policy_loss: -0.06310250047180388\n",
      "          total_loss: -0.07506484331356154\n",
      "          vf_explained_var: -0.7098937630653381\n",
      "          vf_loss: 0.0021724864054704085\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.274193548387096\n",
      "    ram_util_percent: 61.05806451612903\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701784930264282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.147269589799327\n",
      "    mean_inference_ms: 1.3114864022610877\n",
      "    mean_raw_obs_processing_ms: 1.2464672675582276\n",
      "  time_since_restore: 2385.929076194763\n",
      "  time_this_iter_s: 21.425282955169678\n",
      "  time_total_s: 2385.929076194763\n",
      "  timers:\n",
      "    learn_throughput: 1530.186\n",
      "    learn_time_ms: 653.515\n",
      "    load_throughput: 63473.892\n",
      "    load_time_ms: 15.755\n",
      "    sample_throughput: 52.386\n",
      "    sample_time_ms: 19089.043\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1633531817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         2385.93</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            462.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-50-54\n",
      "  done: false\n",
      "  episode_len_mean: 459.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 272\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8757422102822199\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018138005630186013\n",
      "          policy_loss: -0.00015947425531016456\n",
      "          total_loss: -0.011363971398936377\n",
      "          vf_explained_var: 0.20793242752552032\n",
      "          vf_loss: 0.0014313499585518407\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.535185185185185\n",
      "    ram_util_percent: 60.414814814814825\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701742674657503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.149394508621159\n",
      "    mean_inference_ms: 1.3114637515474699\n",
      "    mean_raw_obs_processing_ms: 1.2508524343358907\n",
      "  time_since_restore: 2423.6578364372253\n",
      "  time_this_iter_s: 37.72876024246216\n",
      "  time_total_s: 2423.6578364372253\n",
      "  timers:\n",
      "    learn_throughput: 1532.522\n",
      "    learn_time_ms: 652.519\n",
      "    load_throughput: 58385.183\n",
      "    load_time_ms: 17.128\n",
      "    sample_throughput: 46.929\n",
      "    sample_time_ms: 21308.921\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1633531854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         2423.66</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-51-19\n",
      "  done: false\n",
      "  episode_len_mean: 456.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 274\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.399029016494751\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00799323413422953\n",
      "          policy_loss: -0.008726393514209323\n",
      "          total_loss: -0.017944432215558158\n",
      "          vf_explained_var: -0.04851456731557846\n",
      "          vf_loss: 0.002074537011018644\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.08857142857143\n",
      "    ram_util_percent: 59.99714285714286\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037017209710226345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.152324057836019\n",
      "    mean_inference_ms: 1.3114528617013799\n",
      "    mean_raw_obs_processing_ms: 1.25396441349373\n",
      "  time_since_restore: 2448.7390854358673\n",
      "  time_this_iter_s: 25.081248998641968\n",
      "  time_total_s: 2448.7390854358673\n",
      "  timers:\n",
      "    learn_throughput: 1531.932\n",
      "    learn_time_ms: 652.77\n",
      "    load_throughput: 58251.229\n",
      "    load_time_ms: 17.167\n",
      "    sample_throughput: 45.728\n",
      "    sample_time_ms: 21868.601\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1633531879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         2448.74</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               456</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-51-41\n",
      "  done: false\n",
      "  episode_len_mean: 454.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 277\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4879621399773493\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012081752864804838\n",
      "          policy_loss: -0.04145595093982087\n",
      "          total_loss: -0.05093408841639757\n",
      "          vf_explained_var: -0.6816033124923706\n",
      "          vf_loss: 0.0013238948663153375\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.24375\n",
      "    ram_util_percent: 60.034375\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701688680763238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.157571316374968\n",
      "    mean_inference_ms: 1.3114384159611179\n",
      "    mean_raw_obs_processing_ms: 1.2586649081723928\n",
      "  time_since_restore: 2470.6042551994324\n",
      "  time_this_iter_s: 21.865169763565063\n",
      "  time_total_s: 2470.6042551994324\n",
      "  timers:\n",
      "    learn_throughput: 1533.098\n",
      "    learn_time_ms: 652.274\n",
      "    load_throughput: 58896.108\n",
      "    load_time_ms: 16.979\n",
      "    sample_throughput: 45.471\n",
      "    sample_time_ms: 21991.858\n",
      "    update_time_ms: 1.691\n",
      "  timestamp: 1633531901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">          2470.6</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-52-02\n",
      "  done: false\n",
      "  episode_len_mean: 454.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 279\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6371657027138604\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011048554525283806\n",
      "          policy_loss: 0.0012429546978738573\n",
      "          total_loss: -0.009530220553278923\n",
      "          vf_explained_var: -0.9631701111793518\n",
      "          vf_loss: 0.0018695948050460882\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.24827586206897\n",
      "    ram_util_percent: 60.206896551724135\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037016746391501654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.161291473630802\n",
      "    mean_inference_ms: 1.3114296332603879\n",
      "    mean_raw_obs_processing_ms: 1.2600029247895954\n",
      "  time_since_restore: 2491.5020973682404\n",
      "  time_this_iter_s: 20.897842168807983\n",
      "  time_total_s: 2491.5020973682404\n",
      "  timers:\n",
      "    learn_throughput: 1532.82\n",
      "    learn_time_ms: 652.392\n",
      "    load_throughput: 59729.287\n",
      "    load_time_ms: 16.742\n",
      "    sample_throughput: 45.284\n",
      "    sample_time_ms: 22082.776\n",
      "    update_time_ms: 1.691\n",
      "  timestamp: 1633531922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">          2491.5</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-52-23\n",
      "  done: false\n",
      "  episode_len_mean: 455.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 281\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7068028383784823\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016507324504460876\n",
      "          policy_loss: -0.037407032152016954\n",
      "          total_loss: -0.04606042479475339\n",
      "          vf_explained_var: -0.45688918232917786\n",
      "          vf_loss: 0.002843413225814907\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.03333333333333\n",
      "    ram_util_percent: 60.33333333333335\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701666668626459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.165289771572684\n",
      "    mean_inference_ms: 1.3114226198702545\n",
      "    mean_raw_obs_processing_ms: 1.2595243481893579\n",
      "  time_since_restore: 2512.338374376297\n",
      "  time_this_iter_s: 20.83627700805664\n",
      "  time_total_s: 2512.338374376297\n",
      "  timers:\n",
      "    learn_throughput: 1530.1\n",
      "    learn_time_ms: 653.552\n",
      "    load_throughput: 60282.39\n",
      "    load_time_ms: 16.589\n",
      "    sample_throughput: 45.465\n",
      "    sample_time_ms: 21995.179\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1633531943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         2512.34</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-52-46\n",
      "  done: false\n",
      "  episode_len_mean: 452.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 284\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.472089589966668\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008475856804149787\n",
      "          policy_loss: -0.1439684406750732\n",
      "          total_loss: -0.154070759150717\n",
      "          vf_explained_var: -0.2847822606563568\n",
      "          vf_loss: 0.0017579745397799545\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.081818181818186\n",
      "    ram_util_percent: 60.47272727272727\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701656937637333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.172365671856785\n",
      "    mean_inference_ms: 1.3114166593291738\n",
      "    mean_raw_obs_processing_ms: 1.2590125353646031\n",
      "  time_since_restore: 2535.533285140991\n",
      "  time_this_iter_s: 23.194910764694214\n",
      "  time_total_s: 2535.533285140991\n",
      "  timers:\n",
      "    learn_throughput: 1526.956\n",
      "    learn_time_ms: 654.898\n",
      "    load_throughput: 60967.443\n",
      "    load_time_ms: 16.402\n",
      "    sample_throughput: 45.245\n",
      "    sample_time_ms: 22101.892\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1633531966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         2535.53</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            452.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-53-06\n",
      "  done: false\n",
      "  episode_len_mean: 454.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 286\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3542534430821738\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012043805903389283\n",
      "          policy_loss: -0.10440951594048076\n",
      "          total_loss: -0.11246676668524742\n",
      "          vf_explained_var: 0.32548731565475464\n",
      "          vf_loss: 0.0014204992104269979\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.00689655172414\n",
      "    ram_util_percent: 60.68620689655174\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701651713921617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.177269522029519\n",
      "    mean_inference_ms: 1.3114141128720218\n",
      "    mean_raw_obs_processing_ms: 1.2587160896813887\n",
      "  time_since_restore: 2555.319217443466\n",
      "  time_this_iter_s: 19.785932302474976\n",
      "  time_total_s: 2555.319217443466\n",
      "  timers:\n",
      "    learn_throughput: 1526.953\n",
      "    learn_time_ms: 654.899\n",
      "    load_throughput: 61547.076\n",
      "    load_time_ms: 16.248\n",
      "    sample_throughput: 45.402\n",
      "    sample_time_ms: 22025.488\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633531986\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         2555.32</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-53-31\n",
      "  done: false\n",
      "  episode_len_mean: 452.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 289\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5874063664012485\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012779967644060067\n",
      "          policy_loss: -0.08847716661791007\n",
      "          total_loss: -0.0987074277881119\n",
      "          vf_explained_var: -0.3985958993434906\n",
      "          vf_loss: 0.0013305643030131857\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.120000000000005\n",
      "    ram_util_percent: 60.7\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701656802983776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.186025112316267\n",
      "    mean_inference_ms: 1.3114138837063072\n",
      "    mean_raw_obs_processing_ms: 1.2584721124143676\n",
      "  time_since_restore: 2580.062472343445\n",
      "  time_this_iter_s: 24.743254899978638\n",
      "  time_total_s: 2580.062472343445\n",
      "  timers:\n",
      "    learn_throughput: 1537.639\n",
      "    learn_time_ms: 650.348\n",
      "    load_throughput: 61610.995\n",
      "    load_time_ms: 16.231\n",
      "    sample_throughput: 44.421\n",
      "    sample_time_ms: 22511.936\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1633532011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         2580.06</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            452.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-53-56\n",
      "  done: false\n",
      "  episode_len_mean: 448.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 292\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.724783064259423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006358739219283214\n",
      "          policy_loss: 0.03637071359488699\n",
      "          total_loss: 0.022664523724880484\n",
      "          vf_explained_var: 0.04925107955932617\n",
      "          vf_loss: 0.0013955657609686669\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.89722222222221\n",
      "    ram_util_percent: 60.68888888888888\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701667743023977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.197045975888335\n",
      "    mean_inference_ms: 1.311419630613432\n",
      "    mean_raw_obs_processing_ms: 1.2586016932557809\n",
      "  time_since_restore: 2605.355456829071\n",
      "  time_this_iter_s: 25.29298448562622\n",
      "  time_total_s: 2605.355456829071\n",
      "  timers:\n",
      "    learn_throughput: 1542.427\n",
      "    learn_time_ms: 648.329\n",
      "    load_throughput: 59601.548\n",
      "    load_time_ms: 16.778\n",
      "    sample_throughput: 42.707\n",
      "    sample_time_ms: 23415.353\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1633532036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         2605.36</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            448.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-54-23\n",
      "  done: false\n",
      "  episode_len_mean: 445.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 294\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6381484164132012\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009339463385675689\n",
      "          policy_loss: -0.012728391836086908\n",
      "          total_loss: -0.024825343489646913\n",
      "          vf_explained_var: -0.07765395194292068\n",
      "          vf_loss: 0.001132462499339858\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.07894736842105\n",
      "    ram_util_percent: 60.602631578947374\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701675658309967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.205908327342234\n",
      "    mean_inference_ms: 1.311426836083992\n",
      "    mean_raw_obs_processing_ms: 1.2588029931385005\n",
      "  time_since_restore: 2631.823815345764\n",
      "  time_this_iter_s: 26.468358516693115\n",
      "  time_total_s: 2631.823815345764\n",
      "  timers:\n",
      "    learn_throughput: 1547.307\n",
      "    learn_time_ms: 646.284\n",
      "    load_throughput: 59199.44\n",
      "    load_time_ms: 16.892\n",
      "    sample_throughput: 41.803\n",
      "    sample_time_ms: 23921.601\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633532063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         2631.82</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            445.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-54-47\n",
      "  done: false\n",
      "  episode_len_mean: 440.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 297\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7711309353510538\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012232273323728924\n",
      "          policy_loss: -0.03044374403026369\n",
      "          total_loss: -0.04243993312120438\n",
      "          vf_explained_var: -0.3969481885433197\n",
      "          vf_loss: 0.0015867270393452296\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.97647058823529\n",
      "    ram_util_percent: 60.4764705882353\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701695639230246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.221080764517302\n",
      "    mean_inference_ms: 1.3114424360136843\n",
      "    mean_raw_obs_processing_ms: 1.2592793260290136\n",
      "  time_since_restore: 2655.7436048984528\n",
      "  time_this_iter_s: 23.9197895526886\n",
      "  time_total_s: 2655.7436048984528\n",
      "  timers:\n",
      "    learn_throughput: 1544.669\n",
      "    learn_time_ms: 647.388\n",
      "    load_throughput: 59771.846\n",
      "    load_time_ms: 16.73\n",
      "    sample_throughput: 44.366\n",
      "    sample_time_ms: 22539.778\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1633532087\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         2655.74</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            440.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-55-27\n",
      "  done: false\n",
      "  episode_len_mean: 436.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 300\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6869488067097134\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011704860670499508\n",
      "          policy_loss: -0.0512522681719727\n",
      "          total_loss: -0.062469559576776294\n",
      "          vf_explained_var: -0.6770594716072083\n",
      "          vf_loss: 0.001701806432619277\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.05862068965517\n",
      "    ram_util_percent: 60.37586206896551\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701712494933977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.238190442569612\n",
      "    mean_inference_ms: 1.3114644048801918\n",
      "    mean_raw_obs_processing_ms: 1.2636660203408225\n",
      "  time_since_restore: 2696.2843186855316\n",
      "  time_this_iter_s: 40.54071378707886\n",
      "  time_total_s: 2696.2843186855316\n",
      "  timers:\n",
      "    learn_throughput: 1545.2\n",
      "    learn_time_ms: 647.165\n",
      "    load_throughput: 59621.373\n",
      "    load_time_ms: 16.773\n",
      "    sample_throughput: 41.518\n",
      "    sample_time_ms: 24085.891\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1633532127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         2696.28</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            436.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 431.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 303\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7389071570502388\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015707038230547997\n",
      "          policy_loss: -0.08193500886360804\n",
      "          total_loss: -0.09070974712570508\n",
      "          vf_explained_var: 0.10086729377508163\n",
      "          vf_loss: 0.0033132124407630826\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.74411764705882\n",
      "    ram_util_percent: 60.19705882352942\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701732337875301\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.256743463805575\n",
      "    mean_inference_ms: 1.3114904945210915\n",
      "    mean_raw_obs_processing_ms: 1.268235560091962\n",
      "  time_since_restore: 2720.1209864616394\n",
      "  time_this_iter_s: 23.836667776107788\n",
      "  time_total_s: 2720.1209864616394\n",
      "  timers:\n",
      "    learn_throughput: 1544.404\n",
      "    learn_time_ms: 647.499\n",
      "    load_throughput: 59700.041\n",
      "    load_time_ms: 16.75\n",
      "    sample_throughput: 41.181\n",
      "    sample_time_ms: 24282.914\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633532151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         2720.12</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            431.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 430.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 305\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.755229942003886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011913681761513119\n",
      "          policy_loss: -0.12109130608538786\n",
      "          total_loss: -0.13328852421707577\n",
      "          vf_explained_var: -0.2881658673286438\n",
      "          vf_loss: 0.0013342144258785993\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.12222222222223\n",
      "    ram_util_percent: 60.22222222222223\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370174879454698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.269136325979808\n",
      "    mean_inference_ms: 1.3115084016525609\n",
      "    mean_raw_obs_processing_ms: 1.2713947435406754\n",
      "  time_since_restore: 2738.800374031067\n",
      "  time_this_iter_s: 18.67938756942749\n",
      "  time_total_s: 2738.800374031067\n",
      "  timers:\n",
      "    learn_throughput: 1549.175\n",
      "    learn_time_ms: 645.505\n",
      "    load_throughput: 58132.306\n",
      "    load_time_ms: 17.202\n",
      "    sample_throughput: 41.558\n",
      "    sample_time_ms: 24062.631\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633532170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">          2738.8</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            430.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-56-32\n",
      "  done: false\n",
      "  episode_len_mean: 429.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 308\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.88028451734119\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010704362296559071\n",
      "          policy_loss: -0.07196408982078234\n",
      "          total_loss: -0.08555707770089309\n",
      "          vf_explained_var: -0.7299872636795044\n",
      "          vf_loss: 0.0015971386016139554\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.354838709677416\n",
      "    ram_util_percent: 60.222580645161294\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701777970403907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.28800644030867\n",
      "    mean_inference_ms: 1.3115358464940234\n",
      "    mean_raw_obs_processing_ms: 1.2761898426922116\n",
      "  time_since_restore: 2761.105271577835\n",
      "  time_this_iter_s: 22.30489754676819\n",
      "  time_total_s: 2761.105271577835\n",
      "  timers:\n",
      "    learn_throughput: 1546.277\n",
      "    learn_time_ms: 646.714\n",
      "    load_throughput: 57680.224\n",
      "    load_time_ms: 17.337\n",
      "    sample_throughput: 41.308\n",
      "    sample_time_ms: 24208.127\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633532192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         2761.11</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            429.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-56-55\n",
      "  done: false\n",
      "  episode_len_mean: 424.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 311\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8851404203308952\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010911105241107676\n",
      "          policy_loss: -0.04493506734466387\n",
      "          total_loss: -0.05818288474757638\n",
      "          vf_explained_var: -0.5636374354362488\n",
      "          vf_loss: 0.0019210875745759243\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.00294117647059\n",
      "    ram_util_percent: 60.420588235294126\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037018097076454204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.308281980677345\n",
      "    mean_inference_ms: 1.3115678573947835\n",
      "    mean_raw_obs_processing_ms: 1.27617976769476\n",
      "  time_since_restore: 2784.294798851013\n",
      "  time_this_iter_s: 23.1895272731781\n",
      "  time_total_s: 2784.294798851013\n",
      "  timers:\n",
      "    learn_throughput: 1550.746\n",
      "    learn_time_ms: 644.851\n",
      "    load_throughput: 57576.973\n",
      "    load_time_ms: 17.368\n",
      "    sample_throughput: 41.306\n",
      "    sample_time_ms: 24209.433\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1633532215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         2784.29</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            424.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 420.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 314\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7133526417944167\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018090838639147063\n",
      "          policy_loss: -0.035643704815043344\n",
      "          total_loss: 0.043590672893656625\n",
      "          vf_explained_var: -0.428718239068985\n",
      "          vf_loss: 0.09026224770479732\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.654838709677435\n",
      "    ram_util_percent: 60.65483870967743\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701847482945106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.32975099889621\n",
      "    mean_inference_ms: 1.3116040976274064\n",
      "    mean_raw_obs_processing_ms: 1.2763934474588199\n",
      "  time_since_restore: 2806.6108000278473\n",
      "  time_this_iter_s: 22.316001176834106\n",
      "  time_total_s: 2806.6108000278473\n",
      "  timers:\n",
      "    learn_throughput: 1547.002\n",
      "    learn_time_ms: 646.412\n",
      "    load_throughput: 58037.87\n",
      "    load_time_ms: 17.23\n",
      "    sample_throughput: 40.881\n",
      "    sample_time_ms: 24461.019\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633532238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         2806.61</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            420.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-57-40\n",
      "  done: false\n",
      "  episode_len_mean: 420.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 316\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6687975181473627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012038084204451453\n",
      "          policy_loss: -0.07941989170180427\n",
      "          total_loss: -0.07818241599533293\n",
      "          vf_explained_var: 0.208331897854805\n",
      "          vf_loss: 0.013862597468929986\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.18787878787878\n",
      "    ram_util_percent: 60.89999999999999\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701878201088785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.344133008487336\n",
      "    mean_inference_ms: 1.311629054118239\n",
      "    mean_raw_obs_processing_ms: 1.2766734046150292\n",
      "  time_since_restore: 2829.2780668735504\n",
      "  time_this_iter_s: 22.667266845703125\n",
      "  time_total_s: 2829.2780668735504\n",
      "  timers:\n",
      "    learn_throughput: 1549.413\n",
      "    learn_time_ms: 645.406\n",
      "    load_throughput: 57506.399\n",
      "    load_time_ms: 17.389\n",
      "    sample_throughput: 41.23\n",
      "    sample_time_ms: 24254.293\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1633532260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         2829.28</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            420.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-58-03\n",
      "  done: false\n",
      "  episode_len_mean: 415.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 319\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0600599911477833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015593187673464135\n",
      "          policy_loss: 0.03413295253283448\n",
      "          total_loss: 0.020973727396792836\n",
      "          vf_explained_var: -0.4830680787563324\n",
      "          vf_loss: 0.0021786720453140637\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.18484848484848\n",
      "    ram_util_percent: 61.224242424242426\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701923371445035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.367424025054355\n",
      "    mean_inference_ms: 1.311671365257825\n",
      "    mean_raw_obs_processing_ms: 1.2772968292444238\n",
      "  time_since_restore: 2852.406602859497\n",
      "  time_this_iter_s: 23.128535985946655\n",
      "  time_total_s: 2852.406602859497\n",
      "  timers:\n",
      "    learn_throughput: 1547.799\n",
      "    learn_time_ms: 646.079\n",
      "    load_throughput: 57214.039\n",
      "    load_time_ms: 17.478\n",
      "    sample_throughput: 41.602\n",
      "    sample_time_ms: 24037.078\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633532283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         2852.41</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            415.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-58-27\n",
      "  done: false\n",
      "  episode_len_mean: 410.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 322\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.93890952401691\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013152144781511924\n",
      "          policy_loss: -0.12639451051751774\n",
      "          total_loss: -0.13818638631039196\n",
      "          vf_explained_var: 0.003930139355361462\n",
      "          vf_loss: 0.0031583713794437547\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.15151515151515\n",
      "    ram_util_percent: 61.39393939393939\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037019696933748125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.392338935940536\n",
      "    mean_inference_ms: 1.3117183177981995\n",
      "    mean_raw_obs_processing_ms: 1.2781170342680925\n",
      "  time_since_restore: 2875.7343969345093\n",
      "  time_this_iter_s: 23.327794075012207\n",
      "  time_total_s: 2875.7343969345093\n",
      "  timers:\n",
      "    learn_throughput: 1544.232\n",
      "    learn_time_ms: 647.571\n",
      "    load_throughput: 57247.775\n",
      "    load_time_ms: 17.468\n",
      "    sample_throughput: 42.156\n",
      "    sample_time_ms: 23721.528\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1633532307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         2875.73</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            410.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-58-50\n",
      "  done: false\n",
      "  episode_len_mean: 404.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 325\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9246088676982456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013562904761444294\n",
      "          policy_loss: -0.020920328319900566\n",
      "          total_loss: -0.032891102424926225\n",
      "          vf_explained_var: -0.37057816982269287\n",
      "          vf_loss: 0.0026978313004494543\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.13636363636364\n",
      "    ram_util_percent: 61.35151515151515\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702017107298869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.418971436656426\n",
      "    mean_inference_ms: 1.3117716821650702\n",
      "    mean_raw_obs_processing_ms: 1.2792304572830806\n",
      "  time_since_restore: 2898.835066795349\n",
      "  time_this_iter_s: 23.100669860839844\n",
      "  time_total_s: 2898.835066795349\n",
      "  timers:\n",
      "    learn_throughput: 1548.851\n",
      "    learn_time_ms: 645.64\n",
      "    load_throughput: 56956.485\n",
      "    load_time_ms: 17.557\n",
      "    sample_throughput: 42.299\n",
      "    sample_time_ms: 23641.453\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1633532330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         2898.84</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            404.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-59-12\n",
      "  done: false\n",
      "  episode_len_mean: 401.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 328\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.036651113298204\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01576610774994478\n",
      "          policy_loss: -0.011168494075536727\n",
      "          total_loss: -0.023722980419794717\n",
      "          vf_explained_var: -0.26734137535095215\n",
      "          vf_loss: 0.0024909606953668925\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.203125\n",
      "    ram_util_percent: 61.27499999999999\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037020722689992985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.446638231190697\n",
      "    mean_inference_ms: 1.3118280724760194\n",
      "    mean_raw_obs_processing_ms: 1.2805237332758614\n",
      "  time_since_restore: 2921.015730857849\n",
      "  time_this_iter_s: 22.1806640625\n",
      "  time_total_s: 2921.015730857849\n",
      "  timers:\n",
      "    learn_throughput: 1549.229\n",
      "    learn_time_ms: 645.483\n",
      "    load_throughput: 57682.128\n",
      "    load_time_ms: 17.336\n",
      "    sample_throughput: 45.859\n",
      "    sample_time_ms: 21805.83\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633532352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         2921.02</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            401.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_14-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 396.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 331\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.06594408220715\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010509627343751483\n",
      "          policy_loss: -0.021367360113395586\n",
      "          total_loss: -0.03631191274358166\n",
      "          vf_explained_var: -0.6210172772407532\n",
      "          vf_loss: 0.0021678876946680248\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.832203389830504\n",
      "    ram_util_percent: 60.925423728813556\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702133784162919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.476192282118648\n",
      "    mean_inference_ms: 1.3118889551685555\n",
      "    mean_raw_obs_processing_ms: 1.2853932504109624\n",
      "  time_since_restore: 2962.5891304016113\n",
      "  time_this_iter_s: 41.57339954376221\n",
      "  time_total_s: 2962.5891304016113\n",
      "  timers:\n",
      "    learn_throughput: 1545.427\n",
      "    learn_time_ms: 647.07\n",
      "    load_throughput: 57657.15\n",
      "    load_time_ms: 17.344\n",
      "    sample_throughput: 42.413\n",
      "    sample_time_ms: 23577.9\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633532394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         2962.59</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            396.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-00-13\n",
      "  done: false\n",
      "  episode_len_mean: 392.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 334\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1786076333787707\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0111237690106679\n",
      "          policy_loss: -0.05789300257133113\n",
      "          total_loss: -0.07400280978116724\n",
      "          vf_explained_var: -0.32139432430267334\n",
      "          vf_loss: 0.0019219993426102316\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.707142857142856\n",
      "    ram_util_percent: 60.78571428571428\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702194390921928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.50652468070309\n",
      "    mean_inference_ms: 1.3119543933438438\n",
      "    mean_raw_obs_processing_ms: 1.2904053377546\n",
      "  time_since_restore: 2982.246925830841\n",
      "  time_this_iter_s: 19.657795429229736\n",
      "  time_total_s: 2982.246925830841\n",
      "  timers:\n",
      "    learn_throughput: 1540.846\n",
      "    learn_time_ms: 648.994\n",
      "    load_throughput: 58900.822\n",
      "    load_time_ms: 16.978\n",
      "    sample_throughput: 42.24\n",
      "    sample_time_ms: 23674.185\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1633532413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         2982.25</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            392.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-00-36\n",
      "  done: false\n",
      "  episode_len_mean: 387.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 337\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.047853276464674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012211253588384076\n",
      "          policy_loss: -0.01611838150355551\n",
      "          total_loss: -0.03105200682249334\n",
      "          vf_explained_var: -0.4872833788394928\n",
      "          vf_loss: 0.00142361085745506\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.118181818181824\n",
      "    ram_util_percent: 60.836363636363636\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702253508335991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.538106596784376\n",
      "    mean_inference_ms: 1.3120204009749385\n",
      "    mean_raw_obs_processing_ms: 1.2956448748163603\n",
      "  time_since_restore: 3004.787757873535\n",
      "  time_this_iter_s: 22.540832042694092\n",
      "  time_total_s: 3004.787757873535\n",
      "  timers:\n",
      "    learn_throughput: 1547.556\n",
      "    learn_time_ms: 646.18\n",
      "    load_throughput: 58491.682\n",
      "    load_time_ms: 17.096\n",
      "    sample_throughput: 42.193\n",
      "    sample_time_ms: 23700.485\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1633532436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         3004.79</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            387.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-00-57\n",
      "  done: false\n",
      "  episode_len_mean: 385.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 339\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2217971404393513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012454569411420695\n",
      "          policy_loss: -0.060727354677187075\n",
      "          total_loss: -0.07733131378061242\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0014105927419020897\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.17\n",
      "    ram_util_percent: 60.86333333333335\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370228684743881\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.55963507269668\n",
      "    mean_inference_ms: 1.3120637167344846\n",
      "    mean_raw_obs_processing_ms: 1.2977299312483228\n",
      "  time_since_restore: 3026.2798273563385\n",
      "  time_this_iter_s: 21.492069482803345\n",
      "  time_total_s: 3026.2798273563385\n",
      "  timers:\n",
      "    learn_throughput: 1544.16\n",
      "    learn_time_ms: 647.601\n",
      "    load_throughput: 58517.143\n",
      "    load_time_ms: 17.089\n",
      "    sample_throughput: 42.5\n",
      "    sample_time_ms: 23529.296\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1633532457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         3026.28</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            385.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-01-18\n",
      "  done: false\n",
      "  episode_len_mean: 383.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 342\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.182830391989814\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014233085766576378\n",
      "          policy_loss: -0.046628349812494384\n",
      "          total_loss: -0.06187774216135343\n",
      "          vf_explained_var: -0.6052535176277161\n",
      "          vf_loss: 0.0017752420849218551\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.150000000000006\n",
      "    ram_util_percent: 60.95\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702334665169141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.592468186755353\n",
      "    mean_inference_ms: 1.312129420945459\n",
      "    mean_raw_obs_processing_ms: 1.2987721424138354\n",
      "  time_since_restore: 3047.0266740322113\n",
      "  time_this_iter_s: 20.746846675872803\n",
      "  time_total_s: 3047.0266740322113\n",
      "  timers:\n",
      "    learn_throughput: 1547.567\n",
      "    learn_time_ms: 646.176\n",
      "    load_throughput: 58001.914\n",
      "    load_time_ms: 17.241\n",
      "    sample_throughput: 42.783\n",
      "    sample_time_ms: 23373.625\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633532478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         3047.03</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            383.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-01-42\n",
      "  done: false\n",
      "  episode_len_mean: 378.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 345\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.162757831149631\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012327829148335804\n",
      "          policy_loss: 0.07727342065837649\n",
      "          total_loss: 0.06124446400337749\n",
      "          vf_explained_var: -0.3168719410896301\n",
      "          vf_loss: 0.0014379772655148473\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.38484848484848\n",
      "    ram_util_percent: 61.227272727272734\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702398410530427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.626199686621113\n",
      "    mean_inference_ms: 1.3121970015931137\n",
      "    mean_raw_obs_processing_ms: 1.299976719904324\n",
      "  time_since_restore: 3070.528357028961\n",
      "  time_this_iter_s: 23.501682996749878\n",
      "  time_total_s: 3070.528357028961\n",
      "  timers:\n",
      "    learn_throughput: 1546.084\n",
      "    learn_time_ms: 646.796\n",
      "    load_throughput: 58372.101\n",
      "    load_time_ms: 17.131\n",
      "    sample_throughput: 42.632\n",
      "    sample_time_ms: 23456.545\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1633532502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         3070.53</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            378.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 376.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 347\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1722494496239557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014121234567506422\n",
      "          policy_loss: -0.10701605168481668\n",
      "          total_loss: -0.12211683872673247\n",
      "          vf_explained_var: -0.040504589676856995\n",
      "          vf_loss: 0.001855789285360111\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.22222222222222\n",
      "    ram_util_percent: 61.425925925925924\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037024431051432674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.649041690496004\n",
      "    mean_inference_ms: 1.312242884007078\n",
      "    mean_raw_obs_processing_ms: 1.3008209288416697\n",
      "  time_since_restore: 3089.4529423713684\n",
      "  time_this_iter_s: 18.924585342407227\n",
      "  time_total_s: 3089.4529423713684\n",
      "  timers:\n",
      "    learn_throughput: 1544.706\n",
      "    learn_time_ms: 647.372\n",
      "    load_throughput: 58832.744\n",
      "    load_time_ms: 16.997\n",
      "    sample_throughput: 43.411\n",
      "    sample_time_ms: 23035.716\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1633532521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         3089.45</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            376.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-02-22\n",
      "  done: false\n",
      "  episode_len_mean: 374.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 350\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1853133890363905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01485090329184852\n",
      "          policy_loss: -0.0527657262980938\n",
      "          total_loss: -0.06721455018139548\n",
      "          vf_explained_var: -0.45185890793800354\n",
      "          vf_loss: 0.0023921277994910875\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.0\n",
      "    ram_util_percent: 61.6\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702515406233755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.683100869875064\n",
      "    mean_inference_ms: 1.312311034794352\n",
      "    mean_raw_obs_processing_ms: 1.3022443701494941\n",
      "  time_since_restore: 3111.206089735031\n",
      "  time_this_iter_s: 21.75314736366272\n",
      "  time_total_s: 3111.206089735031\n",
      "  timers:\n",
      "    learn_throughput: 1544.248\n",
      "    learn_time_ms: 647.564\n",
      "    load_throughput: 58810.141\n",
      "    load_time_ms: 17.004\n",
      "    sample_throughput: 43.71\n",
      "    sample_time_ms: 22878.068\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1633532542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         3111.21</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            374.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-02-43\n",
      "  done: false\n",
      "  episode_len_mean: 375.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 353\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.166139531135559\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011852927165676909\n",
      "          policy_loss: -0.0631925054722362\n",
      "          total_loss: -0.0795747987098164\n",
      "          vf_explained_var: -0.3551309406757355\n",
      "          vf_loss: 0.0012787395150452437\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.4\n",
      "    ram_util_percent: 61.62068965517241\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702587647649805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.716770281657036\n",
      "    mean_inference_ms: 1.312380487848653\n",
      "    mean_raw_obs_processing_ms: 1.3037255847258769\n",
      "  time_since_restore: 3131.679232120514\n",
      "  time_this_iter_s: 20.473142385482788\n",
      "  time_total_s: 3131.679232120514\n",
      "  timers:\n",
      "    learn_throughput: 1540.308\n",
      "    learn_time_ms: 649.221\n",
      "    load_throughput: 58666.353\n",
      "    load_time_ms: 17.046\n",
      "    sample_throughput: 44.221\n",
      "    sample_time_ms: 22613.62\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633532563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         3131.68</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            375.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 373.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 356\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1608250591490004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012949723226899025\n",
      "          policy_loss: -0.014807979224456681\n",
      "          total_loss: -0.031125796462098756\n",
      "          vf_explained_var: -0.5401312112808228\n",
      "          vf_loss: 0.0009199001703463081\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.346875\n",
      "    ram_util_percent: 61.515625\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702660559294052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.750054822905533\n",
      "    mean_inference_ms: 1.3124503996654027\n",
      "    mean_raw_obs_processing_ms: 1.3053538756669423\n",
      "  time_since_restore: 3154.2239241600037\n",
      "  time_this_iter_s: 22.544692039489746\n",
      "  time_total_s: 3154.2239241600037\n",
      "  timers:\n",
      "    learn_throughput: 1538.799\n",
      "    learn_time_ms: 649.858\n",
      "    load_throughput: 58560.772\n",
      "    load_time_ms: 17.076\n",
      "    sample_throughput: 44.151\n",
      "    sample_time_ms: 22649.346\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633532586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         3154.22</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            373.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 374.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 358\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1298155784606934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008389102401163168\n",
      "          policy_loss: 0.011129713762137625\n",
      "          total_loss: -0.006227450031373236\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0011096705119901648\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.540740740740745\n",
      "    ram_util_percent: 61.55555555555556\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370270793305402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.771732287234103\n",
      "    mean_inference_ms: 1.312497284607536\n",
      "    mean_raw_obs_processing_ms: 1.3064405985073035\n",
      "  time_since_restore: 3173.491726875305\n",
      "  time_this_iter_s: 19.267802715301514\n",
      "  time_total_s: 3173.491726875305\n",
      "  timers:\n",
      "    learn_throughput: 1544.586\n",
      "    learn_time_ms: 647.423\n",
      "    load_throughput: 58726.317\n",
      "    load_time_ms: 17.028\n",
      "    sample_throughput: 48.969\n",
      "    sample_time_ms: 20421.277\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1633532605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         3173.49</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            374.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-04-01\n",
      "  done: false\n",
      "  episode_len_mean: 376.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 360\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.719845023420122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013772020644890127\n",
      "          policy_loss: -0.10151028869052728\n",
      "          total_loss: -0.1117662955282463\n",
      "          vf_explained_var: 0.1113848015666008\n",
      "          vf_loss: 0.0022943877589164507\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.735849056603783\n",
      "    ram_util_percent: 61.215094339622645\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702751615666023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.793194682972997\n",
      "    mean_inference_ms: 1.312543424892983\n",
      "    mean_raw_obs_processing_ms: 1.3095343000187847\n",
      "  time_since_restore: 3210.0845499038696\n",
      "  time_this_iter_s: 36.59282302856445\n",
      "  time_total_s: 3210.0845499038696\n",
      "  timers:\n",
      "    learn_throughput: 1545.616\n",
      "    learn_time_ms: 646.991\n",
      "    load_throughput: 58207.333\n",
      "    load_time_ms: 17.18\n",
      "    sample_throughput: 45.218\n",
      "    sample_time_ms: 22115.051\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1633532641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         3210.08</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            376.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 375.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 363\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1295008182525637\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013619678548268866\n",
      "          policy_loss: -0.05180545722444852\n",
      "          total_loss: -0.06719897670878304\n",
      "          vf_explained_var: -0.4841125011444092\n",
      "          vf_loss: 0.0013048472600833824\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.95\n",
      "    ram_util_percent: 60.4875\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702814204318259\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.825243663194419\n",
      "    mean_inference_ms: 1.3126121601265248\n",
      "    mean_raw_obs_processing_ms: 1.3142973717301896\n",
      "  time_since_restore: 3232.9580097198486\n",
      "  time_this_iter_s: 22.873459815979004\n",
      "  time_total_s: 3232.9580097198486\n",
      "  timers:\n",
      "    learn_throughput: 1544.78\n",
      "    learn_time_ms: 647.341\n",
      "    load_throughput: 59122.752\n",
      "    load_time_ms: 16.914\n",
      "    sample_throughput: 45.15\n",
      "    sample_time_ms: 22148.218\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1633532664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         3232.96</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            375.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-04-48\n",
      "  done: false\n",
      "  episode_len_mean: 372.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 366\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1420669343736436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012879907194232557\n",
      "          policy_loss: -0.09139517032437855\n",
      "          total_loss: -0.10718897899819745\n",
      "          vf_explained_var: -0.7593534588813782\n",
      "          vf_loss: 0.001279891839173312\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.1764705882353\n",
      "    ram_util_percent: 60.57647058823529\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037028549236487816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.857877462465565\n",
      "    mean_inference_ms: 1.3126764440133487\n",
      "    mean_raw_obs_processing_ms: 1.3190935536387263\n",
      "  time_since_restore: 3256.29039978981\n",
      "  time_this_iter_s: 23.332390069961548\n",
      "  time_total_s: 3256.29039978981\n",
      "  timers:\n",
      "    learn_throughput: 1542.278\n",
      "    learn_time_ms: 648.392\n",
      "    load_throughput: 58898.506\n",
      "    load_time_ms: 16.978\n",
      "    sample_throughput: 44.78\n",
      "    sample_time_ms: 22331.157\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1633532688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         3256.29</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             372.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-05-09\n",
      "  done: false\n",
      "  episode_len_mean: 370.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 369\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0327259845203822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011068144495516978\n",
      "          policy_loss: -0.03367552167425553\n",
      "          total_loss: -0.04865365983504388\n",
      "          vf_explained_var: -0.8252345323562622\n",
      "          vf_loss: 0.001613623609430053\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.46666666666667\n",
      "    ram_util_percent: 60.64333333333334\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702879976416224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.89026983894046\n",
      "    mean_inference_ms: 1.312734019824662\n",
      "    mean_raw_obs_processing_ms: 1.3240888134317255\n",
      "  time_since_restore: 3277.6103177070618\n",
      "  time_this_iter_s: 21.319917917251587\n",
      "  time_total_s: 3277.6103177070618\n",
      "  timers:\n",
      "    learn_throughput: 1542.288\n",
      "    learn_time_ms: 648.387\n",
      "    load_throughput: 58681.127\n",
      "    load_time_ms: 17.041\n",
      "    sample_throughput: 44.666\n",
      "    sample_time_ms: 22388.429\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1633532709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         3277.61</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            370.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-05-31\n",
      "  done: false\n",
      "  episode_len_mean: 369.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 372\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.817118227481842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012185015072334653\n",
      "          policy_loss: 0.044052204800148806\n",
      "          total_loss: 0.031174012252853975\n",
      "          vf_explained_var: -0.13943174481391907\n",
      "          vf_loss: 0.0011805459446299614\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.215625\n",
      "    ram_util_percent: 60.85625\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702895672058251\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.922360357382834\n",
      "    mean_inference_ms: 1.3127890048943949\n",
      "    mean_raw_obs_processing_ms: 1.325403118860233\n",
      "  time_since_restore: 3299.8645026683807\n",
      "  time_this_iter_s: 22.25418496131897\n",
      "  time_total_s: 3299.8645026683807\n",
      "  timers:\n",
      "    learn_throughput: 1544.912\n",
      "    learn_time_ms: 647.286\n",
      "    load_throughput: 58765.07\n",
      "    load_time_ms: 17.017\n",
      "    sample_throughput: 44.914\n",
      "    sample_time_ms: 22264.818\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633532731\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         3299.86</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            369.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-05-55\n",
      "  done: false\n",
      "  episode_len_mean: 368.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 375\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9600609527693855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01242908364764435\n",
      "          policy_loss: -0.12052845545113086\n",
      "          total_loss: -0.13490830798529915\n",
      "          vf_explained_var: -0.5337414741516113\n",
      "          vf_loss: 0.0010259395394112086\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.19411764705882\n",
      "    ram_util_percent: 61.09117647058823\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702907331300051\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.953336954685504\n",
      "    mean_inference_ms: 1.3128420519428827\n",
      "    mean_raw_obs_processing_ms: 1.3268597618526028\n",
      "  time_since_restore: 3323.770807504654\n",
      "  time_this_iter_s: 23.906304836273193\n",
      "  time_total_s: 3323.770807504654\n",
      "  timers:\n",
      "    learn_throughput: 1545.338\n",
      "    learn_time_ms: 647.108\n",
      "    load_throughput: 58645.846\n",
      "    load_time_ms: 17.052\n",
      "    sample_throughput: 43.931\n",
      "    sample_time_ms: 22763.138\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1633532755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         3323.77</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            368.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-06-18\n",
      "  done: false\n",
      "  episode_len_mean: 366.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 378\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9266511029667324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009308524213167413\n",
      "          policy_loss: 0.004791793951557742\n",
      "          total_loss: -0.010605439460939831\n",
      "          vf_explained_var: -0.9335291385650635\n",
      "          vf_loss: 0.0007276497739237837\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.02727272727273\n",
      "    ram_util_percent: 61.27575757575758\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702918113855482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.984175325705099\n",
      "    mean_inference_ms: 1.3128941232383726\n",
      "    mean_raw_obs_processing_ms: 1.3283643442054365\n",
      "  time_since_restore: 3346.677291870117\n",
      "  time_this_iter_s: 22.906484365463257\n",
      "  time_total_s: 3346.677291870117\n",
      "  timers:\n",
      "    learn_throughput: 1544.205\n",
      "    learn_time_ms: 647.582\n",
      "    load_throughput: 59015.689\n",
      "    load_time_ms: 16.945\n",
      "    sample_throughput: 43.71\n",
      "    sample_time_ms: 22878.117\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1633532778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         3346.68</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            366.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-06-40\n",
      "  done: false\n",
      "  episode_len_mean: 365.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 380\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0186809685495164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015353557891928047\n",
      "          policy_loss: -0.05748223505086369\n",
      "          total_loss: -0.07118922571341196\n",
      "          vf_explained_var: -0.7982043027877808\n",
      "          vf_loss: 0.001297993248509657\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.06129032258064\n",
      "    ram_util_percent: 61.409677419354864\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702918372382942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.00454977500366\n",
      "    mean_inference_ms: 1.3129283955947317\n",
      "    mean_raw_obs_processing_ms: 1.329426840515913\n",
      "  time_since_restore: 3368.5823228359222\n",
      "  time_this_iter_s: 21.905030965805054\n",
      "  time_total_s: 3368.5823228359222\n",
      "  timers:\n",
      "    learn_throughput: 1548.748\n",
      "    learn_time_ms: 645.683\n",
      "    load_throughput: 59076.452\n",
      "    load_time_ms: 16.927\n",
      "    sample_throughput: 43.434\n",
      "    sample_time_ms: 23023.212\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633532800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         3368.58</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            365.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-07-04\n",
      "  done: false\n",
      "  episode_len_mean: 362.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 383\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.008729174402025\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008757525852345991\n",
      "          policy_loss: -0.08760210817886724\n",
      "          total_loss: -0.10412505873375469\n",
      "          vf_explained_var: -0.43498852849006653\n",
      "          vf_loss: 0.0006086749765219995\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.038235294117655\n",
      "    ram_util_percent: 61.4\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370291294426385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.034796483050062\n",
      "    mean_inference_ms: 1.3129791946647709\n",
      "    mean_raw_obs_processing_ms: 1.3311074570485157\n",
      "  time_since_restore: 3392.1729214191437\n",
      "  time_this_iter_s: 23.590598583221436\n",
      "  time_total_s: 3392.1729214191437\n",
      "  timers:\n",
      "    learn_throughput: 1549.733\n",
      "    learn_time_ms: 645.273\n",
      "    load_throughput: 59155.689\n",
      "    load_time_ms: 16.905\n",
      "    sample_throughput: 43.237\n",
      "    sample_time_ms: 23128.264\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1633532824\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         3392.17</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            362.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-07-27\n",
      "  done: false\n",
      "  episode_len_mean: 360.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 386\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8677106685108609\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017720651084615034\n",
      "          policy_loss: -0.029729729725254908\n",
      "          total_loss: -0.04048621513777309\n",
      "          vf_explained_var: -0.2872082591056824\n",
      "          vf_loss: 0.0019399014631441484\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.169696969696965\n",
      "    ram_util_percent: 61.333333333333336\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037029060063983696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.065124338471563\n",
      "    mean_inference_ms: 1.3130298116040233\n",
      "    mean_raw_obs_processing_ms: 1.3328338326376024\n",
      "  time_since_restore: 3415.465658903122\n",
      "  time_this_iter_s: 23.29273748397827\n",
      "  time_total_s: 3415.465658903122\n",
      "  timers:\n",
      "    learn_throughput: 1549.219\n",
      "    learn_time_ms: 645.487\n",
      "    load_throughput: 58443.677\n",
      "    load_time_ms: 17.11\n",
      "    sample_throughput: 42.498\n",
      "    sample_time_ms: 23530.346\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1633532847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         3415.47</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            360.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-07-49\n",
      "  done: false\n",
      "  episode_len_mean: 359.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 389\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8596998122003343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013038551383537664\n",
      "          policy_loss: -0.01023402073317104\n",
      "          total_loss: -0.023478357411093182\n",
      "          vf_explained_var: -0.5584292411804199\n",
      "          vf_loss: 0.0009521489995273037\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.277419354838706\n",
      "    ram_util_percent: 61.31612903225806\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702887689388627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.09425794132955\n",
      "    mean_inference_ms: 1.3130777303980943\n",
      "    mean_raw_obs_processing_ms: 1.3346811910071217\n",
      "  time_since_restore: 3437.053983449936\n",
      "  time_this_iter_s: 21.588324546813965\n",
      "  time_total_s: 3437.053983449936\n",
      "  timers:\n",
      "    learn_throughput: 1550.043\n",
      "    learn_time_ms: 645.144\n",
      "    load_throughput: 58980.419\n",
      "    load_time_ms: 16.955\n",
      "    sample_throughput: 45.392\n",
      "    sample_time_ms: 22030.369\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633532869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         3437.05</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            359.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-08-30\n",
      "  done: false\n",
      "  episode_len_mean: 358.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 392\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7988107535574172\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015104037971724004\n",
      "          policy_loss: -0.09996020570397376\n",
      "          total_loss: -0.11151375489102469\n",
      "          vf_explained_var: -0.529030442237854\n",
      "          vf_loss: 0.0013369450345635415\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.07118644067797\n",
      "    ram_util_percent: 60.81525423728812\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037028806081732146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.12283057269167\n",
      "    mean_inference_ms: 1.3131241343400188\n",
      "    mean_raw_obs_processing_ms: 1.3394444725437291\n",
      "  time_since_restore: 3478.7241294384003\n",
      "  time_this_iter_s: 41.670145988464355\n",
      "  time_total_s: 3478.7241294384003\n",
      "  timers:\n",
      "    learn_throughput: 1548.632\n",
      "    learn_time_ms: 645.731\n",
      "    load_throughput: 58957.702\n",
      "    load_time_ms: 16.961\n",
      "    sample_throughput: 41.824\n",
      "    sample_time_ms: 23909.439\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1633532910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         3478.72</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            358.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-08-53\n",
      "  done: false\n",
      "  episode_len_mean: 358.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 395\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8955735312567816\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020797005392699162\n",
      "          policy_loss: -0.058331556887262395\n",
      "          total_loss: -0.06844342346820566\n",
      "          vf_explained_var: -0.42725682258605957\n",
      "          vf_loss: 0.0018248793636707382\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.22727272727273\n",
      "    ram_util_percent: 60.584848484848486\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702874027380462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.14979732717484\n",
      "    mean_inference_ms: 1.3131663243240175\n",
      "    mean_raw_obs_processing_ms: 1.3443097445577865\n",
      "  time_since_restore: 3501.557758808136\n",
      "  time_this_iter_s: 22.833629369735718\n",
      "  time_total_s: 3501.557758808136\n",
      "  timers:\n",
      "    learn_throughput: 1553.602\n",
      "    learn_time_ms: 643.665\n",
      "    load_throughput: 59098.26\n",
      "    load_time_ms: 16.921\n",
      "    sample_throughput: 41.908\n",
      "    sample_time_ms: 23861.672\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633532933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         3501.56</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            358.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-09-17\n",
      "  done: false\n",
      "  episode_len_mean: 357.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 398\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9827707886695862\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010315581545711754\n",
      "          policy_loss: -0.04425957024925285\n",
      "          total_loss: -0.058091499283909796\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007735147438425985\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.13823529411764\n",
      "    ram_util_percent: 60.58235294117646\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702867773740313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.176302210435132\n",
      "    mean_inference_ms: 1.3132073565104128\n",
      "    mean_raw_obs_processing_ms: 1.3479990660996486\n",
      "  time_since_restore: 3525.1127128601074\n",
      "  time_this_iter_s: 23.554954051971436\n",
      "  time_total_s: 3525.1127128601074\n",
      "  timers:\n",
      "    learn_throughput: 1548.291\n",
      "    learn_time_ms: 645.874\n",
      "    load_throughput: 58825.896\n",
      "    load_time_ms: 16.999\n",
      "    sample_throughput: 41.523\n",
      "    sample_time_ms: 24082.89\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633532957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         3525.11</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             357.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-09-41\n",
      "  done: false\n",
      "  episode_len_mean: 357.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 401\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9438893755276998\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01283284994277425\n",
      "          policy_loss: -0.03298403273026149\n",
      "          total_loss: -0.0449607118136353\n",
      "          vf_explained_var: -0.6310948133468628\n",
      "          vf_loss: 0.0009655820335158044\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.779411764705884\n",
      "    ram_util_percent: 61.220588235294116\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370289152253057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.202409351022073\n",
      "    mean_inference_ms: 1.3132561149202797\n",
      "    mean_raw_obs_processing_ms: 1.349330267811158\n",
      "  time_since_restore: 3548.973067998886\n",
      "  time_this_iter_s: 23.860355138778687\n",
      "  time_total_s: 3548.973067998886\n",
      "  timers:\n",
      "    learn_throughput: 1538.203\n",
      "    learn_time_ms: 650.109\n",
      "    load_throughput: 58887.922\n",
      "    load_time_ms: 16.981\n",
      "    sample_throughput: 41.255\n",
      "    sample_time_ms: 24239.24\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1633532981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         3548.97</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            357.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-10-05\n",
      "  done: false\n",
      "  episode_len_mean: 357.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 404\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8571387900246514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00783505822157085\n",
      "          policy_loss: -0.00899026518066724\n",
      "          total_loss: -0.022456053189105457\n",
      "          vf_explained_var: 0.06570732593536377\n",
      "          vf_loss: 0.0011390999622461903\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.894285714285715\n",
      "    ram_util_percent: 61.451428571428565\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702935453295787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.228581131649975\n",
      "    mean_inference_ms: 1.3133127419987074\n",
      "    mean_raw_obs_processing_ms: 1.350711779712859\n",
      "  time_since_restore: 3573.5383546352386\n",
      "  time_this_iter_s: 24.56528663635254\n",
      "  time_total_s: 3573.5383546352386\n",
      "  timers:\n",
      "    learn_throughput: 1529.141\n",
      "    learn_time_ms: 653.962\n",
      "    load_throughput: 60729.541\n",
      "    load_time_ms: 16.466\n",
      "    sample_throughput: 41.149\n",
      "    sample_time_ms: 24301.783\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1633533005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         3573.54</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            357.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-10-29\n",
      "  done: false\n",
      "  episode_len_mean: 355.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 406\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9234975179036458\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009836641185118003\n",
      "          policy_loss: -0.07394873723387718\n",
      "          total_loss: -0.0866894110209412\n",
      "          vf_explained_var: -0.7271720170974731\n",
      "          vf_loss: 0.001514499693358731\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.303030303030305\n",
      "    ram_util_percent: 61.42424242424242\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037029760017249995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.24618210765203\n",
      "    mean_inference_ms: 1.3133524701825192\n",
      "    mean_raw_obs_processing_ms: 1.3516870876424776\n",
      "  time_since_restore: 3597.0424790382385\n",
      "  time_this_iter_s: 23.504124402999878\n",
      "  time_total_s: 3597.0424790382385\n",
      "  timers:\n",
      "    learn_throughput: 1532.356\n",
      "    learn_time_ms: 652.59\n",
      "    load_throughput: 60245.071\n",
      "    load_time_ms: 16.599\n",
      "    sample_throughput: 41.046\n",
      "    sample_time_ms: 24362.758\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1633533029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         3597.04</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            355.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 356.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 409\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.668787243631151\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013056370521414746\n",
      "          policy_loss: -0.055780418962240216\n",
      "          total_loss: -0.06442020196053717\n",
      "          vf_explained_var: -0.21967129409313202\n",
      "          vf_loss: 0.0014383029609840983\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.83225806451613\n",
      "    ram_util_percent: 61.516129032258064\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037030503733676466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.272192567456216\n",
      "    mean_inference_ms: 1.313413110325542\n",
      "    mean_raw_obs_processing_ms: 1.353154286657828\n",
      "  time_since_restore: 3618.444167137146\n",
      "  time_this_iter_s: 21.40168809890747\n",
      "  time_total_s: 3618.444167137146\n",
      "  timers:\n",
      "    learn_throughput: 1530.568\n",
      "    learn_time_ms: 653.352\n",
      "    load_throughput: 59857.232\n",
      "    load_time_ms: 16.706\n",
      "    sample_throughput: 41.133\n",
      "    sample_time_ms: 24311.58\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1633533050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         3618.44</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            356.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-11-10\n",
      "  done: false\n",
      "  episode_len_mean: 357.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 412\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6958423150910271\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008862532313620126\n",
      "          policy_loss: -0.05279140066769388\n",
      "          total_loss: -0.06380320315559705\n",
      "          vf_explained_var: -0.5682177543640137\n",
      "          vf_loss: 0.0014599652566377901\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.660714285714285\n",
      "    ram_util_percent: 61.578571428571415\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703130315870312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.29736196781446\n",
      "    mean_inference_ms: 1.3134743481089066\n",
      "    mean_raw_obs_processing_ms: 1.3546595273627082\n",
      "  time_since_restore: 3638.335641860962\n",
      "  time_this_iter_s: 19.891474723815918\n",
      "  time_total_s: 3638.335641860962\n",
      "  timers:\n",
      "    learn_throughput: 1530.077\n",
      "    learn_time_ms: 653.562\n",
      "    load_throughput: 59855.097\n",
      "    load_time_ms: 16.707\n",
      "    sample_throughput: 41.769\n",
      "    sample_time_ms: 23941.435\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1633533070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         3638.34</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            357.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-11-29\n",
      "  done: false\n",
      "  episode_len_mean: 359.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 414\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8522044155332777\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00955412172872428\n",
      "          policy_loss: -0.03760365636812316\n",
      "          total_loss: -0.049153011271523106\n",
      "          vf_explained_var: -0.42821282148361206\n",
      "          vf_loss: 0.0021359154249593203\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.94642857142858\n",
      "    ram_util_percent: 61.53928571428571\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703188437012227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.313808160528133\n",
      "    mean_inference_ms: 1.3135153335177339\n",
      "    mean_raw_obs_processing_ms: 1.3556317786270637\n",
      "  time_since_restore: 3657.706055164337\n",
      "  time_this_iter_s: 19.370413303375244\n",
      "  time_total_s: 3657.706055164337\n",
      "  timers:\n",
      "    learn_throughput: 1527.622\n",
      "    learn_time_ms: 654.612\n",
      "    load_throughput: 60517.405\n",
      "    load_time_ms: 16.524\n",
      "    sample_throughput: 42.466\n",
      "    sample_time_ms: 23548.339\n",
      "    update_time_ms: 1.712\n",
      "  timestamp: 1633533089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         3657.71</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            359.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-11-51\n",
      "  done: false\n",
      "  episode_len_mean: 359.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 417\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9508200301064385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01230279447511461\n",
      "          policy_loss: -0.04571242295205593\n",
      "          total_loss: -0.05716635990473959\n",
      "          vf_explained_var: -0.11575336754322052\n",
      "          vf_loss: 0.0018259704720953274\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.92903225806452\n",
      "    ram_util_percent: 61.519354838709674\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703277547379361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.337613114456826\n",
      "    mean_inference_ms: 1.3135777669730198\n",
      "    mean_raw_obs_processing_ms: 1.3572117541950621\n",
      "  time_since_restore: 3679.165783405304\n",
      "  time_this_iter_s: 21.459728240966797\n",
      "  time_total_s: 3679.165783405304\n",
      "  timers:\n",
      "    learn_throughput: 1528.305\n",
      "    learn_time_ms: 654.32\n",
      "    load_throughput: 60345.183\n",
      "    load_time_ms: 16.571\n",
      "    sample_throughput: 42.489\n",
      "    sample_time_ms: 23535.763\n",
      "    update_time_ms: 1.691\n",
      "  timestamp: 1633533111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         3679.17</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            359.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-12-09\n",
      "  done: false\n",
      "  episode_len_mean: 361.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 419\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8354124400350782\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01212716038027766\n",
      "          policy_loss: -0.04402380400440759\n",
      "          total_loss: -0.05444001755159762\n",
      "          vf_explained_var: -0.2498028427362442\n",
      "          vf_loss: 0.001798534448284449\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.28461538461538\n",
      "    ram_util_percent: 61.44230769230771\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037033446493576046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.352995869605287\n",
      "    mean_inference_ms: 1.3136194105170174\n",
      "    mean_raw_obs_processing_ms: 1.3582352079546305\n",
      "  time_since_restore: 3697.738172531128\n",
      "  time_this_iter_s: 18.572389125823975\n",
      "  time_total_s: 3697.738172531128\n",
      "  timers:\n",
      "    learn_throughput: 1525.919\n",
      "    learn_time_ms: 655.343\n",
      "    load_throughput: 60393.233\n",
      "    load_time_ms: 16.558\n",
      "    sample_throughput: 47.114\n",
      "    sample_time_ms: 21224.975\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1633533129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         3697.74</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            361.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-12-46\n",
      "  done: false\n",
      "  episode_len_mean: 363.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 421\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.989713924460941\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01031682144116175\n",
      "          policy_loss: -0.09854390803310606\n",
      "          total_loss: -0.11197478050986925\n",
      "          vf_explained_var: -0.3486597239971161\n",
      "          vf_loss: 0.001243374336303936\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.68679245283018\n",
      "    ram_util_percent: 61.01509433962264\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703407285078084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.367829811655973\n",
      "    mean_inference_ms: 1.3136611686376762\n",
      "    mean_raw_obs_processing_ms: 1.361081901947939\n",
      "  time_since_restore: 3734.653867959976\n",
      "  time_this_iter_s: 36.91569542884827\n",
      "  time_total_s: 3734.653867959976\n",
      "  timers:\n",
      "    learn_throughput: 1525.299\n",
      "    learn_time_ms: 655.609\n",
      "    load_throughput: 60343.967\n",
      "    load_time_ms: 16.572\n",
      "    sample_throughput: 44.183\n",
      "    sample_time_ms: 22632.89\n",
      "    update_time_ms: 1.696\n",
      "  timestamp: 1633533166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         3734.65</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            363.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-13-07\n",
      "  done: false\n",
      "  episode_len_mean: 365.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 424\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6039399438434176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0027077998955776414\n",
      "          policy_loss: -0.14131515125433605\n",
      "          total_loss: -0.15402036060889562\n",
      "          vf_explained_var: 0.24631722271442413\n",
      "          vf_loss: 0.0019633650178244957\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.544827586206885\n",
      "    ram_util_percent: 61.15862068965519\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037035133147709336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.389671889378047\n",
      "    mean_inference_ms: 1.3137261650429608\n",
      "    mean_raw_obs_processing_ms: 1.3653007975416784\n",
      "  time_since_restore: 3755.145660161972\n",
      "  time_this_iter_s: 20.49179220199585\n",
      "  time_total_s: 3755.145660161972\n",
      "  timers:\n",
      "    learn_throughput: 1531.177\n",
      "    learn_time_ms: 653.092\n",
      "    load_throughput: 60519.588\n",
      "    load_time_ms: 16.524\n",
      "    sample_throughput: 44.785\n",
      "    sample_time_ms: 22329.118\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1633533187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         3755.15</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            365.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-13-26\n",
      "  done: false\n",
      "  episode_len_mean: 368.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 426\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9392181396484376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016829887127788642\n",
      "          policy_loss: -0.037724780498279466\n",
      "          total_loss: -0.05064839344057772\n",
      "          vf_explained_var: -0.47739362716674805\n",
      "          vf_loss: 0.002208499684800497\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.08518518518518\n",
      "    ram_util_percent: 61.15555555555555\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703588490385476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.403753277211198\n",
      "    mean_inference_ms: 1.3137703848056308\n",
      "    mean_raw_obs_processing_ms: 1.368099255013722\n",
      "  time_since_restore: 3774.103036880493\n",
      "  time_this_iter_s: 18.957376718521118\n",
      "  time_total_s: 3774.103036880493\n",
      "  timers:\n",
      "    learn_throughput: 1532.364\n",
      "    learn_time_ms: 652.586\n",
      "    load_throughput: 60809.576\n",
      "    load_time_ms: 16.445\n",
      "    sample_throughput: 45.789\n",
      "    sample_time_ms: 21839.44\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1633533206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">          3774.1</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            368.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-13-45\n",
      "  done: false\n",
      "  episode_len_mean: 369.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 428\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7638010263442994\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017021256002899145\n",
      "          policy_loss: -0.027939970253242387\n",
      "          total_loss: -0.0333556368533108\n",
      "          vf_explained_var: 0.3493032157421112\n",
      "          vf_loss: 0.007913839040944974\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.97777777777778\n",
      "    ram_util_percent: 61.14074074074074\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037036636465318996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.41753811648149\n",
      "    mean_inference_ms: 1.313815135265343\n",
      "    mean_raw_obs_processing_ms: 1.370835652090651\n",
      "  time_since_restore: 3792.7125465869904\n",
      "  time_this_iter_s: 18.609509706497192\n",
      "  time_total_s: 3792.7125465869904\n",
      "  timers:\n",
      "    learn_throughput: 1538.632\n",
      "    learn_time_ms: 649.928\n",
      "    load_throughput: 58807.173\n",
      "    load_time_ms: 17.005\n",
      "    sample_throughput: 47.068\n",
      "    sample_time_ms: 21245.973\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1633533225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         3792.71</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            369.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-14-02\n",
      "  done: false\n",
      "  episode_len_mean: 372.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 430\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7845696104897393\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012223102028673752\n",
      "          policy_loss: -0.11415083055487937\n",
      "          total_loss: -0.12708195220265123\n",
      "          vf_explained_var: -0.35912686586380005\n",
      "          vf_loss: 0.0018206004842391444\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.224\n",
      "    ram_util_percent: 61.168\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703741696785033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.4303396046594\n",
      "    mean_inference_ms: 1.3138597734970006\n",
      "    mean_raw_obs_processing_ms: 1.371379825890558\n",
      "  time_since_restore: 3810.4759435653687\n",
      "  time_this_iter_s: 17.763396978378296\n",
      "  time_total_s: 3810.4759435653687\n",
      "  timers:\n",
      "    learn_throughput: 1540.116\n",
      "    learn_time_ms: 649.302\n",
      "    load_throughput: 62882.93\n",
      "    load_time_ms: 15.903\n",
      "    sample_throughput: 48.371\n",
      "    sample_time_ms: 20673.652\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1633533242\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         3810.48</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            372.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-14-21\n",
      "  done: false\n",
      "  episode_len_mean: 375.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 432\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0130669289165075\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011900796560085346\n",
      "          policy_loss: 0.006978114280435774\n",
      "          total_loss: 0.0315433735648791\n",
      "          vf_explained_var: -0.42403602600097656\n",
      "          vf_loss: 0.04168354282155633\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.47407407407406\n",
      "    ram_util_percent: 61.285185185185185\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037038199936210076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.443009805171606\n",
      "    mean_inference_ms: 1.3139045885922982\n",
      "    mean_raw_obs_processing_ms: 1.371871453158687\n",
      "  time_since_restore: 3828.9993488788605\n",
      "  time_this_iter_s: 18.52340531349182\n",
      "  time_total_s: 3828.9993488788605\n",
      "  timers:\n",
      "    learn_throughput: 1537.309\n",
      "    learn_time_ms: 650.487\n",
      "    load_throughput: 63180.836\n",
      "    load_time_ms: 15.828\n",
      "    sample_throughput: 49.056\n",
      "    sample_time_ms: 20384.7\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1633533261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">            3829</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            375.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-14-39\n",
      "  done: false\n",
      "  episode_len_mean: 376.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 434\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.686013850900862\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014417790998703791\n",
      "          policy_loss: -0.03516579104794396\n",
      "          total_loss: -0.04188737140761482\n",
      "          vf_explained_var: 0.5206788182258606\n",
      "          vf_loss: 0.006489053939003497\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.28799999999999\n",
      "    ram_util_percent: 61.52\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703897830615274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.455490981916693\n",
      "    mean_inference_ms: 1.3139498879830136\n",
      "    mean_raw_obs_processing_ms: 1.3723119074640875\n",
      "  time_since_restore: 3846.9742789268494\n",
      "  time_this_iter_s: 17.97493004798889\n",
      "  time_total_s: 3846.9742789268494\n",
      "  timers:\n",
      "    learn_throughput: 1534.568\n",
      "    learn_time_ms: 651.649\n",
      "    load_throughput: 63314.072\n",
      "    load_time_ms: 15.794\n",
      "    sample_throughput: 49.525\n",
      "    sample_time_ms: 20191.924\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1633533279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         3846.97</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            376.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-14-55\n",
      "  done: false\n",
      "  episode_len_mean: 380.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 436\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.552247620953454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014357457594032917\n",
      "          policy_loss: 0.05601380674375428\n",
      "          total_loss: 0.04914367637700505\n",
      "          vf_explained_var: 0.34849119186401367\n",
      "          vf_loss: 0.005018113994608737\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.70434782608695\n",
      "    ram_util_percent: 61.6086956521739\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037039789157003254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.467083410155816\n",
      "    mean_inference_ms: 1.3139948230508802\n",
      "    mean_raw_obs_processing_ms: 1.3727794620802423\n",
      "  time_since_restore: 3862.65381526947\n",
      "  time_this_iter_s: 15.67953634262085\n",
      "  time_total_s: 3862.65381526947\n",
      "  timers:\n",
      "    learn_throughput: 1532.568\n",
      "    learn_time_ms: 652.499\n",
      "    load_throughput: 69257.623\n",
      "    load_time_ms: 14.439\n",
      "    sample_throughput: 50.446\n",
      "    sample_time_ms: 19823.327\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1633533295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         3862.65</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            380.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-15-13\n",
      "  done: false\n",
      "  episode_len_mean: 382.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 438\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9525668091244168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01694052910390786\n",
      "          policy_loss: -0.040208646613690585\n",
      "          total_loss: -0.05255649644467566\n",
      "          vf_explained_var: -0.5546086430549622\n",
      "          vf_loss: 0.002889747519253029\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.83703703703704\n",
      "    ram_util_percent: 61.73333333333334\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704065037277173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.4784546756109\n",
      "    mean_inference_ms: 1.3140413958907196\n",
      "    mean_raw_obs_processing_ms: 1.373197437280066\n",
      "  time_since_restore: 3881.393373966217\n",
      "  time_this_iter_s: 18.739558696746826\n",
      "  time_total_s: 3881.393373966217\n",
      "  timers:\n",
      "    learn_throughput: 1533.242\n",
      "    learn_time_ms: 652.213\n",
      "    load_throughput: 72115.9\n",
      "    load_time_ms: 13.867\n",
      "    sample_throughput: 51.145\n",
      "    sample_time_ms: 19552.138\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1633533313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         3881.39</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            382.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-15-33\n",
      "  done: false\n",
      "  episode_len_mean: 384.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 441\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8615676919619242\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015569476489244179\n",
      "          policy_loss: -0.11874061189591885\n",
      "          total_loss: -0.13166276684237851\n",
      "          vf_explained_var: 0.3092082738876343\n",
      "          vf_loss: 0.0017524983045101787\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.3888888888889\n",
      "    ram_util_percent: 61.77407407407406\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704214643195669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.494982200150293\n",
      "    mean_inference_ms: 1.314114795453106\n",
      "    mean_raw_obs_processing_ms: 1.3739096838871314\n",
      "  time_since_restore: 3900.9193172454834\n",
      "  time_this_iter_s: 19.525943279266357\n",
      "  time_total_s: 3900.9193172454834\n",
      "  timers:\n",
      "    learn_throughput: 1531.832\n",
      "    learn_time_ms: 652.813\n",
      "    load_throughput: 71972.353\n",
      "    load_time_ms: 13.894\n",
      "    sample_throughput: 50.899\n",
      "    sample_time_ms: 19646.883\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1633533333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         3900.92</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            384.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 385.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 443\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7897493309444852\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018175695455049972\n",
      "          policy_loss: -0.08487277788420518\n",
      "          total_loss: -0.09317674297425482\n",
      "          vf_explained_var: 0.07351525872945786\n",
      "          vf_loss: 0.0049928092184321335\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.00666666666667\n",
      "    ram_util_percent: 61.46666666666668\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704316341330955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.505722129604777\n",
      "    mean_inference_ms: 1.3141643794068694\n",
      "    mean_raw_obs_processing_ms: 1.3743849032570545\n",
      "  time_since_restore: 3921.3141796588898\n",
      "  time_this_iter_s: 20.394862413406372\n",
      "  time_total_s: 3921.3141796588898\n",
      "  timers:\n",
      "    learn_throughput: 1528.113\n",
      "    learn_time_ms: 654.402\n",
      "    load_throughput: 72637.571\n",
      "    load_time_ms: 13.767\n",
      "    sample_throughput: 55.576\n",
      "    sample_time_ms: 17993.316\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1633533353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         3921.31</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            385.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-16-14\n",
      "  done: false\n",
      "  episode_len_mean: 387.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 446\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9683954649501376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015308410485286252\n",
      "          policy_loss: 0.019012845390372807\n",
      "          total_loss: 0.004625326808955934\n",
      "          vf_explained_var: -0.6120924353599548\n",
      "          vf_loss: 0.0014214950639547573\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.6551724137931\n",
      "    ram_util_percent: 61.300000000000004\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704467978246541\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.521566835599273\n",
      "    mean_inference_ms: 1.3142404376996248\n",
      "    mean_raw_obs_processing_ms: 1.3751049698961482\n",
      "  time_since_restore: 3941.9725048542023\n",
      "  time_this_iter_s: 20.6583251953125\n",
      "  time_total_s: 3941.9725048542023\n",
      "  timers:\n",
      "    learn_throughput: 1523.428\n",
      "    learn_time_ms: 656.414\n",
      "    load_throughput: 72977.289\n",
      "    load_time_ms: 13.703\n",
      "    sample_throughput: 55.531\n",
      "    sample_time_ms: 18008.029\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1633533374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         3941.97</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            387.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-16-33\n",
      "  done: false\n",
      "  episode_len_mean: 388.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 448\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.747755049334632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02099083869114484\n",
      "          policy_loss: -0.00227513187047508\n",
      "          total_loss: -0.01213143900450733\n",
      "          vf_explained_var: -0.8848100304603577\n",
      "          vf_loss: 0.002307937054946605\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.07037037037038\n",
      "    ram_util_percent: 61.20370370370369\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704572493235819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.531890350894088\n",
      "    mean_inference_ms: 1.314292645750801\n",
      "    mean_raw_obs_processing_ms: 1.3756361402465915\n",
      "  time_since_restore: 3961.0696659088135\n",
      "  time_this_iter_s: 19.097161054611206\n",
      "  time_total_s: 3961.0696659088135\n",
      "  timers:\n",
      "    learn_throughput: 1530.778\n",
      "    learn_time_ms: 653.262\n",
      "    load_throughput: 72648.139\n",
      "    load_time_ms: 13.765\n",
      "    sample_throughput: 55.478\n",
      "    sample_time_ms: 18025.088\n",
      "    update_time_ms: 1.696\n",
      "  timestamp: 1633533393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         3961.07</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            388.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-17-12\n",
      "  done: false\n",
      "  episode_len_mean: 388.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 451\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.79244884385003\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01651359657574629\n",
      "          policy_loss: -0.07197956846406063\n",
      "          total_loss: -0.0811955826357007\n",
      "          vf_explained_var: -0.44770148396492004\n",
      "          vf_loss: 0.0024384679434458828\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.25438596491228\n",
      "    ram_util_percent: 60.82280701754386\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704736036876397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.547487532767217\n",
      "    mean_inference_ms: 1.3143722683258512\n",
      "    mean_raw_obs_processing_ms: 1.3789739449697087\n",
      "  time_since_restore: 4000.4733860492706\n",
      "  time_this_iter_s: 39.40372014045715\n",
      "  time_total_s: 4000.4733860492706\n",
      "  timers:\n",
      "    learn_throughput: 1525.588\n",
      "    learn_time_ms: 655.485\n",
      "    load_throughput: 70229.645\n",
      "    load_time_ms: 14.239\n",
      "    sample_throughput: 49.747\n",
      "    sample_time_ms: 20101.8\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1633533432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         4000.47</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">               388</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-17-31\n",
      "  done: false\n",
      "  episode_len_mean: 389.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 453\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.796681527296702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013244854785041997\n",
      "          policy_loss: -0.05391899368001355\n",
      "          total_loss: -0.06556187127199438\n",
      "          vf_explained_var: -0.08302871882915497\n",
      "          vf_loss: 0.00129503295950902\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.93076923076923\n",
      "    ram_util_percent: 60.911538461538456\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037048458148942566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.557699639842422\n",
      "    mean_inference_ms: 1.3144255441794364\n",
      "    mean_raw_obs_processing_ms: 1.3811655114069423\n",
      "  time_since_restore: 4019.2357456684113\n",
      "  time_this_iter_s: 18.762359619140625\n",
      "  time_total_s: 4019.2357456684113\n",
      "  timers:\n",
      "    learn_throughput: 1525.023\n",
      "    learn_time_ms: 655.728\n",
      "    load_throughput: 63443.264\n",
      "    load_time_ms: 15.762\n",
      "    sample_throughput: 49.505\n",
      "    sample_time_ms: 20199.914\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1633533451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         4019.24</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            389.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 390.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 455\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8934671070840623\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014288168465628262\n",
      "          policy_loss: 0.10875184966458215\n",
      "          total_loss: 0.09628962646755908\n",
      "          vf_explained_var: -0.7462552189826965\n",
      "          vf_loss: 0.0010474091877565822\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.9896551724138\n",
      "    ram_util_percent: 61.09310344827585\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037049589767401025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.567520659610906\n",
      "    mean_inference_ms: 1.314478913685752\n",
      "    mean_raw_obs_processing_ms: 1.383371170017865\n",
      "  time_since_restore: 4039.360410928726\n",
      "  time_this_iter_s: 20.12466526031494\n",
      "  time_total_s: 4039.360410928726\n",
      "  timers:\n",
      "    learn_throughput: 1524.574\n",
      "    learn_time_ms: 655.921\n",
      "    load_throughput: 63416.118\n",
      "    load_time_ms: 15.769\n",
      "    sample_throughput: 49.116\n",
      "    sample_time_ms: 20359.836\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1633533471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         4039.36</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            390.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 392.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 458\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5346080621083578\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010317741738702797\n",
      "          policy_loss: -0.20470884999053346\n",
      "          total_loss: -0.21395584086163177\n",
      "          vf_explained_var: -0.37259531021118164\n",
      "          vf_loss: 0.0021815764083940948\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.74814814814815\n",
      "    ram_util_percent: 61.09259259259258\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370513071042702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.582127882730006\n",
      "    mean_inference_ms: 1.3145577537941893\n",
      "    mean_raw_obs_processing_ms: 1.3866399409512844\n",
      "  time_since_restore: 4058.2117149829865\n",
      "  time_this_iter_s: 18.851304054260254\n",
      "  time_total_s: 4058.2117149829865\n",
      "  timers:\n",
      "    learn_throughput: 1522.571\n",
      "    learn_time_ms: 656.784\n",
      "    load_throughput: 63210.543\n",
      "    load_time_ms: 15.82\n",
      "    sample_throughput: 48.908\n",
      "    sample_time_ms: 20446.56\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1633533490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         4058.21</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            392.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-18-29\n",
      "  done: false\n",
      "  episode_len_mean: 391.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 460\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9121108068360222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013251229695353325\n",
      "          policy_loss: -0.015445235081844859\n",
      "          total_loss: -0.028128819995456272\n",
      "          vf_explained_var: -0.5699512362480164\n",
      "          vf_loss: 0.0014061961432970647\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.74814814814815\n",
      "    ram_util_percent: 61.144444444444446\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037052528760378844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.591674969992646\n",
      "    mean_inference_ms: 1.314610431385706\n",
      "    mean_raw_obs_processing_ms: 1.3868219387525866\n",
      "  time_since_restore: 4077.2427139282227\n",
      "  time_this_iter_s: 19.030998945236206\n",
      "  time_total_s: 4077.2427139282227\n",
      "  timers:\n",
      "    learn_throughput: 1523.839\n",
      "    learn_time_ms: 656.237\n",
      "    load_throughput: 57945.982\n",
      "    load_time_ms: 17.257\n",
      "    sample_throughput: 48.121\n",
      "    sample_time_ms: 20780.818\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1633533509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         4077.24</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            391.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-18-50\n",
      "  done: false\n",
      "  episode_len_mean: 391.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 462\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7282828503184848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0014920266397209325\n",
      "          policy_loss: -0.0843162778351042\n",
      "          total_loss: -0.0998087477352884\n",
      "          vf_explained_var: 0.3195388913154602\n",
      "          vf_loss: 0.001223854862877892\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.95\n",
      "    ram_util_percent: 61.34000000000002\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037053774731546425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.600889910972718\n",
      "    mean_inference_ms: 1.3146632260857904\n",
      "    mean_raw_obs_processing_ms: 1.3870293354810967\n",
      "  time_since_restore: 4098.269511461258\n",
      "  time_this_iter_s: 21.02679753303528\n",
      "  time_total_s: 4098.269511461258\n",
      "  timers:\n",
      "    learn_throughput: 1521.32\n",
      "    learn_time_ms: 657.324\n",
      "    load_throughput: 55910.039\n",
      "    load_time_ms: 17.886\n",
      "    sample_throughput: 47.601\n",
      "    sample_time_ms: 21007.852\n",
      "    update_time_ms: 1.696\n",
      "  timestamp: 1633533530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         4098.27</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            391.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-19-12\n",
      "  done: false\n",
      "  episode_len_mean: 394.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 465\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9717708561155531\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009461863397696332\n",
      "          policy_loss: -0.04107375236021148\n",
      "          total_loss: -0.04952573006351789\n",
      "          vf_explained_var: -0.42398884892463684\n",
      "          vf_loss: 0.009469455456645745\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.022580645161284\n",
      "    ram_util_percent: 61.51612903225805\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037055724751265326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.614533113035776\n",
      "    mean_inference_ms: 1.3147433997257585\n",
      "    mean_raw_obs_processing_ms: 1.38730920004681\n",
      "  time_since_restore: 4119.996238231659\n",
      "  time_this_iter_s: 21.726726770401\n",
      "  time_total_s: 4119.996238231659\n",
      "  timers:\n",
      "    learn_throughput: 1525.432\n",
      "    learn_time_ms: 655.552\n",
      "    load_throughput: 55455.711\n",
      "    load_time_ms: 18.032\n",
      "    sample_throughput: 47.104\n",
      "    sample_time_ms: 21229.539\n",
      "    update_time_ms: 1.71\n",
      "  timestamp: 1633533552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">            4120</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            394.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-19-33\n",
      "  done: false\n",
      "  episode_len_mean: 395.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 467\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7572815736134848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03143558698254129\n",
      "          policy_loss: -0.03408896674712499\n",
      "          total_loss: -0.04214694698651632\n",
      "          vf_explained_var: 0.3005373775959015\n",
      "          vf_loss: 0.003546980822445928\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.848387096774196\n",
      "    ram_util_percent: 61.725806451612904\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705703130387579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.62348886689973\n",
      "    mean_inference_ms: 1.314796710751988\n",
      "    mean_raw_obs_processing_ms: 1.3874965593238386\n",
      "  time_since_restore: 4141.2348783016205\n",
      "  time_this_iter_s: 21.238640069961548\n",
      "  time_total_s: 4141.2348783016205\n",
      "  timers:\n",
      "    learn_throughput: 1530.389\n",
      "    learn_time_ms: 653.429\n",
      "    load_throughput: 55120.108\n",
      "    load_time_ms: 18.142\n",
      "    sample_throughput: 46.913\n",
      "    sample_time_ms: 21315.947\n",
      "    update_time_ms: 1.706\n",
      "  timestamp: 1633533573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         4141.23</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            395.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-19-53\n",
      "  done: false\n",
      "  episode_len_mean: 394.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 470\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9947074717945523\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012912568625011625\n",
      "          policy_loss: -0.07890326413843367\n",
      "          total_loss: -0.09408770973483721\n",
      "          vf_explained_var: 0.04421741142868996\n",
      "          vf_loss: 0.0010855722865219125\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.63214285714285\n",
      "    ram_util_percent: 61.80357142857143\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370590508817165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.63655156343935\n",
      "    mean_inference_ms: 1.314876911544962\n",
      "    mean_raw_obs_processing_ms: 1.3877821673848303\n",
      "  time_since_restore: 4160.976374387741\n",
      "  time_this_iter_s: 19.741496086120605\n",
      "  time_total_s: 4160.976374387741\n",
      "  timers:\n",
      "    learn_throughput: 1525.343\n",
      "    learn_time_ms: 655.59\n",
      "    load_throughput: 55251.532\n",
      "    load_time_ms: 18.099\n",
      "    sample_throughput: 47.121\n",
      "    sample_time_ms: 21222.131\n",
      "    update_time_ms: 1.718\n",
      "  timestamp: 1633533593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         4160.98</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            394.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-20-13\n",
      "  done: false\n",
      "  episode_len_mean: 396.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 472\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6949232074949476\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006762512648192577\n",
      "          policy_loss: -0.05499947526388698\n",
      "          total_loss: -0.0028721228034959898\n",
      "          vf_explained_var: 0.3596898317337036\n",
      "          vf_loss: 0.06715085351671506\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.10357142857142\n",
      "    ram_util_percent: 61.696428571428555\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037060412675235584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.645055580849398\n",
      "    mean_inference_ms: 1.3149305448823183\n",
      "    mean_raw_obs_processing_ms: 1.3879518230088377\n",
      "  time_since_restore: 4180.921407461166\n",
      "  time_this_iter_s: 19.945033073425293\n",
      "  time_total_s: 4180.921407461166\n",
      "  timers:\n",
      "    learn_throughput: 1526.345\n",
      "    learn_time_ms: 655.16\n",
      "    load_throughput: 54476.645\n",
      "    load_time_ms: 18.356\n",
      "    sample_throughput: 46.933\n",
      "    sample_time_ms: 21307.114\n",
      "    update_time_ms: 1.704\n",
      "  timestamp: 1633533613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         4180.92</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            396.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-20-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 475\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5236819465955098\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010743749902852241\n",
      "          policy_loss: 0.033764152394400705\n",
      "          total_loss: 0.04338355130619473\n",
      "          vf_explained_var: 0.13710154592990875\n",
      "          vf_loss: 0.021796771645959882\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.13103448275862\n",
      "    ram_util_percent: 61.45172413793103\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706245207848599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.657054015982265\n",
      "    mean_inference_ms: 1.3150106452850219\n",
      "    mean_raw_obs_processing_ms: 1.3882492498557102\n",
      "  time_since_restore: 4201.295150279999\n",
      "  time_this_iter_s: 20.373742818832397\n",
      "  time_total_s: 4201.295150279999\n",
      "  timers:\n",
      "    learn_throughput: 1534.314\n",
      "    learn_time_ms: 651.757\n",
      "    load_throughput: 56066.687\n",
      "    load_time_ms: 17.836\n",
      "    sample_throughput: 51.525\n",
      "    sample_time_ms: 19408.026\n",
      "    update_time_ms: 1.711\n",
      "  timestamp: 1633533634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">          4201.3</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">   -0.14</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            400.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-20-52\n",
      "  done: false\n",
      "  episode_len_mean: 403.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 477\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.463067384560903\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006483986061035946\n",
      "          policy_loss: -0.03546879589557648\n",
      "          total_loss: 0.025003352926837072\n",
      "          vf_explained_var: 0.1543092578649521\n",
      "          vf_loss: 0.07325640870258213\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.6074074074074\n",
      "    ram_util_percent: 61.25925925925927\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706380756179193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.664499374025457\n",
      "    mean_inference_ms: 1.3150650674906723\n",
      "    mean_raw_obs_processing_ms: 1.3884733621447416\n",
      "  time_since_restore: 4219.852957725525\n",
      "  time_this_iter_s: 18.557807445526123\n",
      "  time_total_s: 4219.852957725525\n",
      "  timers:\n",
      "    learn_throughput: 1527.604\n",
      "    learn_time_ms: 654.62\n",
      "    load_throughput: 59647.403\n",
      "    load_time_ms: 16.765\n",
      "    sample_throughput: 51.584\n",
      "    sample_time_ms: 19385.766\n",
      "    update_time_ms: 1.717\n",
      "  timestamp: 1633533652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         4219.85</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\">   -0.15</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            403.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-21-11\n",
      "  done: false\n",
      "  episode_len_mean: 404.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 479\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.549419429567125\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00741301395875554\n",
      "          policy_loss: -0.012538059376594093\n",
      "          total_loss: 0.05989722694373793\n",
      "          vf_explained_var: 0.5337805151939392\n",
      "          vf_loss: 0.0858185073464281\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.77407407407408\n",
      "    ram_util_percent: 61.13333333333333\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037065183329275275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.671751267197166\n",
      "    mean_inference_ms: 1.3151195597765124\n",
      "    mean_raw_obs_processing_ms: 1.3886529660716855\n",
      "  time_since_restore: 4238.858176708221\n",
      "  time_this_iter_s: 19.005218982696533\n",
      "  time_total_s: 4238.858176708221\n",
      "  timers:\n",
      "    learn_throughput: 1528.433\n",
      "    learn_time_ms: 654.265\n",
      "    load_throughput: 59755.496\n",
      "    load_time_ms: 16.735\n",
      "    sample_throughput: 51.883\n",
      "    sample_time_ms: 19274.203\n",
      "    update_time_ms: 1.733\n",
      "  timestamp: 1633533671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         4238.86</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\">   -0.15</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            404.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-21-50\n",
      "  done: false\n",
      "  episode_len_mean: 405.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 482\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6047101153267755\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012698690646814172\n",
      "          policy_loss: -0.11856550532910559\n",
      "          total_loss: -0.11349004689190123\n",
      "          vf_explained_var: 0.5027313828468323\n",
      "          vf_loss: 0.01750641026948061\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.64107142857143\n",
      "    ram_util_percent: 61.10178571428571\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037067238556402725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.682128819203133\n",
      "    mean_inference_ms: 1.3152007232299356\n",
      "    mean_raw_obs_processing_ms: 1.3914652451204936\n",
      "  time_since_restore: 4277.826780796051\n",
      "  time_this_iter_s: 38.96860408782959\n",
      "  time_total_s: 4277.826780796051\n",
      "  timers:\n",
      "    learn_throughput: 1532.83\n",
      "    learn_time_ms: 652.388\n",
      "    load_throughput: 59182.817\n",
      "    load_time_ms: 16.897\n",
      "    sample_throughput: 46.976\n",
      "    sample_time_ms: 21287.637\n",
      "    update_time_ms: 1.726\n",
      "  timestamp: 1633533710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         4277.83</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\">   -0.15</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            405.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-22-12\n",
      "  done: false\n",
      "  episode_len_mean: 405.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 484\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3907375819153256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012569698925534596\n",
      "          policy_loss: -0.07466514205767048\n",
      "          total_loss: 0.008990140424834358\n",
      "          vf_explained_var: 0.7858733534812927\n",
      "          vf_loss: 0.09398324142417146\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.680645161290336\n",
      "    ram_util_percent: 61.28387096774192\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706861768952826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.688856695965978\n",
      "    mean_inference_ms: 1.3152546391605202\n",
      "    mean_raw_obs_processing_ms: 1.3933336271677508\n",
      "  time_since_restore: 4299.6961581707\n",
      "  time_this_iter_s: 21.869377374649048\n",
      "  time_total_s: 4299.6961581707\n",
      "  timers:\n",
      "    learn_throughput: 1536.422\n",
      "    learn_time_ms: 650.863\n",
      "    load_throughput: 58869.821\n",
      "    load_time_ms: 16.987\n",
      "    sample_throughput: 46.354\n",
      "    sample_time_ms: 21572.938\n",
      "    update_time_ms: 1.706\n",
      "  timestamp: 1633533732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">          4299.7</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            405.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-22-31\n",
      "  done: false\n",
      "  episode_len_mean: 407.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 486\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3622781104511685\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0122329896085403\n",
      "          policy_loss: -0.04006696738716629\n",
      "          total_loss: 0.11766472104936838\n",
      "          vf_explained_var: 0.6333124041557312\n",
      "          vf_loss: 0.1678709359218677\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.49999999999999\n",
      "    ram_util_percent: 61.29629629629629\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706996089775422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.69532281133344\n",
      "    mean_inference_ms: 1.315308563476224\n",
      "    mean_raw_obs_processing_ms: 1.3951504496915517\n",
      "  time_since_restore: 4318.79594707489\n",
      "  time_this_iter_s: 19.099788904190063\n",
      "  time_total_s: 4318.79594707489\n",
      "  timers:\n",
      "    learn_throughput: 1534.968\n",
      "    learn_time_ms: 651.479\n",
      "    load_throughput: 58671.523\n",
      "    load_time_ms: 17.044\n",
      "    sample_throughput: 46.774\n",
      "    sample_time_ms: 21379.561\n",
      "    update_time_ms: 1.712\n",
      "  timestamp: 1633533751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">          4318.8</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            407.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-22-49\n",
      "  done: false\n",
      "  episode_len_mean: 411.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.2\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 488\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2320400502946642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010511391934735768\n",
      "          policy_loss: -0.040071031699577965\n",
      "          total_loss: 0.0635955804751979\n",
      "          vf_explained_var: 0.44597527384757996\n",
      "          vf_loss: 0.1129937297768063\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.73846153846154\n",
      "    ram_util_percent: 61.27307692307693\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707134219644516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.701356481907652\n",
      "    mean_inference_ms: 1.3153625159989297\n",
      "    mean_raw_obs_processing_ms: 1.3969837187514178\n",
      "  time_since_restore: 4336.769358873367\n",
      "  time_this_iter_s: 17.973411798477173\n",
      "  time_total_s: 4336.769358873367\n",
      "  timers:\n",
      "    learn_throughput: 1531.766\n",
      "    learn_time_ms: 652.841\n",
      "    load_throughput: 58579.91\n",
      "    load_time_ms: 17.071\n",
      "    sample_throughput: 47.613\n",
      "    sample_time_ms: 21002.845\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1633533769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         4336.77</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\">    -0.2</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            411.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-23-05\n",
      "  done: false\n",
      "  episode_len_mean: 413.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 490\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5503473957379659\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012963450359157481\n",
      "          policy_loss: -0.025762749815152752\n",
      "          total_loss: -0.011513589612311787\n",
      "          vf_explained_var: 0.20142440497875214\n",
      "          vf_loss: 0.02606108703960975\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.7391304347826\n",
      "    ram_util_percent: 61.61739130434782\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707267145438414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.706778007110007\n",
      "    mean_inference_ms: 1.3154158045680233\n",
      "    mean_raw_obs_processing_ms: 1.3978050788603382\n",
      "  time_since_restore: 4353.016636371613\n",
      "  time_this_iter_s: 16.24727749824524\n",
      "  time_total_s: 4353.016636371613\n",
      "  timers:\n",
      "    learn_throughput: 1534.209\n",
      "    learn_time_ms: 651.802\n",
      "    load_throughput: 62095.612\n",
      "    load_time_ms: 16.104\n",
      "    sample_throughput: 48.767\n",
      "    sample_time_ms: 20505.724\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1633533785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         4353.02</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            413.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-23-22\n",
      "  done: false\n",
      "  episode_len_mean: 417.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 492\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5841576682196723\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012166248229907634\n",
      "          policy_loss: -0.06672520024908914\n",
      "          total_loss: -0.021548920538690355\n",
      "          vf_explained_var: 0.48568305373191833\n",
      "          vf_loss: 0.05755333417198724\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.025\n",
      "    ram_util_percent: 61.81666666666666\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707398889226029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.711631706981958\n",
      "    mean_inference_ms: 1.315469127521309\n",
      "    mean_raw_obs_processing_ms: 1.3976164490044962\n",
      "  time_since_restore: 4369.754555463791\n",
      "  time_this_iter_s: 16.737919092178345\n",
      "  time_total_s: 4369.754555463791\n",
      "  timers:\n",
      "    learn_throughput: 1539.899\n",
      "    learn_time_ms: 649.393\n",
      "    load_throughput: 65973.175\n",
      "    load_time_ms: 15.158\n",
      "    sample_throughput: 49.484\n",
      "    sample_time_ms: 20208.734\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1633533802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         4369.75</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            417.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 420.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 494\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5899519907103645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010026027014908939\n",
      "          policy_loss: 0.053962827887800005\n",
      "          total_loss: 0.08107474599447516\n",
      "          vf_explained_var: 0.3919687569141388\n",
      "          vf_loss: 0.040156370013331374\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.98461538461539\n",
      "    ram_util_percent: 61.98076923076923\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707529016025242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.71591728001236\n",
      "    mean_inference_ms: 1.3155218204137455\n",
      "    mean_raw_obs_processing_ms: 1.3974553481506014\n",
      "  time_since_restore: 4387.6344883441925\n",
      "  time_this_iter_s: 17.87993288040161\n",
      "  time_total_s: 4387.6344883441925\n",
      "  timers:\n",
      "    learn_throughput: 1538.044\n",
      "    learn_time_ms: 650.176\n",
      "    load_throughput: 66837.131\n",
      "    load_time_ms: 14.962\n",
      "    sample_throughput: 49.996\n",
      "    sample_time_ms: 20001.629\n",
      "    update_time_ms: 1.696\n",
      "  timestamp: 1633533820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         4387.63</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            420.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-24-00\n",
      "  done: false\n",
      "  episode_len_mean: 422.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 497\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7774225804540846\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013633027029045083\n",
      "          policy_loss: -0.0811694189078278\n",
      "          total_loss: 0.1258425972941849\n",
      "          vf_explained_var: 0.3179531395435333\n",
      "          vf_loss: 0.22090402436442674\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.357142857142854\n",
      "    ram_util_percent: 62.11071428571427\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707724352186319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.721915173594798\n",
      "    mean_inference_ms: 1.3155995297956369\n",
      "    mean_raw_obs_processing_ms: 1.397187317758909\n",
      "  time_since_restore: 4407.438504457474\n",
      "  time_this_iter_s: 19.80401611328125\n",
      "  time_total_s: 4407.438504457474\n",
      "  timers:\n",
      "    learn_throughput: 1538.39\n",
      "    learn_time_ms: 650.03\n",
      "    load_throughput: 66381.325\n",
      "    load_time_ms: 15.064\n",
      "    sample_throughput: 50.139\n",
      "    sample_time_ms: 19944.705\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1633533840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         4407.44</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            422.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-24-17\n",
      "  done: false\n",
      "  episode_len_mean: 425.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 499\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7420897483825684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013185399828090914\n",
      "          policy_loss: -0.0325418084859848\n",
      "          total_loss: 0.022454662269188297\n",
      "          vf_explained_var: 0.12477654218673706\n",
      "          vf_loss: 0.06866261667520222\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.675000000000004\n",
      "    ram_util_percent: 62.19583333333333\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707845212324012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.725242365596877\n",
      "    mean_inference_ms: 1.3156464225219813\n",
      "    mean_raw_obs_processing_ms: 1.3970115613517566\n",
      "  time_since_restore: 4424.08301281929\n",
      "  time_this_iter_s: 16.644508361816406\n",
      "  time_total_s: 4424.08301281929\n",
      "  timers:\n",
      "    learn_throughput: 1543.228\n",
      "    learn_time_ms: 647.992\n",
      "    load_throughput: 68447.031\n",
      "    load_time_ms: 14.61\n",
      "    sample_throughput: 50.618\n",
      "    sample_time_ms: 19755.887\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1633533857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         4424.08</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            425.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-24-32\n",
      "  done: false\n",
      "  episode_len_mean: 430.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 501\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.639753778775533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015545712419586818\n",
      "          policy_loss: -0.14424142978257604\n",
      "          total_loss: -0.12060673911538389\n",
      "          vf_explained_var: 0.3256220817565918\n",
      "          vf_loss: 0.03560534461090962\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.554545454545455\n",
      "    ram_util_percent: 62.13636363636362\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707956479935679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.727963801498888\n",
      "    mean_inference_ms: 1.3156888314029975\n",
      "    mean_raw_obs_processing_ms: 1.396792427174829\n",
      "  time_since_restore: 4439.445126056671\n",
      "  time_this_iter_s: 15.362113237380981\n",
      "  time_total_s: 4439.445126056671\n",
      "  timers:\n",
      "    learn_throughput: 1543.271\n",
      "    learn_time_ms: 647.974\n",
      "    load_throughput: 75303.445\n",
      "    load_time_ms: 13.28\n",
      "    sample_throughput: 51.565\n",
      "    sample_time_ms: 19392.931\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1633533872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         4439.45</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            430.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-24-49\n",
      "  done: false\n",
      "  episode_len_mean: 433.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: -0.09\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 503\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5162927826245627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007872283811132124\n",
      "          policy_loss: -0.20067011424236827\n",
      "          total_loss: -0.2009511356552442\n",
      "          vf_explained_var: 0.436064749956131\n",
      "          vf_loss: 0.012640150642902073\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.55833333333334\n",
      "    ram_util_percent: 62.0375\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708053835736705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.729909135997204\n",
      "    mean_inference_ms: 1.315724892084733\n",
      "    mean_raw_obs_processing_ms: 1.3965963846951543\n",
      "  time_since_restore: 4456.764625072479\n",
      "  time_this_iter_s: 17.319499015808105\n",
      "  time_total_s: 4456.764625072479\n",
      "  timers:\n",
      "    learn_throughput: 1546.386\n",
      "    learn_time_ms: 646.669\n",
      "    load_throughput: 82276.424\n",
      "    load_time_ms: 12.154\n",
      "    sample_throughput: 58.037\n",
      "    sample_time_ms: 17230.436\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1633533889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         4456.76</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\">   -0.09</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            433.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-25-06\n",
      "  done: false\n",
      "  episode_len_mean: 434.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: -0.09\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 504\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6802881757418315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012400327702364006\n",
      "          policy_loss: -0.12851264207727378\n",
      "          total_loss: -0.13313227776024078\n",
      "          vf_explained_var: 0.6871883869171143\n",
      "          vf_loss: 0.008652059273380372\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.38399999999999\n",
      "    ram_util_percent: 61.944\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708102304277593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.730800491234696\n",
      "    mean_inference_ms: 1.3157426452585759\n",
      "    mean_raw_obs_processing_ms: 1.3964410874900235\n",
      "  time_since_restore: 4473.838886022568\n",
      "  time_this_iter_s: 17.0742609500885\n",
      "  time_total_s: 4473.838886022568\n",
      "  timers:\n",
      "    learn_throughput: 1544.326\n",
      "    learn_time_ms: 647.532\n",
      "    load_throughput: 90697.851\n",
      "    load_time_ms: 11.026\n",
      "    sample_throughput: 59.697\n",
      "    sample_time_ms: 16751.178\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1633533906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         4473.84</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\">   -0.09</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            434.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-25-25\n",
      "  done: false\n",
      "  episode_len_mean: 438.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: -0.09\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 507\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5498856557740106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013620741687699405\n",
      "          policy_loss: -0.02073767234881719\n",
      "          total_loss: -0.02501654616660542\n",
      "          vf_explained_var: 0.5831881761550903\n",
      "          vf_loss: 0.00734126056647963\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.5923076923077\n",
      "    ram_util_percent: 61.78846153846154\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708279385902446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.73246351008777\n",
      "    mean_inference_ms: 1.3157924360949176\n",
      "    mean_raw_obs_processing_ms: 1.3960895957724428\n",
      "  time_since_restore: 4492.12738442421\n",
      "  time_this_iter_s: 18.288498401641846\n",
      "  time_total_s: 4492.12738442421\n",
      "  timers:\n",
      "    learn_throughput: 1544.713\n",
      "    learn_time_ms: 647.369\n",
      "    load_throughput: 92021.317\n",
      "    load_time_ms: 10.867\n",
      "    sample_throughput: 59.987\n",
      "    sample_time_ms: 16670.361\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1633533925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         4492.13</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\">   -0.09</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            438.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-25-42\n",
      "  done: false\n",
      "  episode_len_mean: 440.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 509\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6992681980133058\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010857915732754304\n",
      "          policy_loss: -0.10277600238720576\n",
      "          total_loss: 0.020994133833381865\n",
      "          vf_explained_var: 0.21076057851314545\n",
      "          vf_loss: 0.13767085713851784\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.07916666666667\n",
      "    ram_util_percent: 61.72083333333333\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037083922921845806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.733206510516013\n",
      "    mean_inference_ms: 1.3158245151877273\n",
      "    mean_raw_obs_processing_ms: 1.3958358558424373\n",
      "  time_since_restore: 4509.019139051437\n",
      "  time_this_iter_s: 16.891754627227783\n",
      "  time_total_s: 4509.019139051437\n",
      "  timers:\n",
      "    learn_throughput: 1547.061\n",
      "    learn_time_ms: 646.387\n",
      "    load_throughput: 101877.183\n",
      "    load_time_ms: 9.816\n",
      "    sample_throughput: 60.371\n",
      "    sample_time_ms: 16564.235\n",
      "    update_time_ms: 1.711\n",
      "  timestamp: 1633533942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         4509.02</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            440.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-26-17\n",
      "  done: false\n",
      "  episode_len_mean: 441.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 511\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6564648800426058\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02182026878309434\n",
      "          policy_loss: -0.003712109559112125\n",
      "          total_loss: 0.0016702586577998267\n",
      "          vf_explained_var: 0.31537920236587524\n",
      "          vf_loss: 0.015733355307020247\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.913725490196075\n",
      "    ram_util_percent: 61.23333333333333\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708499831960232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.733847641935064\n",
      "    mean_inference_ms: 1.3158552335923595\n",
      "    mean_raw_obs_processing_ms: 1.3970618821190532\n",
      "  time_since_restore: 4544.381741762161\n",
      "  time_this_iter_s: 35.36260271072388\n",
      "  time_total_s: 4544.381741762161\n",
      "  timers:\n",
      "    learn_throughput: 1541.195\n",
      "    learn_time_ms: 648.847\n",
      "    load_throughput: 91179.134\n",
      "    load_time_ms: 10.967\n",
      "    sample_throughput: 54.136\n",
      "    sample_time_ms: 18472.131\n",
      "    update_time_ms: 1.724\n",
      "  timestamp: 1633533977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         4544.38</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            441.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-26-38\n",
      "  done: false\n",
      "  episode_len_mean: 441.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 514\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6993258264329698\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013083689704287435\n",
      "          policy_loss: -0.06510227918624878\n",
      "          total_loss: 0.06478564333584573\n",
      "          vf_explained_var: 0.43406447768211365\n",
      "          vf_loss: 0.1412925072428253\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.196551724137926\n",
      "    ram_util_percent: 60.9655172413793\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708660859955705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.73500650118971\n",
      "    mean_inference_ms: 1.315900569585261\n",
      "    mean_raw_obs_processing_ms: 1.3988725186069926\n",
      "  time_since_restore: 4565.24737071991\n",
      "  time_this_iter_s: 20.865628957748413\n",
      "  time_total_s: 4565.24737071991\n",
      "  timers:\n",
      "    learn_throughput: 1541.257\n",
      "    learn_time_ms: 648.821\n",
      "    load_throughput: 82099.914\n",
      "    load_time_ms: 12.18\n",
      "    sample_throughput: 52.956\n",
      "    sample_time_ms: 18883.69\n",
      "    update_time_ms: 1.733\n",
      "  timestamp: 1633533998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         4565.25</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            441.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-26-53\n",
      "  done: false\n",
      "  episode_len_mean: 443.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.02\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 515\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5194018218252394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00834522217123735\n",
      "          policy_loss: -0.1524125191072623\n",
      "          total_loss: -0.11177187727557289\n",
      "          vf_explained_var: 0.7351833581924438\n",
      "          vf_loss: 0.052270011878055\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.60952380952381\n",
      "    ram_util_percent: 61.12380952380953\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037087118065450196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.735065017677503\n",
      "    mean_inference_ms: 1.3159143728394829\n",
      "    mean_raw_obs_processing_ms: 1.3995064412416858\n",
      "  time_since_restore: 4580.047273159027\n",
      "  time_this_iter_s: 14.799902439117432\n",
      "  time_total_s: 4580.047273159027\n",
      "  timers:\n",
      "    learn_throughput: 1538.921\n",
      "    learn_time_ms: 649.806\n",
      "    load_throughput: 92618.116\n",
      "    load_time_ms: 10.797\n",
      "    sample_throughput: 53.833\n",
      "    sample_time_ms: 18576.1\n",
      "    update_time_ms: 1.726\n",
      "  timestamp: 1633534013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         4580.05</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\">    0.02</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            443.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-27-12\n",
      "  done: false\n",
      "  episode_len_mean: 444.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.02\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 518\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.602643566661411\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011431628842892236\n",
      "          policy_loss: -0.06120569325155682\n",
      "          total_loss: -0.006748779780334896\n",
      "          vf_explained_var: 0.56852126121521\n",
      "          vf_loss: 0.06560034525270263\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.625\n",
      "    ram_util_percent: 61.228571428571435\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708857969100399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.735335223881133\n",
      "    mean_inference_ms: 1.3159551317411904\n",
      "    mean_raw_obs_processing_ms: 1.4013161152636124\n",
      "  time_since_restore: 4599.592627286911\n",
      "  time_this_iter_s: 19.54535412788391\n",
      "  time_total_s: 4599.592627286911\n",
      "  timers:\n",
      "    learn_throughput: 1541.599\n",
      "    learn_time_ms: 648.677\n",
      "    load_throughput: 92205.405\n",
      "    load_time_ms: 10.845\n",
      "    sample_throughput: 53.905\n",
      "    sample_time_ms: 18551.314\n",
      "    update_time_ms: 1.717\n",
      "  timestamp: 1633534032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         4599.59</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">    0.02</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             444.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-27-31\n",
      "  done: false\n",
      "  episode_len_mean: 445.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.1\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 520\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8964812848303052\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009027266759228238\n",
      "          policy_loss: -0.09796808461348215\n",
      "          total_loss: 0.13871586016482776\n",
      "          vf_explained_var: 0.07882210612297058\n",
      "          vf_loss: 0.25179277724172505\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.01481481481481\n",
      "    ram_util_percent: 61.45185185185184\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708951290469233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.735429806193803\n",
      "    mean_inference_ms: 1.315981705718729\n",
      "    mean_raw_obs_processing_ms: 1.4016661912703556\n",
      "  time_since_restore: 4618.134742259979\n",
      "  time_this_iter_s: 18.542114973068237\n",
      "  time_total_s: 4618.134742259979\n",
      "  timers:\n",
      "    learn_throughput: 1534.902\n",
      "    learn_time_ms: 651.507\n",
      "    load_throughput: 83397.206\n",
      "    load_time_ms: 11.991\n",
      "    sample_throughput: 53.37\n",
      "    sample_time_ms: 18737.103\n",
      "    update_time_ms: 1.71\n",
      "  timestamp: 1633534051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         4618.13</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\">     0.1</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            445.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-27-50\n",
      "  done: false\n",
      "  episode_len_mean: 445.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.09\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 522\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5395117786195542\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022451010468919227\n",
      "          policy_loss: 0.11256813560095098\n",
      "          total_loss: 0.16372129174156322\n",
      "          vf_explained_var: 0.6816317439079285\n",
      "          vf_loss: 0.056958358962502745\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.89629629629629\n",
      "    ram_util_percent: 61.62962962962964\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370904080231978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.73539356800394\n",
      "    mean_inference_ms: 1.3160066239205739\n",
      "    mean_raw_obs_processing_ms: 1.4011372456206643\n",
      "  time_since_restore: 4637.310072422028\n",
      "  time_this_iter_s: 19.17533016204834\n",
      "  time_total_s: 4637.310072422028\n",
      "  timers:\n",
      "    learn_throughput: 1536.172\n",
      "    learn_time_ms: 650.969\n",
      "    load_throughput: 77522.849\n",
      "    load_time_ms: 12.899\n",
      "    sample_throughput: 52.307\n",
      "    sample_time_ms: 19118.043\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1633534070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         4637.31</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\">    0.09</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            445.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-28-10\n",
      "  done: false\n",
      "  episode_len_mean: 444.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.16\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 525\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8977167725563049\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012457062747378907\n",
      "          policy_loss: -0.03946738690137863\n",
      "          total_loss: -0.033497603899902764\n",
      "          vf_explained_var: 0.4780016243457794\n",
      "          vf_loss: 0.016965429981549582\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.625\n",
      "    ram_util_percent: 61.792857142857144\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709166507892196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.735309104516425\n",
      "    mean_inference_ms: 1.3160410800791293\n",
      "    mean_raw_obs_processing_ms: 1.400354424338754\n",
      "  time_since_restore: 4656.987942457199\n",
      "  time_this_iter_s: 19.67787003517151\n",
      "  time_total_s: 4656.987942457199\n",
      "  timers:\n",
      "    learn_throughput: 1530.903\n",
      "    learn_time_ms: 653.209\n",
      "    load_throughput: 70621.641\n",
      "    load_time_ms: 14.16\n",
      "    sample_throughput: 51.679\n",
      "    sample_time_ms: 19350.407\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1633534090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         4656.99</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\">    0.16</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            444.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-28-28\n",
      "  done: false\n",
      "  episode_len_mean: 444.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.19\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 527\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7688631825976902\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009966281031854863\n",
      "          policy_loss: -0.07617586743500497\n",
      "          total_loss: -0.010966309077209897\n",
      "          vf_explained_var: -0.4258210062980652\n",
      "          vf_loss: 0.0765125673600576\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.47407407407408\n",
      "    ram_util_percent: 62.0037037037037\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709246792420623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.735246191924457\n",
      "    mean_inference_ms: 1.316062164734572\n",
      "    mean_raw_obs_processing_ms: 1.3998849397177002\n",
      "  time_since_restore: 4675.639325380325\n",
      "  time_this_iter_s: 18.65138292312622\n",
      "  time_total_s: 4675.639325380325\n",
      "  timers:\n",
      "    learn_throughput: 1529.648\n",
      "    learn_time_ms: 653.745\n",
      "    load_throughput: 65761.438\n",
      "    load_time_ms: 15.206\n",
      "    sample_throughput: 51.265\n",
      "    sample_time_ms: 19506.546\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1633534108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         4675.64</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">    0.19</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            444.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-28-48\n",
      "  done: false\n",
      "  episode_len_mean: 443.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.21\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 529\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7683011015256247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007648494795893216\n",
      "          policy_loss: -0.13719354586468802\n",
      "          total_loss: -0.09991046736637751\n",
      "          vf_explained_var: 0.6352459192276001\n",
      "          vf_loss: 0.05006552324112919\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.39999999999999\n",
      "    ram_util_percent: 62.08214285714285\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037093239877726655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.73533212684724\n",
      "    mean_inference_ms: 1.3160820183953132\n",
      "    mean_raw_obs_processing_ms: 1.3994430590636904\n",
      "  time_since_restore: 4695.362118244171\n",
      "  time_this_iter_s: 19.722792863845825\n",
      "  time_total_s: 4695.362118244171\n",
      "  timers:\n",
      "    learn_throughput: 1535.492\n",
      "    learn_time_ms: 651.257\n",
      "    load_throughput: 65031.195\n",
      "    load_time_ms: 15.377\n",
      "    sample_throughput: 50.885\n",
      "    sample_time_ms: 19652.295\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1633534128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         4695.36</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\">    0.21</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            443.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-29-08\n",
      "  done: false\n",
      "  episode_len_mean: 441.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.34\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 532\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.437370389699936\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010741267473591323\n",
      "          policy_loss: 0.03956619302431742\n",
      "          total_loss: 0.10127337492174572\n",
      "          vf_explained_var: 0.7236528992652893\n",
      "          vf_loss: 0.06919871204429202\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.67142857142857\n",
      "    ram_util_percent: 62.025\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709435425282904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.73569219555513\n",
      "    mean_inference_ms: 1.3161098771334954\n",
      "    mean_raw_obs_processing_ms: 1.3988604493282335\n",
      "  time_since_restore: 4715.101941585541\n",
      "  time_this_iter_s: 19.73982334136963\n",
      "  time_total_s: 4715.101941585541\n",
      "  timers:\n",
      "    learn_throughput: 1533.668\n",
      "    learn_time_ms: 652.032\n",
      "    load_throughput: 60703.789\n",
      "    load_time_ms: 16.473\n",
      "    sample_throughput: 50.162\n",
      "    sample_time_ms: 19935.231\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1633534148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">          4715.1</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\">    0.34</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            441.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 439.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.35\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 534\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7557645744747585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013875800858523689\n",
      "          policy_loss: -0.02190773296687338\n",
      "          total_loss: 0.15093680077956784\n",
      "          vf_explained_var: 0.8038637638092041\n",
      "          vf_loss: 0.1815116409626272\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.75333333333334\n",
      "    ram_util_percent: 61.92666666666669\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370950475091607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.736198635992864\n",
      "    mean_inference_ms: 1.3161271104171142\n",
      "    mean_raw_obs_processing_ms: 1.3985431871324752\n",
      "  time_since_restore: 4735.831470251083\n",
      "  time_this_iter_s: 20.729528665542603\n",
      "  time_total_s: 4735.831470251083\n",
      "  timers:\n",
      "    learn_throughput: 1535.796\n",
      "    learn_time_ms: 651.128\n",
      "    load_throughput: 60292.095\n",
      "    load_time_ms: 16.586\n",
      "    sample_throughput: 54.134\n",
      "    sample_time_ms: 18472.743\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1633534169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         4735.83</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\">    0.35</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            439.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-29-49\n",
      "  done: false\n",
      "  episode_len_mean: 437.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.47\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 537\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7886817706955804\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011409410396695168\n",
      "          policy_loss: -0.052583065463436975\n",
      "          total_loss: 0.12441169809963969\n",
      "          vf_explained_var: 0.8590144515037537\n",
      "          vf_loss: 0.18757131761974757\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.800000000000004\n",
      "    ram_util_percent: 61.80344827586208\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709594192147447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.73760156184072\n",
      "    mean_inference_ms: 1.3161510389902693\n",
      "    mean_raw_obs_processing_ms: 1.3981787111972093\n",
      "  time_since_restore: 4755.853656053543\n",
      "  time_this_iter_s: 20.022185802459717\n",
      "  time_total_s: 4755.853656053543\n",
      "  timers:\n",
      "    learn_throughput: 1540.229\n",
      "    learn_time_ms: 649.254\n",
      "    load_throughput: 60087.474\n",
      "    load_time_ms: 16.642\n",
      "    sample_throughput: 54.377\n",
      "    sample_time_ms: 18390.246\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1633534189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         4755.85</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">    0.47</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             437.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-30-27\n",
      "  done: false\n",
      "  episode_len_mean: 435.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.5\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 540\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.48690531651179\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024250310375824095\n",
      "          policy_loss: 0.003066879345311059\n",
      "          total_loss: 0.6931225150409672\n",
      "          vf_explained_var: 0.5861101150512695\n",
      "          vf_loss: 0.689386957221561\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.162962962962965\n",
      "    ram_util_percent: 61.53703703703704\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709664304014526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.739330792219242\n",
      "    mean_inference_ms: 1.316169525526864\n",
      "    mean_raw_obs_processing_ms: 1.400007960926541\n",
      "  time_since_restore: 4794.258619785309\n",
      "  time_this_iter_s: 38.40496373176575\n",
      "  time_total_s: 4794.258619785309\n",
      "  timers:\n",
      "    learn_throughput: 1543.477\n",
      "    learn_time_ms: 647.888\n",
      "    load_throughput: 55146.923\n",
      "    load_time_ms: 18.133\n",
      "    sample_throughput: 48.191\n",
      "    sample_time_ms: 20750.612\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1633534227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         4794.26</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\">     0.5</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            435.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-30-51\n",
      "  done: false\n",
      "  episode_len_mean: 434.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.53\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 542\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7484933972358703\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009413070882391135\n",
      "          policy_loss: -0.004561316221952438\n",
      "          total_loss: 0.08640915287865533\n",
      "          vf_explained_var: 0.1311817765235901\n",
      "          vf_loss: 0.0994086522815956\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.342857142857135\n",
      "    ram_util_percent: 61.054285714285705\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037097050618554145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.740799340292803\n",
      "    mean_inference_ms: 1.3161806800345017\n",
      "    mean_raw_obs_processing_ms: 1.4012244724010754\n",
      "  time_since_restore: 4818.117100715637\n",
      "  time_this_iter_s: 23.85848093032837\n",
      "  time_total_s: 4818.117100715637\n",
      "  timers:\n",
      "    learn_throughput: 1540.033\n",
      "    learn_time_ms: 649.337\n",
      "    load_throughput: 55977.792\n",
      "    load_time_ms: 17.864\n",
      "    sample_throughput: 47.213\n",
      "    sample_time_ms: 21180.738\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1633534251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         4818.12</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\">    0.53</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            434.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-31-13\n",
      "  done: false\n",
      "  episode_len_mean: 434.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.57\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 545\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6625593993398877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008414328859286217\n",
      "          policy_loss: -0.04530797716644075\n",
      "          total_loss: 0.2006429712391562\n",
      "          vf_explained_var: 0.7659650444984436\n",
      "          vf_loss: 0.25448966208431456\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.303225806451614\n",
      "    ram_util_percent: 61.554838709677405\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709764633196211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.743101912917545\n",
      "    mean_inference_ms: 1.316196758314965\n",
      "    mean_raw_obs_processing_ms: 1.4031117190183926\n",
      "  time_since_restore: 4840.266937017441\n",
      "  time_this_iter_s: 22.14983630180359\n",
      "  time_total_s: 4840.266937017441\n",
      "  timers:\n",
      "    learn_throughput: 1543.442\n",
      "    learn_time_ms: 647.903\n",
      "    load_throughput: 56417.215\n",
      "    load_time_ms: 17.725\n",
      "    sample_throughput: 46.419\n",
      "    sample_time_ms: 21543.038\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1633534273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         4840.27</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\">    0.57</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            434.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-32-15\n",
      "  done: false\n",
      "  episode_len_mean: 431.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.69\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 548\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7685914701885648\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004475557801943333\n",
      "          policy_loss: 0.03126644910209709\n",
      "          total_loss: 0.1572801137963931\n",
      "          vf_explained_var: 0.751720666885376\n",
      "          vf_loss: 0.13939819203482734\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.872727272727275\n",
      "    ram_util_percent: 61.45113636363635\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370982870209058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.74564569412357\n",
      "    mean_inference_ms: 1.3162121589283924\n",
      "    mean_raw_obs_processing_ms: 1.4100207093093686\n",
      "  time_since_restore: 4902.022491693497\n",
      "  time_this_iter_s: 61.75555467605591\n",
      "  time_total_s: 4902.022491693497\n",
      "  timers:\n",
      "    learn_throughput: 1542.241\n",
      "    learn_time_ms: 648.407\n",
      "    load_throughput: 54361.346\n",
      "    load_time_ms: 18.395\n",
      "    sample_throughput: 38.76\n",
      "    sample_time_ms: 25799.89\n",
      "    update_time_ms: 1.696\n",
      "  timestamp: 1633534335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         4902.02</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">    0.69</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             431.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-33-01\n",
      "  done: false\n",
      "  episode_len_mean: 431.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.81\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 550\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6149441440900167\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014815183706722256\n",
      "          policy_loss: -0.10221825299991502\n",
      "          total_loss: 0.08715670191579394\n",
      "          vf_explained_var: 0.7223109602928162\n",
      "          vf_loss: 0.1984050799989038\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.75384615384616\n",
      "    ram_util_percent: 62.17384615384615\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709868816907625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.747207346184478\n",
      "    mean_inference_ms: 1.3162204091790657\n",
      "    mean_raw_obs_processing_ms: 1.414940027111334\n",
      "  time_since_restore: 4947.596491575241\n",
      "  time_this_iter_s: 45.573999881744385\n",
      "  time_total_s: 4947.596491575241\n",
      "  timers:\n",
      "    learn_throughput: 1547.35\n",
      "    learn_time_ms: 646.266\n",
      "    load_throughput: 54271.592\n",
      "    load_time_ms: 18.426\n",
      "    sample_throughput: 35.222\n",
      "    sample_time_ms: 28391.598\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1633534381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">          4947.6</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\">    0.81</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             431.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-33-24\n",
      "  done: false\n",
      "  episode_len_mean: 429.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.91\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 553\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5132343848546346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01075688963399928\n",
      "          policy_loss: -0.10354930924044715\n",
      "          total_loss: -0.013398862712913089\n",
      "          vf_explained_var: 0.8518863916397095\n",
      "          vf_loss: 0.10011365010092656\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.04705882352941\n",
      "    ram_util_percent: 62.50588235294117\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370993027482621\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.75011614208093\n",
      "    mean_inference_ms: 1.3162325567471203\n",
      "    mean_raw_obs_processing_ms: 1.4222655492389966\n",
      "  time_since_restore: 4971.097603559494\n",
      "  time_this_iter_s: 23.50111198425293\n",
      "  time_total_s: 4971.097603559494\n",
      "  timers:\n",
      "    learn_throughput: 1547.749\n",
      "    learn_time_ms: 646.1\n",
      "    load_throughput: 54127.315\n",
      "    load_time_ms: 18.475\n",
      "    sample_throughput: 34.63\n",
      "    sample_time_ms: 28876.658\n",
      "    update_time_ms: 1.727\n",
      "  timestamp: 1633534404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">          4971.1</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\">    0.91</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            429.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-33-47\n",
      "  done: false\n",
      "  episode_len_mean: 427.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 0.98\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 556\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6422847867012025\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011893741107332505\n",
      "          policy_loss: -0.11582093615498808\n",
      "          total_loss: 0.058503003263225155\n",
      "          vf_explained_var: 0.6608307361602783\n",
      "          vf_loss: 0.18503134710093339\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.88125\n",
      "    ram_util_percent: 61.865624999999994\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037099816073773106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.753277032312486\n",
      "    mean_inference_ms: 1.3162429837567455\n",
      "    mean_raw_obs_processing_ms: 1.429728846923718\n",
      "  time_since_restore: 4993.559632539749\n",
      "  time_this_iter_s: 22.462028980255127\n",
      "  time_total_s: 4993.559632539749\n",
      "  timers:\n",
      "    learn_throughput: 1541.26\n",
      "    learn_time_ms: 648.82\n",
      "    load_throughput: 53632.035\n",
      "    load_time_ms: 18.646\n",
      "    sample_throughput: 34.308\n",
      "    sample_time_ms: 29147.691\n",
      "    update_time_ms: 1.715\n",
      "  timestamp: 1633534427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         4993.56</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\">    0.98</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            427.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 424.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.06\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 559\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5113967961735195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009964033848671821\n",
      "          policy_loss: 0.030517485075526766\n",
      "          total_loss: 0.3287543192505836\n",
      "          vf_explained_var: 0.5442683696746826\n",
      "          vf_loss: 0.3085626669228077\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.642857142857146\n",
      "    ram_util_percent: 61.714285714285715\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710031659978405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.757143111010137\n",
      "    mean_inference_ms: 1.3162536879907958\n",
      "    mean_raw_obs_processing_ms: 1.4371994902151357\n",
      "  time_since_restore: 5018.321475505829\n",
      "  time_this_iter_s: 24.761842966079712\n",
      "  time_total_s: 5018.321475505829\n",
      "  timers:\n",
      "    learn_throughput: 1545.601\n",
      "    learn_time_ms: 646.997\n",
      "    load_throughput: 53342.151\n",
      "    load_time_ms: 18.747\n",
      "    sample_throughput: 33.725\n",
      "    sample_time_ms: 29651.598\n",
      "    update_time_ms: 1.716\n",
      "  timestamp: 1633534451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         5018.32</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">    1.06</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            424.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-34-34\n",
      "  done: false\n",
      "  episode_len_mean: 423.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.16\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 562\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7282441443867154\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010214353663393475\n",
      "          policy_loss: -0.06314913953344027\n",
      "          total_loss: 0.05335738737550047\n",
      "          vf_explained_var: 0.7637475728988647\n",
      "          vf_loss: 0.12888053953647613\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.53030303030303\n",
      "    ram_util_percent: 61.77878787878788\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710073189182649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.76128869707747\n",
      "    mean_inference_ms: 1.3162630220030789\n",
      "    mean_raw_obs_processing_ms: 1.4447431108141655\n",
      "  time_since_restore: 5041.486780405045\n",
      "  time_this_iter_s: 23.1653048992157\n",
      "  time_total_s: 5041.486780405045\n",
      "  timers:\n",
      "    learn_throughput: 1545.051\n",
      "    learn_time_ms: 647.228\n",
      "    load_throughput: 53857.851\n",
      "    load_time_ms: 18.567\n",
      "    sample_throughput: 33.45\n",
      "    sample_time_ms: 29895.135\n",
      "    update_time_ms: 1.72\n",
      "  timestamp: 1633534474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         5041.49</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\">    1.16</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             423.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 246000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-35-45\n",
      "  done: false\n",
      "  episode_len_mean: 419.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.31\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 565\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5049924784236484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008124183866102195\n",
      "          policy_loss: -0.12051109431518449\n",
      "          total_loss: -0.02421238687303331\n",
      "          vf_explained_var: 0.8612977862358093\n",
      "          vf_loss: 0.10744461909764343\n",
      "    num_agent_steps_sampled: 246000\n",
      "    num_agent_steps_trained: 246000\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.672277227722773\n",
      "    ram_util_percent: 61.94752475247524\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037101050557376725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.76543085824475\n",
      "    mean_inference_ms: 1.316269710915937\n",
      "    mean_raw_obs_processing_ms: 1.4581150922930461\n",
      "  time_since_restore: 5112.011761903763\n",
      "  time_this_iter_s: 70.52498149871826\n",
      "  time_total_s: 5112.011761903763\n",
      "  timers:\n",
      "    learn_throughput: 1540.557\n",
      "    learn_time_ms: 649.116\n",
      "    load_throughput: 54180.594\n",
      "    load_time_ms: 18.457\n",
      "    sample_throughput: 28.618\n",
      "    sample_time_ms: 34943.653\n",
      "    update_time_ms: 1.716\n",
      "  timestamp: 1633534545\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 246\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         5112.01</td><td style=\"text-align: right;\">246000</td><td style=\"text-align: right;\">    1.31</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            419.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 247000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-36-07\n",
      "  done: false\n",
      "  episode_len_mean: 419.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.43\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 567\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4634953949186538\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010679612042098845\n",
      "          policy_loss: -0.116810149865018\n",
      "          total_loss: 0.05095439379413923\n",
      "          vf_explained_var: 0.7310662865638733\n",
      "          vf_loss: 0.17726749084475968\n",
      "    num_agent_steps_sampled: 247000\n",
      "    num_agent_steps_trained: 247000\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0625\n",
      "    ram_util_percent: 62.4875\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037101279208158845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.76821866259934\n",
      "    mean_inference_ms: 1.3162735037262963\n",
      "    mean_raw_obs_processing_ms: 1.4670163049187368\n",
      "  time_since_restore: 5134.215275764465\n",
      "  time_this_iter_s: 22.203513860702515\n",
      "  time_total_s: 5134.215275764465\n",
      "  timers:\n",
      "    learn_throughput: 1537.535\n",
      "    learn_time_ms: 650.392\n",
      "    load_throughput: 54004.586\n",
      "    load_time_ms: 18.517\n",
      "    sample_throughput: 30.01\n",
      "    sample_time_ms: 33322.179\n",
      "    update_time_ms: 1.72\n",
      "  timestamp: 1633534567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         5134.22</td><td style=\"text-align: right;\">247000</td><td style=\"text-align: right;\">    1.43</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            419.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-36-29\n",
      "  done: false\n",
      "  episode_len_mean: 420.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.5\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 570\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7805629266632927\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01048383044334642\n",
      "          policy_loss: -0.03924378504355749\n",
      "          total_loss: 0.2613405943744712\n",
      "          vf_explained_var: 0.6309980750083923\n",
      "          vf_loss: 0.31335209243827394\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.84666666666667\n",
      "    ram_util_percent: 62.57666666666667\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037101588998618366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.77254682266724\n",
      "    mean_inference_ms: 1.3162783091319377\n",
      "    mean_raw_obs_processing_ms: 1.4803500713828952\n",
      "  time_since_restore: 5155.470295906067\n",
      "  time_this_iter_s: 21.255020141601562\n",
      "  time_total_s: 5155.470295906067\n",
      "  timers:\n",
      "    learn_throughput: 1536.681\n",
      "    learn_time_ms: 650.753\n",
      "    load_throughput: 53397.633\n",
      "    load_time_ms: 18.727\n",
      "    sample_throughput: 30.247\n",
      "    sample_time_ms: 33061.252\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1633534589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 248\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         5155.47</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">     1.5</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            420.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 249000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-36-52\n",
      "  done: false\n",
      "  episode_len_mean: 416.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.62\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 573\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5770594875017803\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012832997553820589\n",
      "          policy_loss: -0.11697027534246444\n",
      "          total_loss: 0.17863952798975838\n",
      "          vf_explained_var: 0.8637399673461914\n",
      "          vf_loss: 0.30521359625789857\n",
      "    num_agent_steps_sampled: 249000\n",
      "    num_agent_steps_trained: 249000\n",
      "    num_steps_sampled: 249000\n",
      "    num_steps_trained: 249000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.63333333333333\n",
      "    ram_util_percent: 62.58484848484849\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710189828122854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.777151313767114\n",
      "    mean_inference_ms: 1.3162818612007448\n",
      "    mean_raw_obs_processing_ms: 1.4937234711572913\n",
      "  time_since_restore: 5178.6327023506165\n",
      "  time_this_iter_s: 23.16240644454956\n",
      "  time_total_s: 5178.6327023506165\n",
      "  timers:\n",
      "    learn_throughput: 1541.771\n",
      "    learn_time_ms: 648.605\n",
      "    load_throughput: 53683.312\n",
      "    load_time_ms: 18.628\n",
      "    sample_throughput: 30.152\n",
      "    sample_time_ms: 33164.789\n",
      "    update_time_ms: 1.697\n",
      "  timestamp: 1633534612\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249000\n",
      "  training_iteration: 249\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         5178.63</td><td style=\"text-align: right;\">249000</td><td style=\"text-align: right;\">    1.62</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            416.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 250000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-37-15\n",
      "  done: false\n",
      "  episode_len_mean: 414.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.65\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 576\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6143743912378947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023034766672452692\n",
      "          policy_loss: 0.0020492202291886013\n",
      "          total_loss: 0.42461539266837967\n",
      "          vf_explained_var: 0.6126837134361267\n",
      "          vf_loss: 0.4276407450437546\n",
      "    num_agent_steps_sampled: 250000\n",
      "    num_agent_steps_trained: 250000\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.54411764705883\n",
      "    ram_util_percent: 62.55\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710220437483343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.782158459418135\n",
      "    mean_inference_ms: 1.3162839468275727\n",
      "    mean_raw_obs_processing_ms: 1.5070749468269908\n",
      "  time_since_restore: 5202.057663440704\n",
      "  time_this_iter_s: 23.42496109008789\n",
      "  time_total_s: 5202.057663440704\n",
      "  timers:\n",
      "    learn_throughput: 1546.051\n",
      "    learn_time_ms: 646.809\n",
      "    load_throughput: 53895.36\n",
      "    load_time_ms: 18.554\n",
      "    sample_throughput: 34.091\n",
      "    sample_time_ms: 29333.601\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1633534635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         5202.06</td><td style=\"text-align: right;\">250000</td><td style=\"text-align: right;\">    1.65</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            414.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 251000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-37-39\n",
      "  done: false\n",
      "  episode_len_mean: 410.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.75\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 579\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4955595956908332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010242087403930553\n",
      "          policy_loss: -0.04627964571118355\n",
      "          total_loss: 0.16329300676782926\n",
      "          vf_explained_var: 0.6443601250648499\n",
      "          vf_loss: 0.21714561904470125\n",
      "    num_agent_steps_sampled: 251000\n",
      "    num_agent_steps_trained: 251000\n",
      "    num_steps_sampled: 251000\n",
      "    num_steps_trained: 251000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.829411764705874\n",
      "    ram_util_percent: 62.64117647058823\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710249325960981\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.787739116999155\n",
      "    mean_inference_ms: 1.3162842089215558\n",
      "    mean_raw_obs_processing_ms: 1.5204648327563688\n",
      "  time_since_restore: 5225.67981338501\n",
      "  time_this_iter_s: 23.62214994430542\n",
      "  time_total_s: 5225.67981338501\n",
      "  timers:\n",
      "    learn_throughput: 1547.633\n",
      "    learn_time_ms: 646.148\n",
      "    load_throughput: 53761.066\n",
      "    load_time_ms: 18.601\n",
      "    sample_throughput: 36.847\n",
      "    sample_time_ms: 27139.049\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1633534659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 251000\n",
      "  training_iteration: 251\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         5225.68</td><td style=\"text-align: right;\">251000</td><td style=\"text-align: right;\">    1.75</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            410.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-38-02\n",
      "  done: false\n",
      "  episode_len_mean: 411.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.78\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 581\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6414585365189447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01338993012471599\n",
      "          policy_loss: 0.034759603854682714\n",
      "          total_loss: 0.3785180561658409\n",
      "          vf_explained_var: 0.8030505776405334\n",
      "          vf_loss: 0.35052140195750525\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.715625\n",
      "    ram_util_percent: 62.768750000000004\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710268159906807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.791499153871104\n",
      "    mean_inference_ms: 1.3162833699352359\n",
      "    mean_raw_obs_processing_ms: 1.5277687765974346\n",
      "  time_since_restore: 5248.489241838455\n",
      "  time_this_iter_s: 22.809428453445435\n",
      "  time_total_s: 5248.489241838455\n",
      "  timers:\n",
      "    learn_throughput: 1549.351\n",
      "    learn_time_ms: 645.431\n",
      "    load_throughput: 53483.085\n",
      "    load_time_ms: 18.698\n",
      "    sample_throughput: 36.941\n",
      "    sample_time_ms: 27070.514\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633534682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 252\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         5248.49</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">    1.78</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            411.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 253000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-38-25\n",
      "  done: false\n",
      "  episode_len_mean: 410.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.91\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 584\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6014987296528287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013492394645816337\n",
      "          policy_loss: -0.1626103507147895\n",
      "          total_loss: -0.059310869582825235\n",
      "          vf_explained_var: 0.883992612361908\n",
      "          vf_loss: 0.10958897550072935\n",
      "    num_agent_steps_sampled: 253000\n",
      "    num_agent_steps_trained: 253000\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.41515151515151\n",
      "    ram_util_percent: 62.778787878787874\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710287845978204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.79729750820919\n",
      "    mean_inference_ms: 1.3162822157399154\n",
      "    mean_raw_obs_processing_ms: 1.5386548486110734\n",
      "  time_since_restore: 5271.335767030716\n",
      "  time_this_iter_s: 22.846525192260742\n",
      "  time_total_s: 5271.335767030716\n",
      "  timers:\n",
      "    learn_throughput: 1553.254\n",
      "    learn_time_ms: 643.81\n",
      "    load_throughput: 54178.005\n",
      "    load_time_ms: 18.458\n",
      "    sample_throughput: 36.886\n",
      "    sample_time_ms: 27110.821\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633534705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         5271.34</td><td style=\"text-align: right;\">253000</td><td style=\"text-align: right;\">    1.91</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            410.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 254000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-38-44\n",
      "  done: false\n",
      "  episode_len_mean: 409.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.91\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 587\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4944244980812074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011470902839062823\n",
      "          policy_loss: -0.03820277262065146\n",
      "          total_loss: 0.24129727698034711\n",
      "          vf_explained_var: 0.302024781703949\n",
      "          vf_loss: 0.2861759235461553\n",
      "    num_agent_steps_sampled: 254000\n",
      "    num_agent_steps_trained: 254000\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.65185185185185\n",
      "    ram_util_percent: 62.6888888888889\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710309685782367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.803010623443708\n",
      "    mean_inference_ms: 1.316279800937902\n",
      "    mean_raw_obs_processing_ms: 1.5496515231703913\n",
      "  time_since_restore: 5290.685276031494\n",
      "  time_this_iter_s: 19.3495090007782\n",
      "  time_total_s: 5290.685276031494\n",
      "  timers:\n",
      "    learn_throughput: 1548.234\n",
      "    learn_time_ms: 645.897\n",
      "    load_throughput: 54739.491\n",
      "    load_time_ms: 18.268\n",
      "    sample_throughput: 37.64\n",
      "    sample_time_ms: 26567.702\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1633534724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 254\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         5290.69</td><td style=\"text-align: right;\">254000</td><td style=\"text-align: right;\">    1.91</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            409.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-39-05\n",
      "  done: false\n",
      "  episode_len_mean: 407.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 1.98\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 589\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.789976261721717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010046879028074236\n",
      "          policy_loss: -0.05773945202430089\n",
      "          total_loss: 0.11759109364615547\n",
      "          vf_explained_var: 0.7757536768913269\n",
      "          vf_loss: 0.18598838568561607\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.58\n",
      "    ram_util_percent: 62.56666666666665\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710329848037164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.807124476415783\n",
      "    mean_inference_ms: 1.3162781065469238\n",
      "    mean_raw_obs_processing_ms: 1.5569935718483903\n",
      "  time_since_restore: 5311.343711137772\n",
      "  time_this_iter_s: 20.658435106277466\n",
      "  time_total_s: 5311.343711137772\n",
      "  timers:\n",
      "    learn_throughput: 1548.124\n",
      "    learn_time_ms: 645.943\n",
      "    load_throughput: 54551.891\n",
      "    load_time_ms: 18.331\n",
      "    sample_throughput: 37.998\n",
      "    sample_time_ms: 26316.914\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1633534745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 255\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         5311.34</td><td style=\"text-align: right;\">255000</td><td style=\"text-align: right;\">    1.98</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            407.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-39-25\n",
      "  done: false\n",
      "  episode_len_mean: 403.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.07\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 592\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6075809412532382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0076357351254213454\n",
      "          policy_loss: -0.04091589537759622\n",
      "          total_loss: 0.2059994500544336\n",
      "          vf_explained_var: 0.6837142705917358\n",
      "          vf_loss: 0.25748722325596546\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.69655172413793\n",
      "    ram_util_percent: 62.42413793103449\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037103597379680005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.813787555272853\n",
      "    mean_inference_ms: 1.3162749429371672\n",
      "    mean_raw_obs_processing_ms: 1.5680273682889998\n",
      "  time_since_restore: 5331.40191245079\n",
      "  time_this_iter_s: 20.0582013130188\n",
      "  time_total_s: 5331.40191245079\n",
      "  timers:\n",
      "    learn_throughput: 1548.757\n",
      "    learn_time_ms: 645.679\n",
      "    load_throughput: 54754.497\n",
      "    load_time_ms: 18.263\n",
      "    sample_throughput: 47.013\n",
      "    sample_time_ms: 21270.556\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633534765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">          5331.4</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">    2.07</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            403.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 257000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-40-04\n",
      "  done: false\n",
      "  episode_len_mean: 400.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.13\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 594\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5827876753277248\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009288672010151839\n",
      "          policy_loss: 0.033836421370506284\n",
      "          total_loss: 0.12317054126825598\n",
      "          vf_explained_var: 0.9049570560455322\n",
      "          vf_loss: 0.0984666043271621\n",
      "    num_agent_steps_sampled: 257000\n",
      "    num_agent_steps_trained: 257000\n",
      "    num_steps_sampled: 257000\n",
      "    num_steps_trained: 257000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.25535714285714\n",
      "    ram_util_percent: 62.13928571428572\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710380922048085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.818615572719047\n",
      "    mean_inference_ms: 1.316272941316221\n",
      "    mean_raw_obs_processing_ms: 1.5767537612986229\n",
      "  time_since_restore: 5370.638487577438\n",
      "  time_this_iter_s: 39.23657512664795\n",
      "  time_total_s: 5370.638487577438\n",
      "  timers:\n",
      "    learn_throughput: 1548.462\n",
      "    learn_time_ms: 645.802\n",
      "    load_throughput: 54879.011\n",
      "    load_time_ms: 18.222\n",
      "    sample_throughput: 43.528\n",
      "    sample_time_ms: 22973.782\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633534804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257000\n",
      "  training_iteration: 257\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         5370.64</td><td style=\"text-align: right;\">257000</td><td style=\"text-align: right;\">    2.13</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            400.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 258000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-40-25\n",
      "  done: false\n",
      "  episode_len_mean: 399.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.0\n",
      "  episode_reward_mean: 2.14\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 597\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4691212587886386\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006979093360251751\n",
      "          policy_loss: -0.021245882246229385\n",
      "          total_loss: 0.1254440998037656\n",
      "          vf_explained_var: 0.7898406982421875\n",
      "          vf_loss: 0.15635057468381192\n",
      "    num_agent_steps_sampled: 258000\n",
      "    num_agent_steps_trained: 258000\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.31333333333334\n",
      "    ram_util_percent: 61.516666666666666\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710413918317093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.82598804723496\n",
      "    mean_inference_ms: 1.3162703356159011\n",
      "    mean_raw_obs_processing_ms: 1.5898219822486857\n",
      "  time_since_restore: 5391.973644018173\n",
      "  time_this_iter_s: 21.335156440734863\n",
      "  time_total_s: 5391.973644018173\n",
      "  timers:\n",
      "    learn_throughput: 1553.684\n",
      "    learn_time_ms: 643.632\n",
      "    load_throughput: 54448.004\n",
      "    load_time_ms: 18.366\n",
      "    sample_throughput: 43.509\n",
      "    sample_time_ms: 22983.841\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1633534825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 258\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         5391.97</td><td style=\"text-align: right;\">258000</td><td style=\"text-align: right;\">    2.14</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            399.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 259000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-41-22\n",
      "  done: false\n",
      "  episode_len_mean: 393.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.31\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 600\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.568004768424564\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013379701849715374\n",
      "          policy_loss: -0.07306657623913553\n",
      "          total_loss: 0.5226974303523699\n",
      "          vf_explained_var: 0.6328169703483582\n",
      "          vf_loss: 0.601799797018369\n",
      "    num_agent_steps_sampled: 259000\n",
      "    num_agent_steps_trained: 259000\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.456790123456784\n",
      "    ram_util_percent: 61.49135802469135\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710440663558192\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.834014768574413\n",
      "    mean_inference_ms: 1.3162700729026853\n",
      "    mean_raw_obs_processing_ms: 1.6070768959672554\n",
      "  time_since_restore: 5448.579250335693\n",
      "  time_this_iter_s: 56.60560631752014\n",
      "  time_total_s: 5448.579250335693\n",
      "  timers:\n",
      "    learn_throughput: 1551.2\n",
      "    learn_time_ms: 644.662\n",
      "    load_throughput: 53919.818\n",
      "    load_time_ms: 18.546\n",
      "    sample_throughput: 37.984\n",
      "    sample_time_ms: 26326.957\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633534882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         5448.58</td><td style=\"text-align: right;\">259000</td><td style=\"text-align: right;\">    2.31</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            393.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-41-43\n",
      "  done: false\n",
      "  episode_len_mean: 389.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.44\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 603\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.482285287645128\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012171741590911446\n",
      "          policy_loss: -0.034854304956065285\n",
      "          total_loss: 0.1221585122247537\n",
      "          vf_explained_var: 0.8744493126869202\n",
      "          vf_loss: 0.16306212455448177\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.86896551724138\n",
      "    ram_util_percent: 62.31724137931033\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710477090612208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.842667137466584\n",
      "    mean_inference_ms: 1.3162714225126932\n",
      "    mean_raw_obs_processing_ms: 1.6243518899653728\n",
      "  time_since_restore: 5469.203279733658\n",
      "  time_this_iter_s: 20.624029397964478\n",
      "  time_total_s: 5469.203279733658\n",
      "  timers:\n",
      "    learn_throughput: 1546.462\n",
      "    learn_time_ms: 646.637\n",
      "    load_throughput: 53897.022\n",
      "    load_time_ms: 18.554\n",
      "    sample_throughput: 38.395\n",
      "    sample_time_ms: 26044.863\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1633534903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 260\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">          5469.2</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">    2.44</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            389.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 261000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-42-02\n",
      "  done: false\n",
      "  episode_len_mean: 387.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.47\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 605\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5889564871788024\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012682061630487872\n",
      "          policy_loss: 0.009778976109292773\n",
      "          total_loss: 0.18385834784971344\n",
      "          vf_explained_var: 0.667473554611206\n",
      "          vf_loss: 0.18082753982808855\n",
      "    num_agent_steps_sampled: 261000\n",
      "    num_agent_steps_trained: 261000\n",
      "    num_steps_sampled: 261000\n",
      "    num_steps_trained: 261000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.60714285714285\n",
      "    ram_util_percent: 62.585714285714275\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037104873725116116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.84869967426151\n",
      "    mean_inference_ms: 1.316272862604745\n",
      "    mean_raw_obs_processing_ms: 1.6359360068683884\n",
      "  time_since_restore: 5488.725899934769\n",
      "  time_this_iter_s: 19.52262020111084\n",
      "  time_total_s: 5488.725899934769\n",
      "  timers:\n",
      "    learn_throughput: 1544.866\n",
      "    learn_time_ms: 647.305\n",
      "    load_throughput: 54582.844\n",
      "    load_time_ms: 18.321\n",
      "    sample_throughput: 39.01\n",
      "    sample_time_ms: 25634.47\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1633534922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 261000\n",
      "  training_iteration: 261\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         5488.73</td><td style=\"text-align: right;\">261000</td><td style=\"text-align: right;\">    2.47</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            387.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 262000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-42-24\n",
      "  done: false\n",
      "  episode_len_mean: 384.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.6\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 608\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4819390853246053\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009558608620285616\n",
      "          policy_loss: -0.10634719894991981\n",
      "          total_loss: 0.16439855396747588\n",
      "          vf_explained_var: 0.8179656863212585\n",
      "          vf_loss: 0.27867517471313474\n",
      "    num_agent_steps_sampled: 262000\n",
      "    num_agent_steps_trained: 262000\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.645161290322584\n",
      "    ram_util_percent: 62.39354838709677\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037104773642803825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.85818045093376\n",
      "    mean_inference_ms: 1.3162752597713552\n",
      "    mean_raw_obs_processing_ms: 1.6532459483990478\n",
      "  time_since_restore: 5510.579209804535\n",
      "  time_this_iter_s: 21.853309869766235\n",
      "  time_total_s: 5510.579209804535\n",
      "  timers:\n",
      "    learn_throughput: 1546.676\n",
      "    learn_time_ms: 646.548\n",
      "    load_throughput: 53412.185\n",
      "    load_time_ms: 18.722\n",
      "    sample_throughput: 39.155\n",
      "    sample_time_ms: 25539.21\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1633534944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         5510.58</td><td style=\"text-align: right;\">262000</td><td style=\"text-align: right;\">     2.6</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            384.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 263000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-44-27\n",
      "  done: false\n",
      "  episode_len_mean: 374.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.82\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 613\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4265837497181362\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012667885630335857\n",
      "          policy_loss: 0.11476906140645345\n",
      "          total_loss: 0.4723599268330468\n",
      "          vf_explained_var: 0.8668168783187866\n",
      "          vf_loss: 0.36272552775012123\n",
      "    num_agent_steps_sampled: 263000\n",
      "    num_agent_steps_trained: 263000\n",
      "    num_steps_sampled: 263000\n",
      "    num_steps_trained: 263000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.994886363636365\n",
      "    ram_util_percent: 62.3875\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710469251723426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.875231949168327\n",
      "    mean_inference_ms: 1.3162817248617884\n",
      "    mean_raw_obs_processing_ms: 1.6980216321834845\n",
      "  time_since_restore: 5633.974476575851\n",
      "  time_this_iter_s: 123.39526677131653\n",
      "  time_total_s: 5633.974476575851\n",
      "  timers:\n",
      "    learn_throughput: 1545.926\n",
      "    learn_time_ms: 646.861\n",
      "    load_throughput: 53198.381\n",
      "    load_time_ms: 18.798\n",
      "    sample_throughput: 28.095\n",
      "    sample_time_ms: 35593.701\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1633535067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263000\n",
      "  training_iteration: 263\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         5633.97</td><td style=\"text-align: right;\">263000</td><td style=\"text-align: right;\">    2.82</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             374.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-45-25\n",
      "  done: false\n",
      "  episode_len_mean: 368.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.8\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 616\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.410341027047899\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011673724232452937\n",
      "          policy_loss: -0.014420795854594973\n",
      "          total_loss: 0.24210533127188683\n",
      "          vf_explained_var: 0.8677324652671814\n",
      "          vf_loss: 0.2622149690157837\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.297560975609755\n",
      "    ram_util_percent: 62.46829268292682\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710463980349939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.885972311873864\n",
      "    mean_inference_ms: 1.3162875487299397\n",
      "    mean_raw_obs_processing_ms: 1.7284866497601377\n",
      "  time_since_restore: 5691.195636510849\n",
      "  time_this_iter_s: 57.22115993499756\n",
      "  time_total_s: 5691.195636510849\n",
      "  timers:\n",
      "    learn_throughput: 1545.126\n",
      "    learn_time_ms: 647.197\n",
      "    load_throughput: 52900.34\n",
      "    load_time_ms: 18.903\n",
      "    sample_throughput: 25.393\n",
      "    sample_time_ms: 39380.425\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1633535125\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 264\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">          5691.2</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">     2.8</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            368.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 265000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-46-26\n",
      "  done: false\n",
      "  episode_len_mean: 363.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.9\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 619\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6519897447692022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007945928351349984\n",
      "          policy_loss: -0.10040852112902535\n",
      "          total_loss: 0.22031225967738363\n",
      "          vf_explained_var: 0.8296580910682678\n",
      "          vf_loss: 0.33151315504478085\n",
      "    num_agent_steps_sampled: 265000\n",
      "    num_agent_steps_trained: 265000\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.86590909090909\n",
      "    ram_util_percent: 62.525\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710463432631552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.89728217879519\n",
      "    mean_inference_ms: 1.316295069770284\n",
      "    mean_raw_obs_processing_ms: 1.7632497921878771\n",
      "  time_since_restore: 5752.548577070236\n",
      "  time_this_iter_s: 61.35294055938721\n",
      "  time_total_s: 5752.548577070236\n",
      "  timers:\n",
      "    learn_throughput: 1545.379\n",
      "    learn_time_ms: 647.09\n",
      "    load_throughput: 52633.941\n",
      "    load_time_ms: 18.999\n",
      "    sample_throughput: 23.015\n",
      "    sample_time_ms: 43449.866\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1633535186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 265\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         5752.55</td><td style=\"text-align: right;\">265000</td><td style=\"text-align: right;\">     2.9</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             363.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 266000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-47-27\n",
      "  done: false\n",
      "  episode_len_mean: 358.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.05\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 623\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5323172648747763\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008958193061564214\n",
      "          policy_loss: 0.021396387117500935\n",
      "          total_loss: 0.2012006935560041\n",
      "          vf_explained_var: 0.846246063709259\n",
      "          vf_loss: 0.1886702968014611\n",
      "    num_agent_steps_sampled: 266000\n",
      "    num_agent_steps_trained: 266000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.147674418604645\n",
      "    ram_util_percent: 62.445348837209295\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710466469010487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.912571984152844\n",
      "    mean_inference_ms: 1.31630504444999\n",
      "    mean_raw_obs_processing_ms: 1.8155938891384187\n",
      "  time_since_restore: 5813.227463006973\n",
      "  time_this_iter_s: 60.67888593673706\n",
      "  time_total_s: 5813.227463006973\n",
      "  timers:\n",
      "    learn_throughput: 1545.248\n",
      "    learn_time_ms: 647.145\n",
      "    load_throughput: 52682.268\n",
      "    load_time_ms: 18.982\n",
      "    sample_throughput: 21.047\n",
      "    sample_time_ms: 47511.887\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1633535247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         5813.23</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\">    3.05</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            358.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 267000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-48-17\n",
      "  done: false\n",
      "  episode_len_mean: 358.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.1\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 625\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6427098910013835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010126506831626298\n",
      "          policy_loss: 0.048211582915650475\n",
      "          total_loss: 0.256659109890461\n",
      "          vf_explained_var: 0.5474787950515747\n",
      "          vf_loss: 0.21757530776990786\n",
      "    num_agent_steps_sampled: 267000\n",
      "    num_agent_steps_trained: 267000\n",
      "    num_steps_sampled: 267000\n",
      "    num_steps_trained: 267000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.854166666666668\n",
      "    ram_util_percent: 62.548611111111114\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710468384018566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.920127306579356\n",
      "    mean_inference_ms: 1.316310173306972\n",
      "    mean_raw_obs_processing_ms: 1.8440301971801307\n",
      "  time_since_restore: 5863.391382217407\n",
      "  time_this_iter_s: 50.16391921043396\n",
      "  time_total_s: 5863.391382217407\n",
      "  timers:\n",
      "    learn_throughput: 1549.218\n",
      "    learn_time_ms: 645.487\n",
      "    load_throughput: 52929.981\n",
      "    load_time_ms: 18.893\n",
      "    sample_throughput: 20.573\n",
      "    sample_time_ms: 48606.373\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1633535297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 267000\n",
      "  training_iteration: 267\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         5863.39</td><td style=\"text-align: right;\">267000</td><td style=\"text-align: right;\">     3.1</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             358.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-48-36\n",
      "  done: false\n",
      "  episode_len_mean: 356.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.16\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 627\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3830525914827982\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005963053805006741\n",
      "          policy_loss: -0.13296142025954194\n",
      "          total_loss: -0.0861344626173377\n",
      "          vf_explained_var: 0.4489865303039551\n",
      "          vf_loss: 0.05635923591131965\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.45555555555556\n",
      "    ram_util_percent: 62.38888888888888\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710469817151933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.92767400726261\n",
      "    mean_inference_ms: 1.3163157834733779\n",
      "    mean_raw_obs_processing_ms: 1.8723753752989185\n",
      "  time_since_restore: 5882.261497735977\n",
      "  time_this_iter_s: 18.870115518569946\n",
      "  time_total_s: 5882.261497735977\n",
      "  timers:\n",
      "    learn_throughput: 1548.731\n",
      "    learn_time_ms: 645.69\n",
      "    load_throughput: 53306.22\n",
      "    load_time_ms: 18.76\n",
      "    sample_throughput: 20.678\n",
      "    sample_time_ms: 48359.799\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1633535316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 268\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         5882.26</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">    3.16</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             356.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 269000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-48-56\n",
      "  done: false\n",
      "  episode_len_mean: 357.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.07\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 630\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4340746402740479\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006884376636337998\n",
      "          policy_loss: -0.05769959294961558\n",
      "          total_loss: 0.22116997755236095\n",
      "          vf_explained_var: 0.8122016787528992\n",
      "          vf_loss: 0.28824797063652013\n",
      "    num_agent_steps_sampled: 269000\n",
      "    num_agent_steps_trained: 269000\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.94827586206897\n",
      "    ram_util_percent: 61.82413793103448\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710472898834864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.938938084808857\n",
      "    mean_inference_ms: 1.3163246466888165\n",
      "    mean_raw_obs_processing_ms: 1.9148155571257848\n",
      "  time_since_restore: 5902.38357758522\n",
      "  time_this_iter_s: 20.122079849243164\n",
      "  time_total_s: 5902.38357758522\n",
      "  timers:\n",
      "    learn_throughput: 1547.816\n",
      "    learn_time_ms: 646.072\n",
      "    load_throughput: 52738.905\n",
      "    load_time_ms: 18.961\n",
      "    sample_throughput: 22.366\n",
      "    sample_time_ms: 44710.861\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1633535336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         5902.38</td><td style=\"text-align: right;\">269000</td><td style=\"text-align: right;\">    3.07</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             357.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-49-12\n",
      "  done: false\n",
      "  episode_len_mean: 360.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.02\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 632\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.394374680519104\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010796308837802832\n",
      "          policy_loss: 0.07359701262580023\n",
      "          total_loss: 0.1836264110273785\n",
      "          vf_explained_var: 0.6089895963668823\n",
      "          vf_loss: 0.11619102518177694\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.313043478260866\n",
      "    ram_util_percent: 61.63478260869563\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710472233065116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.946171056793183\n",
      "    mean_inference_ms: 1.3163300159356424\n",
      "    mean_raw_obs_processing_ms: 1.9429816982436967\n",
      "  time_since_restore: 5918.517119884491\n",
      "  time_this_iter_s: 16.13354229927063\n",
      "  time_total_s: 5918.517119884491\n",
      "  timers:\n",
      "    learn_throughput: 1554.036\n",
      "    learn_time_ms: 643.486\n",
      "    load_throughput: 56100.583\n",
      "    load_time_ms: 17.825\n",
      "    sample_throughput: 22.591\n",
      "    sample_time_ms: 44265.556\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633535352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 270\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         5918.52</td><td style=\"text-align: right;\">270000</td><td style=\"text-align: right;\">    3.02</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            360.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 271000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-49-33\n",
      "  done: false\n",
      "  episode_len_mean: 359.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.07\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 634\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.634030709001753\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006378937451827345\n",
      "          policy_loss: -0.020132802261246575\n",
      "          total_loss: 0.2970927477710777\n",
      "          vf_explained_var: 0.5888665914535522\n",
      "          vf_loss: 0.3289678368303511\n",
      "    num_agent_steps_sampled: 271000\n",
      "    num_agent_steps_trained: 271000\n",
      "    num_steps_sampled: 271000\n",
      "    num_steps_trained: 271000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.58333333333334\n",
      "    ram_util_percent: 61.669999999999995\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710470290774783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.953388614734347\n",
      "    mean_inference_ms: 1.3163353959852448\n",
      "    mean_raw_obs_processing_ms: 1.9710570266360259\n",
      "  time_since_restore: 5939.662636041641\n",
      "  time_this_iter_s: 21.14551615715027\n",
      "  time_total_s: 5939.662636041641\n",
      "  timers:\n",
      "    learn_throughput: 1551.456\n",
      "    learn_time_ms: 644.556\n",
      "    load_throughput: 56275.966\n",
      "    load_time_ms: 17.77\n",
      "    sample_throughput: 22.509\n",
      "    sample_time_ms: 44426.836\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1633535373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271000\n",
      "  training_iteration: 271\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         5939.66</td><td style=\"text-align: right;\">271000</td><td style=\"text-align: right;\">    3.07</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            359.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-49-55\n",
      "  done: false\n",
      "  episode_len_mean: 359.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.05\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 637\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7194752083884346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00627906492447191\n",
      "          policy_loss: 0.024589606415894298\n",
      "          total_loss: 0.187931258065833\n",
      "          vf_explained_var: 0.7805097699165344\n",
      "          vf_loss: 0.17601037093748648\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.19354838709677\n",
      "    ram_util_percent: 61.80645161290323\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710468333791206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.964331861207775\n",
      "    mean_inference_ms: 1.3163442771624176\n",
      "    mean_raw_obs_processing_ms: 2.0130335738262333\n",
      "  time_since_restore: 5961.276356220245\n",
      "  time_this_iter_s: 21.613720178604126\n",
      "  time_total_s: 5961.276356220245\n",
      "  timers:\n",
      "    learn_throughput: 1547.991\n",
      "    learn_time_ms: 645.999\n",
      "    load_throughput: 57853.427\n",
      "    load_time_ms: 17.285\n",
      "    sample_throughput: 22.522\n",
      "    sample_time_ms: 44401.931\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1633535395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         5961.28</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">    3.05</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            359.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 273000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 359.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.11\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 640\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5600853403409323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00975085980220377\n",
      "          policy_loss: 0.04476670856691069\n",
      "          total_loss: 0.26269305013120176\n",
      "          vf_explained_var: 0.8297775387763977\n",
      "          vf_loss: 0.22649864852428436\n",
      "    num_agent_steps_sampled: 273000\n",
      "    num_agent_steps_trained: 273000\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.43333333333333\n",
      "    ram_util_percent: 62.05151515151515\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037104630570729914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.975335108640444\n",
      "    mean_inference_ms: 1.3163535345965292\n",
      "    mean_raw_obs_processing_ms: 2.052785923693895\n",
      "  time_since_restore: 5984.537728071213\n",
      "  time_this_iter_s: 23.261371850967407\n",
      "  time_total_s: 5984.537728071213\n",
      "  timers:\n",
      "    learn_throughput: 1546.898\n",
      "    learn_time_ms: 646.455\n",
      "    load_throughput: 57348.279\n",
      "    load_time_ms: 17.437\n",
      "    sample_throughput: 29.08\n",
      "    sample_time_ms: 34387.937\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633535418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 273\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         5984.54</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\">    3.11</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            359.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 274000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-51-21\n",
      "  done: false\n",
      "  episode_len_mean: 355.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.23\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 643\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5256856189833747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009733604981716423\n",
      "          policy_loss: -0.14409605372283193\n",
      "          total_loss: 0.07864438547856278\n",
      "          vf_explained_var: 0.7115103602409363\n",
      "          vf_loss: 0.23098118597020706\n",
      "    num_agent_steps_sampled: 274000\n",
      "    num_agent_steps_trained: 274000\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.692134831460677\n",
      "    ram_util_percent: 62.1370786516854\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037104574160971605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.98585335361833\n",
      "    mean_inference_ms: 1.3163608292669864\n",
      "    mean_raw_obs_processing_ms: 2.0969462485965256\n",
      "  time_since_restore: 6047.157415866852\n",
      "  time_this_iter_s: 62.61968779563904\n",
      "  time_total_s: 6047.157415866852\n",
      "  timers:\n",
      "    learn_throughput: 1552.276\n",
      "    learn_time_ms: 644.215\n",
      "    load_throughput: 57968.886\n",
      "    load_time_ms: 17.251\n",
      "    sample_throughput: 28.628\n",
      "    sample_time_ms: 34930.237\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1633535481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 274\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         6047.16</td><td style=\"text-align: right;\">274000</td><td style=\"text-align: right;\">    3.23</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            355.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 275000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-52-50\n",
      "  done: false\n",
      "  episode_len_mean: 351.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.29\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 647\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.624490745862325\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011854845908697195\n",
      "          policy_loss: 0.062030095193121165\n",
      "          total_loss: 0.23066684442261856\n",
      "          vf_explained_var: 0.9107239842414856\n",
      "          vf_loss: 0.1763365244285928\n",
      "    num_agent_steps_sampled: 275000\n",
      "    num_agent_steps_trained: 275000\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.09375\n",
      "    ram_util_percent: 62.73515625000002\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710445072494949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.99966809822256\n",
      "    mean_inference_ms: 1.3163672526913555\n",
      "    mean_raw_obs_processing_ms: 2.1621368547423385\n",
      "  time_since_restore: 6136.378744363785\n",
      "  time_this_iter_s: 89.22132849693298\n",
      "  time_total_s: 6136.378744363785\n",
      "  timers:\n",
      "    learn_throughput: 1550.905\n",
      "    learn_time_ms: 644.785\n",
      "    load_throughput: 58397.865\n",
      "    load_time_ms: 17.124\n",
      "    sample_throughput: 26.514\n",
      "    sample_time_ms: 37716.614\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633535570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         6136.38</td><td style=\"text-align: right;\">275000</td><td style=\"text-align: right;\">    3.29</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             351.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-53-10\n",
      "  done: false\n",
      "  episode_len_mean: 355.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.26\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 649\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.528537486659156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008848168754145489\n",
      "          policy_loss: -0.0019972315678993863\n",
      "          total_loss: 0.13412226852443482\n",
      "          vf_explained_var: 0.8560252785682678\n",
      "          vf_loss: 0.14502700029147997\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0\n",
      "    ram_util_percent: 62.57931034482757\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037104340548971146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.006401776053746\n",
      "    mean_inference_ms: 1.316369880128451\n",
      "    mean_raw_obs_processing_ms: 2.191981455538061\n",
      "  time_since_restore: 6156.600855588913\n",
      "  time_this_iter_s: 20.222111225128174\n",
      "  time_total_s: 6156.600855588913\n",
      "  timers:\n",
      "    learn_throughput: 1550.592\n",
      "    learn_time_ms: 644.915\n",
      "    load_throughput: 58327.05\n",
      "    load_time_ms: 17.145\n",
      "    sample_throughput: 29.699\n",
      "    sample_time_ms: 33670.752\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1633535590\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 276\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">          6156.6</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">    3.26</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            355.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 277000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-53-31\n",
      "  done: false\n",
      "  episode_len_mean: 355.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.16\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 652\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3957236303223504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009258410773163916\n",
      "          policy_loss: -0.023570441951354344\n",
      "          total_loss: 0.13767235146628487\n",
      "          vf_explained_var: 0.8353255391120911\n",
      "          vf_loss: 0.16852644756436347\n",
      "    num_agent_steps_sampled: 277000\n",
      "    num_agent_steps_trained: 277000\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.78965517241379\n",
      "    ram_util_percent: 62.16551724137933\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710411363883179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.016113257988106\n",
      "    mean_inference_ms: 1.3163724920319106\n",
      "    mean_raw_obs_processing_ms: 2.235172082664153\n",
      "  time_since_restore: 6177.473753452301\n",
      "  time_this_iter_s: 20.87289786338806\n",
      "  time_total_s: 6177.473753452301\n",
      "  timers:\n",
      "    learn_throughput: 1548.897\n",
      "    learn_time_ms: 645.621\n",
      "    load_throughput: 57486.931\n",
      "    load_time_ms: 17.395\n",
      "    sample_throughput: 32.53\n",
      "    sample_time_ms: 30740.693\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1633535611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         6177.47</td><td style=\"text-align: right;\">277000</td><td style=\"text-align: right;\">    3.16</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            355.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 278000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-53-51\n",
      "  done: false\n",
      "  episode_len_mean: 356.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.19\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 654\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5167070653703478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010807097396145604\n",
      "          policy_loss: 0.00459175631403923\n",
      "          total_loss: 0.19467181091507277\n",
      "          vf_explained_var: 0.8160535097122192\n",
      "          vf_loss: 0.19745722694529427\n",
      "    num_agent_steps_sampled: 278000\n",
      "    num_agent_steps_trained: 278000\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.41379310344828\n",
      "    ram_util_percent: 61.73103448275862\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710396934406631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.02235470653406\n",
      "    mean_inference_ms: 1.3163734787975148\n",
      "    mean_raw_obs_processing_ms: 2.2638545693806393\n",
      "  time_since_restore: 6197.193759202957\n",
      "  time_this_iter_s: 19.720005750656128\n",
      "  time_total_s: 6197.193759202957\n",
      "  timers:\n",
      "    learn_throughput: 1549.238\n",
      "    learn_time_ms: 645.479\n",
      "    load_throughput: 57449.136\n",
      "    load_time_ms: 17.407\n",
      "    sample_throughput: 32.44\n",
      "    sample_time_ms: 30825.821\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1633535631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 278\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         6197.19</td><td style=\"text-align: right;\">278000</td><td style=\"text-align: right;\">    3.19</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            356.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 279000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-54-14\n",
      "  done: false\n",
      "  episode_len_mean: 357.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.28\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 657\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5745687007904052\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012101206972253328\n",
      "          policy_loss: -0.13586597707536485\n",
      "          total_loss: -0.0069079882775743805\n",
      "          vf_explained_var: 0.8837577700614929\n",
      "          vf_loss: 0.1359809694604741\n",
      "    num_agent_steps_sampled: 279000\n",
      "    num_agent_steps_trained: 279000\n",
      "    num_steps_sampled: 279000\n",
      "    num_steps_trained: 279000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.475\n",
      "    ram_util_percent: 61.7\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710375511493757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.031639369355638\n",
      "    mean_inference_ms: 1.3163744623418958\n",
      "    mean_raw_obs_processing_ms: 2.3067163157375097\n",
      "  time_since_restore: 6219.95615530014\n",
      "  time_this_iter_s: 22.762396097183228\n",
      "  time_total_s: 6219.95615530014\n",
      "  timers:\n",
      "    learn_throughput: 1551.065\n",
      "    learn_time_ms: 644.718\n",
      "    load_throughput: 58090.439\n",
      "    load_time_ms: 17.215\n",
      "    sample_throughput: 32.164\n",
      "    sample_time_ms: 31090.83\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1633535654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279000\n",
      "  training_iteration: 279\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         6219.96</td><td style=\"text-align: right;\">279000</td><td style=\"text-align: right;\">    3.28</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             357.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-55-33\n",
      "  done: false\n",
      "  episode_len_mean: 352.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.4\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 661\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4741780042648316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010072419594918086\n",
      "          policy_loss: -0.1703796790705787\n",
      "          total_loss: 0.06806797397633393\n",
      "          vf_explained_var: 0.8001817464828491\n",
      "          vf_loss: 0.24592910317911043\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.107964601769915\n",
      "    ram_util_percent: 62.04247787610618\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710348460479843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.043262136646778\n",
      "    mean_inference_ms: 1.3163756315793886\n",
      "    mean_raw_obs_processing_ms: 2.372027175789217\n",
      "  time_since_restore: 6298.791226148605\n",
      "  time_this_iter_s: 78.83507084846497\n",
      "  time_total_s: 6298.791226148605\n",
      "  timers:\n",
      "    learn_throughput: 1548.431\n",
      "    learn_time_ms: 645.815\n",
      "    load_throughput: 54644.854\n",
      "    load_time_ms: 18.3\n",
      "    sample_throughput: 26.767\n",
      "    sample_time_ms: 37358.811\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1633535733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         6298.79</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">     3.4</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             352.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 281000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-55-55\n",
      "  done: false\n",
      "  episode_len_mean: 354.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.38\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 664\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4143705368041992\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011758093137970329\n",
      "          policy_loss: 0.061229637430773844\n",
      "          total_loss: 0.24058679615457854\n",
      "          vf_explained_var: 0.8527283668518066\n",
      "          vf_loss: 0.18502547736797068\n",
      "    num_agent_steps_sampled: 281000\n",
      "    num_agent_steps_trained: 281000\n",
      "    num_steps_sampled: 281000\n",
      "    num_steps_trained: 281000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.603125\n",
      "    ram_util_percent: 62.596875\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710331788376173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.051729566473583\n",
      "    mean_inference_ms: 1.3163763603028356\n",
      "    mean_raw_obs_processing_ms: 2.4170435439842026\n",
      "  time_since_restore: 6321.313596010208\n",
      "  time_this_iter_s: 22.522369861602783\n",
      "  time_total_s: 6321.313596010208\n",
      "  timers:\n",
      "    learn_throughput: 1547.927\n",
      "    learn_time_ms: 646.025\n",
      "    load_throughput: 54406.969\n",
      "    load_time_ms: 18.38\n",
      "    sample_throughput: 26.669\n",
      "    sample_time_ms: 37496.165\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1633535755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281000\n",
      "  training_iteration: 281\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         6321.31</td><td style=\"text-align: right;\">281000</td><td style=\"text-align: right;\">    3.38</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            354.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 282000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-56-57\n",
      "  done: false\n",
      "  episode_len_mean: 351.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.4\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 667\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4574663546350268\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006475855048009017\n",
      "          policy_loss: -0.06720246287683646\n",
      "          total_loss: 0.1080109816768931\n",
      "          vf_explained_var: 0.869222104549408\n",
      "          vf_loss: 0.1851202296713988\n",
      "    num_agent_steps_sampled: 282000\n",
      "    num_agent_steps_trained: 282000\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.104545454545452\n",
      "    ram_util_percent: 61.83295454545455\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710317370473476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.059916415880735\n",
      "    mean_inference_ms: 1.3163767083259668\n",
      "    mean_raw_obs_processing_ms: 2.4643592996851806\n",
      "  time_since_restore: 6382.923136472702\n",
      "  time_this_iter_s: 61.6095404624939\n",
      "  time_total_s: 6382.923136472702\n",
      "  timers:\n",
      "    learn_throughput: 1552.641\n",
      "    learn_time_ms: 644.064\n",
      "    load_throughput: 54428.433\n",
      "    load_time_ms: 18.373\n",
      "    sample_throughput: 24.098\n",
      "    sample_time_ms: 41497.705\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1633535817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         6382.92</td><td style=\"text-align: right;\">282000</td><td style=\"text-align: right;\">     3.4</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            351.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 283000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-57-21\n",
      "  done: false\n",
      "  episode_len_mean: 350.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.48\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 670\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3819619099299112\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007265701144803908\n",
      "          policy_loss: -0.11043927189376619\n",
      "          total_loss: 0.034486665783656965\n",
      "          vf_explained_var: 0.8872184753417969\n",
      "          vf_loss: 0.15350834735565716\n",
      "    num_agent_steps_sampled: 283000\n",
      "    num_agent_steps_trained: 283000\n",
      "    num_steps_sampled: 283000\n",
      "    num_steps_trained: 283000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.3235294117647\n",
      "    ram_util_percent: 62.329411764705874\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710300219315396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.068229435600394\n",
      "    mean_inference_ms: 1.3163769320767784\n",
      "    mean_raw_obs_processing_ms: 2.5115970022314804\n",
      "  time_since_restore: 6407.075209379196\n",
      "  time_this_iter_s: 24.15207290649414\n",
      "  time_total_s: 6407.075209379196\n",
      "  timers:\n",
      "    learn_throughput: 1553.972\n",
      "    learn_time_ms: 643.512\n",
      "    load_throughput: 54948.822\n",
      "    load_time_ms: 18.199\n",
      "    sample_throughput: 24.046\n",
      "    sample_time_ms: 41587.501\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633535841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 283000\n",
      "  training_iteration: 283\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         6407.08</td><td style=\"text-align: right;\">283000</td><td style=\"text-align: right;\">    3.48</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            350.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-57-41\n",
      "  done: false\n",
      "  episode_len_mean: 350.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.5\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 673\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.327705935637156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01121060131859445\n",
      "          policy_loss: -0.038133108533091015\n",
      "          total_loss: 0.1822519310646587\n",
      "          vf_explained_var: 0.7423457503318787\n",
      "          vf_loss: 0.2255813534061114\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.63333333333334\n",
      "    ram_util_percent: 62.55666666666665\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037102831061567684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.07617902530702\n",
      "    mean_inference_ms: 1.3163760037312613\n",
      "    mean_raw_obs_processing_ms: 2.5586883403379486\n",
      "  time_since_restore: 6427.58362197876\n",
      "  time_this_iter_s: 20.5084125995636\n",
      "  time_total_s: 6427.58362197876\n",
      "  timers:\n",
      "    learn_throughput: 1554.285\n",
      "    learn_time_ms: 643.383\n",
      "    load_throughput: 54428.009\n",
      "    load_time_ms: 18.373\n",
      "    sample_throughput: 26.755\n",
      "    sample_time_ms: 37376.308\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1633535861\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         6427.58</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">     3.5</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            350.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 349.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.59\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 676\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.370302599006229\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021748061639625386\n",
      "          policy_loss: -0.004416384796301524\n",
      "          total_loss: 0.34703451540941993\n",
      "          vf_explained_var: 0.5058361291885376\n",
      "          vf_loss: 0.349477647865812\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.68461538461538\n",
      "    ram_util_percent: 62.46153846153845\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037102638972361685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.083844755319276\n",
      "    mean_inference_ms: 1.3163744704811418\n",
      "    mean_raw_obs_processing_ms: 2.6101359019689223\n",
      "  time_since_restore: 6491.898221731186\n",
      "  time_this_iter_s: 64.31459975242615\n",
      "  time_total_s: 6491.898221731186\n",
      "  timers:\n",
      "    learn_throughput: 1559.021\n",
      "    learn_time_ms: 641.428\n",
      "    load_throughput: 53586.195\n",
      "    load_time_ms: 18.662\n",
      "    sample_throughput: 28.664\n",
      "    sample_time_ms: 34887.303\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1633535926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 285\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">          6491.9</td><td style=\"text-align: right;\">285000</td><td style=\"text-align: right;\">    3.59</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            349.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 286000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_15-59-07\n",
      "  done: false\n",
      "  episode_len_mean: 350.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.58\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 679\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0812194824218753\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5642086002561781\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008496297681765854\n",
      "          policy_loss: 0.011351447221305636\n",
      "          total_loss: 0.15297594029042455\n",
      "          vf_explained_var: 0.9022506475448608\n",
      "          vf_loss: 0.14808021742436622\n",
      "    num_agent_steps_sampled: 286000\n",
      "    num_agent_steps_trained: 286000\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.52333333333333\n",
      "    ram_util_percent: 62.786666666666655\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710243411387791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.091145198947455\n",
      "    mean_inference_ms: 1.3163724207081102\n",
      "    mean_raw_obs_processing_ms: 2.6614272215492405\n",
      "  time_since_restore: 6512.912140369415\n",
      "  time_this_iter_s: 21.01391863822937\n",
      "  time_total_s: 6512.912140369415\n",
      "  timers:\n",
      "    learn_throughput: 1563.434\n",
      "    learn_time_ms: 639.618\n",
      "    load_throughput: 53434.708\n",
      "    load_time_ms: 18.714\n",
      "    sample_throughput: 28.597\n",
      "    sample_time_ms: 34968.283\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633535947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         6512.91</td><td style=\"text-align: right;\">286000</td><td style=\"text-align: right;\">    3.58</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            350.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 287000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 349.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.56\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 681\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0812194824218753\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4348193248112997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02017245132526989\n",
      "          policy_loss: -0.010848510969016288\n",
      "          total_loss: 0.4839755290912257\n",
      "          vf_explained_var: 0.7992798686027527\n",
      "          vf_loss: 0.48736138625277414\n",
      "    num_agent_steps_sampled: 287000\n",
      "    num_agent_steps_trained: 287000\n",
      "    num_steps_sampled: 287000\n",
      "    num_steps_trained: 287000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.432499999999997\n",
      "    ram_util_percent: 62.334999999999994\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710230451548753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.095695316206573\n",
      "    mean_inference_ms: 1.316370397480349\n",
      "    mean_raw_obs_processing_ms: 2.698089413717001\n",
      "  time_since_restore: 6568.809199094772\n",
      "  time_this_iter_s: 55.897058725357056\n",
      "  time_total_s: 6568.809199094772\n",
      "  timers:\n",
      "    learn_throughput: 1565.31\n",
      "    learn_time_ms: 638.851\n",
      "    load_throughput: 53632.172\n",
      "    load_time_ms: 18.646\n",
      "    sample_throughput: 25.993\n",
      "    sample_time_ms: 38471.506\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1633536003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287000\n",
      "  training_iteration: 287\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         6568.81</td><td style=\"text-align: right;\">287000</td><td style=\"text-align: right;\">    3.56</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            349.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-00-23\n",
      "  done: false\n",
      "  episode_len_mean: 350.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.58\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 684\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6218292236328118\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0346508151955074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0046258398056348285\n",
      "          policy_loss: -0.1308536926905314\n",
      "          total_loss: -0.009126826375722884\n",
      "          vf_explained_var: 0.7644844055175781\n",
      "          vf_loss: 0.12457105360097355\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29655172413793\n",
      "    ram_util_percent: 62.7551724137931\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710217673393336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.10213692982186\n",
      "    mean_inference_ms: 1.3163671586896941\n",
      "    mean_raw_obs_processing_ms: 2.7529151414155795\n",
      "  time_since_restore: 6588.7105531692505\n",
      "  time_this_iter_s: 19.90135407447815\n",
      "  time_total_s: 6588.7105531692505\n",
      "  timers:\n",
      "    learn_throughput: 1563.415\n",
      "    learn_time_ms: 639.626\n",
      "    load_throughput: 53793.818\n",
      "    load_time_ms: 18.589\n",
      "    sample_throughput: 25.982\n",
      "    sample_time_ms: 38488.911\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1633536023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 288\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         6588.71</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">    3.58</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            350.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 289000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-00-38\n",
      "  done: false\n",
      "  episode_len_mean: 352.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.64\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 686\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0668544054031373\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007754034035629647\n",
      "          policy_loss: -0.11402069545454449\n",
      "          total_loss: 0.027680615045958094\n",
      "          vf_explained_var: 0.7819294333457947\n",
      "          vf_loss: 0.14608199667159674\n",
      "    num_agent_steps_sampled: 289000\n",
      "    num_agent_steps_trained: 289000\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.71818181818182\n",
      "    ram_util_percent: 62.80909090909092\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710208908981097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.106151831404272\n",
      "    mean_inference_ms: 1.316364170297275\n",
      "    mean_raw_obs_processing_ms: 2.7893518721769954\n",
      "  time_since_restore: 6604.192917108536\n",
      "  time_this_iter_s: 15.482363939285278\n",
      "  time_total_s: 6604.192917108536\n",
      "  timers:\n",
      "    learn_throughput: 1562.102\n",
      "    learn_time_ms: 640.163\n",
      "    load_throughput: 58419.257\n",
      "    load_time_ms: 17.118\n",
      "    sample_throughput: 26.482\n",
      "    sample_time_ms: 37761.805\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1633536038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         6604.19</td><td style=\"text-align: right;\">289000</td><td style=\"text-align: right;\">    3.64</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            352.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 290000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-01-32\n",
      "  done: false\n",
      "  episode_len_mean: 351.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.56\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 689\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2376967244678074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021187483656466395\n",
      "          policy_loss: 0.12317179549071523\n",
      "          total_loss: 0.38849691030465894\n",
      "          vf_explained_var: 0.5116358399391174\n",
      "          vf_loss: 0.26052084436847106\n",
      "    num_agent_steps_sampled: 290000\n",
      "    num_agent_steps_trained: 290000\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.196052631578947\n",
      "    ram_util_percent: 62.5078947368421\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037101864663770155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.11194177671043\n",
      "    mean_inference_ms: 1.3163588044503043\n",
      "    mean_raw_obs_processing_ms: 2.8474536375126784\n",
      "  time_since_restore: 6657.9695036411285\n",
      "  time_this_iter_s: 53.77658653259277\n",
      "  time_total_s: 6657.9695036411285\n",
      "  timers:\n",
      "    learn_throughput: 1561.615\n",
      "    learn_time_ms: 640.363\n",
      "    load_throughput: 58272.432\n",
      "    load_time_ms: 17.161\n",
      "    sample_throughput: 28.364\n",
      "    sample_time_ms: 35255.702\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1633536092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 290\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         6657.97</td><td style=\"text-align: right;\">290000</td><td style=\"text-align: right;\">    3.56</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            351.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 291000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-02-33\n",
      "  done: false\n",
      "  episode_len_mean: 349.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.52\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 692\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2163719177246097\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3433722694714865\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01037722179817939\n",
      "          policy_loss: 0.07389433946874406\n",
      "          total_loss: 0.31041123213039507\n",
      "          vf_explained_var: 0.6135828495025635\n",
      "          vf_loss: 0.23732805794311895\n",
      "    num_agent_steps_sampled: 291000\n",
      "    num_agent_steps_trained: 291000\n",
      "    num_steps_sampled: 291000\n",
      "    num_steps_trained: 291000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.206896551724142\n",
      "    ram_util_percent: 62.9333333333333\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371015583932568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.117822238164777\n",
      "    mean_inference_ms: 1.3163537013980235\n",
      "    mean_raw_obs_processing_ms: 2.9094813920999147\n",
      "  time_since_restore: 6718.9287276268005\n",
      "  time_this_iter_s: 60.959223985672\n",
      "  time_total_s: 6718.9287276268005\n",
      "  timers:\n",
      "    learn_throughput: 1565.599\n",
      "    learn_time_ms: 638.733\n",
      "    load_throughput: 58176.169\n",
      "    load_time_ms: 17.189\n",
      "    sample_throughput: 25.575\n",
      "    sample_time_ms: 39101.019\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633536153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291000\n",
      "  training_iteration: 291\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         6718.93</td><td style=\"text-align: right;\">291000</td><td style=\"text-align: right;\">    3.52</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            349.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 346.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.57\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 695\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2163719177246097\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2036692414018844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008226369288330204\n",
      "          policy_loss: -0.3239123629199134\n",
      "          total_loss: 0.10257713649835852\n",
      "          vf_explained_var: 0.60862797498703\n",
      "          vf_loss: 0.4285198698441188\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.50238095238095\n",
      "    ram_util_percent: 62.82380952380953\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710123767126008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.123678485966284\n",
      "    mean_inference_ms: 1.3163495951996647\n",
      "    mean_raw_obs_processing_ms: 2.972976755159633\n",
      "  time_since_restore: 6777.148218631744\n",
      "  time_this_iter_s: 58.21949100494385\n",
      "  time_total_s: 6777.148218631744\n",
      "  timers:\n",
      "    learn_throughput: 1562.066\n",
      "    learn_time_ms: 640.178\n",
      "    load_throughput: 58232.87\n",
      "    load_time_ms: 17.172\n",
      "    sample_throughput: 25.799\n",
      "    sample_time_ms: 38760.588\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1633536211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         6777.15</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">    3.57</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            346.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 293000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-03-47\n",
      "  done: false\n",
      "  episode_len_mean: 350.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.33\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 697\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2163719177246097\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1119874960846372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009730506883105377\n",
      "          policy_loss: -0.059902429083983105\n",
      "          total_loss: 0.12491568449056811\n",
      "          vf_explained_var: 0.44606316089630127\n",
      "          vf_loss: 0.18410207620925373\n",
      "    num_agent_steps_sampled: 293000\n",
      "    num_agent_steps_trained: 293000\n",
      "    num_steps_sampled: 293000\n",
      "    num_steps_trained: 293000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.98636363636364\n",
      "    ram_util_percent: 62.709090909090904\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710098985297877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.127248576791484\n",
      "    mean_inference_ms: 1.3163461462437953\n",
      "    mean_raw_obs_processing_ms: 3.0151321783080527\n",
      "  time_since_restore: 6793.212619066238\n",
      "  time_this_iter_s: 16.06440043449402\n",
      "  time_total_s: 6793.212619066238\n",
      "  timers:\n",
      "    learn_throughput: 1564.857\n",
      "    learn_time_ms: 639.036\n",
      "    load_throughput: 60568.966\n",
      "    load_time_ms: 16.51\n",
      "    sample_throughput: 26.348\n",
      "    sample_time_ms: 37953.624\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1633536227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 293000\n",
      "  training_iteration: 293\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         6793.21</td><td style=\"text-align: right;\">293000</td><td style=\"text-align: right;\">    3.33</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            350.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 294000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-05-31\n",
      "  done: false\n",
      "  episode_len_mean: 344.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.45\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 702\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2163719177246097\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.124357400337855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004028810953653463\n",
      "          policy_loss: 0.18653010775645573\n",
      "          total_loss: 0.2955810974041621\n",
      "          vf_explained_var: 0.8182350993156433\n",
      "          vf_loss: 0.11539403159792225\n",
      "    num_agent_steps_sampled: 294000\n",
      "    num_agent_steps_trained: 294000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.861486486486488\n",
      "    ram_util_percent: 62.21554054054054\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710034035421248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.13560673826602\n",
      "    mean_inference_ms: 1.3163354515305699\n",
      "    mean_raw_obs_processing_ms: 3.1278968644771963\n",
      "  time_since_restore: 6896.764444828033\n",
      "  time_this_iter_s: 103.55182576179504\n",
      "  time_total_s: 6896.764444828033\n",
      "  timers:\n",
      "    learn_throughput: 1560.953\n",
      "    learn_time_ms: 640.634\n",
      "    load_throughput: 60508.413\n",
      "    load_time_ms: 16.527\n",
      "    sample_throughput: 21.619\n",
      "    sample_time_ms: 46256.34\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1633536331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 294\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         6896.76</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\">    3.45</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            344.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 295000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 344.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.36\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 704\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3055957516034444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011349736959017222\n",
      "          policy_loss: -0.28223281113637816\n",
      "          total_loss: -0.0009646870195865632\n",
      "          vf_explained_var: 0.7192257642745972\n",
      "          vf_loss: 0.2874213377220763\n",
      "    num_agent_steps_sampled: 295000\n",
      "    num_agent_steps_trained: 295000\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.34444444444444\n",
      "    ram_util_percent: 62.777777777777786\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710006180542703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.1388311158278\n",
      "    mean_inference_ms: 1.3163308192553547\n",
      "    mean_raw_obs_processing_ms: 3.1728781082775908\n",
      "  time_since_restore: 6915.4778616428375\n",
      "  time_this_iter_s: 18.713416814804077\n",
      "  time_total_s: 6915.4778616428375\n",
      "  timers:\n",
      "    learn_throughput: 1560.654\n",
      "    learn_time_ms: 640.757\n",
      "    load_throughput: 61725.874\n",
      "    load_time_ms: 16.201\n",
      "    sample_throughput: 23.983\n",
      "    sample_time_ms: 41696.444\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1633536350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         6915.48</td><td style=\"text-align: right;\">295000</td><td style=\"text-align: right;\">    3.36</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            344.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-06-47\n",
      "  done: false\n",
      "  episode_len_mean: 343.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.14\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 707\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2408648755815295\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015931016084490126\n",
      "          policy_loss: -0.03347990976439582\n",
      "          total_loss: 0.3129203280640973\n",
      "          vf_explained_var: 0.4957929849624634\n",
      "          vf_loss: 0.34911986506647535\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.17530864197531\n",
      "    ram_util_percent: 62.51481481481481\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037099668040098004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.14325778729969\n",
      "    mean_inference_ms: 1.3163235356285654\n",
      "    mean_raw_obs_processing_ms: 3.2441904986557226\n",
      "  time_since_restore: 6972.536374807358\n",
      "  time_this_iter_s: 57.058513164520264\n",
      "  time_total_s: 6972.536374807358\n",
      "  timers:\n",
      "    learn_throughput: 1559.19\n",
      "    learn_time_ms: 641.359\n",
      "    load_throughput: 61302.757\n",
      "    load_time_ms: 16.312\n",
      "    sample_throughput: 22.075\n",
      "    sample_time_ms: 45300.174\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633536407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 296\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         6972.54</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">    3.14</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            343.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 297000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-09-05\n",
      "  done: false\n",
      "  episode_len_mean: 335.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.33\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 714\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2590063585175408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0135673984024952\n",
      "          policy_loss: 0.06766288015577528\n",
      "          total_loss: 0.49781326833698486\n",
      "          vf_explained_var: 0.9314782619476318\n",
      "          vf_loss: 0.4344889546434085\n",
      "    num_agent_steps_sampled: 297000\n",
      "    num_agent_steps_trained: 297000\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.8989898989899\n",
      "    ram_util_percent: 62.68535353535354\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709880590132766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.151917846581142\n",
      "    mean_inference_ms: 1.316304595029676\n",
      "    mean_raw_obs_processing_ms: 3.414552994286148\n",
      "  time_since_restore: 7110.973558187485\n",
      "  time_this_iter_s: 138.43718338012695\n",
      "  time_total_s: 7110.973558187485\n",
      "  timers:\n",
      "    learn_throughput: 1559.737\n",
      "    learn_time_ms: 641.134\n",
      "    load_throughput: 61692.645\n",
      "    load_time_ms: 16.209\n",
      "    sample_throughput: 18.673\n",
      "    sample_time_ms: 53554.502\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1633536545\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 297\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         7110.97</td><td style=\"text-align: right;\">297000</td><td style=\"text-align: right;\">    3.33</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            335.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 298000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-09-49\n",
      "  done: false\n",
      "  episode_len_mean: 337.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.23\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 716\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2157577481534747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024426871905718597\n",
      "          policy_loss: 0.07369429336653815\n",
      "          total_loss: 0.6207766611542966\n",
      "          vf_explained_var: 0.6565805077552795\n",
      "          vf_loss: 0.5443838660915693\n",
      "    num_agent_steps_sampled: 298000\n",
      "    num_agent_steps_trained: 298000\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.20793650793651\n",
      "    ram_util_percent: 62.70634920634923\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709855991968346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.153940596716946\n",
      "    mean_inference_ms: 1.3162982200246462\n",
      "    mean_raw_obs_processing_ms: 3.461605654064997\n",
      "  time_since_restore: 7155.197904586792\n",
      "  time_this_iter_s: 44.22434639930725\n",
      "  time_total_s: 7155.197904586792\n",
      "  timers:\n",
      "    learn_throughput: 1557.472\n",
      "    learn_time_ms: 642.066\n",
      "    load_throughput: 61306.521\n",
      "    load_time_ms: 16.311\n",
      "    sample_throughput: 17.862\n",
      "    sample_time_ms: 55985.77\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1633536589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">          7155.2</td><td style=\"text-align: right;\">298000</td><td style=\"text-align: right;\">    3.23</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            337.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 299000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-11-50\n",
      "  done: false\n",
      "  episode_len_mean: 327.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.34\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 724\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2551017264525095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00947064329585159\n",
      "          policy_loss: -0.0698968936999639\n",
      "          total_loss: 0.3935093881562352\n",
      "          vf_explained_var: 0.6252464056015015\n",
      "          vf_loss: 0.46731742877099247\n",
      "    num_agent_steps_sampled: 299000\n",
      "    num_agent_steps_trained: 299000\n",
      "    num_steps_sampled: 299000\n",
      "    num_steps_trained: 299000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.58554913294797\n",
      "    ram_util_percent: 62.845086705202306\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709749822632979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.161609127166397\n",
      "    mean_inference_ms: 1.3162726598053436\n",
      "    mean_raw_obs_processing_ms: 3.655775568362261\n",
      "  time_since_restore: 7276.129480600357\n",
      "  time_this_iter_s: 120.93157601356506\n",
      "  time_total_s: 7276.129480600357\n",
      "  timers:\n",
      "    learn_throughput: 1560.124\n",
      "    learn_time_ms: 640.975\n",
      "    load_throughput: 56002.083\n",
      "    load_time_ms: 17.856\n",
      "    sample_throughput: 15.031\n",
      "    sample_time_ms: 66530.249\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1633536710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299000\n",
      "  training_iteration: 299\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         7276.13</td><td style=\"text-align: right;\">299000</td><td style=\"text-align: right;\">    3.34</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            327.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-12-05\n",
      "  done: false\n",
      "  episode_len_mean: 327.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.16\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 725\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4623382012049357\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01688666004115053\n",
      "          policy_loss: 0.10404444336891175\n",
      "          total_loss: 0.4688803907897737\n",
      "          vf_explained_var: 0.5862985253334045\n",
      "          vf_loss: 0.36405398638712033\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53809523809524\n",
      "    ram_util_percent: 62.90000000000001\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709736771634212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.162412543070744\n",
      "    mean_inference_ms: 1.31626932898278\n",
      "    mean_raw_obs_processing_ms: 3.678408835110951\n",
      "  time_since_restore: 7290.960682630539\n",
      "  time_this_iter_s: 14.831202030181885\n",
      "  time_total_s: 7290.960682630539\n",
      "  timers:\n",
      "    learn_throughput: 1560.769\n",
      "    learn_time_ms: 640.71\n",
      "    load_throughput: 61171.865\n",
      "    load_time_ms: 16.347\n",
      "    sample_throughput: 15.965\n",
      "    sample_time_ms: 62637.496\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1633536725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 300\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         7290.96</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">    3.16</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            327.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 301000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-13-20\n",
      "  done: false\n",
      "  episode_len_mean: 324.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.15\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 729\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4522547509935166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011868962232623126\n",
      "          policy_loss: -0.07418458147181405\n",
      "          total_loss: 0.24709044208543168\n",
      "          vf_explained_var: 0.7495694756507874\n",
      "          vf_loss: 0.32496976984871756\n",
      "    num_agent_steps_sampled: 301000\n",
      "    num_agent_steps_trained: 301000\n",
      "    num_steps_sampled: 301000\n",
      "    num_steps_trained: 301000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.651401869158878\n",
      "    ram_util_percent: 62.7467289719626\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370968691200384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.16513184334393\n",
      "    mean_inference_ms: 1.3162563143820358\n",
      "    mean_raw_obs_processing_ms: 3.776727741560169\n",
      "  time_since_restore: 7365.643299341202\n",
      "  time_this_iter_s: 74.68261671066284\n",
      "  time_total_s: 7365.643299341202\n",
      "  timers:\n",
      "    learn_throughput: 1560.236\n",
      "    learn_time_ms: 640.929\n",
      "    load_throughput: 60195.701\n",
      "    load_time_ms: 16.612\n",
      "    sample_throughput: 15.623\n",
      "    sample_time_ms: 64009.356\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633536800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 301000\n",
      "  training_iteration: 301\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         7365.64</td><td style=\"text-align: right;\">301000</td><td style=\"text-align: right;\">    3.15</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            324.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 302000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 285.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.82\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 742\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1746571017636194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0164227700727474\n",
      "          policy_loss: 0.0027149746815363566\n",
      "          total_loss: 0.38558704290125106\n",
      "          vf_explained_var: 0.8520147800445557\n",
      "          vf_loss: 0.3796364893515905\n",
      "    num_agent_steps_sampled: 302000\n",
      "    num_agent_steps_trained: 302000\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.163559322033898\n",
      "    ram_util_percent: 62.70564971751412\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037095775581012284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.17133779395954\n",
      "    mean_inference_ms: 1.3162163030612877\n",
      "    mean_raw_obs_processing_ms: 4.195580317833775\n",
      "  time_since_restore: 7614.308944702148\n",
      "  time_this_iter_s: 248.66564536094666\n",
      "  time_total_s: 7614.308944702148\n",
      "  timers:\n",
      "    learn_throughput: 1563.182\n",
      "    learn_time_ms: 639.721\n",
      "    load_throughput: 60235.986\n",
      "    load_time_ms: 16.601\n",
      "    sample_throughput: 12.04\n",
      "    sample_time_ms: 83055.18\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1633537049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         7614.31</td><td style=\"text-align: right;\">302000</td><td style=\"text-align: right;\">    3.82</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            285.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 303000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 287.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.68\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 744\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1017085777388678\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0123465581379687\n",
      "          policy_loss: -0.08640804919931623\n",
      "          total_loss: 0.3559584463222159\n",
      "          vf_explained_var: 0.7659948468208313\n",
      "          vf_loss: 0.4421200793650415\n",
      "    num_agent_steps_sampled: 303000\n",
      "    num_agent_steps_trained: 303000\n",
      "    num_steps_sampled: 303000\n",
      "    num_steps_trained: 303000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.515254237288133\n",
      "    ram_util_percent: 62.72203389830508\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709561078992502\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.171467321016234\n",
      "    mean_inference_ms: 1.3162103488587258\n",
      "    mean_raw_obs_processing_ms: 4.2569307212765715\n",
      "  time_since_restore: 7655.3880569934845\n",
      "  time_this_iter_s: 41.07911229133606\n",
      "  time_total_s: 7655.3880569934845\n",
      "  timers:\n",
      "    learn_throughput: 1562.195\n",
      "    learn_time_ms: 640.125\n",
      "    load_throughput: 57473.067\n",
      "    load_time_ms: 17.399\n",
      "    sample_throughput: 11.688\n",
      "    sample_time_ms: 85555.448\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1633537090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303000\n",
      "  training_iteration: 303\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         7655.39</td><td style=\"text-align: right;\">303000</td><td style=\"text-align: right;\">    3.68</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            287.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-19-38\n",
      "  done: false\n",
      "  episode_len_mean: 277.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.78\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 750\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3838346163431803\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01593318519785213\n",
      "          policy_loss: 0.08578578109542528\n",
      "          total_loss: 0.351652571807305\n",
      "          vf_explained_var: 0.6141075491905212\n",
      "          vf_loss: 0.26516962945461275\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.80629921259842\n",
      "    ram_util_percent: 62.862204724409445\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709505609870668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.17107605352996\n",
      "    mean_inference_ms: 1.3161942478378827\n",
      "    mean_raw_obs_processing_ms: 4.447261090827756\n",
      "  time_since_restore: 7744.223789215088\n",
      "  time_this_iter_s: 88.8357322216034\n",
      "  time_total_s: 7744.223789215088\n",
      "  timers:\n",
      "    learn_throughput: 1561.253\n",
      "    learn_time_ms: 640.511\n",
      "    load_throughput: 58306.941\n",
      "    load_time_ms: 17.151\n",
      "    sample_throughput: 11.893\n",
      "    sample_time_ms: 84083.688\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1633537178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         7744.22</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">    3.78</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            277.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 305000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-22-29\n",
      "  done: false\n",
      "  episode_len_mean: 255.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.09\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 759\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.349812462594774\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01424433111538082\n",
      "          policy_loss: -0.1157142589489619\n",
      "          total_loss: 0.23719272495557864\n",
      "          vf_explained_var: 0.5153036117553711\n",
      "          vf_loss: 0.3534103030959765\n",
      "    num_agent_steps_sampled: 305000\n",
      "    num_agent_steps_trained: 305000\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.369262295081963\n",
      "    ram_util_percent: 62.72459016393442\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709418292262187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.16818681683627\n",
      "    mean_inference_ms: 1.3161710595991014\n",
      "    mean_raw_obs_processing_ms: 4.7752549381409635\n",
      "  time_since_restore: 7915.2102727890015\n",
      "  time_this_iter_s: 170.98648357391357\n",
      "  time_total_s: 7915.2102727890015\n",
      "  timers:\n",
      "    learn_throughput: 1562.037\n",
      "    learn_time_ms: 640.19\n",
      "    load_throughput: 58014.911\n",
      "    load_time_ms: 17.237\n",
      "    sample_throughput: 10.069\n",
      "    sample_time_ms: 99311.229\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1633537349\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 305\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         7915.21</td><td style=\"text-align: right;\">305000</td><td style=\"text-align: right;\">    4.09</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            255.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 306000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-24-01\n",
      "  done: false\n",
      "  episode_len_mean: 248.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.19\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 764\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1152304808298747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02613986302586117\n",
      "          policy_loss: -0.037515283789899614\n",
      "          total_loss: 1.317635363340378\n",
      "          vf_explained_var: 0.8681626915931702\n",
      "          vf_loss: 1.3424561301867166\n",
      "    num_agent_steps_sampled: 306000\n",
      "    num_agent_steps_trained: 306000\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.013076923076923\n",
      "    ram_util_percent: 62.55461538461539\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709369198214703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.16533905130702\n",
      "    mean_inference_ms: 1.3161567018560325\n",
      "    mean_raw_obs_processing_ms: 4.961566926002671\n",
      "  time_since_restore: 8006.283461093903\n",
      "  time_this_iter_s: 91.07318830490112\n",
      "  time_total_s: 8006.283461093903\n",
      "  timers:\n",
      "    learn_throughput: 1559.435\n",
      "    learn_time_ms: 641.258\n",
      "    load_throughput: 58631.418\n",
      "    load_time_ms: 17.056\n",
      "    sample_throughput: 9.736\n",
      "    sample_time_ms: 102711.828\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1633537441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 306\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         8006.28</td><td style=\"text-align: right;\">306000</td><td style=\"text-align: right;\">    4.19</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">               248</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 307000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-25-53\n",
      "  done: false\n",
      "  episode_len_mean: 237.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.18\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 771\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3684184074401855\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.147027807103263\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008544963948250049\n",
      "          policy_loss: -0.02747060428890917\n",
      "          total_loss: 0.44427492568890253\n",
      "          vf_explained_var: 0.863278865814209\n",
      "          vf_loss: 0.47152272661527\n",
      "    num_agent_steps_sampled: 307000\n",
      "    num_agent_steps_trained: 307000\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.56875\n",
      "    ram_util_percent: 62.87249999999999\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037092943218899756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.160238229623488\n",
      "    mean_inference_ms: 1.3161390791316034\n",
      "    mean_raw_obs_processing_ms: 5.233303570284648\n",
      "  time_since_restore: 8118.5854506492615\n",
      "  time_this_iter_s: 112.30198955535889\n",
      "  time_total_s: 8118.5854506492615\n",
      "  timers:\n",
      "    learn_throughput: 1561.232\n",
      "    learn_time_ms: 640.52\n",
      "    load_throughput: 58232.385\n",
      "    load_time_ms: 17.173\n",
      "    sample_throughput: 9.99\n",
      "    sample_time_ms: 100098.939\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1633537553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         8118.59</td><td style=\"text-align: right;\">307000</td><td style=\"text-align: right;\">    4.18</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            237.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-28-16\n",
      "  done: false\n",
      "  episode_len_mean: 218.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.49\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 779\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3684184074401855\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8930764648649427\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02126606688661915\n",
      "          policy_loss: 0.023411026762591467\n",
      "          total_loss: 1.3072572569052379\n",
      "          vf_explained_var: 0.8183909058570862\n",
      "          vf_loss: 1.2636761128902436\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.969117647058823\n",
      "    ram_util_percent: 62.988235294117636\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709212029605926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.15166158991967\n",
      "    mean_inference_ms: 1.3161195614755663\n",
      "    mean_raw_obs_processing_ms: 5.56852395673331\n",
      "  time_since_restore: 8261.220651388168\n",
      "  time_this_iter_s: 142.63520073890686\n",
      "  time_total_s: 8261.220651388168\n",
      "  timers:\n",
      "    learn_throughput: 1564.933\n",
      "    learn_time_ms: 639.005\n",
      "    load_throughput: 58250.258\n",
      "    load_time_ms: 17.167\n",
      "    sample_throughput: 9.096\n",
      "    sample_time_ms: 109941.539\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633537696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 308\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         8261.22</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">    4.49</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            218.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 309000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-29-06\n",
      "  done: false\n",
      "  episode_len_mean: 218.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.54\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 781\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.052627611160279\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0933166576756372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00990755978021964\n",
      "          policy_loss: 0.011152873188257218\n",
      "          total_loss: 0.6494562087787522\n",
      "          vf_explained_var: 0.6010357141494751\n",
      "          vf_loss: 0.628899968167146\n",
      "    num_agent_steps_sampled: 309000\n",
      "    num_agent_steps_trained: 309000\n",
      "    num_steps_sampled: 309000\n",
      "    num_steps_trained: 309000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.74583333333333\n",
      "    ram_util_percent: 62.47638888888889\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709188078719497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.149166084241394\n",
      "    mean_inference_ms: 1.316114577983229\n",
      "    mean_raw_obs_processing_ms: 5.651242739804345\n",
      "  time_since_restore: 8311.617753744125\n",
      "  time_this_iter_s: 50.39710235595703\n",
      "  time_total_s: 8311.617753744125\n",
      "  timers:\n",
      "    learn_throughput: 1562.058\n",
      "    learn_time_ms: 640.181\n",
      "    load_throughput: 58403.313\n",
      "    load_time_ms: 17.122\n",
      "    sample_throughput: 9.719\n",
      "    sample_time_ms: 102886.964\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1633537746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309000\n",
      "  training_iteration: 309\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         8311.62</td><td style=\"text-align: right;\">309000</td><td style=\"text-align: right;\">    4.54</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            218.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 310000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-30-35\n",
      "  done: false\n",
      "  episode_len_mean: 203.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.92\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 787\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.052627611160279\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0617945280339982\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008038321174635875\n",
      "          policy_loss: -0.0399888245595826\n",
      "          total_loss: 0.649500594039758\n",
      "          vf_explained_var: 0.8186778426170349\n",
      "          vf_loss: 0.6836076882150438\n",
      "    num_agent_steps_sampled: 310000\n",
      "    num_agent_steps_trained: 310000\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.91574803149606\n",
      "    ram_util_percent: 62.61732283464567\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709116591892015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.141353562647875\n",
      "    mean_inference_ms: 1.316101314288822\n",
      "    mean_raw_obs_processing_ms: 5.912065319434401\n",
      "  time_since_restore: 8400.6234292984\n",
      "  time_this_iter_s: 89.00567555427551\n",
      "  time_total_s: 8400.6234292984\n",
      "  timers:\n",
      "    learn_throughput: 1556.221\n",
      "    learn_time_ms: 642.582\n",
      "    load_throughput: 53680.632\n",
      "    load_time_ms: 18.629\n",
      "    sample_throughput: 9.066\n",
      "    sample_time_ms: 110300.486\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1633537835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         8400.62</td><td style=\"text-align: right;\">310000</td><td style=\"text-align: right;\">    4.92</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            203.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 311000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-31-30\n",
      "  done: false\n",
      "  episode_len_mean: 202.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.92\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 791\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.052627611160279\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5022722178035313\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007743118452307495\n",
      "          policy_loss: -0.11132819006840387\n",
      "          total_loss: 0.37799795385864043\n",
      "          vf_explained_var: 0.7590779066085815\n",
      "          vf_loss: 0.4884551222125689\n",
      "    num_agent_steps_sampled: 311000\n",
      "    num_agent_steps_trained: 311000\n",
      "    num_steps_sampled: 311000\n",
      "    num_steps_trained: 311000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.03076923076923\n",
      "    ram_util_percent: 62.95512820512818\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370907548730647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.136397035594413\n",
      "    mean_inference_ms: 1.3160941701309326\n",
      "    mean_raw_obs_processing_ms: 6.083629759469952\n",
      "  time_since_restore: 8455.66542005539\n",
      "  time_this_iter_s: 55.041990756988525\n",
      "  time_total_s: 8455.66542005539\n",
      "  timers:\n",
      "    learn_throughput: 1550.618\n",
      "    learn_time_ms: 644.904\n",
      "    load_throughput: 54234.749\n",
      "    load_time_ms: 18.438\n",
      "    sample_throughput: 9.231\n",
      "    sample_time_ms: 108334.283\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1633537890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311000\n",
      "  training_iteration: 311\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         8455.67</td><td style=\"text-align: right;\">311000</td><td style=\"text-align: right;\">    4.92</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            202.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-34-13\n",
      "  done: false\n",
      "  episode_len_mean: 181.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.39\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 800\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.052627611160279\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0105137321684095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009796740423229582\n",
      "          policy_loss: 0.045568224787712094\n",
      "          total_loss: 0.4658300408886539\n",
      "          vf_explained_var: 0.8574662208557129\n",
      "          vf_loss: 0.4102578901582294\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.024463519313304\n",
      "    ram_util_percent: 62.92060085836909\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708996163910474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.123919369909746\n",
      "    mean_inference_ms: 1.3160782485428255\n",
      "    mean_raw_obs_processing_ms: 6.4876976767111865\n",
      "  time_since_restore: 8618.783055305481\n",
      "  time_this_iter_s: 163.11763525009155\n",
      "  time_total_s: 8618.783055305481\n",
      "  timers:\n",
      "    learn_throughput: 1547.234\n",
      "    learn_time_ms: 646.315\n",
      "    load_throughput: 54187.104\n",
      "    load_time_ms: 18.455\n",
      "    sample_throughput: 10.022\n",
      "    sample_time_ms: 99778.048\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1633538053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 312\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         8618.78</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">    5.39</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            181.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 313000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-37-00\n",
      "  done: false\n",
      "  episode_len_mean: 166.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.97\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 809\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.052627611160279\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.007694787449307\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005750307756586142\n",
      "          policy_loss: -0.16249897927045823\n",
      "          total_loss: 0.3115932982828882\n",
      "          vf_explained_var: 0.8894006609916687\n",
      "          vf_loss: 0.47236598531405133\n",
      "    num_agent_steps_sampled: 313000\n",
      "    num_agent_steps_trained: 313000\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.148319327731095\n",
      "    ram_util_percent: 62.726890756302524\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708920208475232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.111768418337107\n",
      "    mean_inference_ms: 1.3160676920584562\n",
      "    mean_raw_obs_processing_ms: 6.902434459752021\n",
      "  time_since_restore: 8785.26031756401\n",
      "  time_this_iter_s: 166.47726225852966\n",
      "  time_total_s: 8785.26031756401\n",
      "  timers:\n",
      "    learn_throughput: 1542.806\n",
      "    learn_time_ms: 648.17\n",
      "    load_throughput: 54510.912\n",
      "    load_time_ms: 18.345\n",
      "    sample_throughput: 8.903\n",
      "    sample_time_ms: 112316.105\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1633538220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         8785.26</td><td style=\"text-align: right;\">313000</td><td style=\"text-align: right;\">    5.97</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            166.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 314000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-38-29\n",
      "  done: false\n",
      "  episode_len_mean: 165.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.05\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 814\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.052627611160279\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1746990892622207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0035366948914646594\n",
      "          policy_loss: 0.048251047854622205\n",
      "          total_loss: 0.41330663851565785\n",
      "          vf_explained_var: 0.7081131935119629\n",
      "          vf_loss: 0.3695430571834246\n",
      "    num_agent_steps_sampled: 314000\n",
      "    num_agent_steps_trained: 314000\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.503174603174603\n",
      "    ram_util_percent: 62.77857142857144\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708870962596179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.10477493551499\n",
      "    mean_inference_ms: 1.3160625159296107\n",
      "    mean_raw_obs_processing_ms: 7.125370060305437\n",
      "  time_since_restore: 8874.069311857224\n",
      "  time_this_iter_s: 88.80899429321289\n",
      "  time_total_s: 8874.069311857224\n",
      "  timers:\n",
      "    learn_throughput: 1545.671\n",
      "    learn_time_ms: 646.968\n",
      "    load_throughput: 53976.439\n",
      "    load_time_ms: 18.527\n",
      "    sample_throughput: 8.904\n",
      "    sample_time_ms: 112314.484\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1633538309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 314\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         8874.07</td><td style=\"text-align: right;\">314000</td><td style=\"text-align: right;\">    6.05</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">             165.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-40-17\n",
      "  done: false\n",
      "  episode_len_mean: 163.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.21\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 821\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9679742925696903\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01621156422511625\n",
      "          policy_loss: 0.008306838240888384\n",
      "          total_loss: 0.6636629727151658\n",
      "          vf_explained_var: 0.7957344055175781\n",
      "          vf_loss: 0.648397723502583\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.887096774193548\n",
      "    ram_util_percent: 62.95419354838708\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037088018736179736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.094720567857713\n",
      "    mean_inference_ms: 1.31605696552777\n",
      "    mean_raw_obs_processing_ms: 7.434562859914191\n",
      "  time_since_restore: 8982.404948472977\n",
      "  time_this_iter_s: 108.33563661575317\n",
      "  time_total_s: 8982.404948472977\n",
      "  timers:\n",
      "    learn_throughput: 1541.247\n",
      "    learn_time_ms: 648.825\n",
      "    load_throughput: 54083.554\n",
      "    load_time_ms: 18.49\n",
      "    sample_throughput: 9.43\n",
      "    sample_time_ms: 106047.544\n",
      "    update_time_ms: 1.696\n",
      "  timestamp: 1633538417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 315\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">          8982.4</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\">    6.21</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -19</td><td style=\"text-align: right;\">            163.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 153.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.72\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 826\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7479699671268463\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005455730060836217\n",
      "          policy_loss: 0.004626059118244383\n",
      "          total_loss: 0.8459525034659439\n",
      "          vf_explained_var: 0.878835916519165\n",
      "          vf_loss: 0.8432068462173145\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.92745098039216\n",
      "    ram_util_percent: 63.09738562091504\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037087560277543244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.0869334253194\n",
      "    mean_inference_ms: 1.316053455659999\n",
      "    mean_raw_obs_processing_ms: 7.662792278326942\n",
      "  time_since_restore: 9089.56883430481\n",
      "  time_this_iter_s: 107.16388583183289\n",
      "  time_total_s: 9089.56883430481\n",
      "  timers:\n",
      "    learn_throughput: 1543.961\n",
      "    learn_time_ms: 647.685\n",
      "    load_throughput: 54476.928\n",
      "    load_time_ms: 18.356\n",
      "    sample_throughput: 9.289\n",
      "    sample_time_ms: 107657.888\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1633538524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         9089.57</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">    6.72</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -16</td><td style=\"text-align: right;\">             153.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 317000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-43-08\n",
      "  done: false\n",
      "  episode_len_mean: 155.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.91\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 830\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1759015070067511\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016531276832871238\n",
      "          policy_loss: 0.06017685114509529\n",
      "          total_loss: 0.9137505012874801\n",
      "          vf_explained_var: 0.6707587242126465\n",
      "          vf_loss: 0.8483664012617536\n",
      "    num_agent_steps_sampled: 317000\n",
      "    num_agent_steps_trained: 317000\n",
      "    num_steps_sampled: 317000\n",
      "    num_steps_trained: 317000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.540217391304346\n",
      "    ram_util_percent: 62.915217391304324\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708728232627643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.081505886708598\n",
      "    mean_inference_ms: 1.316052025408322\n",
      "    mean_raw_obs_processing_ms: 7.837043287355971\n",
      "  time_since_restore: 9153.947078227997\n",
      "  time_this_iter_s: 64.37824392318726\n",
      "  time_total_s: 9153.947078227997\n",
      "  timers:\n",
      "    learn_throughput: 1539.248\n",
      "    learn_time_ms: 649.668\n",
      "    load_throughput: 54932.341\n",
      "    load_time_ms: 18.204\n",
      "    sample_throughput: 9.722\n",
      "    sample_time_ms: 102863.71\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1633538588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 317000\n",
      "  training_iteration: 317\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         9153.95</td><td style=\"text-align: right;\">317000</td><td style=\"text-align: right;\">    6.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -16</td><td style=\"text-align: right;\">            155.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 318000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-45-12\n",
      "  done: false\n",
      "  episode_len_mean: 163.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.82\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 837\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8717543734444513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004808365316232531\n",
      "          policy_loss: 0.11154438575936688\n",
      "          total_loss: 0.8458969420442979\n",
      "          vf_explained_var: 0.4088207185268402\n",
      "          vf_loss: 0.7381352003353338\n",
      "    num_agent_steps_sampled: 318000\n",
      "    num_agent_steps_trained: 318000\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.497175141242938\n",
      "    ram_util_percent: 63.099999999999994\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708681052904446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.071537081924944\n",
      "    mean_inference_ms: 1.3160496243301105\n",
      "    mean_raw_obs_processing_ms: 8.124387557587038\n",
      "  time_since_restore: 9277.815865278244\n",
      "  time_this_iter_s: 123.86878705024719\n",
      "  time_total_s: 9277.815865278244\n",
      "  timers:\n",
      "    learn_throughput: 1537.141\n",
      "    learn_time_ms: 650.558\n",
      "    load_throughput: 55412.339\n",
      "    load_time_ms: 18.047\n",
      "    sample_throughput: 9.902\n",
      "    sample_time_ms: 100986.346\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1633538712\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 318\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         9277.82</td><td style=\"text-align: right;\">318000</td><td style=\"text-align: right;\">    6.82</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -16</td><td style=\"text-align: right;\">            163.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 319000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-46-45\n",
      "  done: false\n",
      "  episode_len_mean: 166.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.77\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 842\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9827185193697612\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010129956094601668\n",
      "          policy_loss: -0.008066688146856096\n",
      "          total_loss: 0.6088544910152753\n",
      "          vf_explained_var: 0.9194133281707764\n",
      "          vf_loss: 0.6215501078301006\n",
      "    num_agent_steps_sampled: 319000\n",
      "    num_agent_steps_trained: 319000\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.67786259541985\n",
      "    ram_util_percent: 63.01374045801528\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708647580028832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.064147548404936\n",
      "    mean_inference_ms: 1.3160474049470152\n",
      "    mean_raw_obs_processing_ms: 8.339766967215631\n",
      "  time_since_restore: 9369.931439638138\n",
      "  time_this_iter_s: 92.1155743598938\n",
      "  time_total_s: 9369.931439638138\n",
      "  timers:\n",
      "    learn_throughput: 1537.369\n",
      "    learn_time_ms: 650.462\n",
      "    load_throughput: 55385.265\n",
      "    load_time_ms: 18.055\n",
      "    sample_throughput: 9.509\n",
      "    sample_time_ms: 105158.282\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1633538805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         9369.93</td><td style=\"text-align: right;\">319000</td><td style=\"text-align: right;\">    6.77</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -16</td><td style=\"text-align: right;\">            166.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-47-36\n",
      "  done: false\n",
      "  episode_len_mean: 161.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.17\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 846\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1120110266738468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014183605391355186\n",
      "          policy_loss: 0.10728251760204634\n",
      "          total_loss: 0.39157423608832886\n",
      "          vf_explained_var: 0.5617256164550781\n",
      "          vf_loss: 0.28813340990907615\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.145945945945947\n",
      "    ram_util_percent: 63.131081081081064\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708625840188359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.05859195143111\n",
      "    mean_inference_ms: 1.3160449365864715\n",
      "    mean_raw_obs_processing_ms: 8.508210596264698\n",
      "  time_since_restore: 9421.477502584457\n",
      "  time_this_iter_s: 51.54606294631958\n",
      "  time_total_s: 9421.477502584457\n",
      "  timers:\n",
      "    learn_throughput: 1542.997\n",
      "    learn_time_ms: 648.089\n",
      "    load_throughput: 55446.401\n",
      "    load_time_ms: 18.035\n",
      "    sample_throughput: 9.861\n",
      "    sample_time_ms: 101414.727\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1633538856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 320\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         9421.48</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">    7.17</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            161.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 321000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-51-14\n",
      "  done: false\n",
      "  episode_len_mean: 158.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.31\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 857\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6677364607652029\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007782462113953632\n",
      "          policy_loss: -0.11224654217561086\n",
      "          total_loss: 0.6394668887058894\n",
      "          vf_explained_var: 0.92441725730896\n",
      "          vf_loss: 0.7543971757094066\n",
      "    num_agent_steps_sampled: 321000\n",
      "    num_agent_steps_trained: 321000\n",
      "    num_steps_sampled: 321000\n",
      "    num_steps_trained: 321000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.255948553054665\n",
      "    ram_util_percent: 63.06623794212219\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708579032786773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.04262784071383\n",
      "    mean_inference_ms: 1.3160376750533274\n",
      "    mean_raw_obs_processing_ms: 8.99190325436404\n",
      "  time_since_restore: 9639.678884506226\n",
      "  time_this_iter_s: 218.2013819217682\n",
      "  time_total_s: 9639.678884506226\n",
      "  timers:\n",
      "    learn_throughput: 1547.026\n",
      "    learn_time_ms: 646.401\n",
      "    load_throughput: 55752.855\n",
      "    load_time_ms: 17.936\n",
      "    sample_throughput: 8.494\n",
      "    sample_time_ms: 117732.442\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1633539074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 321000\n",
      "  training_iteration: 321\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         9639.68</td><td style=\"text-align: right;\">321000</td><td style=\"text-align: right;\">    7.31</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            158.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 322000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-53-40\n",
      "  done: false\n",
      "  episode_len_mean: 153.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.7\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 865\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8259361922740937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004877089477808422\n",
      "          policy_loss: -0.1495206269952986\n",
      "          total_loss: 0.17811297666695383\n",
      "          vf_explained_var: 0.745003342628479\n",
      "          vf_loss: 0.3333902570936415\n",
      "    num_agent_steps_sampled: 322000\n",
      "    num_agent_steps_trained: 322000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.425603864734295\n",
      "    ram_util_percent: 63.07149758454107\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708545975212603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.031017123371686\n",
      "    mean_inference_ms: 1.3160342043373958\n",
      "    mean_raw_obs_processing_ms: 9.342869288363463\n",
      "  time_since_restore: 9784.993482351303\n",
      "  time_this_iter_s: 145.31459784507751\n",
      "  time_total_s: 9784.993482351303\n",
      "  timers:\n",
      "    learn_throughput: 1549.29\n",
      "    learn_time_ms: 645.457\n",
      "    load_throughput: 56499.139\n",
      "    load_time_ms: 17.699\n",
      "    sample_throughput: 8.624\n",
      "    sample_time_ms: 115953.332\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633539220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         9784.99</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\">     7.7</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            153.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 323000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_16-54-13\n",
      "  done: false\n",
      "  episode_len_mean: 159.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.65\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 867\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7967734201086892\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005533107912645777\n",
      "          policy_loss: 0.012023859988484117\n",
      "          total_loss: 0.13891808005670706\n",
      "          vf_explained_var: -0.011846128851175308\n",
      "          vf_loss: 0.13344227638509537\n",
      "    num_agent_steps_sampled: 323000\n",
      "    num_agent_steps_trained: 323000\n",
      "    num_steps_sampled: 323000\n",
      "    num_steps_trained: 323000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.4375\n",
      "    ram_util_percent: 63.208333333333336\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037085381586242576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.027904373711866\n",
      "    mean_inference_ms: 1.3160327723981555\n",
      "    mean_raw_obs_processing_ms: 9.42501992157648\n",
      "  time_since_restore: 9818.176607847214\n",
      "  time_this_iter_s: 33.183125495910645\n",
      "  time_total_s: 9818.176607847214\n",
      "  timers:\n",
      "    learn_throughput: 1552.566\n",
      "    learn_time_ms: 644.095\n",
      "    load_throughput: 56772.692\n",
      "    load_time_ms: 17.614\n",
      "    sample_throughput: 9.744\n",
      "    sample_time_ms: 102625.373\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633539253\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323000\n",
      "  training_iteration: 323\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         9818.18</td><td style=\"text-align: right;\">323000</td><td style=\"text-align: right;\">    7.65</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            159.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-01-11\n",
      "  done: false\n",
      "  episode_len_mean: 136.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.27\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 890\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4853212998972999\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004297921952065634\n",
      "          policy_loss: -0.06836126786139277\n",
      "          total_loss: 0.14489054195582868\n",
      "          vf_explained_var: 0.97083580493927\n",
      "          vf_loss: 0.21700226941870318\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.663651591289785\n",
      "    ram_util_percent: 63.10150753768844\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708464364840183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.99514925973896\n",
      "    mean_inference_ms: 1.3160358609918128\n",
      "    mean_raw_obs_processing_ms: 10.541443288079158\n",
      "  time_since_restore: 10236.729800462723\n",
      "  time_this_iter_s: 418.55319261550903\n",
      "  time_total_s: 10236.729800462723\n",
      "  timers:\n",
      "    learn_throughput: 1552.103\n",
      "    learn_time_ms: 644.287\n",
      "    load_throughput: 57547.507\n",
      "    load_time_ms: 17.377\n",
      "    sample_throughput: 7.375\n",
      "    sample_time_ms: 135599.796\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1633539671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 324\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         10236.7</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">    8.27</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            136.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 325000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-04-12\n",
      "  done: false\n",
      "  episode_len_mean: 130.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.6\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 900\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0977517435948054\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.030529697242377217\n",
      "          policy_loss: 0.022483853995800017\n",
      "          total_loss: 0.8173236153191991\n",
      "          vf_explained_var: 0.6591564416885376\n",
      "          vf_loss: 0.8019006436069807\n",
      "    num_agent_steps_sampled: 325000\n",
      "    num_agent_steps_trained: 325000\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.848837209302324\n",
      "    ram_util_percent: 63.24108527131783\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708473028273996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.982126818462298\n",
      "    mean_inference_ms: 1.3160427250514504\n",
      "    mean_raw_obs_processing_ms: 11.001649507504768\n",
      "  time_since_restore: 10417.133757352829\n",
      "  time_this_iter_s: 180.4039568901062\n",
      "  time_total_s: 10417.133757352829\n",
      "  timers:\n",
      "    learn_throughput: 1548.851\n",
      "    learn_time_ms: 645.64\n",
      "    load_throughput: 57990.526\n",
      "    load_time_ms: 17.244\n",
      "    sample_throughput: 7.003\n",
      "    sample_time_ms: 142805.442\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1633539852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         10417.1</td><td style=\"text-align: right;\">325000</td><td style=\"text-align: right;\">     8.6</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            130.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 326000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-07-50\n",
      "  done: false\n",
      "  episode_len_mean: 120.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.82\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 911\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6031590074300766\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009893416459124754\n",
      "          policy_loss: -0.11557549950149325\n",
      "          total_loss: 0.00806232632862197\n",
      "          vf_explained_var: 0.3130629062652588\n",
      "          vf_loss: 0.127765588917666\n",
      "    num_agent_steps_sampled: 326000\n",
      "    num_agent_steps_trained: 326000\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.199354838709677\n",
      "    ram_util_percent: 62.79032258064516\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037084949507392634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.966900271789026\n",
      "    mean_inference_ms: 1.316055054399956\n",
      "    mean_raw_obs_processing_ms: 11.513999822790375\n",
      "  time_since_restore: 10634.957338571548\n",
      "  time_this_iter_s: 217.82358121871948\n",
      "  time_total_s: 10634.957338571548\n",
      "  timers:\n",
      "    learn_throughput: 1546.646\n",
      "    learn_time_ms: 646.56\n",
      "    load_throughput: 58026.067\n",
      "    load_time_ms: 17.234\n",
      "    sample_throughput: 6.499\n",
      "    sample_time_ms: 153870.474\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1633540070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 326\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">           10635</td><td style=\"text-align: right;\">326000</td><td style=\"text-align: right;\">    8.82</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             120.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 327000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-10-33\n",
      "  done: false\n",
      "  episode_len_mean: 116.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.02\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 920\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7476299623648326\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008091808413748463\n",
      "          policy_loss: -0.05792140364646912\n",
      "          total_loss: 0.19216735776927735\n",
      "          vf_explained_var: 0.7119265794754028\n",
      "          vf_loss: 0.2560079181773795\n",
      "    num_agent_steps_sampled: 327000\n",
      "    num_agent_steps_trained: 327000\n",
      "    num_steps_sampled: 327000\n",
      "    num_steps_trained: 327000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.884978540772536\n",
      "    ram_util_percent: 62.910300429184545\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708522813899842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.954212475669966\n",
      "    mean_inference_ms: 1.3160676616234566\n",
      "    mean_raw_obs_processing_ms: 11.939841739815165\n",
      "  time_since_restore: 10798.062293052673\n",
      "  time_this_iter_s: 163.10495448112488\n",
      "  time_total_s: 10798.062293052673\n",
      "  timers:\n",
      "    learn_throughput: 1549.83\n",
      "    learn_time_ms: 645.232\n",
      "    load_throughput: 58391.93\n",
      "    load_time_ms: 17.126\n",
      "    sample_throughput: 6.107\n",
      "    sample_time_ms: 163744.573\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633540233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327000\n",
      "  training_iteration: 327\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         10798.1</td><td style=\"text-align: right;\">327000</td><td style=\"text-align: right;\">    9.02</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            116.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-10-46\n",
      "  done: false\n",
      "  episode_len_mean: 123.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.92\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 921\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9604849835236867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014525803638165947\n",
      "          policy_loss: 0.06551868865887324\n",
      "          total_loss: 0.20870353562964333\n",
      "          vf_explained_var: 0.4842485189437866\n",
      "          vf_loss: 0.14999443996283743\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0\n",
      "    ram_util_percent: 62.978947368421046\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037085253015935245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.952640370715457\n",
      "    mean_inference_ms: 1.3160690896554674\n",
      "    mean_raw_obs_processing_ms: 11.985868091799805\n",
      "  time_since_restore: 10811.240974664688\n",
      "  time_this_iter_s: 13.17868161201477\n",
      "  time_total_s: 10811.240974664688\n",
      "  timers:\n",
      "    learn_throughput: 1549.457\n",
      "    learn_time_ms: 645.388\n",
      "    load_throughput: 63523.592\n",
      "    load_time_ms: 15.742\n",
      "    sample_throughput: 6.55\n",
      "    sample_time_ms: 152676.792\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1633540246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         10811.2</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">    8.92</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            123.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 329000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-13-09\n",
      "  done: false\n",
      "  episode_len_mean: 119.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.04\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 930\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6225079483456082\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010421413084264506\n",
      "          policy_loss: 0.20220056399703026\n",
      "          total_loss: 0.4370239515271452\n",
      "          vf_explained_var: 0.8751910924911499\n",
      "          vf_loss: 0.2490430316577355\n",
      "    num_agent_steps_sampled: 329000\n",
      "    num_agent_steps_trained: 329000\n",
      "    num_steps_sampled: 329000\n",
      "    num_steps_trained: 329000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.34754901960784\n",
      "    ram_util_percent: 63.271078431372544\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708538299135781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.939448034677557\n",
      "    mean_inference_ms: 1.3160829987314826\n",
      "    mean_raw_obs_processing_ms: 12.40238856913903\n",
      "  time_since_restore: 10954.366044282913\n",
      "  time_this_iter_s: 143.1250696182251\n",
      "  time_total_s: 10954.366044282913\n",
      "  timers:\n",
      "    learn_throughput: 1551.516\n",
      "    learn_time_ms: 644.531\n",
      "    load_throughput: 63643.887\n",
      "    load_time_ms: 15.712\n",
      "    sample_throughput: 6.338\n",
      "    sample_time_ms: 157778.619\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1633540389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329000\n",
      "  training_iteration: 329\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         10954.4</td><td style=\"text-align: right;\">329000</td><td style=\"text-align: right;\">    9.04</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            119.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-15-14\n",
      "  done: false\n",
      "  episode_len_mean: 113.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.14\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 936\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1246245145797729\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014237908723417587\n",
      "          policy_loss: -0.04249945547845629\n",
      "          total_loss: 0.31947559813658394\n",
      "          vf_explained_var: 0.6494008302688599\n",
      "          vf_loss: 0.37048144878612627\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.470391061452517\n",
      "    ram_util_percent: 63.02178770949722\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708534669548747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.93066402750268\n",
      "    mean_inference_ms: 1.3160934501463708\n",
      "    mean_raw_obs_processing_ms: 12.675066381382349\n",
      "  time_since_restore: 11079.387766361237\n",
      "  time_this_iter_s: 125.02172207832336\n",
      "  time_total_s: 11079.387766361237\n",
      "  timers:\n",
      "    learn_throughput: 1550.486\n",
      "    learn_time_ms: 644.959\n",
      "    load_throughput: 63258.687\n",
      "    load_time_ms: 15.808\n",
      "    sample_throughput: 6.056\n",
      "    sample_time_ms: 165125.643\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1633540514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 330\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         11079.4</td><td style=\"text-align: right;\">330000</td><td style=\"text-align: right;\">    9.14</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            113.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 331000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-17-39\n",
      "  done: false\n",
      "  episode_len_mean: 110.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.18\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 945\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.218409393231074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01869466506145599\n",
      "          policy_loss: 0.14817738127377297\n",
      "          total_loss: 0.32394151012930605\n",
      "          vf_explained_var: 0.7300186157226562\n",
      "          vf_loss: 0.18435073635644383\n",
      "    num_agent_steps_sampled: 331000\n",
      "    num_agent_steps_trained: 331000\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.22125603864734\n",
      "    ram_util_percent: 63.319806763285015\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037085282928321334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.917429293454415\n",
      "    mean_inference_ms: 1.316110396398934\n",
      "    mean_raw_obs_processing_ms: 13.096550430844488\n",
      "  time_since_restore: 11224.302674770355\n",
      "  time_this_iter_s: 144.91490840911865\n",
      "  time_total_s: 11224.302674770355\n",
      "  timers:\n",
      "    learn_throughput: 1548.296\n",
      "    learn_time_ms: 645.871\n",
      "    load_throughput: 62767.558\n",
      "    load_time_ms: 15.932\n",
      "    sample_throughput: 6.337\n",
      "    sample_time_ms: 157795.975\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1633540659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         11224.3</td><td style=\"text-align: right;\">331000</td><td style=\"text-align: right;\">    9.18</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            110.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-19-06\n",
      "  done: false\n",
      "  episode_len_mean: 110.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.17\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 949\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8892097347312503\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012928404233480961\n",
      "          policy_loss: -0.07825385919875569\n",
      "          total_loss: 0.10572686253322496\n",
      "          vf_explained_var: 0.0851082131266594\n",
      "          vf_loss: 0.1903849585706161\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.70081300813008\n",
      "    ram_util_percent: 62.965853658536595\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708522478667937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.911640137572093\n",
      "    mean_inference_ms: 1.3161175554090974\n",
      "    mean_raw_obs_processing_ms: 13.269732556682332\n",
      "  time_since_restore: 11310.636275291443\n",
      "  time_this_iter_s: 86.33360052108765\n",
      "  time_total_s: 11310.636275291443\n",
      "  timers:\n",
      "    learn_throughput: 1543.628\n",
      "    learn_time_ms: 647.824\n",
      "    load_throughput: 61754.956\n",
      "    load_time_ms: 16.193\n",
      "    sample_throughput: 6.583\n",
      "    sample_time_ms: 151895.668\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633540746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 332\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         11310.6</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">    9.17</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            110.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 333000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-22-46\n",
      "  done: false\n",
      "  episode_len_mean: 112.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.11\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 961\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9677966800000932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01534076403112243\n",
      "          policy_loss: -0.04250385347339842\n",
      "          total_loss: 0.5357241669462787\n",
      "          vf_explained_var: 0.932270884513855\n",
      "          vf_loss: 0.5849539091189703\n",
      "    num_agent_steps_sampled: 333000\n",
      "    num_agent_steps_trained: 333000\n",
      "    num_steps_sampled: 333000\n",
      "    num_steps_trained: 333000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.44031746031746\n",
      "    ram_util_percent: 63.06349206349206\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037085061111720635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.89360168132524\n",
      "    mean_inference_ms: 1.316141586619174\n",
      "    mean_raw_obs_processing_ms: 13.825266758666984\n",
      "  time_since_restore: 11531.420885324478\n",
      "  time_this_iter_s: 220.78461003303528\n",
      "  time_total_s: 11531.420885324478\n",
      "  timers:\n",
      "    learn_throughput: 1541.611\n",
      "    learn_time_ms: 648.672\n",
      "    load_throughput: 61600.046\n",
      "    load_time_ms: 16.234\n",
      "    sample_throughput: 5.86\n",
      "    sample_time_ms: 170654.926\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1633540966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333000\n",
      "  training_iteration: 333\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         11531.4</td><td style=\"text-align: right;\">333000</td><td style=\"text-align: right;\">    9.11</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            112.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 334000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 109.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.09\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 969\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1844646732012432\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023198392397958994\n",
      "          policy_loss: -0.023648806744151644\n",
      "          total_loss: 0.40907857161429195\n",
      "          vf_explained_var: 0.6867461800575256\n",
      "          vf_loss: 0.4401078663766384\n",
      "    num_agent_steps_sampled: 334000\n",
      "    num_agent_steps_trained: 334000\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.089903846153845\n",
      "    ram_util_percent: 63.27067307692307\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037084943442562086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.881617370340777\n",
      "    mean_inference_ms: 1.3161578127529794\n",
      "    mean_raw_obs_processing_ms: 14.178799808527387\n",
      "  time_since_restore: 11677.192924499512\n",
      "  time_this_iter_s: 145.77203917503357\n",
      "  time_total_s: 11677.192924499512\n",
      "  timers:\n",
      "    learn_throughput: 1542.232\n",
      "    learn_time_ms: 648.411\n",
      "    load_throughput: 60758.572\n",
      "    load_time_ms: 16.459\n",
      "    sample_throughput: 6.975\n",
      "    sample_time_ms: 143376.873\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1633541112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         11677.2</td><td style=\"text-align: right;\">334000</td><td style=\"text-align: right;\">    9.09</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            109.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 335000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-26-20\n",
      "  done: false\n",
      "  episode_len_mean: 117.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.06\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 973\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1975420415401459\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00813785930497678\n",
      "          policy_loss: -0.18651367533538077\n",
      "          total_loss: -0.08986349710159831\n",
      "          vf_explained_var: 0.22787073254585266\n",
      "          vf_loss: 0.10627659489659386\n",
      "    num_agent_steps_sampled: 335000\n",
      "    num_agent_steps_trained: 335000\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.621649484536086\n",
      "    ram_util_percent: 63.50000000000001\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708489698274926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.87531200092443\n",
      "    mean_inference_ms: 1.3161620172471873\n",
      "    mean_raw_obs_processing_ms: 14.323675722885568\n",
      "  time_since_restore: 11744.873305797577\n",
      "  time_this_iter_s: 67.68038129806519\n",
      "  time_total_s: 11744.873305797577\n",
      "  timers:\n",
      "    learn_throughput: 1546.206\n",
      "    learn_time_ms: 646.744\n",
      "    load_throughput: 60085.322\n",
      "    load_time_ms: 16.643\n",
      "    sample_throughput: 7.57\n",
      "    sample_time_ms: 132105.979\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1633541180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 335\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         11744.9</td><td style=\"text-align: right;\">335000</td><td style=\"text-align: right;\">    9.06</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            117.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-32-45\n",
      "  done: false\n",
      "  episode_len_mean: 113.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.16\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 993\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.41970283256636726\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005749512251039748\n",
      "          policy_loss: -0.1588339311381181\n",
      "          total_loss: 0.05428872087763415\n",
      "          vf_explained_var: 0.9717044234275818\n",
      "          vf_loss: 0.21566007725066608\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.707846715328465\n",
      "    ram_util_percent: 63.42956204379562\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037084709747421216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.841544869019838\n",
      "    mean_inference_ms: 1.3161916689330502\n",
      "    mean_raw_obs_processing_ms: 15.244655471143794\n",
      "  time_since_restore: 12129.558636665344\n",
      "  time_this_iter_s: 384.68533086776733\n",
      "  time_total_s: 12129.558636665344\n",
      "  timers:\n",
      "    learn_throughput: 1544.496\n",
      "    learn_time_ms: 647.46\n",
      "    load_throughput: 59411.256\n",
      "    load_time_ms: 16.832\n",
      "    sample_throughput: 6.721\n",
      "    sample_time_ms: 148791.254\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1633541565\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 336\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         12129.6</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">    9.16</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            113.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 337000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-36-41\n",
      "  done: false\n",
      "  episode_len_mean: 117.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.09\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1005\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.912316173977322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008736185417021562\n",
      "          policy_loss: -0.20199802768313221\n",
      "          total_loss: -0.054871047867669\n",
      "          vf_explained_var: 0.659887433052063\n",
      "          vf_loss: 0.15372843398816055\n",
      "    num_agent_steps_sampled: 337000\n",
      "    num_agent_steps_trained: 337000\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.24230769230769\n",
      "    ram_util_percent: 63.47899408284024\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037084450295957395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.821563009181983\n",
      "    mean_inference_ms: 1.316207003790245\n",
      "    mean_raw_obs_processing_ms: 15.793150342707373\n",
      "  time_since_restore: 12366.083072662354\n",
      "  time_this_iter_s: 236.52443599700928\n",
      "  time_total_s: 12366.083072662354\n",
      "  timers:\n",
      "    learn_throughput: 1544.522\n",
      "    learn_time_ms: 647.449\n",
      "    load_throughput: 58657.575\n",
      "    load_time_ms: 17.048\n",
      "    sample_throughput: 6.405\n",
      "    sample_time_ms: 156132.988\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1633541801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         12366.1</td><td style=\"text-align: right;\">337000</td><td style=\"text-align: right;\">    9.09</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            117.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 338000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-41-55\n",
      "  done: false\n",
      "  episode_len_mean: 98.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.28\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1022\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8015269279479981\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0120219033577261\n",
      "          policy_loss: 0.02231642338964674\n",
      "          total_loss: 0.1363212063908577\n",
      "          vf_explained_var: 0.9359458088874817\n",
      "          vf_loss: 0.11854992450939285\n",
      "    num_agent_steps_sampled: 338000\n",
      "    num_agent_steps_trained: 338000\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.267037861915366\n",
      "    ram_util_percent: 63.39376391982183\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708401347096909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.797380398672612\n",
      "    mean_inference_ms: 1.3162334131000049\n",
      "    mean_raw_obs_processing_ms: 16.60318273798319\n",
      "  time_since_restore: 12680.46695137024\n",
      "  time_this_iter_s: 314.38387870788574\n",
      "  time_total_s: 12680.46695137024\n",
      "  timers:\n",
      "    learn_throughput: 1547.628\n",
      "    learn_time_ms: 646.15\n",
      "    load_throughput: 53661.883\n",
      "    load_time_ms: 18.635\n",
      "    sample_throughput: 5.369\n",
      "    sample_time_ms: 186253.184\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1633542115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 338\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         12680.5</td><td style=\"text-align: right;\">338000</td><td style=\"text-align: right;\">    9.28</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">             98.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 339000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 96.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.33\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1032\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0080890549553765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013269819887774916\n",
      "          policy_loss: -0.09986715664466222\n",
      "          total_loss: 0.07391123258405262\n",
      "          vf_explained_var: 0.9836329817771912\n",
      "          vf_loss: 0.18002894181344245\n",
      "    num_agent_steps_sampled: 339000\n",
      "    num_agent_steps_trained: 339000\n",
      "    num_steps_sampled: 339000\n",
      "    num_steps_trained: 339000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.38280701754386\n",
      "    ram_util_percent: 63.50456140350878\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708390069378481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.785007937050327\n",
      "    mean_inference_ms: 1.316249246414163\n",
      "    mean_raw_obs_processing_ms: 17.079849292402447\n",
      "  time_since_restore: 12880.20294046402\n",
      "  time_this_iter_s: 199.73598909378052\n",
      "  time_total_s: 12880.20294046402\n",
      "  timers:\n",
      "    learn_throughput: 1548.53\n",
      "    learn_time_ms: 645.774\n",
      "    load_throughput: 53526.973\n",
      "    load_time_ms: 18.682\n",
      "    sample_throughput: 5.211\n",
      "    sample_time_ms: 191914.602\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1633542315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339000\n",
      "  training_iteration: 339\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         12880.2</td><td style=\"text-align: right;\">339000</td><td style=\"text-align: right;\">    9.33</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             96.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-50-45\n",
      "  done: false\n",
      "  episode_len_mean: 87.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.44\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1049\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8469805538654327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019078154780535685\n",
      "          policy_loss: 0.0028832776678933037\n",
      "          total_loss: 0.3131580679780907\n",
      "          vf_explained_var: 0.6225775480270386\n",
      "          vf_loss: 0.3132376682427194\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.048936170212766\n",
      "    ram_util_percent: 63.65276595744681\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708405429550685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.76706414125148\n",
      "    mean_inference_ms: 1.3162842600708475\n",
      "    mean_raw_obs_processing_ms: 17.945157050798713\n",
      "  time_since_restore: 13210.017856836319\n",
      "  time_this_iter_s: 329.8149163722992\n",
      "  time_total_s: 13210.017856836319\n",
      "  timers:\n",
      "    learn_throughput: 1551.437\n",
      "    learn_time_ms: 644.564\n",
      "    load_throughput: 53198.651\n",
      "    load_time_ms: 18.797\n",
      "    sample_throughput: 4.708\n",
      "    sample_time_ms: 212394.978\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1633542645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">           13210</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">    9.44</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             87.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 341000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_17-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 58.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.72\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1076\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2780158370733261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006785574046519906\n",
      "          policy_loss: 0.033510510209533904\n",
      "          total_loss: 0.11070260893967417\n",
      "          vf_explained_var: 0.9876610040664673\n",
      "          vf_loss: 0.07801359650782413\n",
      "    num_agent_steps_sampled: 341000\n",
      "    num_agent_steps_trained: 341000\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.68081081081081\n",
      "    ram_util_percent: 64.27243243243242\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370847275043635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.747284341486548\n",
      "    mean_inference_ms: 1.3163553892553304\n",
      "    mean_raw_obs_processing_ms: 19.396972241340958\n",
      "  time_since_restore: 13728.246433734894\n",
      "  time_this_iter_s: 518.2285768985748\n",
      "  time_total_s: 13728.246433734894\n",
      "  timers:\n",
      "    learn_throughput: 1550.805\n",
      "    learn_time_ms: 644.827\n",
      "    load_throughput: 53540.023\n",
      "    load_time_ms: 18.678\n",
      "    sample_throughput: 4.004\n",
      "    sample_time_ms: 249726.176\n",
      "    update_time_ms: 1.702\n",
      "  timestamp: 1633543163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 341\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         13728.2</td><td style=\"text-align: right;\">341000</td><td style=\"text-align: right;\">    9.72</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             58.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 342000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-00-49\n",
      "  done: false\n",
      "  episode_len_mean: 58.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.71\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2756630692217086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012521930630178129\n",
      "          policy_loss: -0.08664227889643775\n",
      "          total_loss: -0.031017057845989862\n",
      "          vf_explained_var: 0.4983328580856323\n",
      "          vf_loss: 0.0647673876852625\n",
      "    num_agent_steps_sampled: 342000\n",
      "    num_agent_steps_trained: 342000\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.032520325203254\n",
      "    ram_util_percent: 64.84308943089431\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037084832651000973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.74487434106728\n",
      "    mean_inference_ms: 1.3163659829100647\n",
      "    mean_raw_obs_processing_ms: 19.571098863778385\n",
      "  time_since_restore: 13814.420203924179\n",
      "  time_this_iter_s: 86.17377018928528\n",
      "  time_total_s: 13814.420203924179\n",
      "  timers:\n",
      "    learn_throughput: 1549.979\n",
      "    learn_time_ms: 645.17\n",
      "    load_throughput: 53669.024\n",
      "    load_time_ms: 18.633\n",
      "    sample_throughput: 4.005\n",
      "    sample_time_ms: 249709.873\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1633543249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 342\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         13814.4</td><td style=\"text-align: right;\">342000</td><td style=\"text-align: right;\">    9.71</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             58.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 343000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-05-57\n",
      "  done: false\n",
      "  episode_len_mean: 65.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.72\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1097\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4480274332894219\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003846456927931068\n",
      "          policy_loss: -0.06037242785096168\n",
      "          total_loss: 0.11668261446886592\n",
      "          vf_explained_var: 0.9851835370063782\n",
      "          vf_loss: 0.18042503108994828\n",
      "    num_agent_steps_sampled: 343000\n",
      "    num_agent_steps_trained: 343000\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.861731207289292\n",
      "    ram_util_percent: 64.81958997722096\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370852061939908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.732605833419413\n",
      "    mean_inference_ms: 1.3164186281310528\n",
      "    mean_raw_obs_processing_ms: 20.421882090034774\n",
      "  time_since_restore: 14122.196917533875\n",
      "  time_this_iter_s: 307.77671360969543\n",
      "  time_total_s: 14122.196917533875\n",
      "  timers:\n",
      "    learn_throughput: 1548.948\n",
      "    learn_time_ms: 645.6\n",
      "    load_throughput: 53618.46\n",
      "    load_time_ms: 18.65\n",
      "    sample_throughput: 3.87\n",
      "    sample_time_ms: 258408.62\n",
      "    update_time_ms: 1.717\n",
      "  timestamp: 1633543557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         14122.2</td><td style=\"text-align: right;\">343000</td><td style=\"text-align: right;\">    9.72</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             65.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-09-16\n",
      "  done: false\n",
      "  episode_len_mean: 67.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.7\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1108\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14432537890970706\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0958480642901527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027255506674970485\n",
      "          policy_loss: -0.02708881100018819\n",
      "          total_loss: 0.1262448936700821\n",
      "          vf_explained_var: 0.8211331963539124\n",
      "          vf_loss: 0.16035852486060725\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.57605633802817\n",
      "    ram_util_percent: 64.83345070422537\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708543118033639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.724542084739312\n",
      "    mean_inference_ms: 1.3164528625002012\n",
      "    mean_raw_obs_processing_ms: 20.948180662584686\n",
      "  time_since_restore: 14320.802127361298\n",
      "  time_this_iter_s: 198.6052098274231\n",
      "  time_total_s: 14320.802127361298\n",
      "  timers:\n",
      "    learn_throughput: 1549.646\n",
      "    learn_time_ms: 645.309\n",
      "    load_throughput: 53472.653\n",
      "    load_time_ms: 18.701\n",
      "    sample_throughput: 3.792\n",
      "    sample_time_ms: 263692.175\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1633543756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 344\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         14320.8</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">     9.7</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             67.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-10-43\n",
      "  done: false\n",
      "  episode_len_mean: 73.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.67\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1113\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2164880683645607\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2933816605144077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020071450099794833\n",
      "          policy_loss: -0.056552247868643865\n",
      "          total_loss: 0.07029434757100211\n",
      "          vf_explained_var: 0.9293187260627747\n",
      "          vf_loss: 0.13543518282887007\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.07741935483871\n",
      "    ram_util_percent: 64.95887096774193\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708552915389479\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.720097499544003\n",
      "    mean_inference_ms: 1.3164664310408278\n",
      "    mean_raw_obs_processing_ms: 21.16496385086301\n",
      "  time_since_restore: 14408.332461833954\n",
      "  time_this_iter_s: 87.53033447265625\n",
      "  time_total_s: 14408.332461833954\n",
      "  timers:\n",
      "    learn_throughput: 1550.302\n",
      "    learn_time_ms: 645.036\n",
      "    load_throughput: 53354.093\n",
      "    load_time_ms: 18.743\n",
      "    sample_throughput: 3.764\n",
      "    sample_time_ms: 265677.413\n",
      "    update_time_ms: 1.711\n",
      "  timestamp: 1633543843\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 345\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         14408.3</td><td style=\"text-align: right;\">345000</td><td style=\"text-align: right;\">    9.67</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             73.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 346000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-15-56\n",
      "  done: false\n",
      "  episode_len_mean: 73.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.7\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1130\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3247321025468408\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.934649983048439\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004268894003203528\n",
      "          policy_loss: 0.009153933823108673\n",
      "          total_loss: 0.045312822196218704\n",
      "          vf_explained_var: 0.32941216230392456\n",
      "          vf_loss: 0.044119143494430725\n",
      "    num_agent_steps_sampled: 346000\n",
      "    num_agent_steps_trained: 346000\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.96420581655481\n",
      "    ram_util_percent: 64.87203579418345\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037085851152722574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.704281417583925\n",
      "    mean_inference_ms: 1.3165168476199898\n",
      "    mean_raw_obs_processing_ms: 21.99755966288236\n",
      "  time_since_restore: 14721.030509233475\n",
      "  time_this_iter_s: 312.6980473995209\n",
      "  time_total_s: 14721.030509233475\n",
      "  timers:\n",
      "    learn_throughput: 1552.666\n",
      "    learn_time_ms: 644.054\n",
      "    load_throughput: 53108.523\n",
      "    load_time_ms: 18.829\n",
      "    sample_throughput: 3.869\n",
      "    sample_time_ms: 258479.564\n",
      "    update_time_ms: 1.729\n",
      "  timestamp: 1633544156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">           14721</td><td style=\"text-align: right;\">346000</td><td style=\"text-align: right;\">     9.7</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             73.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 347000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-21-48\n",
      "  done: false\n",
      "  episode_len_mean: 67.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.75\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 1148\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.31344637274742126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004869590989207712\n",
      "          policy_loss: -0.18380817787514792\n",
      "          total_loss: -0.1259434224002891\n",
      "          vf_explained_var: 0.9936785101890564\n",
      "          vf_loss: 0.06020855935704377\n",
      "    num_agent_steps_sampled: 347000\n",
      "    num_agent_steps_trained: 347000\n",
      "    num_steps_sampled: 347000\n",
      "    num_steps_trained: 347000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.01996007984032\n",
      "    ram_util_percent: 64.73213572854291\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037086008543916506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.68753039477665\n",
      "    mean_inference_ms: 1.3165712957098403\n",
      "    mean_raw_obs_processing_ms: 22.856534093671772\n",
      "  time_since_restore: 15072.671256542206\n",
      "  time_this_iter_s: 351.6407473087311\n",
      "  time_total_s: 15072.671256542206\n",
      "  timers:\n",
      "    learn_throughput: 1549.15\n",
      "    learn_time_ms: 645.515\n",
      "    load_throughput: 53021.916\n",
      "    load_time_ms: 18.86\n",
      "    sample_throughput: 3.704\n",
      "    sample_time_ms: 269989.7\n",
      "    update_time_ms: 1.715\n",
      "  timestamp: 1633544508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347000\n",
      "  training_iteration: 347\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         15072.7</td><td style=\"text-align: right;\">347000</td><td style=\"text-align: right;\">    9.75</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             67.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-28-37\n",
      "  done: false\n",
      "  episode_len_mean: 72.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.75\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1170\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0811830256367102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5685383621189329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017116240109377075\n",
      "          policy_loss: 0.028635855639974277\n",
      "          total_loss: 0.04020042291118039\n",
      "          vf_explained_var: 0.998080849647522\n",
      "          vf_loss: 0.015860402351245285\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.8597602739726\n",
      "    ram_util_percent: 66.84794520547946\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708739164918247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.669008961958735\n",
      "    mean_inference_ms: 1.3166712474713511\n",
      "    mean_raw_obs_processing_ms: 23.819444676125343\n",
      "  time_since_restore: 15481.91457605362\n",
      "  time_this_iter_s: 409.2433195114136\n",
      "  time_total_s: 15481.91457605362\n",
      "  timers:\n",
      "    learn_throughput: 1539.194\n",
      "    learn_time_ms: 649.691\n",
      "    load_throughput: 53029.425\n",
      "    load_time_ms: 18.857\n",
      "    sample_throughput: 3.578\n",
      "    sample_time_ms: 279470.926\n",
      "    update_time_ms: 1.722\n",
      "  timestamp: 1633544917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 348\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         15481.9</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">    9.75</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             72.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 349000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-36-05\n",
      "  done: false\n",
      "  episode_len_mean: 64.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.77\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1193\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0811830256367102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4976729267173343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007537244277320513\n",
      "          policy_loss: 0.00835903427667088\n",
      "          total_loss: 0.0977929233883818\n",
      "          vf_explained_var: 0.9924057126045227\n",
      "          vf_loss: 0.09379872118847238\n",
      "    num_agent_steps_sampled: 349000\n",
      "    num_agent_steps_trained: 349000\n",
      "    num_steps_sampled: 349000\n",
      "    num_steps_trained: 349000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.21921875\n",
      "    ram_util_percent: 67.2228125\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709117017357979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.65428483253301\n",
      "    mean_inference_ms: 1.3168396245385459\n",
      "    mean_raw_obs_processing_ms: 24.948543344386643\n",
      "  time_since_restore: 15930.025237321854\n",
      "  time_this_iter_s: 448.11066126823425\n",
      "  time_total_s: 15930.025237321854\n",
      "  timers:\n",
      "    learn_throughput: 1530.521\n",
      "    learn_time_ms: 653.372\n",
      "    load_throughput: 53310.76\n",
      "    load_time_ms: 18.758\n",
      "    sample_throughput: 3.286\n",
      "    sample_time_ms: 304304.77\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1633545365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349000\n",
      "  training_iteration: 349\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">           15930</td><td style=\"text-align: right;\">349000</td><td style=\"text-align: right;\">    9.77</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             64.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 350000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-43-55\n",
      "  done: false\n",
      "  episode_len_mean: 44.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.96\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1217\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0811830256367102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2672711397210757\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003156071503932682\n",
      "          policy_loss: -0.14335287974940406\n",
      "          total_loss: -0.11251796326703495\n",
      "          vf_explained_var: 0.9944213032722473\n",
      "          vf_loss: 0.03325140828059779\n",
      "    num_agent_steps_sampled: 350000\n",
      "    num_agent_steps_trained: 350000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.81358208955224\n",
      "    ram_util_percent: 67.03223880597015\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709663958829415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.644259396328845\n",
      "    mean_inference_ms: 1.317056828208456\n",
      "    mean_raw_obs_processing_ms: 26.187405103067178\n",
      "  time_since_restore: 16399.54205918312\n",
      "  time_this_iter_s: 469.5168218612671\n",
      "  time_total_s: 16399.54205918312\n",
      "  timers:\n",
      "    learn_throughput: 1518.049\n",
      "    learn_time_ms: 658.74\n",
      "    load_throughput: 54847.435\n",
      "    load_time_ms: 18.232\n",
      "    sample_throughput: 3.142\n",
      "    sample_time_ms: 318270.147\n",
      "    update_time_ms: 1.732\n",
      "  timestamp: 1633545835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         16399.5</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\">    9.96</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">             44.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 351000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-51-56\n",
      "  done: false\n",
      "  episode_len_mean: 45.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.93\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1242\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0405915128183551\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.34249284168084465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008640013394516978\n",
      "          policy_loss: -0.01911449788345231\n",
      "          total_loss: 0.0036792139212290446\n",
      "          vf_explained_var: 0.9962595701217651\n",
      "          vf_loss: 0.02586792834723989\n",
      "    num_agent_steps_sampled: 351000\n",
      "    num_agent_steps_trained: 351000\n",
      "    num_steps_sampled: 351000\n",
      "    num_steps_trained: 351000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.997816593886462\n",
      "    ram_util_percent: 67.49490538573508\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037104671573821545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.638721035930914\n",
      "    mean_inference_ms: 1.3173526393215027\n",
      "    mean_raw_obs_processing_ms: 27.472451427529286\n",
      "  time_since_restore: 16880.871172189713\n",
      "  time_this_iter_s: 481.3291130065918\n",
      "  time_total_s: 16880.871172189713\n",
      "  timers:\n",
      "    learn_throughput: 1509.535\n",
      "    learn_time_ms: 662.456\n",
      "    load_throughput: 57043.242\n",
      "    load_time_ms: 17.531\n",
      "    sample_throughput: 3.179\n",
      "    sample_time_ms: 314577.138\n",
      "    update_time_ms: 1.734\n",
      "  timestamp: 1633546316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351000\n",
      "  training_iteration: 351\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         16880.9</td><td style=\"text-align: right;\">351000</td><td style=\"text-align: right;\">    9.93</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">             45.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_18-59-49\n",
      "  done: false\n",
      "  episode_len_mean: 41.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1266\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0405915128183551\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.43699824710687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013334546803412966\n",
      "          policy_loss: 0.05538976403574149\n",
      "          total_loss: 0.11893540012339751\n",
      "          vf_explained_var: 0.9935937523841858\n",
      "          vf_loss: 0.06737435165171822\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.35890207715134\n",
      "    ram_util_percent: 67.58961424332345\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711357401555887\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.634263332390663\n",
      "    mean_inference_ms: 1.3176673595927633\n",
      "    mean_raw_obs_processing_ms: 28.69421006439301\n",
      "  time_since_restore: 17353.430981874466\n",
      "  time_this_iter_s: 472.5598096847534\n",
      "  time_total_s: 17353.430981874466\n",
      "  timers:\n",
      "    learn_throughput: 1500.419\n",
      "    learn_time_ms: 666.481\n",
      "    load_throughput: 56886.42\n",
      "    load_time_ms: 17.579\n",
      "    sample_throughput: 2.831\n",
      "    sample_time_ms: 353211.638\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1633546789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 352\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         17353.4</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             41.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 353000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_19-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 45.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.89\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1281\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0405915128183551\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1620630704694324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027190717072608964\n",
      "          policy_loss: 0.12025279468960232\n",
      "          total_loss: 0.1480346476038297\n",
      "          vf_explained_var: 0.9960489273071289\n",
      "          vf_loss: 0.03829877431400948\n",
      "    num_agent_steps_sampled: 353000\n",
      "    num_agent_steps_trained: 353000\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.8029776674938\n",
      "    ram_util_percent: 67.70372208436726\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371193322424464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.63315576228854\n",
      "    mean_inference_ms: 1.3178644058552476\n",
      "    mean_raw_obs_processing_ms: 29.3886220366604\n",
      "  time_since_restore: 17635.995104074478\n",
      "  time_this_iter_s: 282.5641222000122\n",
      "  time_total_s: 17635.995104074478\n",
      "  timers:\n",
      "    learn_throughput: 1501.007\n",
      "    learn_time_ms: 666.22\n",
      "    load_throughput: 57189.7\n",
      "    load_time_ms: 17.486\n",
      "    sample_throughput: 2.852\n",
      "    sample_time_ms: 350690.747\n",
      "    update_time_ms: 1.724\n",
      "  timestamp: 1633547071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">           17636</td><td style=\"text-align: right;\">353000</td><td style=\"text-align: right;\">    9.89</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">              45.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 354000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_19-11-36\n",
      "  done: false\n",
      "  episode_len_mean: 44.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1303\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.29364401631885106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010418733690456166\n",
      "          policy_loss: -0.2544620268874698\n",
      "          total_loss: -0.23633202090859412\n",
      "          vf_explained_var: 0.9967111349105835\n",
      "          vf_loss: 0.020432078807304303\n",
      "    num_agent_steps_sampled: 354000\n",
      "    num_agent_steps_trained: 354000\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.466831683168312\n",
      "    ram_util_percent: 67.40445544554456\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037127779059039245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.630454003315762\n",
      "    mean_inference_ms: 1.318173149875059\n",
      "    mean_raw_obs_processing_ms: 30.452065809446957\n",
      "  time_since_restore: 18060.677689552307\n",
      "  time_this_iter_s: 424.682585477829\n",
      "  time_total_s: 18060.677689552307\n",
      "  timers:\n",
      "    learn_throughput: 1489.289\n",
      "    learn_time_ms: 671.461\n",
      "    load_throughput: 59646.47\n",
      "    load_time_ms: 16.765\n",
      "    sample_throughput: 2.679\n",
      "    sample_time_ms: 373293.926\n",
      "    update_time_ms: 1.735\n",
      "  timestamp: 1633547496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 354\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         18060.7</td><td style=\"text-align: right;\">354000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             44.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 355000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_19-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 51.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1314\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0241462906201682\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02139121975629775\n",
      "          policy_loss: 0.11349379635519452\n",
      "          total_loss: 0.19223201647400856\n",
      "          vf_explained_var: 0.4390571713447571\n",
      "          vf_loss: 0.08767722740562425\n",
      "    num_agent_steps_sampled: 355000\n",
      "    num_agent_steps_trained: 355000\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.40993150684932\n",
      "    ram_util_percent: 67.83527397260275\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713250103556168\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.62871816085403\n",
      "    mean_inference_ms: 1.3183418047346018\n",
      "    mean_raw_obs_processing_ms: 30.959000640990407\n",
      "  time_since_restore: 18265.046567440033\n",
      "  time_this_iter_s: 204.36887788772583\n",
      "  time_total_s: 18265.046567440033\n",
      "  timers:\n",
      "    learn_throughput: 1480.346\n",
      "    learn_time_ms: 675.518\n",
      "    load_throughput: 60149.344\n",
      "    load_time_ms: 16.625\n",
      "    sample_throughput: 2.598\n",
      "    sample_time_ms: 384973.84\n",
      "    update_time_ms: 1.739\n",
      "  timestamp: 1633547700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">           18265</td><td style=\"text-align: right;\">355000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             51.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_19-23-25\n",
      "  done: false\n",
      "  episode_len_mean: 50.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.88\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1340\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09133090384129905\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2582437339756224\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005683721345262743\n",
      "          policy_loss: -0.013356426192654505\n",
      "          total_loss: -0.004285883179141415\n",
      "          vf_explained_var: 0.998159646987915\n",
      "          vf_loss: 0.01113388143065903\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.575416666666666\n",
      "    ram_util_percent: 67.76388888888889\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037143765307153326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.624197109914434\n",
      "    mean_inference_ms: 1.318741036873257\n",
      "    mean_raw_obs_processing_ms: 32.204033875205816\n",
      "  time_since_restore: 18769.80161166191\n",
      "  time_this_iter_s: 504.75504422187805\n",
      "  time_total_s: 18769.80161166191\n",
      "  timers:\n",
      "    learn_throughput: 1483.043\n",
      "    learn_time_ms: 674.289\n",
      "    load_throughput: 60887.081\n",
      "    load_time_ms: 16.424\n",
      "    sample_throughput: 2.474\n",
      "    sample_time_ms: 404180.976\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1633548205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 356\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         18769.8</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">    9.88</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             50.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 357000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_19-31-25\n",
      "  done: false\n",
      "  episode_len_mean: 50.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1365\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09133090384129905\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.25346632964081234\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006024752538576836\n",
      "          policy_loss: 0.040440792300634916\n",
      "          total_loss: 0.06413704496290949\n",
      "          vf_explained_var: 0.9965187907218933\n",
      "          vf_loss: 0.025680671616767844\n",
      "    num_agent_steps_sampled: 357000\n",
      "    num_agent_steps_trained: 357000\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.611094890510948\n",
      "    ram_util_percent: 66.83299270072993\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03715234950037787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.61885685650202\n",
      "    mean_inference_ms: 1.319067787938176\n",
      "    mean_raw_obs_processing_ms: 33.38824942797095\n",
      "  time_since_restore: 19249.525854587555\n",
      "  time_this_iter_s: 479.7242429256439\n",
      "  time_total_s: 19249.525854587555\n",
      "  timers:\n",
      "    learn_throughput: 1484.107\n",
      "    learn_time_ms: 673.806\n",
      "    load_throughput: 61902.333\n",
      "    load_time_ms: 16.154\n",
      "    sample_throughput: 2.398\n",
      "    sample_time_ms: 416990.065\n",
      "    update_time_ms: 1.712\n",
      "  timestamp: 1633548685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 357\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         19249.5</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             50.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 358000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_19-39-58\n",
      "  done: false\n",
      "  episode_len_mean: 45.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.93\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1392\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09133090384129905\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.23473120563560063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003693263329881279\n",
      "          policy_loss: -0.046794688618845406\n",
      "          total_loss: -0.0394003431002299\n",
      "          vf_explained_var: 0.9983786940574646\n",
      "          vf_loss: 0.009404347192806502\n",
      "    num_agent_steps_sampled: 358000\n",
      "    num_agent_steps_trained: 358000\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.61898907103825\n",
      "    ram_util_percent: 66.8893442622951\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037158800303102486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.609801582066964\n",
      "    mean_inference_ms: 1.3193418994585577\n",
      "    mean_raw_obs_processing_ms: 34.702566476883\n",
      "  time_since_restore: 19762.554921388626\n",
      "  time_this_iter_s: 513.0290668010712\n",
      "  time_total_s: 19762.554921388626\n",
      "  timers:\n",
      "    learn_throughput: 1490.727\n",
      "    learn_time_ms: 670.814\n",
      "    load_throughput: 62247.947\n",
      "    load_time_ms: 16.065\n",
      "    sample_throughput: 2.34\n",
      "    sample_time_ms: 427372.279\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1633549198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         19762.6</td><td style=\"text-align: right;\">358000</td><td style=\"text-align: right;\">    9.93</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             45.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 359000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_19-40-28\n",
      "  done: false\n",
      "  episode_len_mean: 55.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.9\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1394\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.045665451920649525\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6228445318010118\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015967176336345704\n",
      "          policy_loss: -0.03839170361558596\n",
      "          total_loss: 0.0035793816877735985\n",
      "          vf_explained_var: 0.2624513506889343\n",
      "          vf_loss: 0.04747038388417827\n",
      "    num_agent_steps_sampled: 359000\n",
      "    num_agent_steps_trained: 359000\n",
      "    num_steps_sampled: 359000\n",
      "    num_steps_trained: 359000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.083720930232563\n",
      "    ram_util_percent: 67.06976744186048\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037159250691400386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.608888091332183\n",
      "    mean_inference_ms: 1.3193600759324935\n",
      "    mean_raw_obs_processing_ms: 34.78572967337768\n",
      "  time_since_restore: 19792.548393011093\n",
      "  time_this_iter_s: 29.99347162246704\n",
      "  time_total_s: 19792.548393011093\n",
      "  timers:\n",
      "    learn_throughput: 1495.457\n",
      "    learn_time_ms: 668.692\n",
      "    load_throughput: 61880.323\n",
      "    load_time_ms: 16.16\n",
      "    sample_throughput: 2.594\n",
      "    sample_time_ms: 385562.619\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1633549228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359000\n",
      "  training_iteration: 359\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         19792.5</td><td style=\"text-align: right;\">359000</td><td style=\"text-align: right;\">     9.9</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             55.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_19-48-45\n",
      "  done: false\n",
      "  episode_len_mean: 47.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.96\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1420\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.045665451920649525\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.24133872704373466\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0031680704849618674\n",
      "          policy_loss: -0.022601399425831108\n",
      "          total_loss: -0.00855522229232722\n",
      "          vf_explained_var: 0.9968703389167786\n",
      "          vf_loss: 0.01631489291580187\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.567136812411846\n",
      "    ram_util_percent: 67.06741889985896\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716360531990977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.59624014480149\n",
      "    mean_inference_ms: 1.3195647703130104\n",
      "    mean_raw_obs_processing_ms: 36.03682851526864\n",
      "  time_since_restore: 20289.343456983566\n",
      "  time_this_iter_s: 496.79506397247314\n",
      "  time_total_s: 20289.343456983566\n",
      "  timers:\n",
      "    learn_throughput: 1505.341\n",
      "    learn_time_ms: 664.302\n",
      "    load_throughput: 60345.964\n",
      "    load_time_ms: 16.571\n",
      "    sample_throughput: 2.575\n",
      "    sample_time_ms: 388294.42\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633549725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 360\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         20289.3</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">    9.96</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             47.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 361000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_19-57-23\n",
      "  done: false\n",
      "  episode_len_mean: 47.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.96\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1447\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.022832725960324762\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.22329652425315644\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004372054165244046\n",
      "          policy_loss: 0.0268270765327745\n",
      "          total_loss: 0.031395075822042096\n",
      "          vf_explained_var: 0.9988997578620911\n",
      "          vf_loss: 0.006701138049053649\n",
      "    num_agent_steps_sampled: 361000\n",
      "    num_agent_steps_trained: 361000\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.57807848443843\n",
      "    ram_util_percent: 66.95331529093369\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716611578875909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.583232161243178\n",
      "    mean_inference_ms: 1.3197257290090625\n",
      "    mean_raw_obs_processing_ms: 37.29264601247942\n",
      "  time_since_restore: 20807.27398467064\n",
      "  time_this_iter_s: 517.9305276870728\n",
      "  time_total_s: 20807.27398467064\n",
      "  timers:\n",
      "    learn_throughput: 1516.685\n",
      "    learn_time_ms: 659.332\n",
      "    load_throughput: 57887.92\n",
      "    load_time_ms: 17.275\n",
      "    sample_throughput: 2.551\n",
      "    sample_time_ms: 391958.891\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1633550243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         20807.3</td><td style=\"text-align: right;\">361000</td><td style=\"text-align: right;\">    9.96</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             47.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 362000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-05-22\n",
      "  done: false\n",
      "  episode_len_mean: 46.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.97\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1472\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.011416362980162381\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.25124536520904966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011935507297087809\n",
      "          policy_loss: -0.2018683296110895\n",
      "          total_loss: -0.20152758326795367\n",
      "          vf_explained_var: 0.9996016621589661\n",
      "          vf_loss: 0.002716939706200113\n",
      "    num_agent_steps_sampled: 362000\n",
      "    num_agent_steps_trained: 362000\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.540204678362574\n",
      "    ram_util_percent: 67.11608187134503\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716872494962941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.57115712324746\n",
      "    mean_inference_ms: 1.319878647541057\n",
      "    mean_raw_obs_processing_ms: 38.43549851322074\n",
      "  time_since_restore: 21286.70539188385\n",
      "  time_this_iter_s: 479.43140721321106\n",
      "  time_total_s: 21286.70539188385\n",
      "  timers:\n",
      "    learn_throughput: 1530.836\n",
      "    learn_time_ms: 653.238\n",
      "    load_throughput: 58009.374\n",
      "    load_time_ms: 17.239\n",
      "    sample_throughput: 2.547\n",
      "    sample_time_ms: 392652.206\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1633550722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 362\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         21286.7</td><td style=\"text-align: right;\">362000</td><td style=\"text-align: right;\">    9.97</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             46.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 363000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-09-04\n",
      "  done: false\n",
      "  episode_len_mean: 52.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.94\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1484\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.011416362980162381\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0111489252911674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.09309087448011921\n",
      "          policy_loss: 0.06121718337138494\n",
      "          total_loss: 0.11511788583464093\n",
      "          vf_explained_var: 0.8217215538024902\n",
      "          vf_loss: 0.06294942787951893\n",
      "    num_agent_steps_sampled: 363000\n",
      "    num_agent_steps_trained: 363000\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.872784810126582\n",
      "    ram_util_percent: 67.40727848101265\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03717008841072369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.565604181989137\n",
      "    mean_inference_ms: 1.3199541496186027\n",
      "    mean_raw_obs_processing_ms: 38.9267924698157\n",
      "  time_since_restore: 21508.154445409775\n",
      "  time_this_iter_s: 221.44905352592468\n",
      "  time_total_s: 21508.154445409775\n",
      "  timers:\n",
      "    learn_throughput: 1536.054\n",
      "    learn_time_ms: 651.019\n",
      "    load_throughput: 57281.471\n",
      "    load_time_ms: 17.458\n",
      "    sample_throughput: 2.587\n",
      "    sample_time_ms: 386542.691\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1633550944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 363\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         21508.2</td><td style=\"text-align: right;\">363000</td><td style=\"text-align: right;\">    9.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             52.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-16-08\n",
      "  done: false\n",
      "  episode_len_mean: 44.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.97\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1506\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.017124544470243558\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.34880547324816386\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012192021968624431\n",
      "          policy_loss: -0.24423770159482955\n",
      "          total_loss: -0.2389435009823905\n",
      "          vf_explained_var: 0.9988594651222229\n",
      "          vf_loss: 0.008573471609916951\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.755206611570248\n",
      "    ram_util_percent: 67.4700826446281\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03717266981991828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.55727871169802\n",
      "    mean_inference_ms: 1.320099389530152\n",
      "    mean_raw_obs_processing_ms: 39.90880778698457\n",
      "  time_since_restore: 21932.321341991425\n",
      "  time_this_iter_s: 424.1668965816498\n",
      "  time_total_s: 21932.321341991425\n",
      "  timers:\n",
      "    learn_throughput: 1550.142\n",
      "    learn_time_ms: 645.102\n",
      "    load_throughput: 54612.409\n",
      "    load_time_ms: 18.311\n",
      "    sample_throughput: 2.587\n",
      "    sample_time_ms: 386496.23\n",
      "    update_time_ms: 1.618\n",
      "  timestamp: 1633551368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         21932.3</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">    9.97</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             44.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 365000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-18-20\n",
      "  done: false\n",
      "  episode_len_mean: 52.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.84\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1514\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.017124544470243558\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2170654773712157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.05717110140088937\n",
      "          policy_loss: 0.07569731324911118\n",
      "          total_loss: 0.25001439654992685\n",
      "          vf_explained_var: 0.8176006078720093\n",
      "          vf_loss: 0.18550870596534677\n",
      "    num_agent_steps_sampled: 365000\n",
      "    num_agent_steps_trained: 365000\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.3031914893617\n",
      "    ram_util_percent: 67.45212765957447\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03717363427514689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.555962630034436\n",
      "    mean_inference_ms: 1.3201550804525803\n",
      "    mean_raw_obs_processing_ms: 40.23729131881047\n",
      "  time_since_restore: 22064.077367067337\n",
      "  time_this_iter_s: 131.75602507591248\n",
      "  time_total_s: 22064.077367067337\n",
      "  timers:\n",
      "    learn_throughput: 1558.931\n",
      "    learn_time_ms: 641.465\n",
      "    load_throughput: 54512.259\n",
      "    load_time_ms: 18.344\n",
      "    sample_throughput: 2.637\n",
      "    sample_time_ms: 379238.588\n",
      "    update_time_ms: 1.615\n",
      "  timestamp: 1633551500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 365\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         22064.1</td><td style=\"text-align: right;\">365000</td><td style=\"text-align: right;\">    9.84</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             52.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 366000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-19-17\n",
      "  done: false\n",
      "  episode_len_mean: 57.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.81\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1517\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.025686816705365344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3604958666695488\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01775120083598891\n",
      "          policy_loss: -0.027449126541614532\n",
      "          total_loss: 0.04068652126524183\n",
      "          vf_explained_var: 0.8851577639579773\n",
      "          vf_loss: 0.08128463530706034\n",
      "    num_agent_steps_sampled: 366000\n",
      "    num_agent_steps_trained: 366000\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.521686746987953\n",
      "    ram_util_percent: 67.53373493975904\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03717409019337695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.555628388723783\n",
      "    mean_inference_ms: 1.3201795294862384\n",
      "    mean_raw_obs_processing_ms: 40.36009623630098\n",
      "  time_since_restore: 22121.922845363617\n",
      "  time_this_iter_s: 57.84547829627991\n",
      "  time_total_s: 22121.922845363617\n",
      "  timers:\n",
      "    learn_throughput: 1556.698\n",
      "    learn_time_ms: 642.385\n",
      "    load_throughput: 53572.095\n",
      "    load_time_ms: 18.666\n",
      "    sample_throughput: 2.989\n",
      "    sample_time_ms: 334546.39\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1633551557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 366\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         22121.9</td><td style=\"text-align: right;\">366000</td><td style=\"text-align: right;\">    9.81</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             57.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 367000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-23-55\n",
      "  done: false\n",
      "  episode_len_mean: 65.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.75\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1533\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.025686816705365344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0274640841616525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03459798660509487\n",
      "          policy_loss: 0.11637818449073367\n",
      "          total_loss: 0.19708209799395668\n",
      "          vf_explained_var: 0.9899064898490906\n",
      "          vf_loss: 0.09008984229423933\n",
      "    num_agent_steps_sampled: 367000\n",
      "    num_agent_steps_trained: 367000\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.44116161616161\n",
      "    ram_util_percent: 67.53207070707072\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03717676910786587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.555720491372494\n",
      "    mean_inference_ms: 1.3203088467681265\n",
      "    mean_raw_obs_processing_ms: 40.94159508843219\n",
      "  time_since_restore: 22399.41495347023\n",
      "  time_this_iter_s: 277.49210810661316\n",
      "  time_total_s: 22399.41495347023\n",
      "  timers:\n",
      "    learn_throughput: 1559.08\n",
      "    learn_time_ms: 641.404\n",
      "    load_throughput: 53534.966\n",
      "    load_time_ms: 18.679\n",
      "    sample_throughput: 3.181\n",
      "    sample_time_ms: 314324.132\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1633551835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         22399.4</td><td style=\"text-align: right;\">367000</td><td style=\"text-align: right;\">    9.75</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             65.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-27-17\n",
      "  done: false\n",
      "  episode_len_mean: 71.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.72\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1544\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.038530225058048026\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4733178933461508\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019008380085077253\n",
      "          policy_loss: 0.17401915374729368\n",
      "          total_loss: 0.2454905522366365\n",
      "          vf_explained_var: 0.8116285800933838\n",
      "          vf_loss: 0.08547217866612805\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.673263888888886\n",
      "    ram_util_percent: 67.66805555555555\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037178759754547736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.55496794432008\n",
      "    mean_inference_ms: 1.320406831983749\n",
      "    mean_raw_obs_processing_ms: 41.3578156095371\n",
      "  time_since_restore: 22600.95696425438\n",
      "  time_this_iter_s: 201.54201078414917\n",
      "  time_total_s: 22600.95696425438\n",
      "  timers:\n",
      "    learn_throughput: 1562.371\n",
      "    learn_time_ms: 640.053\n",
      "    load_throughput: 53664.629\n",
      "    load_time_ms: 18.634\n",
      "    sample_throughput: 3.531\n",
      "    sample_time_ms: 283176.847\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1633552037\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 368\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">           22601</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">    9.72</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             71.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 369000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-31-19\n",
      "  done: false\n",
      "  episode_len_mean: 76.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.68\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1557\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.038530225058048026\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7792815446853638\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017304321238883296\n",
      "          policy_loss: -0.20369255741437275\n",
      "          total_loss: -0.12054287360774146\n",
      "          vf_explained_var: 0.788899838924408\n",
      "          vf_loss: 0.09027575680779086\n",
      "    num_agent_steps_sampled: 369000\n",
      "    num_agent_steps_trained: 369000\n",
      "    num_steps_sampled: 369000\n",
      "    num_steps_trained: 369000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.538840579710143\n",
      "    ram_util_percent: 67.70376811594203\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03718123693261836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.554977495933773\n",
      "    mean_inference_ms: 1.320525707516132\n",
      "    mean_raw_obs_processing_ms: 41.796003390822214\n",
      "  time_since_restore: 22843.205229759216\n",
      "  time_this_iter_s: 242.24826550483704\n",
      "  time_total_s: 22843.205229759216\n",
      "  timers:\n",
      "    learn_throughput: 1567.839\n",
      "    learn_time_ms: 637.821\n",
      "    load_throughput: 53502.324\n",
      "    load_time_ms: 18.691\n",
      "    sample_throughput: 3.285\n",
      "    sample_time_ms: 304404.502\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1633552279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369000\n",
      "  training_iteration: 369\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         22843.2</td><td style=\"text-align: right;\">369000</td><td style=\"text-align: right;\">    9.68</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             76.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 370000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-34-43\n",
      "  done: false\n",
      "  episode_len_mean: 82.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.61\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1568\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.038530225058048026\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2527018556992213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.05575265422513931\n",
      "          policy_loss: 0.0315584518843227\n",
      "          total_loss: 0.19943377210034263\n",
      "          vf_explained_var: 0.7655395269393921\n",
      "          vf_loss: 0.1782541724542777\n",
      "    num_agent_steps_sampled: 370000\n",
      "    num_agent_steps_trained: 370000\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.870205479452054\n",
      "    ram_util_percent: 67.76541095890411\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037183496465102654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.555283919718434\n",
      "    mean_inference_ms: 1.320637901321215\n",
      "    mean_raw_obs_processing_ms: 42.17930538078475\n",
      "  time_since_restore: 23047.77247953415\n",
      "  time_this_iter_s: 204.56724977493286\n",
      "  time_total_s: 23047.77247953415\n",
      "  timers:\n",
      "    learn_throughput: 1569.265\n",
      "    learn_time_ms: 637.241\n",
      "    load_throughput: 53510.105\n",
      "    load_time_ms: 18.688\n",
      "    sample_throughput: 3.634\n",
      "    sample_time_ms: 275182.306\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633552483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         23047.8</td><td style=\"text-align: right;\">370000</td><td style=\"text-align: right;\">    9.61</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             82.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 371000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-39-43\n",
      "  done: false\n",
      "  episode_len_mean: 79.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.56\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1584\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.057795337587072004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8193633917305204\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01416319395161793\n",
      "          policy_loss: -0.08643087281121148\n",
      "          total_loss: 0.02964731156826019\n",
      "          vf_explained_var: 0.9866622686386108\n",
      "          vf_loss: 0.12345324847847224\n",
      "    num_agent_steps_sampled: 371000\n",
      "    num_agent_steps_trained: 371000\n",
      "    num_steps_sampled: 371000\n",
      "    num_steps_trained: 371000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.414285714285707\n",
      "    ram_util_percent: 67.67330210772833\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03718679388669798\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.55645079516586\n",
      "    mean_inference_ms: 1.3208113013997809\n",
      "    mean_raw_obs_processing_ms: 42.78450361518892\n",
      "  time_since_restore: 23346.977375984192\n",
      "  time_this_iter_s: 299.2048964500427\n",
      "  time_total_s: 23346.977375984192\n",
      "  timers:\n",
      "    learn_throughput: 1567.464\n",
      "    learn_time_ms: 637.973\n",
      "    load_throughput: 53610.304\n",
      "    load_time_ms: 18.653\n",
      "    sample_throughput: 3.948\n",
      "    sample_time_ms: 253309.044\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1633552783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 371000\n",
      "  training_iteration: 371\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">           23347</td><td style=\"text-align: right;\">371000</td><td style=\"text-align: right;\">    9.56</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             79.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 86.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.49\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1591\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.057795337587072004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.335556408762932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02170101068034565\n",
      "          policy_loss: -0.007849726660384072\n",
      "          total_loss: 0.10754850059747696\n",
      "          vf_explained_var: 0.8223041892051697\n",
      "          vf_loss: 0.1274995766994026\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.4427027027027\n",
      "    ram_util_percent: 68.08432432432434\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037188159335155006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.557680191805815\n",
      "    mean_inference_ms: 1.3208843121555944\n",
      "    mean_raw_obs_processing_ms: 42.98309267503351\n",
      "  time_since_restore: 23476.59779071808\n",
      "  time_this_iter_s: 129.62041473388672\n",
      "  time_total_s: 23476.59779071808\n",
      "  timers:\n",
      "    learn_throughput: 1567.251\n",
      "    learn_time_ms: 638.06\n",
      "    load_throughput: 53506.692\n",
      "    load_time_ms: 18.689\n",
      "    sample_throughput: 4.58\n",
      "    sample_time_ms: 218327.828\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633552912\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 372\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         23476.6</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">    9.49</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              86.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 373000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-45-55\n",
      "  done: false\n",
      "  episode_len_mean: 91.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.44\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1604\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08669300638060806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.103781510061688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01478533778307543\n",
      "          policy_loss: 0.0763790488243103\n",
      "          total_loss: 0.15892019669214885\n",
      "          vf_explained_var: 0.7407925128936768\n",
      "          vf_loss: 0.09229718140429921\n",
      "    num_agent_steps_sampled: 373000\n",
      "    num_agent_steps_trained: 373000\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.595100864553313\n",
      "    ram_util_percent: 68.15100864553314\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03719113331643739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.56036349460745\n",
      "    mean_inference_ms: 1.3210369300708793\n",
      "    mean_raw_obs_processing_ms: 43.414238043179004\n",
      "  time_since_restore: 23719.758697986603\n",
      "  time_this_iter_s: 243.16090726852417\n",
      "  time_total_s: 23719.758697986603\n",
      "  timers:\n",
      "    learn_throughput: 1565.252\n",
      "    learn_time_ms: 638.875\n",
      "    load_throughput: 53626.069\n",
      "    load_time_ms: 18.648\n",
      "    sample_throughput: 4.535\n",
      "    sample_time_ms: 220498.24\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633553155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         23719.8</td><td style=\"text-align: right;\">373000</td><td style=\"text-align: right;\">    9.44</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              91.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 374000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-50-56\n",
      "  done: false\n",
      "  episode_len_mean: 78.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.61\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1620\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08669300638060806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6606071213881175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015108964200095793\n",
      "          policy_loss: 0.02888521800438563\n",
      "          total_loss: 0.37417034982807107\n",
      "          vf_explained_var: 0.9391394853591919\n",
      "          vf_loss: 0.3505813599460655\n",
      "    num_agent_steps_sampled: 374000\n",
      "    num_agent_steps_trained: 374000\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.62634032634033\n",
      "    ram_util_percent: 68.23076923076923\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037194612888483816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.563158201778702\n",
      "    mean_inference_ms: 1.3212208922284623\n",
      "    mean_raw_obs_processing_ms: 44.00198674636367\n",
      "  time_since_restore: 24020.22573208809\n",
      "  time_this_iter_s: 300.4670341014862\n",
      "  time_total_s: 24020.22573208809\n",
      "  timers:\n",
      "    learn_throughput: 1561.34\n",
      "    learn_time_ms: 640.476\n",
      "    load_throughput: 54628.413\n",
      "    load_time_ms: 18.305\n",
      "    sample_throughput: 4.805\n",
      "    sample_time_ms: 208126.989\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1633553456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 374\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         24020.2</td><td style=\"text-align: right;\">374000</td><td style=\"text-align: right;\">    9.61</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             78.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-54-37\n",
      "  done: false\n",
      "  episode_len_mean: 80.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.61\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1632\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08669300638060806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3229487790001764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.032874231064178715\n",
      "          policy_loss: 0.1208570903374089\n",
      "          total_loss: 0.26767173380487497\n",
      "          vf_explained_var: 0.7286304235458374\n",
      "          vf_loss: 0.15719416936238606\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.670569620253165\n",
      "    ram_util_percent: 68.33101265822785\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03719700446503559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.56376505259584\n",
      "    mean_inference_ms: 1.3213498910517216\n",
      "    mean_raw_obs_processing_ms: 44.43444409747317\n",
      "  time_since_restore: 24241.36706161499\n",
      "  time_this_iter_s: 221.14132952690125\n",
      "  time_total_s: 24241.36706161499\n",
      "  timers:\n",
      "    learn_throughput: 1565.804\n",
      "    learn_time_ms: 638.65\n",
      "    load_throughput: 54710.644\n",
      "    load_time_ms: 18.278\n",
      "    sample_throughput: 4.607\n",
      "    sample_time_ms: 217067.338\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1633553677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 375\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         24241.4</td><td style=\"text-align: right;\">375000</td><td style=\"text-align: right;\">    9.61</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             80.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_20-59-55\n",
      "  done: false\n",
      "  episode_len_mean: 78.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.64\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1648\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1300395095709121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6609452264176474\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01841836120829332\n",
      "          policy_loss: -0.06492472274435891\n",
      "          total_loss: 0.03979549258947372\n",
      "          vf_explained_var: 0.9864202737808228\n",
      "          vf_loss: 0.10893455321590105\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.18476821192053\n",
      "    ram_util_percent: 68.22273730684326\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03720025284658935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.56530717824778\n",
      "    mean_inference_ms: 1.3215213759162951\n",
      "    mean_raw_obs_processing_ms: 45.04123179352973\n",
      "  time_since_restore: 24559.3148291111\n",
      "  time_this_iter_s: 317.947767496109\n",
      "  time_total_s: 24559.3148291111\n",
      "  timers:\n",
      "    learn_throughput: 1567.932\n",
      "    learn_time_ms: 637.783\n",
      "    load_throughput: 55104.249\n",
      "    load_time_ms: 18.147\n",
      "    sample_throughput: 4.114\n",
      "    sample_time_ms: 243078.586\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1633553995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         24559.3</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">    9.64</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             78.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 377000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-03-59\n",
      "  done: false\n",
      "  episode_len_mean: 78.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.67\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1660\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1300395095709121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.253672577937444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015243203773184572\n",
      "          policy_loss: 0.10960959361659156\n",
      "          total_loss: 0.2336595892906189\n",
      "          vf_explained_var: 0.9848577976226807\n",
      "          vf_loss: 0.13460450602902307\n",
      "    num_agent_steps_sampled: 377000\n",
      "    num_agent_steps_trained: 377000\n",
      "    num_steps_sampled: 377000\n",
      "    num_steps_trained: 377000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.62155172413793\n",
      "    ram_util_percent: 67.68247126436782\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037202763845492176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.565686274816127\n",
      "    mean_inference_ms: 1.3216505658840405\n",
      "    mean_raw_obs_processing_ms: 45.49530601103257\n",
      "  time_since_restore: 24802.91912484169\n",
      "  time_this_iter_s: 243.60429573059082\n",
      "  time_total_s: 24802.91912484169\n",
      "  timers:\n",
      "    learn_throughput: 1564.183\n",
      "    learn_time_ms: 639.311\n",
      "    load_throughput: 54635.173\n",
      "    load_time_ms: 18.303\n",
      "    sample_throughput: 4.172\n",
      "    sample_time_ms: 239688.152\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633554239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 377000\n",
      "  training_iteration: 377\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         24802.9</td><td style=\"text-align: right;\">377000</td><td style=\"text-align: right;\">    9.67</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             78.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 378000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-11-59\n",
      "  done: false\n",
      "  episode_len_mean: 64.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.89\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1685\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1300395095709121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.40666637155744767\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006531729705457407\n",
      "          policy_loss: 0.02424335161017047\n",
      "          total_loss: 0.07864961102604866\n",
      "          vf_explained_var: 0.9928490519523621\n",
      "          vf_loss: 0.05762354058937894\n",
      "    num_agent_steps_sampled: 378000\n",
      "    num_agent_steps_trained: 378000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.701020408163266\n",
      "    ram_util_percent: 68.26239067055394\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037208159458695364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.56455881323701\n",
      "    mean_inference_ms: 1.321908933520112\n",
      "    mean_raw_obs_processing_ms: 46.53480491134817\n",
      "  time_since_restore: 25283.35520672798\n",
      "  time_this_iter_s: 480.4360818862915\n",
      "  time_total_s: 25283.35520672798\n",
      "  timers:\n",
      "    learn_throughput: 1559.381\n",
      "    learn_time_ms: 641.28\n",
      "    load_throughput: 54462.286\n",
      "    load_time_ms: 18.361\n",
      "    sample_throughput: 3.737\n",
      "    sample_time_ms: 267575.506\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1633554719\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 378\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         25283.4</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">    9.89</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             64.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 379000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-15-26\n",
      "  done: false\n",
      "  episode_len_mean: 63.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.86\n",
      "  episode_reward_min: 3.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1696\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1300395095709121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0265043911006715\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0165181693843626\n",
      "          policy_loss: 0.08944274170531166\n",
      "          total_loss: 0.205848887645536\n",
      "          vf_explained_var: 0.927069365978241\n",
      "          vf_loss: 0.12452317993674013\n",
      "    num_agent_steps_sampled: 379000\n",
      "    num_agent_steps_trained: 379000\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.973220338983047\n",
      "    ram_util_percent: 68.41525423728814\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721034512979138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.564323845660855\n",
      "    mean_inference_ms: 1.3220168947264532\n",
      "    mean_raw_obs_processing_ms: 46.96710828153484\n",
      "  time_since_restore: 25490.56169819832\n",
      "  time_this_iter_s: 207.2064914703369\n",
      "  time_total_s: 25490.56169819832\n",
      "  timers:\n",
      "    learn_throughput: 1556.633\n",
      "    learn_time_ms: 642.412\n",
      "    load_throughput: 54570.132\n",
      "    load_time_ms: 18.325\n",
      "    sample_throughput: 3.787\n",
      "    sample_time_ms: 264070.257\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1633554926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         25490.6</td><td style=\"text-align: right;\">379000</td><td style=\"text-align: right;\">    9.86</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">             63.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 62.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 3.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1711\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1300395095709121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6266145394908057\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008050143742780985\n",
      "          policy_loss: -0.1424001806312137\n",
      "          total_loss: 0.06404493128259976\n",
      "          vf_explained_var: 0.9794126749038696\n",
      "          vf_loss: 0.21166442024211088\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.24693396226415\n",
      "    ram_util_percent: 68.66981132075472\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721334309866111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.56215903737256\n",
      "    mean_inference_ms: 1.3221657144384378\n",
      "    mean_raw_obs_processing_ms: 47.565215337390846\n",
      "  time_since_restore: 25787.31504011154\n",
      "  time_this_iter_s: 296.75334191322327\n",
      "  time_total_s: 25787.31504011154\n",
      "  timers:\n",
      "    learn_throughput: 1554.143\n",
      "    learn_time_ms: 643.441\n",
      "    load_throughput: 54789.258\n",
      "    load_time_ms: 18.252\n",
      "    sample_throughput: 3.659\n",
      "    sample_time_ms: 273287.914\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1633555223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 380\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         25787.3</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">             62.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 381000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-23-14\n",
      "  done: false\n",
      "  episode_len_mean: 69.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.82\n",
      "  episode_reward_min: 3.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1721\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1300395095709121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9878618127769894\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007833253659697528\n",
      "          policy_loss: 0.014386291553576788\n",
      "          total_loss: 0.12120325615008672\n",
      "          vf_explained_var: 0.7308746576309204\n",
      "          vf_loss: 0.1156769511807296\n",
      "    num_agent_steps_sampled: 381000\n",
      "    num_agent_steps_trained: 381000\n",
      "    num_steps_sampled: 381000\n",
      "    num_steps_trained: 381000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.815983606557378\n",
      "    ram_util_percent: 68.66393442622947\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721544952632354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.561539852414725\n",
      "    mean_inference_ms: 1.3222709488528093\n",
      "    mean_raw_obs_processing_ms: 47.95225131150356\n",
      "  time_since_restore: 25958.39401459694\n",
      "  time_this_iter_s: 171.07897448539734\n",
      "  time_total_s: 25958.39401459694\n",
      "  timers:\n",
      "    learn_throughput: 1555.53\n",
      "    learn_time_ms: 642.868\n",
      "    load_throughput: 54743.563\n",
      "    load_time_ms: 18.267\n",
      "    sample_throughput: 3.839\n",
      "    sample_time_ms: 260475.897\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1633555394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 381000\n",
      "  training_iteration: 381\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         25958.4</td><td style=\"text-align: right;\">381000</td><td style=\"text-align: right;\">    9.82</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">             69.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 382000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-31-21\n",
      "  done: false\n",
      "  episode_len_mean: 62.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 3.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1746\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1300395095709121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.26600875324673123\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0027745040092967423\n",
      "          policy_loss: -0.06332759050031503\n",
      "          total_loss: -0.042220966600709495\n",
      "          vf_explained_var: 0.9954119920730591\n",
      "          vf_loss: 0.023405914639847147\n",
      "    num_agent_steps_sampled: 382000\n",
      "    num_agent_steps_trained: 382000\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.937463976945246\n",
      "    ram_util_percent: 68.90489913544668\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03722041098570364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.559925373786097\n",
      "    mean_inference_ms: 1.3225319338202732\n",
      "    mean_raw_obs_processing_ms: 49.00529099377505\n",
      "  time_since_restore: 26444.97222852707\n",
      "  time_this_iter_s: 486.57821393013\n",
      "  time_total_s: 26444.97222852707\n",
      "  timers:\n",
      "    learn_throughput: 1556.64\n",
      "    learn_time_ms: 642.409\n",
      "    load_throughput: 54498.163\n",
      "    load_time_ms: 18.349\n",
      "    sample_throughput: 3.376\n",
      "    sample_time_ms: 296172.035\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1633555881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">           26445</td><td style=\"text-align: right;\">382000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">             62.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 383000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-37-14\n",
      "  done: false\n",
      "  episode_len_mean: 58.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.84\n",
      "  episode_reward_min: 3.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 1764\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06501975478545605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6926059785816404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007327165161113866\n",
      "          policy_loss: -0.12404008482893308\n",
      "          total_loss: -0.04505416295594639\n",
      "          vf_explained_var: 0.9910990595817566\n",
      "          vf_loss: 0.08543557077646255\n",
      "    num_agent_steps_sampled: 383000\n",
      "    num_agent_steps_trained: 383000\n",
      "    num_steps_sampled: 383000\n",
      "    num_steps_trained: 383000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.054653465346536\n",
      "    ram_util_percent: 68.93366336633662\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03722357819964917\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.559247299787028\n",
      "    mean_inference_ms: 1.3227084121860846\n",
      "    mean_raw_obs_processing_ms: 49.72372921641807\n",
      "  time_since_restore: 26798.51885533333\n",
      "  time_this_iter_s: 353.54662680625916\n",
      "  time_total_s: 26798.51885533333\n",
      "  timers:\n",
      "    learn_throughput: 1553.777\n",
      "    learn_time_ms: 643.593\n",
      "    load_throughput: 54736.562\n",
      "    load_time_ms: 18.269\n",
      "    sample_throughput: 3.255\n",
      "    sample_time_ms: 307209.51\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633556234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383000\n",
      "  training_iteration: 383\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         26798.5</td><td style=\"text-align: right;\">383000</td><td style=\"text-align: right;\">    9.84</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">             58.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-42-54\n",
      "  done: false\n",
      "  episode_len_mean: 60.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.78\n",
      "  episode_reward_min: 3.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 1782\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06501975478545605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.39117797447575464\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06510793242347525\n",
      "          policy_loss: 0.00912994361586041\n",
      "          total_loss: 0.1887824700938331\n",
      "          vf_explained_var: 0.9747409224510193\n",
      "          vf_loss: 0.17933101339472665\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.677731958762887\n",
      "    ram_util_percent: 68.85278350515463\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03722668401769699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.561110649352692\n",
      "    mean_inference_ms: 1.322895810815812\n",
      "    mean_raw_obs_processing_ms: 50.40305894582229\n",
      "  time_since_restore: 27138.306030511856\n",
      "  time_this_iter_s: 339.78717517852783\n",
      "  time_total_s: 27138.306030511856\n",
      "  timers:\n",
      "    learn_throughput: 1552.439\n",
      "    learn_time_ms: 644.148\n",
      "    load_throughput: 54090.389\n",
      "    load_time_ms: 18.488\n",
      "    sample_throughput: 3.214\n",
      "    sample_time_ms: 311140.728\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1633556574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 384\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         27138.3</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">    9.78</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">             60.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 385000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-49-44\n",
      "  done: false\n",
      "  episode_len_mean: 56.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.87\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1803\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5728282246324751\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018087116431635786\n",
      "          policy_loss: -0.012523453517092598\n",
      "          total_loss: 0.22093340020833743\n",
      "          vf_explained_var: 0.9620319604873657\n",
      "          vf_loss: 0.23742110609180397\n",
      "    num_agent_steps_sampled: 385000\n",
      "    num_agent_steps_trained: 385000\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.889914529914527\n",
      "    ram_util_percent: 69.01282051282051\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03723020847349384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.561618231059732\n",
      "    mean_inference_ms: 1.323107194320731\n",
      "    mean_raw_obs_processing_ms: 51.27336561177481\n",
      "  time_since_restore: 27548.134353876114\n",
      "  time_this_iter_s: 409.8283233642578\n",
      "  time_total_s: 27548.134353876114\n",
      "  timers:\n",
      "    learn_throughput: 1552.346\n",
      "    learn_time_ms: 644.186\n",
      "    load_throughput: 54015.784\n",
      "    load_time_ms: 18.513\n",
      "    sample_throughput: 3.03\n",
      "    sample_time_ms: 330009.347\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633556984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         27548.1</td><td style=\"text-align: right;\">385000</td><td style=\"text-align: right;\">    9.87</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             56.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 386000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_21-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 47.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1827\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.28285323414537644\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003976976986833853\n",
      "          policy_loss: -0.1807877336939176\n",
      "          total_loss: -0.16519327892197502\n",
      "          vf_explained_var: 0.9968392252922058\n",
      "          vf_loss: 0.018035114980820152\n",
      "    num_agent_steps_sampled: 386000\n",
      "    num_agent_steps_trained: 386000\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.89110105580694\n",
      "    ram_util_percent: 69.1657616892911\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037233961399701705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.560550276628415\n",
      "    mean_inference_ms: 1.3233289816947948\n",
      "    mean_raw_obs_processing_ms: 52.29574879186179\n",
      "  time_since_restore: 28012.890356063843\n",
      "  time_this_iter_s: 464.7560021877289\n",
      "  time_total_s: 28012.890356063843\n",
      "  timers:\n",
      "    learn_throughput: 1548.554\n",
      "    learn_time_ms: 645.764\n",
      "    load_throughput: 54512.613\n",
      "    load_time_ms: 18.344\n",
      "    sample_throughput: 2.901\n",
      "    sample_time_ms: 344688.745\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1633557449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 386\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         28012.9</td><td style=\"text-align: right;\">386000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             47.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 387000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_22-02-47\n",
      "  done: false\n",
      "  episode_len_mean: 50.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.89\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1844\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.048764816089092014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7027947339746687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01762935468869205\n",
      "          policy_loss: 0.004677773349814945\n",
      "          total_loss: 0.016135027011235554\n",
      "          vf_explained_var: 0.9182518124580383\n",
      "          vf_loss: 0.01762550801763104\n",
      "    num_agent_steps_sampled: 387000\n",
      "    num_agent_steps_trained: 387000\n",
      "    num_steps_sampled: 387000\n",
      "    num_steps_trained: 387000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.32797356828194\n",
      "    ram_util_percent: 68.75066079295155\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037236679646488476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.560547757302853\n",
      "    mean_inference_ms: 1.3234856607125534\n",
      "    mean_raw_obs_processing_ms: 52.97189538603175\n",
      "  time_since_restore: 28330.704761743546\n",
      "  time_this_iter_s: 317.81440567970276\n",
      "  time_total_s: 28330.704761743546\n",
      "  timers:\n",
      "    learn_throughput: 1551.386\n",
      "    learn_time_ms: 644.585\n",
      "    load_throughput: 54821.269\n",
      "    load_time_ms: 18.241\n",
      "    sample_throughput: 2.84\n",
      "    sample_time_ms: 352111.032\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1633557767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 387000\n",
      "  training_iteration: 387\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         28330.7</td><td style=\"text-align: right;\">387000</td><td style=\"text-align: right;\">    9.89</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             50.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_22-07-50\n",
      "  done: false\n",
      "  episode_len_mean: 52.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.78\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1860\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.048764816089092014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8846079409122467\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07989766662162678\n",
      "          policy_loss: 0.05912291788392597\n",
      "          total_loss: 0.3715957178837723\n",
      "          vf_explained_var: 0.7693992853164673\n",
      "          vf_loss: 0.317422683040301\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.411111111111108\n",
      "    ram_util_percent: 68.94560185185185\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03723947457998141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.56244190120192\n",
      "    mean_inference_ms: 1.3236377116134932\n",
      "    mean_raw_obs_processing_ms: 53.597279146169534\n",
      "  time_since_restore: 28633.88660955429\n",
      "  time_this_iter_s: 303.18184781074524\n",
      "  time_total_s: 28633.88660955429\n",
      "  timers:\n",
      "    learn_throughput: 1554.579\n",
      "    learn_time_ms: 643.261\n",
      "    load_throughput: 54623.931\n",
      "    load_time_ms: 18.307\n",
      "    sample_throughput: 2.991\n",
      "    sample_time_ms: 334386.894\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1633558070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         28633.9</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">    9.78</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             52.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 389000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_22-16-13\n",
      "  done: false\n",
      "  episode_len_mean: 46.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1886\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07314722413363806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.20162336048152713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015150824244477541\n",
      "          policy_loss: -0.07436487591928906\n",
      "          total_loss: -0.06163956771294276\n",
      "          vf_explained_var: 0.9973024129867554\n",
      "          vf_loss: 0.013633302873414424\n",
      "    num_agent_steps_sampled: 389000\n",
      "    num_agent_steps_trained: 389000\n",
      "    num_steps_sampled: 389000\n",
      "    num_steps_trained: 389000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.743454038997214\n",
      "    ram_util_percent: 69.25933147632311\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724387000478457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.562407460570547\n",
      "    mean_inference_ms: 1.3238751270324665\n",
      "    mean_raw_obs_processing_ms: 54.68956753549859\n",
      "  time_since_restore: 29136.848722696304\n",
      "  time_this_iter_s: 502.96211314201355\n",
      "  time_total_s: 29136.848722696304\n",
      "  timers:\n",
      "    learn_throughput: 1559.101\n",
      "    learn_time_ms: 641.395\n",
      "    load_throughput: 55089.629\n",
      "    load_time_ms: 18.152\n",
      "    sample_throughput: 2.748\n",
      "    sample_time_ms: 363964.454\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1633558573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389000\n",
      "  training_iteration: 389\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         29136.8</td><td style=\"text-align: right;\">389000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             46.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_22-24-51\n",
      "  done: false\n",
      "  episode_len_mean: 46.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1913\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07314722413363806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.20793638891643948\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008871403669518527\n",
      "          policy_loss: -0.06411224024163352\n",
      "          total_loss: -0.061553855240345\n",
      "          vf_explained_var: 0.9992354512214661\n",
      "          vf_loss: 0.00398883189385136\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.739106901217863\n",
      "    ram_util_percent: 69.4\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037248242522941295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.56180291504959\n",
      "    mean_inference_ms: 1.3241082168399354\n",
      "    mean_raw_obs_processing_ms: 55.79897170249826\n",
      "  time_since_restore: 29654.955028533936\n",
      "  time_this_iter_s: 518.1063058376312\n",
      "  time_total_s: 29654.955028533936\n",
      "  timers:\n",
      "    learn_throughput: 1560.219\n",
      "    learn_time_ms: 640.936\n",
      "    load_throughput: 55170.57\n",
      "    load_time_ms: 18.126\n",
      "    sample_throughput: 2.59\n",
      "    sample_time_ms: 386100.228\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1633559091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 390\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">           29655</td><td style=\"text-align: right;\">390000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             46.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 391000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_22-32-57\n",
      "  done: false\n",
      "  episode_len_mean: 45.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.86\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1938\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07314722413363806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.23565107037623723\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003131726311314588\n",
      "          policy_loss: 0.002514624243809117\n",
      "          total_loss: 0.03560446392123898\n",
      "          vf_explained_var: 0.993043839931488\n",
      "          vf_loss: 0.03521727385102875\n",
      "    num_agent_steps_sampled: 391000\n",
      "    num_agent_steps_trained: 391000\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.820172910662823\n",
      "    ram_util_percent: 69.43530259365994\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03725198485994233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.559916571943756\n",
      "    mean_inference_ms: 1.3243248331235487\n",
      "    mean_raw_obs_processing_ms: 56.85948103635272\n",
      "  time_since_restore: 30140.790353536606\n",
      "  time_this_iter_s: 485.8353250026703\n",
      "  time_total_s: 30140.790353536606\n",
      "  timers:\n",
      "    learn_throughput: 1556.409\n",
      "    learn_time_ms: 642.505\n",
      "    load_throughput: 55120.181\n",
      "    load_time_ms: 18.142\n",
      "    sample_throughput: 2.395\n",
      "    sample_time_ms: 417574.253\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1633559577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         30140.8</td><td style=\"text-align: right;\">391000</td><td style=\"text-align: right;\">    9.86</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             45.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_22-35-42\n",
      "  done: false\n",
      "  episode_len_mean: 48.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1947\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03657361206681903\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2549117237329483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.030202076990202366\n",
      "          policy_loss: -0.07803759053349495\n",
      "          total_loss: -0.017236311485370002\n",
      "          vf_explained_var: 0.34683161973953247\n",
      "          vf_loss: 0.07224579819788536\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.140851063829786\n",
      "    ram_util_percent: 69.53148936170213\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03725335414443313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.558074487042166\n",
      "    mean_inference_ms: 1.3244036637753434\n",
      "    mean_raw_obs_processing_ms: 57.212834578142235\n",
      "  time_since_restore: 30305.544350385666\n",
      "  time_this_iter_s: 164.75399684906006\n",
      "  time_total_s: 30305.544350385666\n",
      "  timers:\n",
      "    learn_throughput: 1555.281\n",
      "    learn_time_ms: 642.97\n",
      "    load_throughput: 55487.331\n",
      "    load_time_ms: 18.022\n",
      "    sample_throughput: 2.595\n",
      "    sample_time_ms: 385391.509\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1633559742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 392\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         30305.5</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             48.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 393000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_22-42-31\n",
      "  done: false\n",
      "  episode_len_mean: 45.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.96\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1968\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.054860418100228535\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.28307517055008147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004730882896089413\n",
      "          policy_loss: -0.28368813577625485\n",
      "          total_loss: -0.27106925861703024\n",
      "          vf_explained_var: 0.9978623390197754\n",
      "          vf_loss: 0.015190090689187249\n",
      "    num_agent_steps_sampled: 393000\n",
      "    num_agent_steps_trained: 393000\n",
      "    num_steps_sampled: 393000\n",
      "    num_steps_trained: 393000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.853938356164385\n",
      "    ram_util_percent: 69.78544520547946\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037256374566112464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.551940830615905\n",
      "    mean_inference_ms: 1.3245885914200286\n",
      "    mean_raw_obs_processing_ms: 58.04558083918271\n",
      "  time_since_restore: 30715.33388876915\n",
      "  time_this_iter_s: 409.7895383834839\n",
      "  time_total_s: 30715.33388876915\n",
      "  timers:\n",
      "    learn_throughput: 1560.177\n",
      "    learn_time_ms: 640.953\n",
      "    load_throughput: 54789.831\n",
      "    load_time_ms: 18.252\n",
      "    sample_throughput: 2.557\n",
      "    sample_time_ms: 391017.583\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1633560151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 393000\n",
      "  training_iteration: 393\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         30715.3</td><td style=\"text-align: right;\">393000</td><td style=\"text-align: right;\">    9.96</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             45.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 394000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_22-46-35\n",
      "  done: false\n",
      "  episode_len_mean: 51.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.93\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1981\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.027430209050114267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1735174417495728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06465250921009372\n",
      "          policy_loss: 0.088569505016009\n",
      "          total_loss: 0.13901454353084167\n",
      "          vf_explained_var: 0.6931113600730896\n",
      "          vf_loss: 0.06040678092588981\n",
      "    num_agent_steps_sampled: 394000\n",
      "    num_agent_steps_trained: 394000\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.76551724137931\n",
      "    ram_util_percent: 69.7485632183908\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03725843984102986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.548011307081588\n",
      "    mean_inference_ms: 1.324710238856726\n",
      "    mean_raw_obs_processing_ms: 58.5269095448077\n",
      "  time_since_restore: 30958.601509809494\n",
      "  time_this_iter_s: 243.26762104034424\n",
      "  time_total_s: 30958.601509809494\n",
      "  timers:\n",
      "    learn_throughput: 1565.777\n",
      "    learn_time_ms: 638.661\n",
      "    load_throughput: 55019.244\n",
      "    load_time_ms: 18.175\n",
      "    sample_throughput: 2.622\n",
      "    sample_time_ms: 381368.0\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1633560395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         30958.6</td><td style=\"text-align: right;\">394000</td><td style=\"text-align: right;\">    9.93</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             51.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 395000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_22-54-58\n",
      "  done: false\n",
      "  episode_len_mean: 52.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.94\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2007\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04114531357517139\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.21883991791142357\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0024702566102803598\n",
      "          policy_loss: -0.04197828186055024\n",
      "          total_loss: -0.0284561600536108\n",
      "          vf_explained_var: 0.9973160624504089\n",
      "          vf_loss: 0.015608881465676757\n",
      "    num_agent_steps_sampled: 395000\n",
      "    num_agent_steps_trained: 395000\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.66866295264624\n",
      "    ram_util_percent: 69.76908077994429\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03726263240802771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.539861676206428\n",
      "    mean_inference_ms: 1.3249637377668275\n",
      "    mean_raw_obs_processing_ms: 59.5308380454012\n",
      "  time_since_restore: 31462.025988817215\n",
      "  time_this_iter_s: 503.42447900772095\n",
      "  time_total_s: 31462.025988817215\n",
      "  timers:\n",
      "    learn_throughput: 1560.828\n",
      "    learn_time_ms: 640.685\n",
      "    load_throughput: 54924.645\n",
      "    load_time_ms: 18.207\n",
      "    sample_throughput: 2.559\n",
      "    sample_time_ms: 390725.565\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1633560898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 395\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">           31462</td><td style=\"text-align: right;\">395000</td><td style=\"text-align: right;\">    9.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             52.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-03-37\n",
      "  done: false\n",
      "  episode_len_mean: 51.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.94\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2034\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.020572656787585696\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.17748646719588174\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005161419672021273\n",
      "          policy_loss: -0.030744417756795882\n",
      "          total_loss: -0.0281711146235466\n",
      "          vf_explained_var: 0.9992408156394958\n",
      "          vf_loss: 0.004241984031007936\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.697837837837838\n",
      "    ram_util_percent: 69.86121621621622\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03726700202436373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.531436608933497\n",
      "    mean_inference_ms: 1.3252289576441814\n",
      "    mean_raw_obs_processing_ms: 60.593481741077596\n",
      "  time_since_restore: 31980.563845396042\n",
      "  time_this_iter_s: 518.5378565788269\n",
      "  time_total_s: 31980.563845396042\n",
      "  timers:\n",
      "    learn_throughput: 1560.977\n",
      "    learn_time_ms: 640.625\n",
      "    load_throughput: 54528.984\n",
      "    load_time_ms: 18.339\n",
      "    sample_throughput: 2.525\n",
      "    sample_time_ms: 396103.697\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1633561417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 396\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         31980.6</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">    9.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">              51.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 397000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-12-00\n",
      "  done: false\n",
      "  episode_len_mean: 45.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.97\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2060\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.020572656787585696\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.1942689738339848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0041083370982254825\n",
      "          policy_loss: -0.10451469851864709\n",
      "          total_loss: -0.1025053944852617\n",
      "          vf_explained_var: 0.9993113279342651\n",
      "          vf_loss: 0.0038674723809688457\n",
      "    num_agent_steps_sampled: 397000\n",
      "    num_agent_steps_trained: 397000\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.650625869262864\n",
      "    ram_util_percent: 69.80417246175243\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0372709002531538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.52497546946535\n",
      "    mean_inference_ms: 1.3254655103851252\n",
      "    mean_raw_obs_processing_ms: 61.67230540455625\n",
      "  time_since_restore: 32484.139994859695\n",
      "  time_this_iter_s: 503.57614946365356\n",
      "  time_total_s: 32484.139994859695\n",
      "  timers:\n",
      "    learn_throughput: 1561.62\n",
      "    learn_time_ms: 640.361\n",
      "    load_throughput: 54253.129\n",
      "    load_time_ms: 18.432\n",
      "    sample_throughput: 2.411\n",
      "    sample_time_ms: 414680.03\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1633561920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         32484.1</td><td style=\"text-align: right;\">397000</td><td style=\"text-align: right;\">    9.97</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             45.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 398000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-19-46\n",
      "  done: false\n",
      "  episode_len_mean: 38.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 10.0\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2084\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010286328393792848\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2729147232241101\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005528151017674767\n",
      "          policy_loss: -0.06458315683735741\n",
      "          total_loss: -0.06149415916038884\n",
      "          vf_explained_var: 0.999154806137085\n",
      "          vf_loss: 0.005761279114004638\n",
      "    num_agent_steps_sampled: 398000\n",
      "    num_agent_steps_trained: 398000\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.845632530120483\n",
      "    ram_util_percent: 70.13960843373494\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03727451786715988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.51976506854205\n",
      "    mean_inference_ms: 1.3256726749823033\n",
      "    mean_raw_obs_processing_ms: 62.68977661772662\n",
      "  time_since_restore: 32949.83312487602\n",
      "  time_this_iter_s: 465.6931300163269\n",
      "  time_total_s: 32949.83312487602\n",
      "  timers:\n",
      "    learn_throughput: 1561.591\n",
      "    learn_time_ms: 640.372\n",
      "    load_throughput: 53808.518\n",
      "    load_time_ms: 18.584\n",
      "    sample_throughput: 2.321\n",
      "    sample_time_ms: 430930.98\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1633562386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 398\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         32949.8</td><td style=\"text-align: right;\">398000</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             38.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 399000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-25-39\n",
      "  done: false\n",
      "  episode_len_mean: 38.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 10.0\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2102\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010286328393792848\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3571313351392746\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004444099297633791\n",
      "          policy_loss: -0.2232437307635943\n",
      "          total_loss: -0.1810851232873069\n",
      "          vf_explained_var: 0.9960237145423889\n",
      "          vf_loss: 0.04568420670305689\n",
      "    num_agent_steps_sampled: 399000\n",
      "    num_agent_steps_trained: 399000\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.997817460317464\n",
      "    ram_util_percent: 70.3956349206349\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03727747660956146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.515729140603213\n",
      "    mean_inference_ms: 1.3258229985721384\n",
      "    mean_raw_obs_processing_ms: 63.380425578762576\n",
      "  time_since_restore: 33302.97175335884\n",
      "  time_this_iter_s: 353.1386284828186\n",
      "  time_total_s: 33302.97175335884\n",
      "  timers:\n",
      "    learn_throughput: 1553.679\n",
      "    learn_time_ms: 643.634\n",
      "    load_throughput: 53611.743\n",
      "    load_time_ms: 18.653\n",
      "    sample_throughput: 2.404\n",
      "    sample_time_ms: 415945.307\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1633562739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 399\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">           33303</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             38.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-25-56\n",
      "  done: false\n",
      "  episode_len_mean: 45.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.95\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2103\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005143164196896424\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7887945625517103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022585909833293102\n",
      "          policy_loss: -0.05887875370681286\n",
      "          total_loss: -0.011044324396385087\n",
      "          vf_explained_var: 0.7905305624008179\n",
      "          vf_loss: 0.06560621394051445\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.855999999999995\n",
      "    ram_util_percent: 70.21199999999999\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03727772085092778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.515460818362236\n",
      "    mean_inference_ms: 1.3258331686243832\n",
      "    mean_raw_obs_processing_ms: 63.41717591873626\n",
      "  time_since_restore: 33319.980650901794\n",
      "  time_this_iter_s: 17.00889754295349\n",
      "  time_total_s: 33319.980650901794\n",
      "  timers:\n",
      "    learn_throughput: 1551.622\n",
      "    learn_time_ms: 644.487\n",
      "    load_throughput: 57006.26\n",
      "    load_time_ms: 17.542\n",
      "    sample_throughput: 2.733\n",
      "    sample_time_ms: 365835.795\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633562756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">           33320</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">    9.95</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             45.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 401000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-33-37\n",
      "  done: false\n",
      "  episode_len_mean: 52.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.92\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2128\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007714746295344637\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.41547826098071206\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010782521084633127\n",
      "          policy_loss: 0.028601248231199052\n",
      "          total_loss: 0.052595579044686425\n",
      "          vf_explained_var: 0.9961450099945068\n",
      "          vf_loss: 0.028065928128651448\n",
      "    num_agent_steps_sampled: 401000\n",
      "    num_agent_steps_trained: 401000\n",
      "    num_steps_sampled: 401000\n",
      "    num_steps_trained: 401000\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.787975646879755\n",
      "    ram_util_percent: 71.34307458143074\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037284696675557304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.5088475975942\n",
      "    mean_inference_ms: 1.326098067951399\n",
      "    mean_raw_obs_processing_ms: 64.33856047729054\n",
      "  time_since_restore: 33780.971037864685\n",
      "  time_this_iter_s: 460.9903869628906\n",
      "  time_total_s: 33780.971037864685\n",
      "  timers:\n",
      "    learn_throughput: 1556.553\n",
      "    learn_time_ms: 642.445\n",
      "    load_throughput: 57078.407\n",
      "    load_time_ms: 17.52\n",
      "    sample_throughput: 2.752\n",
      "    sample_time_ms: 363353.361\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633563217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 401000\n",
      "  training_iteration: 401\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">           33781</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\">    9.92</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             52.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 402000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-38-38\n",
      "  done: false\n",
      "  episode_len_mean: 52.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.92\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 2143\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007714746295344637\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.582109675473637\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024385273652444105\n",
      "          policy_loss: -0.29414747489823234\n",
      "          total_loss: -0.25669565399487815\n",
      "          vf_explained_var: 0.9951498508453369\n",
      "          vf_loss: 0.04308479121989674\n",
      "    num_agent_steps_sampled: 402000\n",
      "    num_agent_steps_trained: 402000\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.337529137529135\n",
      "    ram_util_percent: 71.05198135198135\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03728956384329127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.504810686585923\n",
      "    mean_inference_ms: 1.326268327451877\n",
      "    mean_raw_obs_processing_ms: 64.84961074700564\n",
      "  time_since_restore: 34081.250175476074\n",
      "  time_this_iter_s: 300.27913761138916\n",
      "  time_total_s: 34081.250175476074\n",
      "  timers:\n",
      "    learn_throughput: 1558.221\n",
      "    learn_time_ms: 641.758\n",
      "    load_throughput: 56979.155\n",
      "    load_time_ms: 17.55\n",
      "    sample_throughput: 2.653\n",
      "    sample_time_ms: 376906.535\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1633563518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 402\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         34081.3</td><td style=\"text-align: right;\">402000</td><td style=\"text-align: right;\">    9.92</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">              52.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 403000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-46-01\n",
      "  done: false\n",
      "  episode_len_mean: 57.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.88\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2167\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.011572119443016953\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4207546404666371\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019580190558530976\n",
      "          policy_loss: 0.04810736080010732\n",
      "          total_loss: 0.06669452024830712\n",
      "          vf_explained_var: 0.9977976679801941\n",
      "          vf_loss: 0.022568118686063423\n",
      "    num_agent_steps_sampled: 403000\n",
      "    num_agent_steps_trained: 403000\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.978041074249603\n",
      "    ram_util_percent: 71.14486571879937\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037298483228622344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.499928784846723\n",
      "    mean_inference_ms: 1.32655234232565\n",
      "    mean_raw_obs_processing_ms: 65.70970669575428\n",
      "  time_since_restore: 34524.98167133331\n",
      "  time_this_iter_s: 443.73149585723877\n",
      "  time_total_s: 34524.98167133331\n",
      "  timers:\n",
      "    learn_throughput: 1554.284\n",
      "    learn_time_ms: 643.383\n",
      "    load_throughput: 57032.538\n",
      "    load_time_ms: 17.534\n",
      "    sample_throughput: 2.63\n",
      "    sample_time_ms: 380299.101\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1633563961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">           34525</td><td style=\"text-align: right;\">403000</td><td style=\"text-align: right;\">    9.88</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             57.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-52-32\n",
      "  done: false\n",
      "  episode_len_mean: 56.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.88\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2187\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.011572119443016953\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.21538236141204833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007138885377988046\n",
      "          policy_loss: -0.2943386336167653\n",
      "          total_loss: -0.266014274292522\n",
      "          vf_explained_var: 0.9958807229995728\n",
      "          vf_loss: 0.030395570190416443\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.963375224416517\n",
      "    ram_util_percent: 71.59730700179534\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037306875983674465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.49562785742509\n",
      "    mean_inference_ms: 1.3267958444961976\n",
      "    mean_raw_obs_processing_ms: 66.42055877765566\n",
      "  time_since_restore: 34915.34739804268\n",
      "  time_this_iter_s: 390.36572670936584\n",
      "  time_total_s: 34915.34739804268\n",
      "  timers:\n",
      "    learn_throughput: 1554.501\n",
      "    learn_time_ms: 643.293\n",
      "    load_throughput: 56515.279\n",
      "    load_time_ms: 17.694\n",
      "    sample_throughput: 2.532\n",
      "    sample_time_ms: 395008.86\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633564352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 404\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         34915.3</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">    9.88</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             56.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_23-56-50\n",
      "  done: false\n",
      "  episode_len_mean: 63.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.82\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2201\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.011572119443016953\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0142738216453129\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02267104432239507\n",
      "          policy_loss: 0.10285578436321682\n",
      "          total_loss: 0.16348659329944187\n",
      "          vf_explained_var: 0.6835916042327881\n",
      "          vf_loss: 0.07051119226962328\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.335501355013548\n",
      "    ram_util_percent: 71.320054200542\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03731341230058053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.493091685680767\n",
      "    mean_inference_ms: 1.3269678101229525\n",
      "    mean_raw_obs_processing_ms: 66.89662083343431\n",
      "  time_since_restore: 35174.10458660126\n",
      "  time_this_iter_s: 258.7571885585785\n",
      "  time_total_s: 35174.10458660126\n",
      "  timers:\n",
      "    learn_throughput: 1554.18\n",
      "    learn_time_ms: 643.426\n",
      "    load_throughput: 56991.465\n",
      "    load_time_ms: 17.546\n",
      "    sample_throughput: 2.699\n",
      "    sample_time_ms: 370542.184\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633564610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 405\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         35174.1</td><td style=\"text-align: right;\">405000</td><td style=\"text-align: right;\">    9.82</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             63.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 406000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_00-04-02\n",
      "  done: false\n",
      "  episode_len_mean: 50.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.9\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2223\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01735817916452543\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.17181754062573115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004507616438439873\n",
      "          policy_loss: -0.2916649442580011\n",
      "          total_loss: -0.28800704562001755\n",
      "          vf_explained_var: 0.999212384223938\n",
      "          vf_loss: 0.005297826062370506\n",
      "    num_agent_steps_sampled: 406000\n",
      "    num_agent_steps_trained: 406000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.009090909090908\n",
      "    ram_util_percent: 71.60714285714286\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037322954889968625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.489818876826043\n",
      "    mean_inference_ms: 1.3272001618342053\n",
      "    mean_raw_obs_processing_ms: 67.67740516976912\n",
      "  time_since_restore: 35605.77257156372\n",
      "  time_this_iter_s: 431.6679849624634\n",
      "  time_total_s: 35605.77257156372\n",
      "  timers:\n",
      "    learn_throughput: 1557.433\n",
      "    learn_time_ms: 642.082\n",
      "    load_throughput: 57193.131\n",
      "    load_time_ms: 17.485\n",
      "    sample_throughput: 2.764\n",
      "    sample_time_ms: 361856.597\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1633565042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         35605.8</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\">     9.9</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             50.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 407000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_00-08-41\n",
      "  done: false\n",
      "  episode_len_mean: 56.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.87\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 2238\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.008679089582262715\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2663153110278977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03499292578013719\n",
      "          policy_loss: 0.06959822575251261\n",
      "          total_loss: 0.07533338864644369\n",
      "          vf_explained_var: 0.6533120274543762\n",
      "          vf_loss: 0.01809460975540181\n",
      "    num_agent_steps_sampled: 407000\n",
      "    num_agent_steps_trained: 407000\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.60653266331658\n",
      "    ram_util_percent: 71.67236180904523\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03733005818884552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.48820917743778\n",
      "    mean_inference_ms: 1.3273604137108461\n",
      "    mean_raw_obs_processing_ms: 68.2139557663389\n",
      "  time_since_restore: 35884.1442899704\n",
      "  time_this_iter_s: 278.37171840667725\n",
      "  time_total_s: 35884.1442899704\n",
      "  timers:\n",
      "    learn_throughput: 1558.642\n",
      "    learn_time_ms: 641.584\n",
      "    load_throughput: 57186.503\n",
      "    load_time_ms: 17.487\n",
      "    sample_throughput: 2.947\n",
      "    sample_time_ms: 339336.646\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1633565321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 407\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         35884.1</td><td style=\"text-align: right;\">407000</td><td style=\"text-align: right;\">    9.87</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              56.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_00-17-19\n",
      "  done: false\n",
      "  episode_len_mean: 50.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2265\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.013018634373394069\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.15960404723882676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00259320979845064\n",
      "          policy_loss: -0.03164799677001105\n",
      "          total_loss: -0.023951005045738487\n",
      "          vf_explained_var: 0.9983144998550415\n",
      "          vf_loss: 0.009259273481762243\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.756621621621623\n",
      "    ram_util_percent: 71.8308108108108\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037342846496828036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.484185416027525\n",
      "    mean_inference_ms: 1.327647477048082\n",
      "    mean_raw_obs_processing_ms: 69.22139436647063\n",
      "  time_since_restore: 36402.732399225235\n",
      "  time_this_iter_s: 518.588109254837\n",
      "  time_total_s: 36402.732399225235\n",
      "  timers:\n",
      "    learn_throughput: 1558.063\n",
      "    learn_time_ms: 641.822\n",
      "    load_throughput: 58346.523\n",
      "    load_time_ms: 17.139\n",
      "    sample_throughput: 2.902\n",
      "    sample_time_ms: 344626.235\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1633565839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 408\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         36402.7</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             50.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 409000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_00-26-02\n",
      "  done: false\n",
      "  episode_len_mean: 43.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.97\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2292\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006509317186697034\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.16885686318079632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01193101035595807\n",
      "          policy_loss: 0.022779914860924084\n",
      "          total_loss: 0.02207124042842123\n",
      "          vf_explained_var: 0.9998335838317871\n",
      "          vf_loss: 0.0009022354963235558\n",
      "    num_agent_steps_sampled: 409000\n",
      "    num_agent_steps_trained: 409000\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.906836461126005\n",
      "    ram_util_percent: 71.96796246648793\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037354828526198626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.48075957374944\n",
      "    mean_inference_ms: 1.3279297965677344\n",
      "    mean_raw_obs_processing_ms: 70.26018405216486\n",
      "  time_since_restore: 36925.64593195915\n",
      "  time_this_iter_s: 522.9135327339172\n",
      "  time_total_s: 36925.64593195915\n",
      "  timers:\n",
      "    learn_throughput: 1564.823\n",
      "    learn_time_ms: 639.05\n",
      "    load_throughput: 58623.469\n",
      "    load_time_ms: 17.058\n",
      "    sample_throughput: 2.765\n",
      "    sample_time_ms: 361606.551\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1633566362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         36925.6</td><td style=\"text-align: right;\">409000</td><td style=\"text-align: right;\">    9.97</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             43.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 410000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_00-29-44\n",
      "  done: false\n",
      "  episode_len_mean: 49.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2304\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006509317186697034\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.854605392118295\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04026606089597012\n",
      "          policy_loss: -0.09838341606987847\n",
      "          total_loss: -0.04281732944978608\n",
      "          vf_explained_var: 0.8120838403701782\n",
      "          vf_loss: 0.06385003911952178\n",
      "    num_agent_steps_sampled: 410000\n",
      "    num_agent_steps_trained: 410000\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.080126182965298\n",
      "    ram_util_percent: 72.17981072555204\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037359855142058765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.479527033241617\n",
      "    mean_inference_ms: 1.3280549537762\n",
      "    mean_raw_obs_processing_ms: 70.68631770618214\n",
      "  time_since_restore: 37147.5650331974\n",
      "  time_this_iter_s: 221.91910123825073\n",
      "  time_total_s: 37147.5650331974\n",
      "  timers:\n",
      "    learn_throughput: 1565.004\n",
      "    learn_time_ms: 638.976\n",
      "    load_throughput: 54809.162\n",
      "    load_time_ms: 18.245\n",
      "    sample_throughput: 2.617\n",
      "    sample_time_ms: 382096.497\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1633566584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 410\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">         37147.6</td><td style=\"text-align: right;\">410000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             49.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 411000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_00-35-20\n",
      "  done: false\n",
      "  episode_len_mean: 49.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2321\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009763975780045554\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.40508854389190674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010848968775318222\n",
      "          policy_loss: -0.11124309566285875\n",
      "          total_loss: -0.05707364943292406\n",
      "          vf_explained_var: 0.9954920411109924\n",
      "          vf_loss: 0.058114403838084805\n",
      "    num_agent_steps_sampled: 411000\n",
      "    num_agent_steps_trained: 411000\n",
      "    num_steps_sampled: 411000\n",
      "    num_steps_trained: 411000\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.089979123173276\n",
      "    ram_util_percent: 71.9052192066806\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03736722790597182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.477982271376135\n",
      "    mean_inference_ms: 1.3282459710095214\n",
      "    mean_raw_obs_processing_ms: 71.28299342206914\n",
      "  time_since_restore: 37483.54678606987\n",
      "  time_this_iter_s: 335.98175287246704\n",
      "  time_total_s: 37483.54678606987\n",
      "  timers:\n",
      "    learn_throughput: 1561.129\n",
      "    learn_time_ms: 640.562\n",
      "    load_throughput: 55052.029\n",
      "    load_time_ms: 18.165\n",
      "    sample_throughput: 2.706\n",
      "    sample_time_ms: 369594.117\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633566920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411000\n",
      "  training_iteration: 411\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         37483.5</td><td style=\"text-align: right;\">411000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             49.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_00-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 48.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.88\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2342\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.009763975780045554\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6247342692481147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.036836105736871794\n",
      "          policy_loss: 0.037269164787398446\n",
      "          total_loss: 0.08632295355200767\n",
      "          vf_explained_var: 0.9560325741767883\n",
      "          vf_loss: 0.054941470010413065\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.209302325581394\n",
      "    ram_util_percent: 71.99803220035777\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03737580091482783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.476162571218744\n",
      "    mean_inference_ms: 1.328477764494431\n",
      "    mean_raw_obs_processing_ms: 72.04185989855004\n",
      "  time_since_restore: 37875.111904621124\n",
      "  time_this_iter_s: 391.5651185512543\n",
      "  time_total_s: 37875.111904621124\n",
      "  timers:\n",
      "    learn_throughput: 1557.442\n",
      "    learn_time_ms: 642.079\n",
      "    load_throughput: 55064.388\n",
      "    load_time_ms: 18.161\n",
      "    sample_throughput: 2.64\n",
      "    sample_time_ms: 378721.18\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1633567312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         37875.1</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">    9.88</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             48.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 413000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_00-47-15\n",
      "  done: false\n",
      "  episode_len_mean: 52.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2359\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01464596367006833\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6587796969546212\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025629075482544384\n",
      "          policy_loss: 0.09493327902423011\n",
      "          total_loss: 0.1600319458378686\n",
      "          vf_explained_var: 0.9908716082572937\n",
      "          vf_loss: 0.07131110530139671\n",
      "    num_agent_steps_sampled: 413000\n",
      "    num_agent_steps_trained: 413000\n",
      "    num_steps_sampled: 413000\n",
      "    num_steps_trained: 413000\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.782863340563996\n",
      "    ram_util_percent: 72.34251626898048\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0373830672680899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.47652768143293\n",
      "    mean_inference_ms: 1.3286733003511366\n",
      "    mean_raw_obs_processing_ms: 72.59676920126283\n",
      "  time_since_restore: 38198.3303899765\n",
      "  time_this_iter_s: 323.2184853553772\n",
      "  time_total_s: 38198.3303899765\n",
      "  timers:\n",
      "    learn_throughput: 1558.756\n",
      "    learn_time_ms: 641.537\n",
      "    load_throughput: 55278.256\n",
      "    load_time_ms: 18.09\n",
      "    sample_throughput: 2.727\n",
      "    sample_time_ms: 366670.517\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1633567635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 413000\n",
      "  training_iteration: 413\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         38198.3</td><td style=\"text-align: right;\">413000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             52.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 414000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_00-55-53\n",
      "  done: false\n",
      "  episode_len_mean: 52.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2386\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0219689455051025\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.19342709084351858\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004434006985433076\n",
      "          policy_loss: -0.054221560226546395\n",
      "          total_loss: -0.03961558604819907\n",
      "          vf_explained_var: 0.9969868063926697\n",
      "          vf_loss: 0.016442834756647547\n",
      "    num_agent_steps_sampled: 414000\n",
      "    num_agent_steps_trained: 414000\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.82216216216216\n",
      "    ram_util_percent: 72.32810810810811\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037395641080651576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.477028596579252\n",
      "    mean_inference_ms: 1.3289900713671239\n",
      "    mean_raw_obs_processing_ms: 73.53307897207154\n",
      "  time_since_restore: 38716.4672293663\n",
      "  time_this_iter_s: 518.136839389801\n",
      "  time_total_s: 38716.4672293663\n",
      "  timers:\n",
      "    learn_throughput: 1555.546\n",
      "    learn_time_ms: 642.861\n",
      "    load_throughput: 55817.479\n",
      "    load_time_ms: 17.916\n",
      "    sample_throughput: 2.635\n",
      "    sample_time_ms: 379446.455\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1633568153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 414\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         38716.5</td><td style=\"text-align: right;\">414000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             52.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 415000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_01-04-37\n",
      "  done: false\n",
      "  episode_len_mean: 46.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2413\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01098447275255125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.17236345592472288\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004442623286995162\n",
      "          policy_loss: -0.050802675137917204\n",
      "          total_loss: -0.047529347323709066\n",
      "          vf_explained_var: 0.9991070032119751\n",
      "          vf_loss: 0.0049481606861162515\n",
      "    num_agent_steps_sampled: 415000\n",
      "    num_agent_steps_trained: 415000\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.844444444444445\n",
      "    ram_util_percent: 72.1536813922356\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03740817275082789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.477451297233497\n",
      "    mean_inference_ms: 1.329288288165467\n",
      "    mean_raw_obs_processing_ms: 74.5644946637095\n",
      "  time_since_restore: 39240.028834819794\n",
      "  time_this_iter_s: 523.5616054534912\n",
      "  time_total_s: 39240.028834819794\n",
      "  timers:\n",
      "    learn_throughput: 1557.563\n",
      "    learn_time_ms: 642.029\n",
      "    load_throughput: 55394.263\n",
      "    load_time_ms: 18.052\n",
      "    sample_throughput: 2.463\n",
      "    sample_time_ms: 405927.562\n",
      "    update_time_ms: 1.704\n",
      "  timestamp: 1633568677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">           39240</td><td style=\"text-align: right;\">415000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             46.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_01-13-19\n",
      "  done: false\n",
      "  episode_len_mean: 40.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.97\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2440\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005492236376275625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.186288135084841\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007778375548004821\n",
      "          policy_loss: -0.0024368269162045584\n",
      "          total_loss: -0.0007827210343546337\n",
      "          vf_explained_var: 0.9993687868118286\n",
      "          vf_loss: 0.003474267548881471\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.766487935656837\n",
      "    ram_util_percent: 71.75348525469168\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03742050797887093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.47767047963571\n",
      "    mean_inference_ms: 1.3295566690631833\n",
      "    mean_raw_obs_processing_ms: 75.60885748395164\n",
      "  time_since_restore: 39762.834569215775\n",
      "  time_this_iter_s: 522.8057343959808\n",
      "  time_total_s: 39762.834569215775\n",
      "  timers:\n",
      "    learn_throughput: 1556.871\n",
      "    learn_time_ms: 642.314\n",
      "    load_throughput: 55249.13\n",
      "    load_time_ms: 18.1\n",
      "    sample_throughput: 2.409\n",
      "    sample_time_ms: 415040.988\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1633569199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 416\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         39762.8</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">    9.97</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             40.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 417000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_01-22-05\n",
      "  done: false\n",
      "  episode_len_mean: 37.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.99\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2467\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005492236376275625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2555969364113278\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006247550312345017\n",
      "          policy_loss: -0.04023364500866996\n",
      "          total_loss: -0.028176356106996536\n",
      "          vf_explained_var: 0.998226523399353\n",
      "          vf_loss: 0.014578944985340866\n",
      "    num_agent_steps_sampled: 417000\n",
      "    num_agent_steps_trained: 417000\n",
      "    num_steps_sampled: 417000\n",
      "    num_steps_trained: 417000\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.87970627503338\n",
      "    ram_util_percent: 72.06141522029372\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03743246123513501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.475956966528237\n",
      "    mean_inference_ms: 1.329795072227648\n",
      "    mean_raw_obs_processing_ms: 76.65287414152417\n",
      "  time_since_restore: 40288.1850438118\n",
      "  time_this_iter_s: 525.3504745960236\n",
      "  time_total_s: 40288.1850438118\n",
      "  timers:\n",
      "    learn_throughput: 1555.313\n",
      "    learn_time_ms: 642.958\n",
      "    load_throughput: 55435.409\n",
      "    load_time_ms: 18.039\n",
      "    sample_throughput: 2.274\n",
      "    sample_time_ms: 439738.298\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1633569725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417000\n",
      "  training_iteration: 417\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">         40288.2</td><td style=\"text-align: right;\">417000</td><td style=\"text-align: right;\">    9.99</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">              37.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 418000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_01-30-55\n",
      "  done: false\n",
      "  episode_len_mean: 37.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.98\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2494\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005492236376275625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.22822539624240662\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003719363133922708\n",
      "          policy_loss: 0.03562237839731905\n",
      "          total_loss: 0.05001556966453791\n",
      "          vf_explained_var: 0.9975521564483643\n",
      "          vf_loss: 0.016655015358183946\n",
      "    num_agent_steps_sampled: 418000\n",
      "    num_agent_steps_trained: 418000\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.150198150594452\n",
      "    ram_util_percent: 72.94385733157199\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037444934353824705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.474648797792188\n",
      "    mean_inference_ms: 1.3300324033715158\n",
      "    mean_raw_obs_processing_ms: 77.68711329000155\n",
      "  time_since_restore: 40818.37812590599\n",
      "  time_this_iter_s: 530.1930820941925\n",
      "  time_total_s: 40818.37812590599\n",
      "  timers:\n",
      "    learn_throughput: 1557.806\n",
      "    learn_time_ms: 641.929\n",
      "    load_throughput: 55128.222\n",
      "    load_time_ms: 18.14\n",
      "    sample_throughput: 2.268\n",
      "    sample_time_ms: 440899.729\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1633570255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         40818.4</td><td style=\"text-align: right;\">418000</td><td style=\"text-align: right;\">    9.98</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             37.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 419000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_01-39-18\n",
      "  done: false\n",
      "  episode_len_mean: 37.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.98\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2520\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0027461181881378127\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.23545740909046597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007011463799953526\n",
      "          policy_loss: -0.01554549526837137\n",
      "          total_loss: -0.008552145833770433\n",
      "          vf_explained_var: 0.9986149072647095\n",
      "          vf_loss: 0.009328671577127858\n",
      "    num_agent_steps_sampled: 419000\n",
      "    num_agent_steps_trained: 419000\n",
      "    num_steps_sampled: 419000\n",
      "    num_steps_trained: 419000\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.926462395543176\n",
      "    ram_util_percent: 73.31601671309193\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037457669046980245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.473707809043372\n",
      "    mean_inference_ms: 1.3302671998212217\n",
      "    mean_raw_obs_processing_ms: 78.67279395131331\n",
      "  time_since_restore: 41321.68657588959\n",
      "  time_this_iter_s: 503.3084499835968\n",
      "  time_total_s: 41321.68657588959\n",
      "  timers:\n",
      "    learn_throughput: 1557.632\n",
      "    learn_time_ms: 642.0\n",
      "    load_throughput: 54698.016\n",
      "    load_time_ms: 18.282\n",
      "    sample_throughput: 2.278\n",
      "    sample_time_ms: 438939.028\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1633570758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419000\n",
      "  training_iteration: 419\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         41321.7</td><td style=\"text-align: right;\">419000</td><td style=\"text-align: right;\">    9.98</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             37.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_01-48-23\n",
      "  done: false\n",
      "  episode_len_mean: 37.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.98\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2548\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0027461181881378127\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.17378531065252092\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006668109540218797\n",
      "          policy_loss: -0.09052783966892296\n",
      "          total_loss: -0.0906307159198655\n",
      "          vf_explained_var: 0.9997170567512512\n",
      "          vf_loss: 0.0016166655107453052\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.90064267352185\n",
      "    ram_util_percent: 73.45668380462725\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03747128782153428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.47269007518034\n",
      "    mean_inference_ms: 1.3305196210743273\n",
      "    mean_raw_obs_processing_ms: 79.73860314496613\n",
      "  time_since_restore: 41866.39814782143\n",
      "  time_this_iter_s: 544.711571931839\n",
      "  time_total_s: 41866.39814782143\n",
      "  timers:\n",
      "    learn_throughput: 1559.08\n",
      "    learn_time_ms: 641.404\n",
      "    load_throughput: 54739.991\n",
      "    load_time_ms: 18.268\n",
      "    sample_throughput: 2.122\n",
      "    sample_time_ms: 471218.876\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1633571303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 420\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         41866.4</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">    9.98</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             37.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 421000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_01-51-49\n",
      "  done: false\n",
      "  episode_len_mean: 43.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.94\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2558\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0027461181881378127\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9842894653479258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.031238585549274875\n",
      "          policy_loss: 0.037071999328003986\n",
      "          total_loss: 0.17729925157295334\n",
      "          vf_explained_var: 0.6485320329666138\n",
      "          vf_loss: 0.1499843576302131\n",
      "    num_agent_steps_sampled: 421000\n",
      "    num_agent_steps_trained: 421000\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.125597269624574\n",
      "    ram_util_percent: 73.30921501706484\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037476519132173994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.472080454477606\n",
      "    mean_inference_ms: 1.3306111912330376\n",
      "    mean_raw_obs_processing_ms: 80.07102178832982\n",
      "  time_since_restore: 42071.94372630119\n",
      "  time_this_iter_s: 205.54557847976685\n",
      "  time_total_s: 42071.94372630119\n",
      "  timers:\n",
      "    learn_throughput: 1560.057\n",
      "    learn_time_ms: 641.002\n",
      "    load_throughput: 54475.089\n",
      "    load_time_ms: 18.357\n",
      "    sample_throughput: 2.183\n",
      "    sample_time_ms: 458175.603\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1633571509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         42071.9</td><td style=\"text-align: right;\">421000</td><td style=\"text-align: right;\">    9.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">              43.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 422000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_02-00-31\n",
      "  done: false\n",
      "  episode_len_mean: 43.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.95\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2585\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004119177282206718\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.15820346110396916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016163398766668693\n",
      "          policy_loss: -0.06541370087199741\n",
      "          total_loss: -0.06199328348868423\n",
      "          vf_explained_var: 0.9990152716636658\n",
      "          vf_loss: 0.004935873369686305\n",
      "    num_agent_steps_sampled: 422000\n",
      "    num_agent_steps_trained: 422000\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.856912751677854\n",
      "    ram_util_percent: 73.7889932885906\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03749200150372745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.47033344786973\n",
      "    mean_inference_ms: 1.3308772513570524\n",
      "    mean_raw_obs_processing_ms: 81.05211121751948\n",
      "  time_since_restore: 42594.173315286636\n",
      "  time_this_iter_s: 522.2295889854431\n",
      "  time_total_s: 42594.173315286636\n",
      "  timers:\n",
      "    learn_throughput: 1562.385\n",
      "    learn_time_ms: 640.047\n",
      "    load_throughput: 54438.393\n",
      "    load_time_ms: 18.369\n",
      "    sample_throughput: 2.122\n",
      "    sample_time_ms: 471243.006\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1633572031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 422\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         42594.2</td><td style=\"text-align: right;\">422000</td><td style=\"text-align: right;\">    9.95</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             43.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 423000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_02-09-37\n",
      "  done: false\n",
      "  episode_len_mean: 42.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.94\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2613\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004119177282206718\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.12674451230300798\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00343580510502247\n",
      "          policy_loss: -0.11424134853813384\n",
      "          total_loss: -0.10466922705786096\n",
      "          vf_explained_var: 0.9977912902832031\n",
      "          vf_loss: 0.010825413102025373\n",
      "    num_agent_steps_sampled: 423000\n",
      "    num_agent_steps_trained: 423000\n",
      "    num_steps_sampled: 423000\n",
      "    num_steps_trained: 423000\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.889216944801024\n",
      "    ram_util_percent: 73.71360718870346\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037507949707873176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.46886732846322\n",
      "    mean_inference_ms: 1.3311458867692345\n",
      "    mean_raw_obs_processing_ms: 82.08282949544686\n",
      "  time_since_restore: 43139.84516596794\n",
      "  time_this_iter_s: 545.6718506813049\n",
      "  time_total_s: 43139.84516596794\n",
      "  timers:\n",
      "    learn_throughput: 1565.911\n",
      "    learn_time_ms: 638.606\n",
      "    load_throughput: 54863.362\n",
      "    load_time_ms: 18.227\n",
      "    sample_throughput: 2.026\n",
      "    sample_time_ms: 493489.917\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1633572577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423000\n",
      "  training_iteration: 423\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         43139.8</td><td style=\"text-align: right;\">423000</td><td style=\"text-align: right;\">    9.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             42.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_02-18-42\n",
      "  done: false\n",
      "  episode_len_mean: 42.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.94\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2641\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.002059588641103359\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.13057645724879371\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0039013712098325626\n",
      "          policy_loss: -0.021278336892525354\n",
      "          total_loss: -0.013489455170929431\n",
      "          vf_explained_var: 0.9979986548423767\n",
      "          vf_loss: 0.009086610739015871\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.89074550128535\n",
      "    ram_util_percent: 73.64228791773779\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037523332515326704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.467655384778297\n",
      "    mean_inference_ms: 1.3314000853247816\n",
      "    mean_raw_obs_processing_ms: 83.09611469758991\n",
      "  time_since_restore: 43685.50121593475\n",
      "  time_this_iter_s: 545.6560499668121\n",
      "  time_total_s: 43685.50121593475\n",
      "  timers:\n",
      "    learn_throughput: 1564.986\n",
      "    learn_time_ms: 638.983\n",
      "    load_throughput: 54937.882\n",
      "    load_time_ms: 18.202\n",
      "    sample_throughput: 2.015\n",
      "    sample_time_ms: 496241.501\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1633573122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         43685.5</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">    9.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             42.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 425000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_02-27-43\n",
      "  done: false\n",
      "  episode_len_mean: 35.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.99\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2669\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0010297943205516795\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.19145269923739963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01568172849232378\n",
      "          policy_loss: -0.04522459838125441\n",
      "          total_loss: -0.04438877370622423\n",
      "          vf_explained_var: 0.9995026588439941\n",
      "          vf_loss: 0.0027342030752657187\n",
      "    num_agent_steps_sampled: 425000\n",
      "    num_agent_steps_trained: 425000\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.912807244501938\n",
      "    ram_util_percent: 73.81267787839586\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037536779058263525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.46721001760568\n",
      "    mean_inference_ms: 1.3316328276825005\n",
      "    mean_raw_obs_processing_ms: 84.1601071745779\n",
      "  time_since_restore: 44226.70799732208\n",
      "  time_this_iter_s: 541.2067813873291\n",
      "  time_total_s: 44226.70799732208\n",
      "  timers:\n",
      "    learn_throughput: 1565.821\n",
      "    learn_time_ms: 638.643\n",
      "    load_throughput: 54998.682\n",
      "    load_time_ms: 18.182\n",
      "    sample_throughput: 2.008\n",
      "    sample_time_ms: 498006.375\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633573663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 425\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         44226.7</td><td style=\"text-align: right;\">425000</td><td style=\"text-align: right;\">    9.99</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             35.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 426000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_02-36-52\n",
      "  done: false\n",
      "  episode_len_mean: 35.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.98\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2697\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0010297943205516795\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.15550366093715032\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003078783411256245\n",
      "          policy_loss: -0.08344658650457859\n",
      "          total_loss: -0.08224405099948247\n",
      "          vf_explained_var: 0.999416172504425\n",
      "          vf_loss: 0.0027544029005285766\n",
      "    num_agent_steps_sampled: 426000\n",
      "    num_agent_steps_trained: 426000\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.043989769820975\n",
      "    ram_util_percent: 74.02864450127878\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03754792878314783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.4672188929724\n",
      "    mean_inference_ms: 1.3318499608769887\n",
      "    mean_raw_obs_processing_ms: 85.2016277740172\n",
      "  time_since_restore: 44774.80917453766\n",
      "  time_this_iter_s: 548.1011772155762\n",
      "  time_total_s: 44774.80917453766\n",
      "  timers:\n",
      "    learn_throughput: 1566.005\n",
      "    learn_time_ms: 638.567\n",
      "    load_throughput: 55262.524\n",
      "    load_time_ms: 18.095\n",
      "    sample_throughput: 1.998\n",
      "    sample_time_ms: 500536.076\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1633574212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 426\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         44774.8</td><td style=\"text-align: right;\">426000</td><td style=\"text-align: right;\">    9.98</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             35.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 427000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_02-40-32\n",
      "  done: false\n",
      "  episode_len_mean: 35.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.98\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2708\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005148971602758397\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6512993269496494\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.13977637838877757\n",
      "          policy_loss: -0.09137499406933784\n",
      "          total_loss: -0.06868764023400015\n",
      "          vf_explained_var: 0.7935608625411987\n",
      "          vf_loss: 0.029128375887456866\n",
      "    num_agent_steps_sampled: 427000\n",
      "    num_agent_steps_trained: 427000\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.55968253968254\n",
      "    ram_util_percent: 73.95809523809524\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037552327235814066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.466537152895594\n",
      "    mean_inference_ms: 1.3319393024392168\n",
      "    mean_raw_obs_processing_ms: 85.57514221136424\n",
      "  time_since_restore: 44995.408188819885\n",
      "  time_this_iter_s: 220.59901428222656\n",
      "  time_total_s: 44995.408188819885\n",
      "  timers:\n",
      "    learn_throughput: 1566.23\n",
      "    learn_time_ms: 638.476\n",
      "    load_throughput: 55125.976\n",
      "    load_time_ms: 18.14\n",
      "    sample_throughput: 2.127\n",
      "    sample_time_ms: 470060.957\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1633574432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         44995.4</td><td style=\"text-align: right;\">427000</td><td style=\"text-align: right;\">    9.98</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             35.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_02-44-22\n",
      "  done: false\n",
      "  episode_len_mean: 47.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2721\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007723457404137598\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9974282357427809\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07274798116289899\n",
      "          policy_loss: 0.12897009290754796\n",
      "          total_loss: 0.2542429584595892\n",
      "          vf_explained_var: 0.9865066409111023\n",
      "          vf_loss: 0.1351909663114283\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.407645259938835\n",
      "    ram_util_percent: 73.54831804281346\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037557739580493674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.46770952530968\n",
      "    mean_inference_ms: 1.3320514125508915\n",
      "    mean_raw_obs_processing_ms: 85.96858296810704\n",
      "  time_since_restore: 45224.942902088165\n",
      "  time_this_iter_s: 229.53471326828003\n",
      "  time_total_s: 45224.942902088165\n",
      "  timers:\n",
      "    learn_throughput: 1561.255\n",
      "    learn_time_ms: 640.51\n",
      "    load_throughput: 54997.024\n",
      "    load_time_ms: 18.183\n",
      "    sample_throughput: 2.273\n",
      "    sample_time_ms: 439993.057\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1633574662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 428\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">         45224.9</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             47.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 429000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_02-48-47\n",
      "  done: false\n",
      "  episode_len_mean: 52.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2735\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011585186106206396\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0137239426374436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020186166041570728\n",
      "          policy_loss: 0.10869759817918141\n",
      "          total_loss: 0.17629533641868167\n",
      "          vf_explained_var: 0.9507269859313965\n",
      "          vf_loss: 0.07771159114523066\n",
      "    num_agent_steps_sampled: 429000\n",
      "    num_agent_steps_trained: 429000\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.84802110817942\n",
      "    ram_util_percent: 74.15857519788918\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03756415969844765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.46995048056918\n",
      "    mean_inference_ms: 1.3321862566440044\n",
      "    mean_raw_obs_processing_ms: 86.38707601495459\n",
      "  time_since_restore: 45490.33102321625\n",
      "  time_this_iter_s: 265.3881211280823\n",
      "  time_total_s: 45490.33102321625\n",
      "  timers:\n",
      "    learn_throughput: 1558.019\n",
      "    learn_time_ms: 641.841\n",
      "    load_throughput: 55496.435\n",
      "    load_time_ms: 18.019\n",
      "    sample_throughput: 2.403\n",
      "    sample_time_ms: 416199.843\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1633574927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 429\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         45490.3</td><td style=\"text-align: right;\">429000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             52.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 430000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_02-57-14\n",
      "  done: false\n",
      "  episode_len_mean: 52.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.85\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2761\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0017377779159309594\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.16389611545536253\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007476586071234505\n",
      "          policy_loss: -0.1611191901895735\n",
      "          total_loss: -0.1585328015188376\n",
      "          vf_explained_var: 0.9991185665130615\n",
      "          vf_loss: 0.004212356268221306\n",
      "    num_agent_steps_sampled: 430000\n",
      "    num_agent_steps_trained: 430000\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.99889502762431\n",
      "    ram_util_percent: 74.30966850828729\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03757552380861526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.474712380053028\n",
      "    mean_inference_ms: 1.3324432229781893\n",
      "    mean_raw_obs_processing_ms: 87.20007181414043\n",
      "  time_since_restore: 45997.59627866745\n",
      "  time_this_iter_s: 507.2652554512024\n",
      "  time_total_s: 45997.59627866745\n",
      "  timers:\n",
      "    learn_throughput: 1556.938\n",
      "    learn_time_ms: 642.287\n",
      "    load_throughput: 55674.927\n",
      "    load_time_ms: 17.961\n",
      "    sample_throughput: 2.425\n",
      "    sample_time_ms: 412454.821\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1633575434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         45997.6</td><td style=\"text-align: right;\">430000</td><td style=\"text-align: right;\">    9.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             52.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 431000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_03-00-39\n",
      "  done: false\n",
      "  episode_len_mean: 59.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.82\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2772\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0017377779159309594\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.77193650106589\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.16798822817821943\n",
      "          policy_loss: 0.18715107730693287\n",
      "          total_loss: 0.199093672964308\n",
      "          vf_explained_var: -0.06679344922304153\n",
      "          vf_loss: 0.019370034797531036\n",
      "    num_agent_steps_sampled: 431000\n",
      "    num_agent_steps_trained: 431000\n",
      "    num_steps_sampled: 431000\n",
      "    num_steps_trained: 431000\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.089078498293517\n",
      "    ram_util_percent: 74.3617747440273\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037580527439824256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.47630551710889\n",
      "    mean_inference_ms: 1.332558719544301\n",
      "    mean_raw_obs_processing_ms: 87.51203048154258\n",
      "  time_since_restore: 46202.61662364006\n",
      "  time_this_iter_s: 205.02034497261047\n",
      "  time_total_s: 46202.61662364006\n",
      "  timers:\n",
      "    learn_throughput: 1560.368\n",
      "    learn_time_ms: 640.874\n",
      "    load_throughput: 55771.24\n",
      "    load_time_ms: 17.93\n",
      "    sample_throughput: 2.425\n",
      "    sample_time_ms: 412403.749\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1633575639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431000\n",
      "  training_iteration: 431\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         46202.6</td><td style=\"text-align: right;\">431000</td><td style=\"text-align: right;\">    9.82</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             59.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_03-06-20\n",
      "  done: false\n",
      "  episode_len_mean: 62.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.78\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2790\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.002606666873896439\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8222079628043705\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0286963962603464\n",
      "          policy_loss: 0.11056970722145504\n",
      "          total_loss: 0.16624281737539504\n",
      "          vf_explained_var: 0.9919306635856628\n",
      "          vf_loss: 0.06382039058953523\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.64485596707819\n",
      "    ram_util_percent: 74.41296296296298\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037588880480529926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.481291942349955\n",
      "    mean_inference_ms: 1.33275316355665\n",
      "    mean_raw_obs_processing_ms: 87.98099888832536\n",
      "  time_since_restore: 46543.40641665459\n",
      "  time_this_iter_s: 340.78979301452637\n",
      "  time_total_s: 46543.40641665459\n",
      "  timers:\n",
      "    learn_throughput: 1558.694\n",
      "    learn_time_ms: 641.563\n",
      "    load_throughput: 55711.606\n",
      "    load_time_ms: 17.95\n",
      "    sample_throughput: 2.536\n",
      "    sample_time_ms: 394259.07\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1633575980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 432\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         46543.4</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">    9.78</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             62.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 433000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_03-15-05\n",
      "  done: false\n",
      "  episode_len_mean: 51.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.87\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2817\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0039100003108446585\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.1773126459783978\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011385215841681933\n",
      "          policy_loss: -0.03959035393264559\n",
      "          total_loss: -0.03451693302227391\n",
      "          vf_explained_var: 0.9988584518432617\n",
      "          vf_loss: 0.0068020303086895086\n",
      "    num_agent_steps_sampled: 433000\n",
      "    num_agent_steps_trained: 433000\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.002673796791445\n",
      "    ram_util_percent: 74.46336898395724\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03760078977992407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.48885557157636\n",
      "    mean_inference_ms: 1.3330515629319195\n",
      "    mean_raw_obs_processing_ms: 88.86391100313428\n",
      "  time_since_restore: 47067.72099161148\n",
      "  time_this_iter_s: 524.3145749568939\n",
      "  time_total_s: 47067.72099161148\n",
      "  timers:\n",
      "    learn_throughput: 1553.618\n",
      "    learn_time_ms: 643.659\n",
      "    load_throughput: 55431.086\n",
      "    load_time_ms: 18.04\n",
      "    sample_throughput: 2.55\n",
      "    sample_time_ms: 392121.158\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1633576505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         47067.7</td><td style=\"text-align: right;\">433000</td><td style=\"text-align: right;\">    9.87</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              51.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 434000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_03-24-09\n",
      "  done: false\n",
      "  episode_len_mean: 46.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.93\n",
      "  episode_reward_min: 6.0\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2845\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0039100003108446585\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.15225693467590545\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0027889614911111712\n",
      "          policy_loss: -0.04199977144598961\n",
      "          total_loss: -0.0410387870338228\n",
      "          vf_explained_var: 0.999521017074585\n",
      "          vf_loss: 0.002472648777378102\n",
      "    num_agent_steps_sampled: 434000\n",
      "    num_agent_steps_trained: 434000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.897812097812096\n",
      "    ram_util_percent: 74.45945945945945\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03760973645393495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.49284997680572\n",
      "    mean_inference_ms: 1.3333051113168437\n",
      "    mean_raw_obs_processing_ms: 89.80796028030059\n",
      "  time_since_restore: 47611.93394780159\n",
      "  time_this_iter_s: 544.2129561901093\n",
      "  time_total_s: 47611.93394780159\n",
      "  timers:\n",
      "    learn_throughput: 1554.976\n",
      "    learn_time_ms: 643.097\n",
      "    load_throughput: 55299.61\n",
      "    load_time_ms: 18.083\n",
      "    sample_throughput: 2.551\n",
      "    sample_time_ms: 391977.374\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1633577049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 434\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         47611.9</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\">    9.93</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">              46.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_03-29-34\n",
      "  done: false\n",
      "  episode_len_mean: 43.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.93\n",
      "  episode_reward_min: 6.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2862\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019550001554223292\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6993767600920465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013104048463527463\n",
      "          policy_loss: 0.04864727904399236\n",
      "          total_loss: 0.08696574014094141\n",
      "          vf_explained_var: 0.32974234223365784\n",
      "          vf_loss: 0.04528660679029094\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.798272138228942\n",
      "    ram_util_percent: 74.3879049676026\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03761512215318988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.49736609886132\n",
      "    mean_inference_ms: 1.3334626528608486\n",
      "    mean_raw_obs_processing_ms: 90.34984918703074\n",
      "  time_since_restore: 47936.842131614685\n",
      "  time_this_iter_s: 324.9081838130951\n",
      "  time_total_s: 47936.842131614685\n",
      "  timers:\n",
      "    learn_throughput: 1552.959\n",
      "    learn_time_ms: 643.932\n",
      "    load_throughput: 55481.606\n",
      "    load_time_ms: 18.024\n",
      "    sample_throughput: 2.7\n",
      "    sample_time_ms: 370346.766\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1633577374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 435\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         47936.8</td><td style=\"text-align: right;\">435000</td><td style=\"text-align: right;\">    9.93</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">             43.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_03-34-41\n",
      "  done: false\n",
      "  episode_len_mean: 43.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.94\n",
      "  episode_reward_min: 7.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2878\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019550001554223292\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7426646189557181\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01646014812855207\n",
      "          policy_loss: -0.09749362481137117\n",
      "          total_loss: -0.08406951849659285\n",
      "          vf_explained_var: 0.887431263923645\n",
      "          vf_loss: 0.02081857088746296\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.15398633257403\n",
      "    ram_util_percent: 74.46173120728929\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03761957179362411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.503860413417943\n",
      "    mean_inference_ms: 1.3336014731875208\n",
      "    mean_raw_obs_processing_ms: 90.86337558711304\n",
      "  time_since_restore: 48244.494040727615\n",
      "  time_this_iter_s: 307.6519091129303\n",
      "  time_total_s: 48244.494040727615\n",
      "  timers:\n",
      "    learn_throughput: 1546.052\n",
      "    learn_time_ms: 646.809\n",
      "    load_throughput: 55521.706\n",
      "    load_time_ms: 18.011\n",
      "    sample_throughput: 2.888\n",
      "    sample_time_ms: 346299.009\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633577681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         48244.5</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">    9.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">             43.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_03-42-53\n",
      "  done: false\n",
      "  episode_len_mean: 45.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.89\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2903\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019550001554223292\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3731069521771537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011511400518838302\n",
      "          policy_loss: 0.02085147549708684\n",
      "          total_loss: 0.05403999967707528\n",
      "          vf_explained_var: 0.996947169303894\n",
      "          vf_loss: 0.036897092731669544\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.732336182336184\n",
      "    ram_util_percent: 74.12507122507122\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03762589603109027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.51392352737258\n",
      "    mean_inference_ms: 1.3338048264341515\n",
      "    mean_raw_obs_processing_ms: 91.67426639285735\n",
      "  time_since_restore: 48736.14012217522\n",
      "  time_this_iter_s: 491.6460814476013\n",
      "  time_total_s: 48736.14012217522\n",
      "  timers:\n",
      "    learn_throughput: 1545.543\n",
      "    learn_time_ms: 647.022\n",
      "    load_throughput: 55519.428\n",
      "    load_time_ms: 18.012\n",
      "    sample_throughput: 2.678\n",
      "    sample_time_ms: 373403.495\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633578173\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>RUNNING </td><td>192.168.3.5:175</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         48736.1</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\">    9.89</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             45.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-10-07 03:53:30,240\tERROR trial_runner.py:773 -- Trial PPO_my_env_21374_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 739, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 746, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 82, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/worker.py\", line 1621, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(timeout): \u001b[36mray::PPO.train_buffered()\u001b[39m (pid=175, ip=192.168.3.5, repr=PPO)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/trainable.py\", line 178, in train_buffered\n",
      "    result = self.train()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 648, in train\n",
      "    raise e\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 637, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/trainable.py\", line 237, in train\n",
      "    result = self.step()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 193, in step\n",
      "    res = next(self.train_exec_impl)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 828, in add_wait_hooks\n",
      "    item = next(it)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "    yield ray.get(futures, timeout=timeout)\n",
      "ray.exceptions.RayTaskError(timeout): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=176, ip=192.168.3.5, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ffb60a540f0>)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "    return next(self.local_it)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "    yield self.sample()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 615, in _env_runner\n",
      "    sample_collector=sample_collector,\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 940, in _process_observations\n",
      "    env_id)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 370, in try_reset\n",
      "    return {_DUMMY_AGENT_ID: self.vector_env.reset_at(env_id)}\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/env/vector_env.py\", line 167, in reset_at\n",
      "    return self.envs[index].reset()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/core.py\", line 237, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/core.py\", line 237, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/core.py\", line 264, in reset\n",
      "    observation = self.env.reset(**kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/wrappers/time_limit.py\", line 25, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/iglu/env.py\", line 107, in reset\n",
      "    obs = self.real_reset()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/iglu/env.py\", line 127, in real_reset\n",
      "    obs = super().reset()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/_singleagent.py\", line 23, in reset\n",
      "    multi_obs = super().reset()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/_multiagent.py\", line 477, in reset\n",
      "    return self._peek_obs()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/_multiagent.py\", line 636, in _peek_obs\n",
      "    obs = comms.recv_message(instance.client_socket)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/comms.py\", line 63, in recv_message\n",
      "    lengthbuf = recvall(sock, 4)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/comms.py\", line 73, in recvall\n",
      "    newbuf = sock.recv(count)\n",
      "socket.timeout: timed out\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 396<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_21374_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-07_03-42-53\n",
      "  done: false\n",
      "  episode_len_mean: 45.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.89\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2903\n",
      "  experiment_id: 49db762336a445169df010321c0a100e\n",
      "  experiment_tag: '0'\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019550001554223292\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3731069521771537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011511400518838302\n",
      "          policy_loss: 0.02085147549708684\n",
      "          total_loss: 0.05403999967707528\n",
      "          vf_explained_var: 0.996947169303894\n",
      "          vf_loss: 0.036897092731669544\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.732336182336184\n",
      "    ram_util_percent: 74.12507122507122\n",
      "  pid: 175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03762589603109027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.51392352737258\n",
      "    mean_inference_ms: 1.3338048264341515\n",
      "    mean_raw_obs_processing_ms: 91.67426639285735\n",
      "  time_since_restore: 48736.14012217522\n",
      "  time_this_iter_s: 491.6460814476013\n",
      "  time_total_s: 48736.14012217522\n",
      "  timers:\n",
      "    learn_throughput: 1545.543\n",
      "    learn_time_ms: 647.022\n",
      "    load_throughput: 55519.428\n",
      "    load_time_ms: 18.012\n",
      "    sample_throughput: 2.678\n",
      "    sample_time_ms: 373403.495\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633578173\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: '21374_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/IGLU-Minecraft/wandb/run-20211006_141016-21374_00000/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/IGLU-Minecraft/wandb/run-20211006_141016-21374_00000/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>agent_timesteps_total</td><td>437000</td></tr><tr><td>episode_len_mean</td><td>45.24</td></tr><tr><td>episode_reward_max</td><td>10.0</td></tr><tr><td>episode_reward_mean</td><td>9.89</td></tr><tr><td>episode_reward_min</td><td>5.0</td></tr><tr><td>episodes_this_iter</td><td>25</td></tr><tr><td>episodes_total</td><td>2903</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_kl_coeff</td><td>0.00196</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_lr</td><td>5e-05</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy</td><td>0.37311</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy_coeff</td><td>0.01</td></tr><tr><td>info/learner/default_policy/learner_stats/kl</td><td>0.01151</td></tr><tr><td>info/learner/default_policy/learner_stats/policy_loss</td><td>0.02085</td></tr><tr><td>info/learner/default_policy/learner_stats/total_loss</td><td>0.05404</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_explained_var</td><td>0.99695</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_loss</td><td>0.0369</td></tr><tr><td>info/num_agent_steps_sampled</td><td>437000</td></tr><tr><td>info/num_agent_steps_trained</td><td>437000</td></tr><tr><td>info/num_steps_sampled</td><td>437000</td></tr><tr><td>info/num_steps_trained</td><td>437000</td></tr><tr><td>iterations_since_restore</td><td>437</td></tr><tr><td>num_healthy_workers</td><td>1</td></tr><tr><td>perf/cpu_util_percent</td><td>22.73234</td></tr><tr><td>perf/ram_util_percent</td><td>74.12507</td></tr><tr><td>sampler_perf/mean_action_processing_ms</td><td>0.03763</td></tr><tr><td>sampler_perf/mean_env_render_ms</td><td>0.0</td></tr><tr><td>sampler_perf/mean_env_wait_ms</td><td>16.51392</td></tr><tr><td>sampler_perf/mean_inference_ms</td><td>1.3338</td></tr><tr><td>sampler_perf/mean_raw_obs_processing_ms</td><td>91.67427</td></tr><tr><td>time_since_restore</td><td>48736.14012</td></tr><tr><td>time_this_iter_s</td><td>491.64608</td></tr><tr><td>time_total_s</td><td>48736.14012</td></tr><tr><td>timers/learn_throughput</td><td>1545.543</td></tr><tr><td>timers/learn_time_ms</td><td>647.022</td></tr><tr><td>timers/load_throughput</td><td>55519.428</td></tr><tr><td>timers/load_time_ms</td><td>18.012</td></tr><tr><td>timers/sample_throughput</td><td>2.678</td></tr><tr><td>timers/sample_time_ms</td><td>373403.495</td></tr><tr><td>timers/update_time_ms</td><td>1.653</td></tr><tr><td>timestamp</td><td>1633578173</td></tr><tr><td>timesteps_since_restore</td><td>0</td></tr><tr><td>timesteps_total</td><td>437000</td></tr><tr><td>training_iteration</td><td>437</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>agent_timesteps_total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode_len_mean</td><td>█▇▆▆▆▅▅▅▅▅▅▅▅▅▄▄▄▄▄▅▅▅▅▅▄▄▄▄▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode_reward_max</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▆▆▆████████████████</td></tr><tr><td>episode_reward_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▃▄▃▅▇▇█████████</td></tr><tr><td>episode_reward_min</td><td>▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▃▁▁▃▆▆▇▆▇▆▇▇▇█</td></tr><tr><td>episodes_this_iter</td><td>▁▁▁▂▁▁▂▁▁▁▁▂▁▂▂▂▂▁▁▁▁▂▂▂▁▂▂▁▃▇▆▄▇▄▅▅▁▆█▅</td></tr><tr><td>episodes_total</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_kl_coeff</td><td>▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▃▃▃▃▃▅▄█▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy</td><td>▄▆█▇▇▇▇█▇▇▇▅▅▇▇▆▆▅▆▅▆▆▅▅▅▅▄▅▄▂▂▄▁▅▂▃▆▂▁▃</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy_coeff</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>info/learner/default_policy/learner_stats/kl</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▃▁▂▁█▃▄▁▂</td></tr><tr><td>info/learner/default_policy/learner_stats/policy_loss</td><td>▆▅▄▆▆▆▅▆▆▆▅▄▄▆▅▄▅▆▇▅▄▅▆▅▄▃▁▇▆▅▃▅▅█▄▆▅▆▅▄</td></tr><tr><td>info/learner/default_policy/learner_stats/total_loss</td><td>▃▂▁▃▃▃▂▃▃▃▂▂▂▃▂▁▂▃▄▃▃▄▆▅▂▄▄██▄▃▄▃▆▄▇▃▄▃▂</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_explained_var</td><td>▇▇▃▃▂▂▂▁▁▂▁▃▆▃▂▃▄▆▂▇▅█▆▇▆▇▇▇███▇█▇█▇▇███</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▄▆▅▂▅█▇█▅▅▄▁▂▄▆▂▂▁▁</td></tr><tr><td>info/num_agent_steps_sampled</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>info/num_agent_steps_trained</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>info/num_steps_sampled</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>info/num_steps_trained</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iterations_since_restore</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>num_healthy_workers</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/cpu_util_percent</td><td>█▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▂▃█▁▁▁▁▃▁▁▁█▁▁▁</td></tr><tr><td>perf/ram_util_percent</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▃▃▃▃▃▄▅▅▅▆▆▇██</td></tr><tr><td>sampler_perf/mean_action_processing_ms</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▃▃▄▅</td></tr><tr><td>sampler_perf/mean_env_render_ms</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sampler_perf/mean_env_wait_ms</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>sampler_perf/mean_inference_ms</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▄▄</td></tr><tr><td>sampler_perf/mean_raw_obs_processing_ms</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▅▅▆▇▇█</td></tr><tr><td>time_since_restore</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▄▄▅▅▆▆▇█</td></tr><tr><td>time_this_iter_s</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▃▆▆▃▇▃▅▅▁▆█▅</td></tr><tr><td>time_total_s</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▄▄▅▅▆▆▇█</td></tr><tr><td>timers/learn_throughput</td><td>▅▆▅▇▇▇▇▆▆▆█▆▅▇▇▆▅▅▅▆▆▆▆▇▇▇██▆▇▆▇▁█▇▇▇▇█▆</td></tr><tr><td>timers/learn_time_ms</td><td>▄▃▄▂▂▂▂▂▃▃▁▃▄▂▂▃▄▄▄▃▂▃▃▂▂▂▁▁▂▂▃▂█▁▂▂▂▂▁▃</td></tr><tr><td>timers/load_throughput</td><td>▄█▂▂▁▂▁▁▂▃▃▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>timers/load_time_ms</td><td>▂▁▅▅▇▅▇▇▆▃▃▆▇▇▇▇▇▆▇▇▄▇████▇▇█▇▇█▇███▇███</td></tr><tr><td>timers/sample_throughput</td><td>▅█▆▆▆▆▆▆▆▇▇▆▅▅▅▅▅▆▅▅▇▆▄▅▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>timers/sample_time_ms</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▃▃▅▇▅▅▆▆▆█▆</td></tr><tr><td>timers/update_time_ms</td><td>█▃▃▁▂▁▂▃▂▅▂▂▂▂▂▂▃▃▃▃▃▂▄▂▂▃▃▃▃▃▂▄▄▂▂▁▂▃▂▂</td></tr><tr><td>timestamp</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▄▄▅▅▆▆▇█</td></tr><tr><td>timesteps_since_restore</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>timesteps_total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">PPO C32 pretrained (AnnaCNN) (3 noops after placement)</strong>: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/21374_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/21374_00000</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 03:53:33,867\tWARNING util.py:164 -- The `process_trial` operation took 3.628 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         48736.1</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\">    9.89</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             45.24</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td style=\"text-align: right;\">           1</td><td>/root/ray_results/PPO_2021-10-06_14-10-15/PPO_my_env_21374_00000_0_2021-10-06_14-10-16/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/8.88 GiB heap, 0.0/4.44 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_14-10-15<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         48736.1</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\">    9.89</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">             45.24</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_21374_00000</td><td style=\"text-align: right;\">           1</td><td>/root/ray_results/PPO_2021-10-06_14-10-15/PPO_my_env_21374_00000_0_2021-10-06_14-10-16/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=176)\u001b[0m Attempted to send kill command to minecraft process and failed with exception timed out\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_my_env_21374_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_69/3078919067.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         },\n\u001b[0;32m---> 28\u001b[0;31m         loggers=[WandbLogger])\n\u001b[0m",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_my_env_21374_00000])"
     ]
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C32 pretrained (AnnaCNN) (3 noops after placement)\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
