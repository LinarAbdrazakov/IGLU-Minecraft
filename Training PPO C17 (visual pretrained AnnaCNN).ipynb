{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C17']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-09-18 16:06:42,958\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-09-18 16:06:42,975\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 7b99f_00000 but id 6a34a_00000 is set.\n",
      "\u001b[2m\u001b[36m(pid=168620)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168620)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C17 pretrained (AnnaCNN)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/6a34a_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/6a34a_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20210918_160643-6a34a_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168620)\u001b[0m 2021-09-18 16:06:46,535\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=168620)\u001b[0m 2021-09-18 16:06:46,535\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=168620)\u001b[0m 2021-09-18 16:06:52,383\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12.0\n",
      "  episode_reward_mean: -12.0\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3670889682239957\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023380577262915696\n",
      "          policy_loss: 0.05648387355936898\n",
      "          total_loss: 0.18125435608542628\n",
      "          vf_explained_var: 0.5717211365699768\n",
      "          vf_loss: 0.1337652596541577\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.94202898550725\n",
      "    ram_util_percent: 52.61159420289855\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042494717654172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.37682266502114\n",
      "    mean_inference_ms: 1.510613686316735\n",
      "    mean_raw_obs_processing_ms: 0.20027565551209045\n",
      "  time_since_restore: 47.94692063331604\n",
      "  time_this_iter_s: 47.94692063331604\n",
      "  time_total_s: 47.94692063331604\n",
      "  timers:\n",
      "    learn_throughput: 1429.395\n",
      "    learn_time_ms: 699.597\n",
      "    load_throughput: 75875.179\n",
      "    load_time_ms: 13.18\n",
      "    sample_throughput: 21.173\n",
      "    sample_time_ms: 47228.973\n",
      "    update_time_ms: 2.473\n",
      "  timestamp: 1631981260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         47.9469</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">     -12</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-07-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -9.0\n",
      "  episode_reward_mean: -10.5\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.304554573694865\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00966196364176197\n",
      "          policy_loss: 0.06383168548345566\n",
      "          total_loss: 0.10338386793931326\n",
      "          vf_explained_var: 0.6174427270889282\n",
      "          vf_loss: 0.04969913876718945\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.846666666666664\n",
      "    ram_util_percent: 59.79999999999999\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04256657897775286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.03058233141892\n",
      "    mean_inference_ms: 1.500041292225643\n",
      "    mean_raw_obs_processing_ms: 0.19057019736378286\n",
      "  time_since_restore: 58.343839168548584\n",
      "  time_this_iter_s: 10.396918535232544\n",
      "  time_total_s: 58.343839168548584\n",
      "  timers:\n",
      "    learn_throughput: 1475.402\n",
      "    learn_time_ms: 677.781\n",
      "    load_throughput: 117429.943\n",
      "    load_time_ms: 8.516\n",
      "    sample_throughput: 35.112\n",
      "    sample_time_ms: 28480.667\n",
      "    update_time_ms: 2.384\n",
      "  timestamp: 1631981270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         58.3438</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">   -10.5</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.0\n",
      "  episode_reward_mean: -9.666666666666666\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1480003045664893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017902980215514692\n",
      "          policy_loss: -0.025454937294125558\n",
      "          total_loss: 0.054351726671059926\n",
      "          vf_explained_var: 0.8110604286193848\n",
      "          vf_loss: 0.08591577326878905\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.307142857142864\n",
      "    ram_util_percent: 59.87142857142857\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04249234813329747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.79511279018506\n",
      "    mean_inference_ms: 1.4932677415922615\n",
      "    mean_raw_obs_processing_ms: 0.18170797161970673\n",
      "  time_since_restore: 68.31076741218567\n",
      "  time_this_iter_s: 9.966928243637085\n",
      "  time_total_s: 68.31076741218567\n",
      "  timers:\n",
      "    learn_throughput: 1457.748\n",
      "    learn_time_ms: 685.99\n",
      "    load_throughput: 147808.199\n",
      "    load_time_ms: 6.766\n",
      "    sample_throughput: 45.305\n",
      "    sample_time_ms: 22072.764\n",
      "    update_time_ms: 2.185\n",
      "  timestamp: 1631981280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         68.3108</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-9.66667</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8.0\n",
      "  episode_reward_mean: -10.25\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8596882647938198\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011328951106469948\n",
      "          policy_loss: 0.016690279915928842\n",
      "          total_loss: 0.07627820964488718\n",
      "          vf_explained_var: 0.6837578415870667\n",
      "          vf_loss: 0.07478612440948686\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.85000000000001\n",
      "    ram_util_percent: 59.64285714285713\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04227069619022855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.357407884730087\n",
      "    mean_inference_ms: 1.4825432158768095\n",
      "    mean_raw_obs_processing_ms: 0.17462213946016955\n",
      "  time_since_restore: 77.75259160995483\n",
      "  time_this_iter_s: 9.441824197769165\n",
      "  time_total_s: 77.75259160995483\n",
      "  timers:\n",
      "    learn_throughput: 1459.462\n",
      "    learn_time_ms: 685.184\n",
      "    load_throughput: 165428.045\n",
      "    load_time_ms: 6.045\n",
      "    sample_throughput: 53.355\n",
      "    sample_time_ms: 18742.321\n",
      "    update_time_ms: 2.063\n",
      "  timestamp: 1631981290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         77.7526</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  -10.25</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-08-53\n",
      "  done: false\n",
      "  episode_len_mean: 824.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -7.0\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9570701294475132\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03312427827807017\n",
      "          policy_loss: -0.03640692879756292\n",
      "          total_loss: 1.01073770125707\n",
      "          vf_explained_var: 0.28020626306533813\n",
      "          vf_loss: 1.0567780323326588\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.36451612903226\n",
      "    ram_util_percent: 58.54193548387097\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04206579211491592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.934820828043836\n",
      "    mean_inference_ms: 1.4734043715383112\n",
      "    mean_raw_obs_processing_ms: 1.4946456855963768\n",
      "  time_since_restore: 121.20984840393066\n",
      "  time_this_iter_s: 43.45725679397583\n",
      "  time_total_s: 121.20984840393066\n",
      "  timers:\n",
      "    learn_throughput: 1471.982\n",
      "    learn_time_ms: 679.356\n",
      "    load_throughput: 121133.734\n",
      "    load_time_ms: 8.255\n",
      "    sample_throughput: 42.463\n",
      "    sample_time_ms: 23549.822\n",
      "    update_time_ms: 1.987\n",
      "  timestamp: 1631981333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          121.21</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">      -7</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             824.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-09-03\n",
      "  done: false\n",
      "  episode_len_mean: 853.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -7.833333333333333\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9388237396876018\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01243431468141068\n",
      "          policy_loss: -0.008174773181478183\n",
      "          total_loss: 0.12707871823675101\n",
      "          vf_explained_var: 0.5083362460136414\n",
      "          vf_loss: 0.1490462856988112\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.87692307692308\n",
      "    ram_util_percent: 59.73846153846155\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04187056806469472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.103763573674936\n",
      "    mean_inference_ms: 1.4649928096457039\n",
      "    mean_raw_obs_processing_ms: 2.189948800247253\n",
      "  time_since_restore: 130.8676495552063\n",
      "  time_this_iter_s: 9.657801151275635\n",
      "  time_total_s: 130.8676495552063\n",
      "  timers:\n",
      "    learn_throughput: 1486.609\n",
      "    learn_time_ms: 672.672\n",
      "    load_throughput: 134891.104\n",
      "    load_time_ms: 7.413\n",
      "    sample_throughput: 47.333\n",
      "    sample_time_ms: 21126.735\n",
      "    update_time_ms: 1.919\n",
      "  timestamp: 1631981343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         130.868</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-7.83333</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             853.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-09-13\n",
      "  done: false\n",
      "  episode_len_mean: 874.4285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -7.428571428571429\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.056898648209042\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01353828184062584\n",
      "          policy_loss: -0.03745961909492811\n",
      "          total_loss: 0.24459801192084948\n",
      "          vf_explained_var: 0.6537075042724609\n",
      "          vf_loss: 0.2965343894229995\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.01428571428573\n",
      "    ram_util_percent: 59.52142857142856\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04171588153546235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.661931957428973\n",
      "    mean_inference_ms: 1.4577316217318852\n",
      "    mean_raw_obs_processing_ms: 2.5736472644864223\n",
      "  time_since_restore: 140.51339197158813\n",
      "  time_this_iter_s: 9.645742416381836\n",
      "  time_total_s: 140.51339197158813\n",
      "  timers:\n",
      "    learn_throughput: 1487.727\n",
      "    learn_time_ms: 672.166\n",
      "    load_throughput: 147184.054\n",
      "    load_time_ms: 6.794\n",
      "    sample_throughput: 51.573\n",
      "    sample_time_ms: 19389.911\n",
      "    update_time_ms: 1.889\n",
      "  timestamp: 1631981353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         140.513</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-7.42857</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           874.429</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-10-03\n",
      "  done: false\n",
      "  episode_len_mean: 803.1111111111111\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -7.0\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 9\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6656837317678663\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009691746205677513\n",
      "          policy_loss: -0.04750806490580241\n",
      "          total_loss: 0.7492489092051983\n",
      "          vf_explained_var: 0.4128987193107605\n",
      "          vf_loss: 0.80905252252188\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.043055555555554\n",
      "    ram_util_percent: 56.8125\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415055717788206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.59234204864046\n",
      "    mean_inference_ms: 1.4473557848571041\n",
      "    mean_raw_obs_processing_ms: 4.078998560609662\n",
      "  time_since_restore: 191.0059130191803\n",
      "  time_this_iter_s: 50.49252104759216\n",
      "  time_total_s: 191.0059130191803\n",
      "  timers:\n",
      "    learn_throughput: 1495.715\n",
      "    learn_time_ms: 668.576\n",
      "    load_throughput: 120239.2\n",
      "    load_time_ms: 8.317\n",
      "    sample_throughput: 43.114\n",
      "    sample_time_ms: 23194.371\n",
      "    update_time_ms: 1.879\n",
      "  timestamp: 1631981403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         191.006</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">      -7</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           803.111</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-10-15\n",
      "  done: false\n",
      "  episode_len_mean: 822.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -7.1\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9088264160686068\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0077701195581519125\n",
      "          policy_loss: 0.04112128756112522\n",
      "          total_loss: 0.09781551866067781\n",
      "          vf_explained_var: 0.3502279818058014\n",
      "          vf_loss: 0.07228594259358942\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.55294117647058\n",
      "    ram_util_percent: 56.11176470588235\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04143858900850355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.833862845705283\n",
      "    mean_inference_ms: 1.4436903526385756\n",
      "    mean_raw_obs_processing_ms: 4.5034261940169955\n",
      "  time_since_restore: 202.61821389198303\n",
      "  time_this_iter_s: 11.612300872802734\n",
      "  time_total_s: 202.61821389198303\n",
      "  timers:\n",
      "    learn_throughput: 1482.748\n",
      "    learn_time_ms: 674.423\n",
      "    load_throughput: 127850.002\n",
      "    load_time_ms: 7.822\n",
      "    sample_throughput: 45.816\n",
      "    sample_time_ms: 21826.331\n",
      "    update_time_ms: 1.904\n",
      "  timestamp: 1631981415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         202.618</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">    -7.1</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">             822.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-10-24\n",
      "  done: false\n",
      "  episode_len_mean: 838.9090909090909\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -6.7272727272727275\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.971891744931539\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011741300626494599\n",
      "          policy_loss: -0.0011010727948612636\n",
      "          total_loss: 0.12622878899176915\n",
      "          vf_explained_var: 0.61668461561203\n",
      "          vf_loss: 0.14176519563835527\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.87857142857143\n",
      "    ram_util_percent: 56.442857142857136\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0413768421651406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.171789965320222\n",
      "    mean_inference_ms: 1.440251951750414\n",
      "    mean_raw_obs_processing_ms: 4.77613910531808\n",
      "  time_since_restore: 212.28332543373108\n",
      "  time_this_iter_s: 9.665111541748047\n",
      "  time_total_s: 212.28332543373108\n",
      "  timers:\n",
      "    learn_throughput: 1485.394\n",
      "    learn_time_ms: 673.222\n",
      "    load_throughput: 133131.799\n",
      "    load_time_ms: 7.511\n",
      "    sample_throughput: 48.678\n",
      "    sample_time_ms: 20543.054\n",
      "    update_time_ms: 1.901\n",
      "  timestamp: 1631981424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         212.283</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">-6.72727</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           838.909</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-11-14\n",
      "  done: false\n",
      "  episode_len_mean: 793.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -5.461538461538462\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 13\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9707191096411811\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011683577840766821\n",
      "          policy_loss: -0.053177201996246974\n",
      "          total_loss: 0.583914022313224\n",
      "          vf_explained_var: 0.5343300700187683\n",
      "          vf_loss: 0.6515407888011799\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.649295774647886\n",
      "    ram_util_percent: 55.85352112676056\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041252061731060934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.090985181231765\n",
      "    mean_inference_ms: 1.4335935908344342\n",
      "    mean_raw_obs_processing_ms: 5.658651426705204\n",
      "  time_since_restore: 262.0750205516815\n",
      "  time_this_iter_s: 49.79169511795044\n",
      "  time_total_s: 262.0750205516815\n",
      "  timers:\n",
      "    learn_throughput: 1487.354\n",
      "    learn_time_ms: 672.335\n",
      "    load_throughput: 128940.85\n",
      "    load_time_ms: 7.755\n",
      "    sample_throughput: 48.243\n",
      "    sample_time_ms: 20728.276\n",
      "    update_time_ms: 1.814\n",
      "  timestamp: 1631981474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         262.075</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-5.46154</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">               793</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-11-26\n",
      "  done: false\n",
      "  episode_len_mean: 807.7857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -5.5\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1383425328466625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01069537233033954\n",
      "          policy_loss: 0.06140308487746451\n",
      "          total_loss: 0.15643544323328468\n",
      "          vf_explained_var: 0.0781848207116127\n",
      "          vf_loss: 0.11160286520090368\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.23125\n",
      "    ram_util_percent: 56.212500000000006\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04119623063421697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.655096436122832\n",
      "    mean_inference_ms: 1.430692539724968\n",
      "    mean_raw_obs_processing_ms: 5.943500414373368\n",
      "  time_since_restore: 273.50720620155334\n",
      "  time_this_iter_s: 11.432185649871826\n",
      "  time_total_s: 273.50720620155334\n",
      "  timers:\n",
      "    learn_throughput: 1491.341\n",
      "    learn_time_ms: 670.538\n",
      "    load_throughput: 130082.156\n",
      "    load_time_ms: 7.687\n",
      "    sample_throughput: 47.999\n",
      "    sample_time_ms: 20833.704\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1631981486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         273.507</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">    -5.5</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           807.786</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-11-35\n",
      "  done: false\n",
      "  episode_len_mean: 820.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -5.0\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.045928826597002\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013125653957697223\n",
      "          policy_loss: 0.029312341494692695\n",
      "          total_loss: 0.13808400246004263\n",
      "          vf_explained_var: 0.24759584665298462\n",
      "          vf_loss: 0.12332440658679439\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.96923076923076\n",
      "    ram_util_percent: 56.29230769230768\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041138879502978294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.25620334977991\n",
      "    mean_inference_ms: 1.427812812128527\n",
      "    mean_raw_obs_processing_ms: 6.141509005735433\n",
      "  time_since_restore: 282.53361892700195\n",
      "  time_this_iter_s: 9.026412725448608\n",
      "  time_total_s: 282.53361892700195\n",
      "  timers:\n",
      "    learn_throughput: 1504.353\n",
      "    learn_time_ms: 664.738\n",
      "    load_throughput: 130244.54\n",
      "    load_time_ms: 7.678\n",
      "    sample_throughput: 48.203\n",
      "    sample_time_ms: 20745.457\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1631981495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         282.534</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">      -5</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">             820.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-11-44\n",
      "  done: false\n",
      "  episode_len_mean: 831.8125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -5.0625\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.25733843114641\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011378293740579753\n",
      "          policy_loss: -0.06985723161035114\n",
      "          total_loss: -0.025295359227392407\n",
      "          vf_explained_var: -0.3458927571773529\n",
      "          vf_loss: 0.062015028256509036\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.65384615384615\n",
      "    ram_util_percent: 56.20769230769231\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041081056393679656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.890419887816133\n",
      "    mean_inference_ms: 1.4250034664092488\n",
      "    mean_raw_obs_processing_ms: 6.275516313327617\n",
      "  time_since_restore: 291.6324803829193\n",
      "  time_this_iter_s: 9.098861455917358\n",
      "  time_total_s: 291.6324803829193\n",
      "  timers:\n",
      "    learn_throughput: 1508.67\n",
      "    learn_time_ms: 662.836\n",
      "    load_throughput: 131188.84\n",
      "    load_time_ms: 7.623\n",
      "    sample_throughput: 48.279\n",
      "    sample_time_ms: 20713.091\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1631981504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         291.632</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\"> -5.0625</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           831.812</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-11-53\n",
      "  done: false\n",
      "  episode_len_mean: 841.7058823529412\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -4.764705882352941\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.508435853322347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009501845399831666\n",
      "          policy_loss: -0.03988076539503203\n",
      "          total_loss: -0.03233566027548578\n",
      "          vf_explained_var: -0.14936764538288116\n",
      "          vf_loss: 0.028353631724086073\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.223076923076924\n",
      "    ram_util_percent: 55.84615384615384\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04102521790945608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.554780642383268\n",
      "    mean_inference_ms: 1.4223210528959433\n",
      "    mean_raw_obs_processing_ms: 6.361702796731075\n",
      "  time_since_restore: 300.90337085723877\n",
      "  time_this_iter_s: 9.270890474319458\n",
      "  time_total_s: 300.90337085723877\n",
      "  timers:\n",
      "    learn_throughput: 1512.36\n",
      "    learn_time_ms: 661.218\n",
      "    load_throughput: 160300.245\n",
      "    load_time_ms: 6.238\n",
      "    sample_throughput: 57.812\n",
      "    sample_time_ms: 17297.473\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1631981513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         300.903</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-4.76471</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           841.706</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-12-03\n",
      "  done: false\n",
      "  episode_len_mean: 850.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -4.5\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.39866370095147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013497753540101224\n",
      "          policy_loss: -0.06663913782685996\n",
      "          total_loss: -0.061146514924863976\n",
      "          vf_explained_var: 0.37064480781555176\n",
      "          vf_loss: 0.023405268902166022\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.721428571428575\n",
      "    ram_util_percent: 55.23571428571429\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04097180237195174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.24639309491846\n",
      "    mean_inference_ms: 1.4197486004233346\n",
      "    mean_raw_obs_processing_ms: 6.41182154810069\n",
      "  time_since_restore: 310.34687304496765\n",
      "  time_this_iter_s: 9.443502187728882\n",
      "  time_total_s: 310.34687304496765\n",
      "  timers:\n",
      "    learn_throughput: 1511.718\n",
      "    learn_time_ms: 661.499\n",
      "    load_throughput: 159174.81\n",
      "    load_time_ms: 6.282\n",
      "    sample_throughput: 57.885\n",
      "    sample_time_ms: 17275.712\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1631981523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         310.347</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">    -4.5</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">             850.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-12-12\n",
      "  done: false\n",
      "  episode_len_mean: 858.3684210526316\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -4.2631578947368425\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.60370774269104\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008118200573161694\n",
      "          policy_loss: -0.1102167909240557\n",
      "          total_loss: -0.12192499397529497\n",
      "          vf_explained_var: -0.5022750496864319\n",
      "          vf_loss: 0.010675684072905117\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.035714285714285\n",
      "    ram_util_percent: 54.778571428571425\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04092374639370346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.962666105588031\n",
      "    mean_inference_ms: 1.4173833195740195\n",
      "    mean_raw_obs_processing_ms: 6.434534955942821\n",
      "  time_since_restore: 320.1295530796051\n",
      "  time_this_iter_s: 9.782680034637451\n",
      "  time_total_s: 320.1295530796051\n",
      "  timers:\n",
      "    learn_throughput: 1500.983\n",
      "    learn_time_ms: 666.23\n",
      "    load_throughput: 159026.95\n",
      "    load_time_ms: 6.288\n",
      "    sample_throughput: 57.855\n",
      "    sample_time_ms: 17284.679\n",
      "    update_time_ms: 1.774\n",
      "  timestamp: 1631981532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          320.13</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-4.26316</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           858.368</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-12-23\n",
      "  done: false\n",
      "  episode_len_mean: 865.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -4.05\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6035052193535697\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006822143986906244\n",
      "          policy_loss: -0.03238322544429037\n",
      "          total_loss: -0.04681883574359947\n",
      "          vf_explained_var: -0.26939353346824646\n",
      "          vf_loss: 0.00852947877202597\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.15714285714285\n",
      "    ram_util_percent: 53.88571428571428\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0408843571675853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.702009607797976\n",
      "    mean_inference_ms: 1.415272722169122\n",
      "    mean_raw_obs_processing_ms: 6.436306041314775\n",
      "  time_since_restore: 330.39667797088623\n",
      "  time_this_iter_s: 10.267124891281128\n",
      "  time_total_s: 330.39667797088623\n",
      "  timers:\n",
      "    learn_throughput: 1479.377\n",
      "    learn_time_ms: 675.96\n",
      "    load_throughput: 212389.18\n",
      "    load_time_ms: 4.708\n",
      "    sample_throughput: 75.449\n",
      "    sample_time_ms: 13254.006\n",
      "    update_time_ms: 1.774\n",
      "  timestamp: 1631981543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         330.397</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">   -4.05</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            865.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-12-33\n",
      "  done: false\n",
      "  episode_len_mean: 871.8571428571429\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -3.857142857142857\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5005279302597048\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0148615653162509\n",
      "          policy_loss: -0.2157234638929367\n",
      "          total_loss: -0.21506182932191426\n",
      "          vf_explained_var: -0.3178735077381134\n",
      "          vf_loss: 0.01897920976496405\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.95333333333333\n",
      "    ram_util_percent: 53.766666666666666\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040851434719869115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.461157215545754\n",
      "    mean_inference_ms: 1.41340077961753\n",
      "    mean_raw_obs_processing_ms: 6.4219911171845725\n",
      "  time_since_restore: 340.3927490711212\n",
      "  time_this_iter_s: 9.996071100234985\n",
      "  time_total_s: 340.3927490711212\n",
      "  timers:\n",
      "    learn_throughput: 1492.239\n",
      "    learn_time_ms: 670.134\n",
      "    load_throughput: 214294.677\n",
      "    load_time_ms: 4.666\n",
      "    sample_throughput: 76.346\n",
      "    sample_time_ms: 13098.342\n",
      "    update_time_ms: 1.733\n",
      "  timestamp: 1631981553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         340.393</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-3.85714</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           871.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-12-42\n",
      "  done: false\n",
      "  episode_len_mean: 877.6818181818181\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -3.6818181818181817\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4068985833062064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03700042223651964\n",
      "          policy_loss: 0.0678404011660152\n",
      "          total_loss: 0.0648525142007404\n",
      "          vf_explained_var: 0.2182598114013672\n",
      "          vf_loss: 0.004430911392490897\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.376923076923084\n",
      "    ram_util_percent: 53.48461538461538\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040818271776927734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.236584692902307\n",
      "    mean_inference_ms: 1.4115659629035129\n",
      "    mean_raw_obs_processing_ms: 6.395287735997145\n",
      "  time_since_restore: 349.7370722293854\n",
      "  time_this_iter_s: 9.34432315826416\n",
      "  time_total_s: 349.7370722293854\n",
      "  timers:\n",
      "    learn_throughput: 1490.738\n",
      "    learn_time_ms: 670.809\n",
      "    load_throughput: 221789.416\n",
      "    load_time_ms: 4.509\n",
      "    sample_throughput: 76.536\n",
      "    sample_time_ms: 13065.775\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1631981562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         349.737</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-3.68182</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           877.682</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 883.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -3.5217391304347827\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5570175674226547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009949949233425166\n",
      "          policy_loss: -0.03594713871263795\n",
      "          total_loss: -0.050255102167526884\n",
      "          vf_explained_var: -0.9111191630363464\n",
      "          vf_loss: 0.00454599377569846\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.6\n",
      "    ram_util_percent: 53.42857142857142\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040784435593623485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.027560926092745\n",
      "    mean_inference_ms: 1.409772852996322\n",
      "    mean_raw_obs_processing_ms: 6.35905531485423\n",
      "  time_since_restore: 359.50020933151245\n",
      "  time_this_iter_s: 9.763137102127075\n",
      "  time_total_s: 359.50020933151245\n",
      "  timers:\n",
      "    learn_throughput: 1496.195\n",
      "    learn_time_ms: 668.362\n",
      "    load_throughput: 305569.203\n",
      "    load_time_ms: 3.273\n",
      "    sample_throughput: 110.295\n",
      "    sample_time_ms: 9066.601\n",
      "    update_time_ms: 1.717\n",
      "  timestamp: 1631981572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">           359.5</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-3.52174</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">               883</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-13-01\n",
      "  done: false\n",
      "  episode_len_mean: 887.875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -3.375\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.413970539304945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009946620388181993\n",
      "          policy_loss: -0.031489395846923195\n",
      "          total_loss: -0.03735377879606353\n",
      "          vf_explained_var: 0.23393145203590393\n",
      "          vf_loss: 0.011561354847314458\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.57142857142857\n",
      "    ram_util_percent: 54.36428571428571\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04075061685393778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.831942669106331\n",
      "    mean_inference_ms: 1.4080099626688616\n",
      "    mean_raw_obs_processing_ms: 6.315519821974422\n",
      "  time_since_restore: 368.94692063331604\n",
      "  time_this_iter_s: 9.446711301803589\n",
      "  time_total_s: 368.94692063331604\n",
      "  timers:\n",
      "    learn_throughput: 1494.555\n",
      "    learn_time_ms: 669.095\n",
      "    load_throughput: 305702.832\n",
      "    load_time_ms: 3.271\n",
      "    sample_throughput: 112.773\n",
      "    sample_time_ms: 8867.339\n",
      "    update_time_ms: 1.705\n",
      "  timestamp: 1631981581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         368.947</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">  -3.375</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           887.875</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-13-11\n",
      "  done: false\n",
      "  episode_len_mean: 892.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -3.24\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5622371620602076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007846308017997005\n",
      "          policy_loss: -0.0954566051148706\n",
      "          total_loss: -0.11288728693293201\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00289543038363465\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.892307692307696\n",
      "    ram_util_percent: 54.49230769230769\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071610535703725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.648472051408035\n",
      "    mean_inference_ms: 1.4062629701177236\n",
      "    mean_raw_obs_processing_ms: 6.266414681808844\n",
      "  time_since_restore: 378.38005447387695\n",
      "  time_this_iter_s: 9.433133840560913\n",
      "  time_total_s: 378.38005447387695\n",
      "  timers:\n",
      "    learn_throughput: 1495.882\n",
      "    learn_time_ms: 668.502\n",
      "    load_throughput: 304763.233\n",
      "    load_time_ms: 3.281\n",
      "    sample_throughput: 112.251\n",
      "    sample_time_ms: 8908.6\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1631981591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">          378.38</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">   -3.24</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            892.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 896.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -3.1153846153846154\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5388310564888847\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007913037596885367\n",
      "          policy_loss: -0.08542121628092395\n",
      "          total_loss: -0.10038222935464647\n",
      "          vf_explained_var: -0.8031092286109924\n",
      "          vf_loss: 0.005085994591677768\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.20714285714286\n",
      "    ram_util_percent: 54.721428571428554\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040682466378096124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.476443331854407\n",
      "    mean_inference_ms: 1.4045966541883705\n",
      "    mean_raw_obs_processing_ms: 6.21311343545179\n",
      "  time_since_restore: 388.1617338657379\n",
      "  time_this_iter_s: 9.781679391860962\n",
      "  time_total_s: 388.1617338657379\n",
      "  timers:\n",
      "    learn_throughput: 1490.615\n",
      "    learn_time_ms: 670.864\n",
      "    load_throughput: 304084.911\n",
      "    load_time_ms: 3.289\n",
      "    sample_throughput: 111.427\n",
      "    sample_time_ms: 8974.504\n",
      "    update_time_ms: 1.696\n",
      "  timestamp: 1631981601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         388.162</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-3.11538</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">             896.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-13-30\n",
      "  done: false\n",
      "  episode_len_mean: 900.3333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -3.0\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5264961216184827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0036638973529518576\n",
      "          policy_loss: -0.13386898396743668\n",
      "          total_loss: -0.15074449338846738\n",
      "          vf_explained_var: -0.6281957626342773\n",
      "          vf_loss: 0.005916321626864373\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.01428571428572\n",
      "    ram_util_percent: 54.814285714285695\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406497052948991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.314724976525838\n",
      "    mean_inference_ms: 1.4029916871857984\n",
      "    mean_raw_obs_processing_ms: 6.156696598938214\n",
      "  time_since_restore: 397.82965445518494\n",
      "  time_this_iter_s: 9.667920589447021\n",
      "  time_total_s: 397.82965445518494\n",
      "  timers:\n",
      "    learn_throughput: 1493.137\n",
      "    learn_time_ms: 669.731\n",
      "    load_throughput: 305240.084\n",
      "    load_time_ms: 3.276\n",
      "    sample_throughput: 110.922\n",
      "    sample_time_ms: 9015.342\n",
      "    update_time_ms: 1.705\n",
      "  timestamp: 1631981610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">          397.83</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">      -3</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           900.333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-13-40\n",
      "  done: false\n",
      "  episode_len_mean: 903.8928571428571\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.857142857142857\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.467068929142422\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007571735254300351\n",
      "          policy_loss: 0.1309463522500462\n",
      "          total_loss: 0.11203731422622999\n",
      "          vf_explained_var: 0.3144274353981018\n",
      "          vf_loss: 0.003206189464415527\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.90714285714285\n",
      "    ram_util_percent: 55.09285714285715\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061917537952299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.162392496740306\n",
      "    mean_inference_ms: 1.4014854128171286\n",
      "    mean_raw_obs_processing_ms: 6.098035145171553\n",
      "  time_since_restore: 407.55526089668274\n",
      "  time_this_iter_s: 9.725606441497803\n",
      "  time_total_s: 407.55526089668274\n",
      "  timers:\n",
      "    learn_throughput: 1492.394\n",
      "    learn_time_ms: 670.064\n",
      "    load_throughput: 309091.07\n",
      "    load_time_ms: 3.235\n",
      "    sample_throughput: 110.58\n",
      "    sample_time_ms: 9043.236\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1631981620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         407.555</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-2.85714</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           903.893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-13-49\n",
      "  done: false\n",
      "  episode_len_mean: 907.2068965517242\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.7241379310344827\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.484879144032796\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019831591988369486\n",
      "          policy_loss: 0.1188467585378223\n",
      "          total_loss: 0.1046080768108368\n",
      "          vf_explained_var: -0.14281293749809265\n",
      "          vf_loss: 0.0039169466666256385\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.73846153846154\n",
      "    ram_util_percent: 55.03076923076924\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058864493975906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.018234683943986\n",
      "    mean_inference_ms: 1.4000244321427784\n",
      "    mean_raw_obs_processing_ms: 6.037805191981989\n",
      "  time_since_restore: 416.9366412162781\n",
      "  time_this_iter_s: 9.381380319595337\n",
      "  time_total_s: 416.9366412162781\n",
      "  timers:\n",
      "    learn_throughput: 1506.964\n",
      "    learn_time_ms: 663.586\n",
      "    load_throughput: 307967.664\n",
      "    load_time_ms: 3.247\n",
      "    sample_throughput: 110.993\n",
      "    sample_time_ms: 9009.574\n",
      "    update_time_ms: 1.696\n",
      "  timestamp: 1631981629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         416.937</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-2.72414</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           907.207</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-13-59\n",
      "  done: false\n",
      "  episode_len_mean: 910.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.6333333333333333\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.53340417014228\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013735513012403564\n",
      "          policy_loss: -0.014920179090566105\n",
      "          total_loss: -0.032245269334978524\n",
      "          vf_explained_var: 0.25545358657836914\n",
      "          vf_loss: 0.0033732176111597153\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.30769230769231\n",
      "    ram_util_percent: 55.03076923076924\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055834061256048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.88153795569649\n",
      "    mean_inference_ms: 1.3985946539915766\n",
      "    mean_raw_obs_processing_ms: 5.976542332977692\n",
      "  time_since_restore: 426.22730445861816\n",
      "  time_this_iter_s: 9.290663242340088\n",
      "  time_total_s: 426.22730445861816\n",
      "  timers:\n",
      "    learn_throughput: 1532.961\n",
      "    learn_time_ms: 652.332\n",
      "    load_throughput: 307005.124\n",
      "    load_time_ms: 3.257\n",
      "    sample_throughput: 112.068\n",
      "    sample_time_ms: 8923.187\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1631981639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         426.227</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-2.63333</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">             910.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-14-08\n",
      "  done: false\n",
      "  episode_len_mean: 913.1935483870968\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.5483870967741935\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.520556020736694\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014283085154531111\n",
      "          policy_loss: 0.054686250620418125\n",
      "          total_loss: 0.0394025883740849\n",
      "          vf_explained_var: -0.227853924036026\n",
      "          vf_loss: 0.005101357604790893\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.878571428571426\n",
      "    ram_util_percent: 55.092857142857156\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040528357019621424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.751556708540985\n",
      "    mean_inference_ms: 1.397191036603464\n",
      "    mean_raw_obs_processing_ms: 5.914683116649301\n",
      "  time_since_restore: 435.3981132507324\n",
      "  time_this_iter_s: 9.170808792114258\n",
      "  time_total_s: 435.3981132507324\n",
      "  timers:\n",
      "    learn_throughput: 1532.365\n",
      "    learn_time_ms: 652.586\n",
      "    load_throughput: 309533.593\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 113.117\n",
      "    sample_time_ms: 8840.443\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1631981648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         435.398</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-2.54839</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           913.194</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 915.90625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.46875\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.603881984286838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008884891117715111\n",
      "          policy_loss: -0.04468931257724762\n",
      "          total_loss: -0.06572166867554188\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0020078121141220134\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.54615384615384\n",
      "    ram_util_percent: 55.17692307692308\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049932047042701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.628207219397758\n",
      "    mean_inference_ms: 1.395837336601152\n",
      "    mean_raw_obs_processing_ms: 5.85257913458554\n",
      "  time_since_restore: 444.9687261581421\n",
      "  time_this_iter_s: 9.570612907409668\n",
      "  time_total_s: 444.9687261581421\n",
      "  timers:\n",
      "    learn_throughput: 1537.37\n",
      "    learn_time_ms: 650.462\n",
      "    load_throughput: 310057.586\n",
      "    load_time_ms: 3.225\n",
      "    sample_throughput: 112.801\n",
      "    sample_time_ms: 8865.189\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1631981657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         444.969</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-2.46875</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           915.906</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-14-26\n",
      "  done: false\n",
      "  episode_len_mean: 918.4545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.393939393939394\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5448914660347834\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011722076049035815\n",
      "          policy_loss: -0.17997491227255927\n",
      "          total_loss: -0.1998554011185964\n",
      "          vf_explained_var: -0.47906172275543213\n",
      "          vf_loss: 0.0016122254362724358\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.638461538461534\n",
      "    ram_util_percent: 55.16923076923078\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04047090665055986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.510032819266543\n",
      "    mean_inference_ms: 1.3945140264967506\n",
      "    mean_raw_obs_processing_ms: 5.79050825739251\n",
      "  time_since_restore: 453.5516839027405\n",
      "  time_this_iter_s: 8.582957744598389\n",
      "  time_total_s: 453.5516839027405\n",
      "  timers:\n",
      "    learn_throughput: 1541.691\n",
      "    learn_time_ms: 648.639\n",
      "    load_throughput: 310767.453\n",
      "    load_time_ms: 3.218\n",
      "    sample_throughput: 114.299\n",
      "    sample_time_ms: 8748.971\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1631981666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         453.552</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-2.39394</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           918.455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-14-35\n",
      "  done: false\n",
      "  episode_len_mean: 920.8529411764706\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.323529411764706\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5109880844751995\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011955855112892206\n",
      "          policy_loss: 0.08620979251960913\n",
      "          total_loss: 0.06683593235082097\n",
      "          vf_explained_var: 0.13686338067054749\n",
      "          vf_loss: 0.001700916242003182\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.38333333333333\n",
      "    ram_util_percent: 55.199999999999996\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04044306779325743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.397010752082323\n",
      "    mean_inference_ms: 1.3932127414540947\n",
      "    mean_raw_obs_processing_ms: 5.728702100714524\n",
      "  time_since_restore: 462.4393961429596\n",
      "  time_this_iter_s: 8.887712240219116\n",
      "  time_total_s: 462.4393961429596\n",
      "  timers:\n",
      "    learn_throughput: 1544.285\n",
      "    learn_time_ms: 647.549\n",
      "    load_throughput: 309168.534\n",
      "    load_time_ms: 3.234\n",
      "    sample_throughput: 115.02\n",
      "    sample_time_ms: 8694.155\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1631981675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         462.439</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-2.32353</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           920.853</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-14-44\n",
      "  done: false\n",
      "  episode_len_mean: 923.1142857142858\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.257142857142857\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.557257252269321\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008146977846499539\n",
      "          policy_loss: 0.006548690547545751\n",
      "          total_loss: -0.01436559334397316\n",
      "          vf_explained_var: -0.4181974232196808\n",
      "          vf_loss: 0.0019086826989627701\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.900000000000006\n",
      "    ram_util_percent: 55.3\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04041585198655199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.28892023868022\n",
      "    mean_inference_ms: 1.3919465967305182\n",
      "    mean_raw_obs_processing_ms: 5.6673286840814425\n",
      "  time_since_restore: 471.4817969799042\n",
      "  time_this_iter_s: 9.04240083694458\n",
      "  time_total_s: 471.4817969799042\n",
      "  timers:\n",
      "    learn_throughput: 1541.347\n",
      "    learn_time_ms: 648.783\n",
      "    load_throughput: 302811.598\n",
      "    load_time_ms: 3.302\n",
      "    sample_throughput: 115.556\n",
      "    sample_time_ms: 8653.794\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1631981684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         471.482</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-2.25714</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           923.114</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-14-53\n",
      "  done: false\n",
      "  episode_len_mean: 925.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.1944444444444446\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.548848334948222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0076119084525106\n",
      "          policy_loss: -0.0689220764570766\n",
      "          total_loss: -0.09070639262596766\n",
      "          vf_explained_var: -0.3235972225666046\n",
      "          vf_loss: 0.0011351510149122785\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.7\n",
      "    ram_util_percent: 55.383333333333326\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04038891263170847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.185010563036478\n",
      "    mean_inference_ms: 1.3907034040330946\n",
      "    mean_raw_obs_processing_ms: 5.606529173698838\n",
      "  time_since_restore: 479.9846513271332\n",
      "  time_this_iter_s: 8.502854347229004\n",
      "  time_total_s: 479.9846513271332\n",
      "  timers:\n",
      "    learn_throughput: 1550.312\n",
      "    learn_time_ms: 645.031\n",
      "    load_throughput: 302750.397\n",
      "    load_time_ms: 3.303\n",
      "    sample_throughput: 117.238\n",
      "    sample_time_ms: 8529.687\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1631981693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         479.985</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-2.19444</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            925.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 927.2702702702703\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.135135135135135\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.572509010632833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011177547066836012\n",
      "          policy_loss: -0.07413210272789002\n",
      "          total_loss: -0.09513660023609798\n",
      "          vf_explained_var: -0.3870854675769806\n",
      "          vf_loss: 0.0009481699497781745\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.458333333333336\n",
      "    ram_util_percent: 55.4\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040362714837202815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.08494755922456\n",
      "    mean_inference_ms: 1.3894877890469122\n",
      "    mean_raw_obs_processing_ms: 5.546425959774998\n",
      "  time_since_restore: 488.3819465637207\n",
      "  time_this_iter_s: 8.397295236587524\n",
      "  time_total_s: 488.3819465637207\n",
      "  timers:\n",
      "    learn_throughput: 1545.386\n",
      "    learn_time_ms: 647.087\n",
      "    load_throughput: 301286.805\n",
      "    load_time_ms: 3.319\n",
      "    sample_throughput: 119.04\n",
      "    sample_time_ms: 8400.518\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1631981701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         488.382</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-2.13514</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            927.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-15-09\n",
      "  done: false\n",
      "  episode_len_mean: 929.1842105263158\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.0789473684210527\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.598424869113498\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009191391169818727\n",
      "          policy_loss: -0.06474812510940764\n",
      "          total_loss: -0.08685214759574997\n",
      "          vf_explained_var: -0.6681010127067566\n",
      "          vf_loss: 0.000778131173683505\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.71666666666667\n",
      "    ram_util_percent: 55.416666666666664\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033685687158195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.98848158159871\n",
      "    mean_inference_ms: 1.3882866361781248\n",
      "    mean_raw_obs_processing_ms: 5.487093636596691\n",
      "  time_since_restore: 496.6732084751129\n",
      "  time_this_iter_s: 8.291261911392212\n",
      "  time_total_s: 496.6732084751129\n",
      "  timers:\n",
      "    learn_throughput: 1548.398\n",
      "    learn_time_ms: 645.829\n",
      "    load_throughput: 300936.61\n",
      "    load_time_ms: 3.323\n",
      "    sample_throughput: 121.09\n",
      "    sample_time_ms: 8258.333\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1631981709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         496.673</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-2.07895</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           929.184</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 931.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -2.0256410256410255\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.572610428598192\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008653683319882808\n",
      "          policy_loss: -0.04221520647406578\n",
      "          total_loss: -0.06448948333660762\n",
      "          vf_explained_var: -0.5766053199768066\n",
      "          vf_loss: 0.000531206997483322\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.69166666666667\n",
      "    ram_util_percent: 55.4\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04031125777420596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.89522096683165\n",
      "    mean_inference_ms: 1.3871038697781444\n",
      "    mean_raw_obs_processing_ms: 5.428597700492426\n",
      "  time_since_restore: 504.71050333976746\n",
      "  time_this_iter_s: 8.037294864654541\n",
      "  time_total_s: 504.71050333976746\n",
      "  timers:\n",
      "    learn_throughput: 1543.995\n",
      "    learn_time_ms: 647.671\n",
      "    load_throughput: 300210.719\n",
      "    load_time_ms: 3.331\n",
      "    sample_throughput: 123.121\n",
      "    sample_time_ms: 8122.065\n",
      "    update_time_ms: 1.722\n",
      "  timestamp: 1631981717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         504.711</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-2.02564</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">               931</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-15-26\n",
      "  done: false\n",
      "  episode_len_mean: 932.725\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.975\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 40\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.482022409968906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008207910104156902\n",
      "          policy_loss: -0.06294444708360566\n",
      "          total_loss: -0.08355077455441157\n",
      "          vf_explained_var: -0.6399221420288086\n",
      "          vf_loss: 0.0014437250553682032\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.449999999999996\n",
      "    ram_util_percent: 55.46666666666667\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04028617279574638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.805200221981426\n",
      "    mean_inference_ms: 1.3859466095958868\n",
      "    mean_raw_obs_processing_ms: 5.370990746446526\n",
      "  time_since_restore: 513.0150277614594\n",
      "  time_this_iter_s: 8.304524421691895\n",
      "  time_total_s: 513.0150277614594\n",
      "  timers:\n",
      "    learn_throughput: 1540.721\n",
      "    learn_time_ms: 649.047\n",
      "    load_throughput: 301507.717\n",
      "    load_time_ms: 3.317\n",
      "    sample_throughput: 124.656\n",
      "    sample_time_ms: 8022.082\n",
      "    update_time_ms: 1.733\n",
      "  timestamp: 1631981726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         513.015</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">  -1.975</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           932.725</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-15-34\n",
      "  done: false\n",
      "  episode_len_mean: 934.3658536585366\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.9268292682926829\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 41\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4406596395704483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00898419153487071\n",
      "          policy_loss: -0.04131204151651925\n",
      "          total_loss: -0.06128625062604745\n",
      "          vf_explained_var: -0.9638320803642273\n",
      "          vf_loss: 0.0014002223939718937\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.666666666666664\n",
      "    ram_util_percent: 55.49166666666667\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040261577732644896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.71826080228686\n",
      "    mean_inference_ms: 1.3848115464816397\n",
      "    mean_raw_obs_processing_ms: 5.314312728683351\n",
      "  time_since_restore: 521.3321998119354\n",
      "  time_this_iter_s: 8.317172050476074\n",
      "  time_total_s: 521.3321998119354\n",
      "  timers:\n",
      "    learn_throughput: 1545.964\n",
      "    learn_time_ms: 646.846\n",
      "    load_throughput: 300908.543\n",
      "    load_time_ms: 3.323\n",
      "    sample_throughput: 125.962\n",
      "    sample_time_ms: 7938.931\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1631981734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         521.332</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-1.92683</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           934.366</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-15-43\n",
      "  done: false\n",
      "  episode_len_mean: 935.9285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.880952380952381\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 42\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.416805322964986\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00983859686516229\n",
      "          policy_loss: -0.04132922262781196\n",
      "          total_loss: -0.05852732178237703\n",
      "          vf_explained_var: -0.7327122092247009\n",
      "          vf_loss: 0.0036494270637275703\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.99999999999999\n",
      "    ram_util_percent: 55.550000000000004\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04023771607364286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.634487504133633\n",
      "    mean_inference_ms: 1.383708344501125\n",
      "    mean_raw_obs_processing_ms: 5.258590546093389\n",
      "  time_since_restore: 530.0660529136658\n",
      "  time_this_iter_s: 8.733853101730347\n",
      "  time_total_s: 530.0660529136658\n",
      "  timers:\n",
      "    learn_throughput: 1549.192\n",
      "    learn_time_ms: 645.498\n",
      "    load_throughput: 300434.359\n",
      "    load_time_ms: 3.329\n",
      "    sample_throughput: 127.282\n",
      "    sample_time_ms: 7856.574\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1631981743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         530.066</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-1.88095</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           935.929</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-16-09\n",
      "  done: false\n",
      "  episode_len_mean: 934.2325581395348\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.8372093023255813\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 43\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4678532573911878\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00977669746956888\n",
      "          policy_loss: -0.04623100811408626\n",
      "          total_loss: -0.06342430042309893\n",
      "          vf_explained_var: -0.8790987730026245\n",
      "          vf_loss: 0.004185604968289327\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.789473684210527\n",
      "    ram_util_percent: 55.781578947368416\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0402145128831162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.553860771670994\n",
      "    mean_inference_ms: 1.3826373605696147\n",
      "    mean_raw_obs_processing_ms: 5.213772957667587\n",
      "  time_since_restore: 556.6280219554901\n",
      "  time_this_iter_s: 26.56196904182434\n",
      "  time_total_s: 556.6280219554901\n",
      "  timers:\n",
      "    learn_throughput: 1545.876\n",
      "    learn_time_ms: 646.882\n",
      "    load_throughput: 213487.525\n",
      "    load_time_ms: 4.684\n",
      "    sample_throughput: 103.608\n",
      "    sample_time_ms: 9651.761\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1631981769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         556.628</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-1.83721</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           934.233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-16-18\n",
      "  done: false\n",
      "  episode_len_mean: 935.7272727272727\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.7954545454545454\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4775991413328384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009978224203016842\n",
      "          policy_loss: -0.054001608656512365\n",
      "          total_loss: -0.0736454498850637\n",
      "          vf_explained_var: -0.7468075156211853\n",
      "          vf_loss: 0.0017645010548423873\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.80769230769231\n",
      "    ram_util_percent: 56.3153846153846\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040192290665955865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.476076019085898\n",
      "    mean_inference_ms: 1.3815975099866682\n",
      "    mean_raw_obs_processing_ms: 5.169249386867873\n",
      "  time_since_restore: 565.4157605171204\n",
      "  time_this_iter_s: 8.787738561630249\n",
      "  time_total_s: 565.4157605171204\n",
      "  timers:\n",
      "    learn_throughput: 1542.17\n",
      "    learn_time_ms: 648.437\n",
      "    load_throughput: 214219.158\n",
      "    load_time_ms: 4.668\n",
      "    sample_throughput: 103.732\n",
      "    sample_time_ms: 9640.198\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1631981778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         565.416</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-1.79545</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           935.727</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-16-27\n",
      "  done: false\n",
      "  episode_len_mean: 937.1555555555556\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.7555555555555555\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 45\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3562517219119603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013491703683417993\n",
      "          policy_loss: -0.07062424841440386\n",
      "          total_loss: -0.08448361615753835\n",
      "          vf_explained_var: -0.3154928684234619\n",
      "          vf_loss: 0.005149699425480018\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.03076923076923\n",
      "    ram_util_percent: 56.22307692307694\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04017072481400372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.401173811667832\n",
      "    mean_inference_ms: 1.3805899557748005\n",
      "    mean_raw_obs_processing_ms: 5.125088685458132\n",
      "  time_since_restore: 574.5912971496582\n",
      "  time_this_iter_s: 9.175536632537842\n",
      "  time_total_s: 574.5912971496582\n",
      "  timers:\n",
      "    learn_throughput: 1543.342\n",
      "    learn_time_ms: 647.945\n",
      "    load_throughput: 218028.632\n",
      "    load_time_ms: 4.587\n",
      "    sample_throughput: 103.583\n",
      "    sample_time_ms: 9654.09\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1631981787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         574.591</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">-1.75556</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           937.156</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 938.5217391304348\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.7173913043478262\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 46\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.498113160663181\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007620885996063434\n",
      "          policy_loss: -0.03625733604033788\n",
      "          total_loss: -0.05800971653726366\n",
      "          vf_explained_var: -0.8936265707015991\n",
      "          vf_loss: 0.0006567016898164486\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.345454545454544\n",
      "    ram_util_percent: 56.06363636363637\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04014969984927046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.328522781096222\n",
      "    mean_inference_ms: 1.3796006422878009\n",
      "    mean_raw_obs_processing_ms: 5.0813295547536494\n",
      "  time_since_restore: 582.7629356384277\n",
      "  time_this_iter_s: 8.171638488769531\n",
      "  time_total_s: 582.7629356384277\n",
      "  timers:\n",
      "    learn_throughput: 1546.733\n",
      "    learn_time_ms: 646.524\n",
      "    load_throughput: 218749.557\n",
      "    load_time_ms: 4.571\n",
      "    sample_throughput: 103.925\n",
      "    sample_time_ms: 9622.363\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1631981796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         582.763</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-1.71739</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           938.522</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-16-45\n",
      "  done: false\n",
      "  episode_len_mean: 939.8297872340426\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.6808510638297873\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 47\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3478867954678004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008922418692231057\n",
      "          policy_loss: 0.06431468731413285\n",
      "          total_loss: 0.04592250229583846\n",
      "          vf_explained_var: -0.6231013536453247\n",
      "          vf_loss: 0.0020753667272704964\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.64999999999999\n",
      "    ram_util_percent: 55.885714285714265\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04012923322446932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.258584039503322\n",
      "    mean_inference_ms: 1.3786378830591324\n",
      "    mean_raw_obs_processing_ms: 5.038021369670428\n",
      "  time_since_restore: 592.1499443054199\n",
      "  time_this_iter_s: 9.387008666992188\n",
      "  time_total_s: 592.1499443054199\n",
      "  timers:\n",
      "    learn_throughput: 1551.833\n",
      "    learn_time_ms: 644.399\n",
      "    load_throughput: 218021.832\n",
      "    load_time_ms: 4.587\n",
      "    sample_throughput: 102.844\n",
      "    sample_time_ms: 9723.459\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1631981805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">          592.15</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">-1.68085</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            939.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-16-54\n",
      "  done: false\n",
      "  episode_len_mean: 941.0833333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.6458333333333333\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 48\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3808150953716702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009281638785061316\n",
      "          policy_loss: -0.06242520846426487\n",
      "          total_loss: -0.08144021845526166\n",
      "          vf_explained_var: -0.5780569911003113\n",
      "          vf_loss: 0.0016605900524559224\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.28333333333333\n",
      "    ram_util_percent: 55.81666666666666\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040109490838260924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.190954995499704\n",
      "    mean_inference_ms: 1.3777021320474792\n",
      "    mean_raw_obs_processing_ms: 4.995188008607146\n",
      "  time_since_restore: 600.9821889400482\n",
      "  time_this_iter_s: 8.832244634628296\n",
      "  time_total_s: 600.9821889400482\n",
      "  timers:\n",
      "    learn_throughput: 1550.672\n",
      "    learn_time_ms: 644.882\n",
      "    load_throughput: 217416.077\n",
      "    load_time_ms: 4.599\n",
      "    sample_throughput: 102.28\n",
      "    sample_time_ms: 9777.066\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1631981814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         600.982</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">-1.64583</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           941.083</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 942.2857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.6122448979591837\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 49\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3599317603641086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00706868143227217\n",
      "          policy_loss: -0.02746694100399812\n",
      "          total_loss: -0.04710187104841073\n",
      "          vf_explained_var: -0.9661502242088318\n",
      "          vf_loss: 0.0015787058923807408\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.47692307692307\n",
      "    ram_util_percent: 55.630769230769246\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0400900960153661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.125546135250072\n",
      "    mean_inference_ms: 1.376785673222826\n",
      "    mean_raw_obs_processing_ms: 4.952858596104047\n",
      "  time_since_restore: 609.8740661144257\n",
      "  time_this_iter_s: 8.891877174377441\n",
      "  time_total_s: 609.8740661144257\n",
      "  timers:\n",
      "    learn_throughput: 1556.879\n",
      "    learn_time_ms: 642.311\n",
      "    load_throughput: 217527.708\n",
      "    load_time_ms: 4.597\n",
      "    sample_throughput: 101.367\n",
      "    sample_time_ms: 9865.125\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1631981823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         609.874</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">-1.61224</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           942.286</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-17-58\n",
      "  done: false\n",
      "  episode_len_mean: 933.3529411764706\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.4901960784313726\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 51\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.320815255906847\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0072998620462356965\n",
      "          policy_loss: -0.047786881691879696\n",
      "          total_loss: 0.0023759115487337113\n",
      "          vf_explained_var: -0.17321932315826416\n",
      "          vf_loss: 0.07090724084991962\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.01139240506329\n",
      "    ram_util_percent: 55.43037974683546\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04005348675718139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.001805575388826\n",
      "    mean_inference_ms: 1.3750417187174566\n",
      "    mean_raw_obs_processing_ms: 4.908497919672662\n",
      "  time_since_restore: 665.3750479221344\n",
      "  time_this_iter_s: 55.50098180770874\n",
      "  time_total_s: 665.3750479221344\n",
      "  timers:\n",
      "    learn_throughput: 1536.225\n",
      "    learn_time_ms: 650.947\n",
      "    load_throughput: 183394.431\n",
      "    load_time_ms: 5.453\n",
      "    sample_throughput: 68.609\n",
      "    sample_time_ms: 14575.253\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1631981878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         665.375</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -1.4902</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           933.353</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-18-08\n",
      "  done: false\n",
      "  episode_len_mean: 934.6346153846154\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.4615384615384615\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 52\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4784562508265178\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007655309651024409\n",
      "          policy_loss: 0.007878005918529298\n",
      "          total_loss: -0.013123071690400442\n",
      "          vf_explained_var: -0.30670133233070374\n",
      "          vf_loss: 0.0011998180893392095\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.199999999999996\n",
      "    ram_util_percent: 56.37142857142857\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04003706283293147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.943289609746513\n",
      "    mean_inference_ms: 1.3742380091147406\n",
      "    mean_raw_obs_processing_ms: 4.8861423844841285\n",
      "  time_since_restore: 675.0983428955078\n",
      "  time_this_iter_s: 9.723294973373413\n",
      "  time_total_s: 675.0983428955078\n",
      "  timers:\n",
      "    learn_throughput: 1516.602\n",
      "    learn_time_ms: 659.369\n",
      "    load_throughput: 181094.954\n",
      "    load_time_ms: 5.522\n",
      "    sample_throughput: 67.993\n",
      "    sample_time_ms: 14707.301\n",
      "    update_time_ms: 1.708\n",
      "  timestamp: 1631981888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         675.098</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">-1.46154</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           934.635</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-18-17\n",
      "  done: false\n",
      "  episode_len_mean: 935.8679245283018\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.4339622641509433\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 53\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.356647112634447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01010687954799404\n",
      "          policy_loss: -0.016304176011019283\n",
      "          total_loss: -0.0347729154345062\n",
      "          vf_explained_var: -0.8308650255203247\n",
      "          vf_loss: 0.0016866594928109811\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.471428571428575\n",
      "    ram_util_percent: 56.49999999999999\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04002181815301534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.886725057686245\n",
      "    mean_inference_ms: 1.3734742315874913\n",
      "    mean_raw_obs_processing_ms: 4.863260231648054\n",
      "  time_since_restore: 684.5615158081055\n",
      "  time_this_iter_s: 9.463172912597656\n",
      "  time_total_s: 684.5615158081055\n",
      "  timers:\n",
      "    learn_throughput: 1516.638\n",
      "    learn_time_ms: 659.353\n",
      "    load_throughput: 180613.801\n",
      "    load_time_ms: 5.537\n",
      "    sample_throughput: 67.658\n",
      "    sample_time_ms: 14780.244\n",
      "    update_time_ms: 1.708\n",
      "  timestamp: 1631981897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         684.562</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">-1.43396</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           935.868</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-18-26\n",
      "  done: false\n",
      "  episode_len_mean: 937.0555555555555\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.4074074074074074\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 54\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5398691177368162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006295328678075821\n",
      "          policy_loss: -0.00610105622973707\n",
      "          total_loss: -0.029099483456876542\n",
      "          vf_explained_var: 0.07766468077898026\n",
      "          vf_loss: 0.0002755904697551159\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.925000000000004\n",
      "    ram_util_percent: 56.33333333333332\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040006895283606854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.831662393219275\n",
      "    mean_inference_ms: 1.3727258488351577\n",
      "    mean_raw_obs_processing_ms: 4.83993367760599\n",
      "  time_since_restore: 692.9975898265839\n",
      "  time_this_iter_s: 8.436074018478394\n",
      "  time_total_s: 692.9975898265839\n",
      "  timers:\n",
      "    learn_throughput: 1522.232\n",
      "    learn_time_ms: 656.93\n",
      "    load_throughput: 234990.812\n",
      "    load_time_ms: 4.255\n",
      "    sample_throughput: 77.093\n",
      "    sample_time_ms: 12971.373\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1631981906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         692.998</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">-1.40741</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           937.056</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-18-35\n",
      "  done: false\n",
      "  episode_len_mean: 938.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.3818181818181818\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 55\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.475806548860338\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005507781526225841\n",
      "          policy_loss: -0.06082903082585997\n",
      "          total_loss: -0.0830688984443744\n",
      "          vf_explained_var: -0.5110281109809875\n",
      "          vf_loss: 0.0006593216812689207\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.14615384615385\n",
      "    ram_util_percent: 56.23076923076924\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03999269406161898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.778196120176798\n",
      "    mean_inference_ms: 1.3720052776661464\n",
      "    mean_raw_obs_processing_ms: 4.8162366651414725\n",
      "  time_since_restore: 701.9784181118011\n",
      "  time_this_iter_s: 8.980828285217285\n",
      "  time_total_s: 701.9784181118011\n",
      "  timers:\n",
      "    learn_throughput: 1510.534\n",
      "    learn_time_ms: 662.018\n",
      "    load_throughput: 234629.314\n",
      "    load_time_ms: 4.262\n",
      "    sample_throughput: 77.009\n",
      "    sample_time_ms: 12985.538\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1631981915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         701.978</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-1.38182</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">             938.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-18-45\n",
      "  done: false\n",
      "  episode_len_mean: 939.3035714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.3571428571428572\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 56\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4141886552174885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008150332949203214\n",
      "          policy_loss: -0.08046758568121327\n",
      "          total_loss: -0.10067920581334167\n",
      "          vf_explained_var: -0.7163428664207458\n",
      "          vf_loss: 0.001179528148430917\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.25714285714287\n",
      "    ram_util_percent: 56.12857142857144\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03997955476881738\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.726556192484225\n",
      "    mean_inference_ms: 1.3713210438351289\n",
      "    mean_raw_obs_processing_ms: 4.792234898829368\n",
      "  time_since_restore: 711.874587059021\n",
      "  time_this_iter_s: 9.896168947219849\n",
      "  time_total_s: 711.874587059021\n",
      "  timers:\n",
      "    learn_throughput: 1501.077\n",
      "    learn_time_ms: 666.189\n",
      "    load_throughput: 233343.57\n",
      "    load_time_ms: 4.286\n",
      "    sample_throughput: 76.609\n",
      "    sample_time_ms: 13053.379\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1631981925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         711.875</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">-1.35714</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           939.304</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-18-54\n",
      "  done: false\n",
      "  episode_len_mean: 940.3684210526316\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.3333333333333333\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 57\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.500933270984226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0070091107468568635\n",
      "          policy_loss: -0.037294209375977515\n",
      "          total_loss: -0.05950250451763471\n",
      "          vf_explained_var: -0.07140655070543289\n",
      "          vf_loss: 0.00043546247523206855\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.01538461538461\n",
      "    ram_util_percent: 55.83076923076923\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03996668445989774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.676342997436889\n",
      "    mean_inference_ms: 1.3706543513847491\n",
      "    mean_raw_obs_processing_ms: 4.767982936151555\n",
      "  time_since_restore: 720.7214469909668\n",
      "  time_this_iter_s: 8.8468599319458\n",
      "  time_total_s: 720.7214469909668\n",
      "  timers:\n",
      "    learn_throughput: 1497.676\n",
      "    learn_time_ms: 667.701\n",
      "    load_throughput: 233771.451\n",
      "    load_time_ms: 4.278\n",
      "    sample_throughput: 76.223\n",
      "    sample_time_ms: 13119.43\n",
      "    update_time_ms: 1.706\n",
      "  timestamp: 1631981934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         720.721</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">-1.33333</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           940.368</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-19-03\n",
      "  done: false\n",
      "  episode_len_mean: 941.3965517241379\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.3103448275862069\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 58\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3846836434470284\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014819260580153454\n",
      "          policy_loss: 0.04202855825424194\n",
      "          total_loss: 0.03340553790330887\n",
      "          vf_explained_var: 0.4449535012245178\n",
      "          vf_loss: 0.01022231237600661\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.4076923076923\n",
      "    ram_util_percent: 55.56153846153847\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039954284753305265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.627681605749423\n",
      "    mean_inference_ms: 1.3700129795853053\n",
      "    mean_raw_obs_processing_ms: 4.743535732223576\n",
      "  time_since_restore: 730.1800272464752\n",
      "  time_this_iter_s: 9.458580255508423\n",
      "  time_total_s: 730.1800272464752\n",
      "  timers:\n",
      "    learn_throughput: 1496.194\n",
      "    learn_time_ms: 668.363\n",
      "    load_throughput: 235496.14\n",
      "    load_time_ms: 4.246\n",
      "    sample_throughput: 76.185\n",
      "    sample_time_ms: 13125.98\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1631981943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">          730.18</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">-1.31034</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           941.397</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-19-12\n",
      "  done: false\n",
      "  episode_len_mean: 942.3898305084746\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.2881355932203389\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 59\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.494808962610033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009236251177623762\n",
      "          policy_loss: -0.12348763959275352\n",
      "          total_loss: -0.14420984946191312\n",
      "          vf_explained_var: -0.4008418321609497\n",
      "          vf_loss: 0.0011086431433795952\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.76923076923077\n",
      "    ram_util_percent: 55.807692307692314\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03994252625879413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.580387662706332\n",
      "    mean_inference_ms: 1.3693962547873548\n",
      "    mean_raw_obs_processing_ms: 4.718938059133665\n",
      "  time_since_restore: 739.3541321754456\n",
      "  time_this_iter_s: 9.174104928970337\n",
      "  time_total_s: 739.3541321754456\n",
      "  timers:\n",
      "    learn_throughput: 1478.657\n",
      "    learn_time_ms: 676.289\n",
      "    load_throughput: 233352.657\n",
      "    load_time_ms: 4.285\n",
      "    sample_throughput: 76.033\n",
      "    sample_time_ms: 13152.116\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1631981952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         739.354</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-1.28814</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            942.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-19-23\n",
      "  done: false\n",
      "  episode_len_mean: 943.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.2666666666666666\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 60\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3583421309789023\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013568814135639679\n",
      "          policy_loss: -0.04624882864041461\n",
      "          total_loss: -0.0642317364199294\n",
      "          vf_explained_var: 0.07156095653772354\n",
      "          vf_loss: 0.0010210372332949192\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.360000000000014\n",
      "    ram_util_percent: 55.96666666666666\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0399316610387155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.534704424839443\n",
      "    mean_inference_ms: 1.3688093508039498\n",
      "    mean_raw_obs_processing_ms: 4.6942312984190435\n",
      "  time_since_restore: 749.5683312416077\n",
      "  time_this_iter_s: 10.21419906616211\n",
      "  time_total_s: 749.5683312416077\n",
      "  timers:\n",
      "    learn_throughput: 1467.262\n",
      "    learn_time_ms: 681.542\n",
      "    load_throughput: 232882.335\n",
      "    load_time_ms: 4.294\n",
      "    sample_throughput: 75.307\n",
      "    sample_time_ms: 13279.039\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1631981963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         749.568</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">-1.26667</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            943.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-19-33\n",
      "  done: false\n",
      "  episode_len_mean: 944.2786885245902\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.2459016393442623\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 61\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0904591745800443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009728373616449727\n",
      "          policy_loss: 0.02931175749335024\n",
      "          total_loss: 0.013241404874457253\n",
      "          vf_explained_var: -0.8708957433700562\n",
      "          vf_loss: 0.0015509138685754604\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.45\n",
      "    ram_util_percent: 55.957142857142856\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03992149749518292\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.490552342782888\n",
      "    mean_inference_ms: 1.368250899733544\n",
      "    mean_raw_obs_processing_ms: 4.669452649715741\n",
      "  time_since_restore: 759.8375523090363\n",
      "  time_this_iter_s: 10.269221067428589\n",
      "  time_total_s: 759.8375523090363\n",
      "  timers:\n",
      "    learn_throughput: 1463.184\n",
      "    learn_time_ms: 683.441\n",
      "    load_throughput: 281615.985\n",
      "    load_time_ms: 3.551\n",
      "    sample_throughput: 114.226\n",
      "    sample_time_ms: 8754.562\n",
      "    update_time_ms: 1.814\n",
      "  timestamp: 1631981973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         759.838</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\"> -1.2459</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           944.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-19-44\n",
      "  done: false\n",
      "  episode_len_mean: 945.1774193548387\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.2258064516129032\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 62\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2167019208272296\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008282511220372522\n",
      "          policy_loss: -0.009000593465235499\n",
      "          total_loss: -0.024871079706483418\n",
      "          vf_explained_var: -0.060268767178058624\n",
      "          vf_loss: 0.0035011868691071867\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.681250000000006\n",
      "    ram_util_percent: 56.08125\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039912258225053364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.447980249708667\n",
      "    mean_inference_ms: 1.3677231517549047\n",
      "    mean_raw_obs_processing_ms: 4.644634946646816\n",
      "  time_since_restore: 770.5115070343018\n",
      "  time_this_iter_s: 10.673954725265503\n",
      "  time_total_s: 770.5115070343018\n",
      "  timers:\n",
      "    learn_throughput: 1472.132\n",
      "    learn_time_ms: 679.287\n",
      "    load_throughput: 271132.027\n",
      "    load_time_ms: 3.688\n",
      "    sample_throughput: 112.948\n",
      "    sample_time_ms: 8853.635\n",
      "    update_time_ms: 1.828\n",
      "  timestamp: 1631981984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         770.512</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">-1.22581</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           945.177</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-19-53\n",
      "  done: false\n",
      "  episode_len_mean: 946.047619047619\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.2063492063492063\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 63\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4782678365707396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014442453751581317\n",
      "          policy_loss: 0.029237177222967148\n",
      "          total_loss: 0.010512602494822608\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0011837756544183422\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.2\n",
      "    ram_util_percent: 56.07692307692308\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03990344463858025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.406601215179503\n",
      "    mean_inference_ms: 1.367215608446463\n",
      "    mean_raw_obs_processing_ms: 4.6198076245998205\n",
      "  time_since_restore: 779.9512529373169\n",
      "  time_this_iter_s: 9.439745903015137\n",
      "  time_total_s: 779.9512529373169\n",
      "  timers:\n",
      "    learn_throughput: 1468.949\n",
      "    learn_time_ms: 680.759\n",
      "    load_throughput: 270698.059\n",
      "    load_time_ms: 3.694\n",
      "    sample_throughput: 112.997\n",
      "    sample_time_ms: 8849.795\n",
      "    update_time_ms: 1.827\n",
      "  timestamp: 1631981993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         779.951</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-1.20635</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           946.048</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-20-02\n",
      "  done: false\n",
      "  episode_len_mean: 946.890625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1875\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 64\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.499061197704739\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010512339026236835\n",
      "          policy_loss: -0.0332981423371368\n",
      "          total_loss: -0.05263775148325496\n",
      "          vf_explained_var: -0.2780475914478302\n",
      "          vf_loss: 0.0021030884240947974\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.800000000000004\n",
      "    ram_util_percent: 56.06153846153847\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03989472111708999\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.366253938886958\n",
      "    mean_inference_ms: 1.3667205758558394\n",
      "    mean_raw_obs_processing_ms: 4.594995844668596\n",
      "  time_since_restore: 788.9961524009705\n",
      "  time_this_iter_s: 9.044899463653564\n",
      "  time_total_s: 788.9961524009705\n",
      "  timers:\n",
      "    learn_throughput: 1452.543\n",
      "    learn_time_ms: 688.448\n",
      "    load_throughput: 276494.041\n",
      "    load_time_ms: 3.617\n",
      "    sample_throughput: 112.322\n",
      "    sample_time_ms: 8902.941\n",
      "    update_time_ms: 1.85\n",
      "  timestamp: 1631982002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         788.996</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\"> -1.1875</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           946.891</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-20-12\n",
      "  done: false\n",
      "  episode_len_mean: 947.7076923076924\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1692307692307693\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 65\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.094940814707014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011099380751596695\n",
      "          policy_loss: 0.0022414557635784147\n",
      "          total_loss: -0.013357095668713253\n",
      "          vf_explained_var: -0.5701809525489807\n",
      "          vf_loss: 0.001604816184974172\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.74666666666667\n",
      "    ram_util_percent: 56.17333333333334\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039886535457270125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.327184148661612\n",
      "    mean_inference_ms: 1.3662504439913568\n",
      "    mean_raw_obs_processing_ms: 4.570224477769659\n",
      "  time_since_restore: 799.1781437397003\n",
      "  time_this_iter_s: 10.181991338729858\n",
      "  time_total_s: 799.1781437397003\n",
      "  timers:\n",
      "    learn_throughput: 1464.471\n",
      "    learn_time_ms: 682.84\n",
      "    load_throughput: 276350.124\n",
      "    load_time_ms: 3.619\n",
      "    sample_throughput: 110.758\n",
      "    sample_time_ms: 9028.712\n",
      "    update_time_ms: 1.847\n",
      "  timestamp: 1631982012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         799.178</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">-1.16923</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           947.708</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-20-22\n",
      "  done: false\n",
      "  episode_len_mean: 948.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1515151515151516\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 66\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1690964804755315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016103836434318158\n",
      "          policy_loss: -0.009063829233249028\n",
      "          total_loss: -0.023897259351280002\n",
      "          vf_explained_var: -0.597825825214386\n",
      "          vf_loss: 0.0014224914251826704\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.04615384615385\n",
      "    ram_util_percent: 56.07692307692308\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039878358461984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.28920598648332\n",
      "    mean_inference_ms: 1.3657917386446605\n",
      "    mean_raw_obs_processing_ms: 4.5455122870059315\n",
      "  time_since_restore: 808.7812831401825\n",
      "  time_this_iter_s: 9.603139400482178\n",
      "  time_total_s: 808.7812831401825\n",
      "  timers:\n",
      "    learn_throughput: 1472.561\n",
      "    learn_time_ms: 679.089\n",
      "    load_throughput: 277536.89\n",
      "    load_time_ms: 3.603\n",
      "    sample_throughput: 111.072\n",
      "    sample_time_ms: 9003.177\n",
      "    update_time_ms: 1.848\n",
      "  timestamp: 1631982022\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         808.781</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">-1.15152</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">             948.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-20-32\n",
      "  done: false\n",
      "  episode_len_mean: 949.2686567164179\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1343283582089552\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 67\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2080962631437515\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010283481526969594\n",
      "          policy_loss: 0.0011737524635261959\n",
      "          total_loss: -0.01619636867609289\n",
      "          vf_explained_var: -0.5366697311401367\n",
      "          vf_loss: 0.0012401644541468056\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.107142857142854\n",
      "    ram_util_percent: 55.96428571428571\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039870246596729886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.252299119686787\n",
      "    mean_inference_ms: 1.3653396323176372\n",
      "    mean_raw_obs_processing_ms: 4.520877904829257\n",
      "  time_since_restore: 818.4689755439758\n",
      "  time_this_iter_s: 9.687692403793335\n",
      "  time_total_s: 818.4689755439758\n",
      "  timers:\n",
      "    learn_throughput: 1473.156\n",
      "    learn_time_ms: 678.815\n",
      "    load_throughput: 277597.506\n",
      "    load_time_ms: 3.602\n",
      "    sample_throughput: 110.041\n",
      "    sample_time_ms: 9087.535\n",
      "    update_time_ms: 1.836\n",
      "  timestamp: 1631982032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         818.469</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-1.13433</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           949.269</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-20-41\n",
      "  done: false\n",
      "  episode_len_mean: 950.0147058823529\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1176470588235294\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 68\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3147110091315377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012112479123064753\n",
      "          policy_loss: -0.0928427712370952\n",
      "          total_loss: -0.10929345625142256\n",
      "          vf_explained_var: -0.2870159447193146\n",
      "          vf_loss: 0.002608460860533847\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.392857142857146\n",
      "    ram_util_percent: 55.89999999999999\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0398621659500489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.216363976818569\n",
      "    mean_inference_ms: 1.3648945449634122\n",
      "    mean_raw_obs_processing_ms: 4.496338927160325\n",
      "  time_since_restore: 827.9418482780457\n",
      "  time_this_iter_s: 9.472872734069824\n",
      "  time_total_s: 827.9418482780457\n",
      "  timers:\n",
      "    learn_throughput: 1466.811\n",
      "    learn_time_ms: 681.751\n",
      "    load_throughput: 277186.569\n",
      "    load_time_ms: 3.608\n",
      "    sample_throughput: 110.06\n",
      "    sample_time_ms: 9085.936\n",
      "    update_time_ms: 1.878\n",
      "  timestamp: 1631982041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         827.942</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">-1.11765</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           950.015</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-20-50\n",
      "  done: false\n",
      "  episode_len_mean: 950.7391304347826\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1014492753623188\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 69\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4446755382749767\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0059911760582319696\n",
      "          policy_loss: -0.0823147031168143\n",
      "          total_loss: -0.103671019938257\n",
      "          vf_explained_var: -0.6271396279335022\n",
      "          vf_loss: 0.001068414035317902\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.900000000000006\n",
      "    ram_util_percent: 55.96923076923076\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03985408261613403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.181284828489161\n",
      "    mean_inference_ms: 1.364457008808348\n",
      "    mean_raw_obs_processing_ms: 4.471911482410951\n",
      "  time_since_restore: 837.0449347496033\n",
      "  time_this_iter_s: 9.103086471557617\n",
      "  time_total_s: 837.0449347496033\n",
      "  timers:\n",
      "    learn_throughput: 1484.496\n",
      "    learn_time_ms: 673.63\n",
      "    load_throughput: 281759.762\n",
      "    load_time_ms: 3.549\n",
      "    sample_throughput: 110.046\n",
      "    sample_time_ms: 9087.102\n",
      "    update_time_ms: 1.811\n",
      "  timestamp: 1631982050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         837.045</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">-1.10145</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           950.739</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-21-00\n",
      "  done: false\n",
      "  episode_len_mean: 951.4428571428572\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.0857142857142856\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 70\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1844130555788674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014051260943567416\n",
      "          policy_loss: -0.04286992487808069\n",
      "          total_loss: -0.05269750650558207\n",
      "          vf_explained_var: 0.39199379086494446\n",
      "          vf_loss: 0.007274249558233552\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.276923076923076\n",
      "    ram_util_percent: 56.03846153846154\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03984609477681643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.14708183615733\n",
      "    mean_inference_ms: 1.3640297179124519\n",
      "    mean_raw_obs_processing_ms: 4.44760878172771\n",
      "  time_since_restore: 846.4143717288971\n",
      "  time_this_iter_s: 9.369436979293823\n",
      "  time_total_s: 846.4143717288971\n",
      "  timers:\n",
      "    learn_throughput: 1495.78\n",
      "    learn_time_ms: 668.548\n",
      "    load_throughput: 283073.767\n",
      "    load_time_ms: 3.533\n",
      "    sample_throughput: 111.015\n",
      "    sample_time_ms: 9007.765\n",
      "    update_time_ms: 1.791\n",
      "  timestamp: 1631982060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         846.414</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">-1.08571</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           951.443</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 952.1267605633802\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.0704225352112675\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 71\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.23444803820716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015374650321640064\n",
      "          policy_loss: -0.07178106498387125\n",
      "          total_loss: -0.08156095900469357\n",
      "          vf_explained_var: 0.4203774631023407\n",
      "          vf_loss: 0.007375640525586076\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.778571428571425\n",
      "    ram_util_percent: 56.10714285714287\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03983818830057882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.11374348854286\n",
      "    mean_inference_ms: 1.3636122854306771\n",
      "    mean_raw_obs_processing_ms: 4.423440976375384\n",
      "  time_since_restore: 855.8775615692139\n",
      "  time_this_iter_s: 9.463189840316772\n",
      "  time_total_s: 855.8775615692139\n",
      "  timers:\n",
      "    learn_throughput: 1521.114\n",
      "    learn_time_ms: 657.413\n",
      "    load_throughput: 290889.319\n",
      "    load_time_ms: 3.438\n",
      "    sample_throughput: 111.875\n",
      "    sample_time_ms: 8938.55\n",
      "    update_time_ms: 1.729\n",
      "  timestamp: 1631982069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         855.878</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-1.07042</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           952.127</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-21-19\n",
      "  done: false\n",
      "  episode_len_mean: 952.7916666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.125\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 72\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3034107552634344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01423460977874657\n",
      "          policy_loss: 0.019342604527870812\n",
      "          total_loss: 0.0714603620270888\n",
      "          vf_explained_var: 0.4094415009021759\n",
      "          vf_loss: 0.07034768282901496\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.800000000000004\n",
      "    ram_util_percent: 56.14285714285716\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03983025544530506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.081273746308092\n",
      "    mean_inference_ms: 1.3632046238175357\n",
      "    mean_raw_obs_processing_ms: 4.399420966999405\n",
      "  time_since_restore: 865.5496394634247\n",
      "  time_this_iter_s: 9.672077894210815\n",
      "  time_total_s: 865.5496394634247\n",
      "  timers:\n",
      "    learn_throughput: 1529.486\n",
      "    learn_time_ms: 653.814\n",
      "    load_throughput: 308993.156\n",
      "    load_time_ms: 3.236\n",
      "    sample_throughput: 113.094\n",
      "    sample_time_ms: 8842.191\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1631982079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">          865.55</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">  -1.125</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           952.792</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 953.4383561643835\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1095890410958904\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 73\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3209650225109524\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011760266609349321\n",
      "          policy_loss: -0.07847798212120931\n",
      "          total_loss: 0.011269927035189337\n",
      "          vf_explained_var: 0.029305381700396538\n",
      "          vf_loss: 0.10898846906299392\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.99285714285714\n",
      "    ram_util_percent: 56.185714285714305\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03982235564291265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.049688498573584\n",
      "    mean_inference_ms: 1.362804904069688\n",
      "    mean_raw_obs_processing_ms: 4.375556463441967\n",
      "  time_since_restore: 875.4643516540527\n",
      "  time_this_iter_s: 9.914712190628052\n",
      "  time_total_s: 875.4643516540527\n",
      "  timers:\n",
      "    learn_throughput: 1529.745\n",
      "    learn_time_ms: 653.704\n",
      "    load_throughput: 309428.55\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 112.489\n",
      "    sample_time_ms: 8889.776\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1631982089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         875.464</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">-1.10959</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           953.438</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-21-38\n",
      "  done: false\n",
      "  episode_len_mean: 954.0675675675676\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1891891891891893\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 74\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1983835948838126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015071017053820175\n",
      "          policy_loss: 0.04954090333647198\n",
      "          total_loss: 0.11397781636979845\n",
      "          vf_explained_var: 0.11887650191783905\n",
      "          vf_loss: 0.08133427773912748\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.19230769230769\n",
      "    ram_util_percent: 56.18461538461539\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03981447552151429\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.01887362474964\n",
      "    mean_inference_ms: 1.3624124129754214\n",
      "    mean_raw_obs_processing_ms: 4.351854926740148\n",
      "  time_since_restore: 884.9964256286621\n",
      "  time_this_iter_s: 9.532073974609375\n",
      "  time_total_s: 884.9964256286621\n",
      "  timers:\n",
      "    learn_throughput: 1537.65\n",
      "    learn_time_ms: 650.343\n",
      "    load_throughput: 305660.504\n",
      "    load_time_ms: 3.272\n",
      "    sample_throughput: 111.833\n",
      "    sample_time_ms: 8941.878\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1631982098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         884.996</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">-1.18919</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           954.068</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-21-48\n",
      "  done: false\n",
      "  episode_len_mean: 954.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1733333333333333\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 75\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.361377713415358\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011009007771668916\n",
      "          policy_loss: -0.0277599251932568\n",
      "          total_loss: -0.04506506994366646\n",
      "          vf_explained_var: -0.508177638053894\n",
      "          vf_loss: 0.0025930927639516693\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.48571428571429\n",
      "    ram_util_percent: 56.25000000000001\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039806643800831494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.988839285408238\n",
      "    mean_inference_ms: 1.362026325393423\n",
      "    mean_raw_obs_processing_ms: 4.3283207438248485\n",
      "  time_since_restore: 894.6787359714508\n",
      "  time_this_iter_s: 9.682310342788696\n",
      "  time_total_s: 894.6787359714508\n",
      "  timers:\n",
      "    learn_throughput: 1538.853\n",
      "    learn_time_ms: 649.835\n",
      "    load_throughput: 306057.515\n",
      "    load_time_ms: 3.267\n",
      "    sample_throughput: 112.455\n",
      "    sample_time_ms: 8892.419\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1631982108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         894.679</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-1.17333</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            954.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-21-58\n",
      "  done: false\n",
      "  episode_len_mean: 955.2763157894736\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1578947368421053\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 76\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.362012667126126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011293318398381104\n",
      "          policy_loss: -0.042689505674772796\n",
      "          total_loss: -0.058618664368987085\n",
      "          vf_explained_var: -0.5122891068458557\n",
      "          vf_loss: 0.0038794744992628693\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.871428571428574\n",
      "    ram_util_percent: 56.25714285714286\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03979890428328322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.959560184984513\n",
      "    mean_inference_ms: 1.361647476813933\n",
      "    mean_raw_obs_processing_ms: 4.304960483044169\n",
      "  time_since_restore: 904.4049541950226\n",
      "  time_this_iter_s: 9.726218223571777\n",
      "  time_total_s: 904.4049541950226\n",
      "  timers:\n",
      "    learn_throughput: 1539.176\n",
      "    learn_time_ms: 649.698\n",
      "    load_throughput: 306207.219\n",
      "    load_time_ms: 3.266\n",
      "    sample_throughput: 112.298\n",
      "    sample_time_ms: 8904.869\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1631982118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         904.405</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">-1.15789</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           955.276</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-22-07\n",
      "  done: false\n",
      "  episode_len_mean: 955.8571428571429\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1428571428571428\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 77\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3327783425649007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011384261376873579\n",
      "          policy_loss: 0.09387120430668196\n",
      "          total_loss: 0.07739107641908857\n",
      "          vf_explained_var: 0.45245233178138733\n",
      "          vf_loss: 0.0030054676301208222\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.99285714285714\n",
      "    ram_util_percent: 56.29285714285713\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039791118461892205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.930943897932165\n",
      "    mean_inference_ms: 1.3612731272084908\n",
      "    mean_raw_obs_processing_ms: 4.28177929700251\n",
      "  time_since_restore: 913.7377758026123\n",
      "  time_this_iter_s: 9.332821607589722\n",
      "  time_total_s: 913.7377758026123\n",
      "  timers:\n",
      "    learn_throughput: 1542.162\n",
      "    learn_time_ms: 648.44\n",
      "    load_throughput: 306124.528\n",
      "    load_time_ms: 3.267\n",
      "    sample_throughput: 112.732\n",
      "    sample_time_ms: 8870.633\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1631982127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         913.738</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">-1.14286</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           955.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-22-17\n",
      "  done: false\n",
      "  episode_len_mean: 956.4230769230769\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1153846153846154\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 78\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2808373477723864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0107847347101261\n",
      "          policy_loss: -0.041977416599790256\n",
      "          total_loss: 0.06924806390371588\n",
      "          vf_explained_var: 0.25379952788352966\n",
      "          vf_loss: 0.13039400776227314\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.24615384615385\n",
      "    ram_util_percent: 56.33076923076923\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03978339671910227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.902995702360537\n",
      "    mean_inference_ms: 1.360906457291755\n",
      "    mean_raw_obs_processing_ms: 4.258783331531378\n",
      "  time_since_restore: 923.2724831104279\n",
      "  time_this_iter_s: 9.534707307815552\n",
      "  time_total_s: 923.2724831104279\n",
      "  timers:\n",
      "    learn_throughput: 1547.182\n",
      "    learn_time_ms: 646.337\n",
      "    load_throughput: 303258.235\n",
      "    load_time_ms: 3.298\n",
      "    sample_throughput: 112.626\n",
      "    sample_time_ms: 8878.948\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1631982137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         923.272</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">-1.11538</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           956.423</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-22-27\n",
      "  done: false\n",
      "  episode_len_mean: 956.9746835443038\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.1012658227848102\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 79\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2726280477311875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010747958550140326\n",
      "          policy_loss: -0.0753200762387779\n",
      "          total_loss: -0.0881874591526058\n",
      "          vf_explained_var: 0.08895084261894226\n",
      "          vf_loss: 0.006231464890556203\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58571428571428\n",
      "    ram_util_percent: 56.49285714285714\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03977584624726612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.87574823079059\n",
      "    mean_inference_ms: 1.3605488491284652\n",
      "    mean_raw_obs_processing_ms: 4.235973619466197\n",
      "  time_since_restore: 933.1331653594971\n",
      "  time_this_iter_s: 9.860682249069214\n",
      "  time_total_s: 933.1331653594971\n",
      "  timers:\n",
      "    learn_throughput: 1547.06\n",
      "    learn_time_ms: 646.387\n",
      "    load_throughput: 302614.969\n",
      "    load_time_ms: 3.305\n",
      "    sample_throughput: 111.674\n",
      "    sample_time_ms: 8954.654\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1631982147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         933.133</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-1.10127</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           956.975</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 957.5125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.0875\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 80\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.333445986111959\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011109602754442996\n",
      "          policy_loss: -0.11983919375472599\n",
      "          total_loss: -0.1308373322089513\n",
      "          vf_explained_var: 0.7625449895858765\n",
      "          vf_loss: 0.00858683723749386\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.50714285714286\n",
      "    ram_util_percent: 56.47142857142857\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03976830627478105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.849115050598531\n",
      "    mean_inference_ms: 1.3601953512846687\n",
      "    mean_raw_obs_processing_ms: 4.213352832601229\n",
      "  time_since_restore: 942.6006989479065\n",
      "  time_this_iter_s: 9.467533588409424\n",
      "  time_total_s: 942.6006989479065\n",
      "  timers:\n",
      "    learn_throughput: 1546.807\n",
      "    learn_time_ms: 646.493\n",
      "    load_throughput: 302931.885\n",
      "    load_time_ms: 3.301\n",
      "    sample_throughput: 111.553\n",
      "    sample_time_ms: 8964.359\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1631982156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         942.601</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\"> -1.0875</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           957.513</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-23-02\n",
      "  done: false\n",
      "  episode_len_mean: 956.1604938271605\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.0740740740740742\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 81\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4134248733520507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012050170816677329\n",
      "          policy_loss: -0.09341504077116648\n",
      "          total_loss: -0.10884499384297265\n",
      "          vf_explained_var: 0.19497640430927277\n",
      "          vf_loss: 0.0046373643925310005\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.983783783783785\n",
      "    ram_util_percent: 56.48918918918919\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039760833191371026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.82304594411764\n",
      "    mean_inference_ms: 1.3598471669732988\n",
      "    mean_raw_obs_processing_ms: 4.1935901700566145\n",
      "  time_since_restore: 968.7457098960876\n",
      "  time_this_iter_s: 26.145010948181152\n",
      "  time_total_s: 968.7457098960876\n",
      "  timers:\n",
      "    learn_throughput: 1546.009\n",
      "    learn_time_ms: 646.827\n",
      "    load_throughput: 214108.711\n",
      "    load_time_ms: 4.671\n",
      "    sample_throughput: 94.066\n",
      "    sample_time_ms: 10630.828\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1631982182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         968.746</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">-1.07407</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            956.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-23-14\n",
      "  done: false\n",
      "  episode_len_mean: 956.6951219512196\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.0365853658536586\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 82\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.388527594672309\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00911301744513991\n",
      "          policy_loss: -0.13510787958900133\n",
      "          total_loss: -0.1367648740609487\n",
      "          vf_explained_var: 0.21167674660682678\n",
      "          vf_loss: 0.019152638847137696\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.16875\n",
      "    ram_util_percent: 56.575\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039753409057285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.797827266057364\n",
      "    mean_inference_ms: 1.359503740577205\n",
      "    mean_raw_obs_processing_ms: 4.173924260875684\n",
      "  time_since_restore: 980.0120730400085\n",
      "  time_this_iter_s: 11.266363143920898\n",
      "  time_total_s: 980.0120730400085\n",
      "  timers:\n",
      "    learn_throughput: 1547.995\n",
      "    learn_time_ms: 645.997\n",
      "    load_throughput: 214454.648\n",
      "    load_time_ms: 4.663\n",
      "    sample_throughput: 92.669\n",
      "    sample_time_ms: 10791.118\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1631982194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         980.012</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">-1.03659</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           956.695</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-23-23\n",
      "  done: false\n",
      "  episode_len_mean: 957.2168674698795\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.0240963855421688\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 83\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2257036156124537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010635602629127543\n",
      "          policy_loss: -0.13361983601417807\n",
      "          total_loss: -0.147148962587946\n",
      "          vf_explained_var: 0.3560786843299866\n",
      "          vf_loss: 0.005138394609093666\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.86428571428572\n",
      "    ram_util_percent: 56.557142857142864\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039746055970214474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.773147907808534\n",
      "    mean_inference_ms: 1.3591678073129447\n",
      "    mean_raw_obs_processing_ms: 4.1543611325846825\n",
      "  time_since_restore: 989.4907517433167\n",
      "  time_this_iter_s: 9.478678703308105\n",
      "  time_total_s: 989.4907517433167\n",
      "  timers:\n",
      "    learn_throughput: 1544.795\n",
      "    learn_time_ms: 647.335\n",
      "    load_throughput: 212844.007\n",
      "    load_time_ms: 4.698\n",
      "    sample_throughput: 93.056\n",
      "    sample_time_ms: 10746.193\n",
      "    update_time_ms: 1.668\n",
      "  timestamp: 1631982203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         989.491</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -1.0241</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           957.217</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 957.7261904761905\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -1.0119047619047619\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 84\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2286994059880576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008141746548463284\n",
      "          policy_loss: -0.014605503115389083\n",
      "          total_loss: -0.027591187175777222\n",
      "          vf_explained_var: 0.7292709350585938\n",
      "          vf_loss: 0.006553474786536147\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.56153846153846\n",
      "    ram_util_percent: 56.4153846153846\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03973873218557053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.748964222920835\n",
      "    mean_inference_ms: 1.3588356291078494\n",
      "    mean_raw_obs_processing_ms: 4.1349070184470555\n",
      "  time_since_restore: 998.785327911377\n",
      "  time_this_iter_s: 9.294576168060303\n",
      "  time_total_s: 998.785327911377\n",
      "  timers:\n",
      "    learn_throughput: 1555.341\n",
      "    learn_time_ms: 642.946\n",
      "    load_throughput: 214343.958\n",
      "    load_time_ms: 4.665\n",
      "    sample_throughput: 93.224\n",
      "    sample_time_ms: 10726.854\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1631982212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         998.785</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\"> -1.0119</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           957.726</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-23-42\n",
      "  done: false\n",
      "  episode_len_mean: 958.2235294117647\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.9882352941176471\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 85\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2796242674191793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009669047117096695\n",
      "          policy_loss: -0.026547039051850636\n",
      "          total_loss: 0.03651258908212185\n",
      "          vf_explained_var: 0.30491766333580017\n",
      "          vf_loss: 0.08259256698139426\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.91428571428571\n",
      "    ram_util_percent: 56.4357142857143\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03973161798534828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.725309194260076\n",
      "    mean_inference_ms: 1.3585126711378765\n",
      "    mean_raw_obs_processing_ms: 4.115567538495472\n",
      "  time_since_restore: 1008.4402174949646\n",
      "  time_this_iter_s: 9.654889583587646\n",
      "  time_total_s: 1008.4402174949646\n",
      "  timers:\n",
      "    learn_throughput: 1552.817\n",
      "    learn_time_ms: 643.991\n",
      "    load_throughput: 212840.767\n",
      "    load_time_ms: 4.698\n",
      "    sample_throughput: 93.257\n",
      "    sample_time_ms: 10723.007\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1631982222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1008.44</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">-0.988235</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           958.224</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-23-52\n",
      "  done: false\n",
      "  episode_len_mean: 958.7093023255813\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.9767441860465116\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 86\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2678010092841254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01660516961763996\n",
      "          policy_loss: -0.03195831129948298\n",
      "          total_loss: -0.03778276294469833\n",
      "          vf_explained_var: 0.09394180774688721\n",
      "          vf_loss: 0.011249315789124617\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.228571428571435\n",
      "    ram_util_percent: 56.47142857142857\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03972481836676991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.702194621357206\n",
      "    mean_inference_ms: 1.3582024703305935\n",
      "    mean_raw_obs_processing_ms: 4.096348070636775\n",
      "  time_since_restore: 1018.4026167392731\n",
      "  time_this_iter_s: 9.962399244308472\n",
      "  time_total_s: 1018.4026167392731\n",
      "  timers:\n",
      "    learn_throughput: 1539.515\n",
      "    learn_time_ms: 649.555\n",
      "    load_throughput: 209264.236\n",
      "    load_time_ms: 4.779\n",
      "    sample_throughput: 93.102\n",
      "    sample_time_ms: 10740.951\n",
      "    update_time_ms: 1.722\n",
      "  timestamp: 1631982232\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">          1018.4</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">-0.976744</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           958.709</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-24-41\n",
      "  done: false\n",
      "  episode_len_mean: 951.9090909090909\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.8636363636363636\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 88\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2274052672915987\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013635757058759938\n",
      "          policy_loss: -0.09400106064147419\n",
      "          total_loss: -0.016075204147232902\n",
      "          vf_explained_var: 0.6156895756721497\n",
      "          vf_loss: 0.095597842703056\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.691428571428574\n",
      "    ram_util_percent: 56.294285714285714\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03971172655319504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.65751816644949\n",
      "    mean_inference_ms: 1.3576058784642633\n",
      "    mean_raw_obs_processing_ms: 4.0692031592166105\n",
      "  time_since_restore: 1067.5353121757507\n",
      "  time_this_iter_s: 49.13269543647766\n",
      "  time_total_s: 1067.5353121757507\n",
      "  timers:\n",
      "    learn_throughput: 1527.088\n",
      "    learn_time_ms: 654.841\n",
      "    load_throughput: 159649.816\n",
      "    load_time_ms: 6.264\n",
      "    sample_throughput: 67.962\n",
      "    sample_time_ms: 14714.148\n",
      "    update_time_ms: 1.725\n",
      "  timestamp: 1631982281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1067.54</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-0.863636</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           951.909</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-24-52\n",
      "  done: false\n",
      "  episode_len_mean: 952.4494382022472\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.8314606741573034\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 89\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3523484177059597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015746227895127272\n",
      "          policy_loss: -0.016598949250247744\n",
      "          total_loss: 0.07769457673033078\n",
      "          vf_explained_var: 0.4678023159503937\n",
      "          vf_loss: 0.11250265799462796\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.14375\n",
      "    ram_util_percent: 56.3625\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03970548755706994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.636095681662402\n",
      "    mean_inference_ms: 1.3573202257536892\n",
      "    mean_raw_obs_processing_ms: 4.055722413324788\n",
      "  time_since_restore: 1078.7112920284271\n",
      "  time_this_iter_s: 11.175979852676392\n",
      "  time_total_s: 1078.7112920284271\n",
      "  timers:\n",
      "    learn_throughput: 1525.982\n",
      "    learn_time_ms: 655.316\n",
      "    load_throughput: 160399.555\n",
      "    load_time_ms: 6.234\n",
      "    sample_throughput: 67.214\n",
      "    sample_time_ms: 14877.814\n",
      "    update_time_ms: 1.739\n",
      "  timestamp: 1631982292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1078.71</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">-0.831461</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           952.449</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-25-02\n",
      "  done: false\n",
      "  episode_len_mean: 952.9777777777778\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.8222222222222222\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 90\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.348871694670783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010833983489930078\n",
      "          policy_loss: 0.13747789308221803\n",
      "          total_loss: 0.13047069729202324\n",
      "          vf_explained_var: 0.26592713594436646\n",
      "          vf_loss: 0.012825053597852173\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.314285714285724\n",
      "    ram_util_percent: 56.664285714285725\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039699409411868954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.615103941715095\n",
      "    mean_inference_ms: 1.357042580563384\n",
      "    mean_raw_obs_processing_ms: 4.04218656875182\n",
      "  time_since_restore: 1088.3066155910492\n",
      "  time_this_iter_s: 9.59532356262207\n",
      "  time_total_s: 1088.3066155910492\n",
      "  timers:\n",
      "    learn_throughput: 1522.819\n",
      "    learn_time_ms: 656.677\n",
      "    load_throughput: 160890.549\n",
      "    load_time_ms: 6.215\n",
      "    sample_throughput: 67.34\n",
      "    sample_time_ms: 14849.922\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1631982302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1088.31</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">-0.822222</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           952.978</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-25-11\n",
      "  done: false\n",
      "  episode_len_mean: 953.4945054945055\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.8131868131868132\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 91\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2103344413969253\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009769614142024154\n",
      "          policy_loss: -0.10783260828918881\n",
      "          total_loss: -0.12060633384519154\n",
      "          vf_explained_var: 0.7229940891265869\n",
      "          vf_loss: 0.006032377740161287\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.323076923076925\n",
      "    ram_util_percent: 56.76153846153845\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03969352085117043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.594493455208996\n",
      "    mean_inference_ms: 1.356772064514283\n",
      "    mean_raw_obs_processing_ms: 4.028605497288207\n",
      "  time_since_restore: 1097.6073274612427\n",
      "  time_this_iter_s: 9.300711870193481\n",
      "  time_total_s: 1097.6073274612427\n",
      "  timers:\n",
      "    learn_throughput: 1522.688\n",
      "    learn_time_ms: 656.733\n",
      "    load_throughput: 161024.586\n",
      "    load_time_ms: 6.21\n",
      "    sample_throughput: 67.417\n",
      "    sample_time_ms: 14833.126\n",
      "    update_time_ms: 1.768\n",
      "  timestamp: 1631982311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1097.61</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">-0.813187</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           953.495</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-25-21\n",
      "  done: false\n",
      "  episode_len_mean: 954.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.8043478260869565\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 92\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4004220883051555\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009986087939026254\n",
      "          policy_loss: -0.1284053170018726\n",
      "          total_loss: -0.14715883450375664\n",
      "          vf_explained_var: 0.3931565284729004\n",
      "          vf_loss: 0.00188040058581262\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.59285714285715\n",
      "    ram_util_percent: 56.907142857142844\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0396877793018365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.57424590266311\n",
      "    mean_inference_ms: 1.3565068222805703\n",
      "    mean_raw_obs_processing_ms: 4.014988460267723\n",
      "  time_since_restore: 1106.9259066581726\n",
      "  time_this_iter_s: 9.318579196929932\n",
      "  time_total_s: 1106.9259066581726\n",
      "  timers:\n",
      "    learn_throughput: 1502.646\n",
      "    learn_time_ms: 665.493\n",
      "    load_throughput: 206338.471\n",
      "    load_time_ms: 4.846\n",
      "    sample_throughput: 76.086\n",
      "    sample_time_ms: 13143.077\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1631982321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1106.93</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-0.804348</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">               954</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-25-31\n",
      "  done: false\n",
      "  episode_len_mean: 954.494623655914\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.7741935483870968\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 93\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2396639611985947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009425203336745921\n",
      "          policy_loss: -0.04106121522684892\n",
      "          total_loss: -0.037973231242762674\n",
      "          vf_explained_var: 0.7053161859512329\n",
      "          vf_loss: 0.022303617554199365\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.9\n",
      "    ram_util_percent: 57.31428571428571\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039682334251256945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.5544529770655\n",
      "    mean_inference_ms: 1.3562513050507121\n",
      "    mean_raw_obs_processing_ms: 4.001344512643538\n",
      "  time_since_restore: 1117.1113889217377\n",
      "  time_this_iter_s: 10.185482263565063\n",
      "  time_total_s: 1117.1113889217377\n",
      "  timers:\n",
      "    learn_throughput: 1484.5\n",
      "    learn_time_ms: 673.628\n",
      "    load_throughput: 204090.467\n",
      "    load_time_ms: 4.9\n",
      "    sample_throughput: 76.765\n",
      "    sample_time_ms: 13026.729\n",
      "    update_time_ms: 1.815\n",
      "  timestamp: 1631982331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1117.11</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">-0.774194</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           954.495</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 954.9787234042553\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.776595744680851\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 94\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7939368844032288\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010585254786628164\n",
      "          policy_loss: 0.022923836650119888\n",
      "          total_loss: 0.02898104041814804\n",
      "          vf_explained_var: -0.1924740970134735\n",
      "          vf_loss: 0.020424042103695684\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.78666666666666\n",
      "    ram_util_percent: 57.22000000000002\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039677185565264665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.535134630874614\n",
      "    mean_inference_ms: 1.356004446317432\n",
      "    mean_raw_obs_processing_ms: 3.9876808652744886\n",
      "  time_since_restore: 1127.5557391643524\n",
      "  time_this_iter_s: 10.444350242614746\n",
      "  time_total_s: 1127.5557391643524\n",
      "  timers:\n",
      "    learn_throughput: 1476.553\n",
      "    learn_time_ms: 677.253\n",
      "    load_throughput: 200043.115\n",
      "    load_time_ms: 4.999\n",
      "    sample_throughput: 76.222\n",
      "    sample_time_ms: 13119.51\n",
      "    update_time_ms: 1.852\n",
      "  timestamp: 1631982341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1127.56</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">-0.776596</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           954.979</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-26-20\n",
      "  done: false\n",
      "  episode_len_mean: 951.7894736842105\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.7263157894736842\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 95\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6599336134062872\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0093572844748536\n",
      "          policy_loss: -0.08784801031773289\n",
      "          total_loss: 0.0333473046310246\n",
      "          vf_explained_var: 0.5752996206283569\n",
      "          vf_loss: 0.13463656764943152\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.09642857142857\n",
      "    ram_util_percent: 56.721428571428575\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03967212715832237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.516233959364973\n",
      "    mean_inference_ms: 1.3557637396357787\n",
      "    mean_raw_obs_processing_ms: 3.977367274920742\n",
      "  time_since_restore: 1166.653052330017\n",
      "  time_this_iter_s: 39.09731316566467\n",
      "  time_total_s: 1166.653052330017\n",
      "  timers:\n",
      "    learn_throughput: 1475.15\n",
      "    learn_time_ms: 677.897\n",
      "    load_throughput: 158044.222\n",
      "    load_time_ms: 6.327\n",
      "    sample_throughput: 62.12\n",
      "    sample_time_ms: 16097.832\n",
      "    update_time_ms: 1.821\n",
      "  timestamp: 1631982380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1166.65</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">-0.726316</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           951.789</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-27-00\n",
      "  done: false\n",
      "  episode_len_mean: 946.7216494845361\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.6288659793814433\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 97\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8749012072881064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008865397957402897\n",
      "          policy_loss: 0.09542159338792165\n",
      "          total_loss: 0.19765625629160138\n",
      "          vf_explained_var: 0.46233507990837097\n",
      "          vf_loss: 0.1179916069118513\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.37017543859649\n",
      "    ram_util_percent: 57.01754385964912\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03966216915693464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.479559122517683\n",
      "    mean_inference_ms: 1.3552929346961267\n",
      "    mean_raw_obs_processing_ms: 3.963495286385644\n",
      "  time_since_restore: 1206.543066740036\n",
      "  time_this_iter_s: 39.89001441001892\n",
      "  time_total_s: 1206.543066740036\n",
      "  timers:\n",
      "    learn_throughput: 1478.55\n",
      "    learn_time_ms: 676.338\n",
      "    load_throughput: 128212.853\n",
      "    load_time_ms: 7.8\n",
      "    sample_throughput: 52.297\n",
      "    sample_time_ms: 19121.461\n",
      "    update_time_ms: 1.799\n",
      "  timestamp: 1631982420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1206.54</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-0.628866</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           946.722</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-27-12\n",
      "  done: false\n",
      "  episode_len_mean: 947.265306122449\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.7551020408163265\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 98\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0256695482465954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03274304560879995\n",
      "          policy_loss: 0.008881798221005334\n",
      "          total_loss: 0.4718125851617919\n",
      "          vf_explained_var: 0.23494894802570343\n",
      "          vf_loss: 0.47213670218156445\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.3625\n",
      "    ram_util_percent: 56.98125\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039657321744810056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.461935304092448\n",
      "    mean_inference_ms: 1.3550645484718062\n",
      "    mean_raw_obs_processing_ms: 3.956422541702702\n",
      "  time_since_restore: 1217.8590550422668\n",
      "  time_this_iter_s: 11.315988302230835\n",
      "  time_total_s: 1217.8590550422668\n",
      "  timers:\n",
      "    learn_throughput: 1490.18\n",
      "    learn_time_ms: 671.06\n",
      "    load_throughput: 129524.586\n",
      "    load_time_ms: 7.721\n",
      "    sample_throughput: 51.915\n",
      "    sample_time_ms: 19262.183\n",
      "    update_time_ms: 1.79\n",
      "  timestamp: 1631982432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1217.86</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">-0.755102</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           947.265</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-27-21\n",
      "  done: false\n",
      "  episode_len_mean: 947.7979797979798\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.7676767676767676\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 99\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2649994532267255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010650995746810644\n",
      "          policy_loss: -0.03370949650804202\n",
      "          total_loss: 0.17711093458864424\n",
      "          vf_explained_var: 0.3298164904117584\n",
      "          vf_loss: 0.2280783564162751\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.121428571428574\n",
      "    ram_util_percent: 57.00714285714285\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039652498893192716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.444622535047202\n",
      "    mean_inference_ms: 1.3548397518825017\n",
      "    mean_raw_obs_processing_ms: 3.9491551999603836\n",
      "  time_since_restore: 1227.359882593155\n",
      "  time_this_iter_s: 9.500827550888062\n",
      "  time_total_s: 1227.359882593155\n",
      "  timers:\n",
      "    learn_throughput: 1500.674\n",
      "    learn_time_ms: 666.367\n",
      "    load_throughput: 160539.533\n",
      "    load_time_ms: 6.229\n",
      "    sample_throughput: 65.337\n",
      "    sample_time_ms: 15305.208\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1631982441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1227.36</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">-0.767677</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           947.798</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-27-31\n",
      "  done: false\n",
      "  episode_len_mean: 948.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.76\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 100\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.618031911055247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006809661680779034\n",
      "          policy_loss: 0.1356133793377214\n",
      "          total_loss: 0.13817643394900692\n",
      "          vf_explained_var: 0.6510093808174133\n",
      "          vf_loss: 0.01529598235178532\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.26153846153846\n",
      "    ram_util_percent: 56.899999999999984\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03964773316645256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.427612681194631\n",
      "    mean_inference_ms: 1.3546175374467015\n",
      "    mean_raw_obs_processing_ms: 3.941704281292489\n",
      "  time_since_restore: 1236.8422005176544\n",
      "  time_this_iter_s: 9.482317924499512\n",
      "  time_total_s: 1236.8422005176544\n",
      "  timers:\n",
      "    learn_throughput: 1501.014\n",
      "    learn_time_ms: 666.216\n",
      "    load_throughput: 160369.504\n",
      "    load_time_ms: 6.236\n",
      "    sample_throughput: 66.068\n",
      "    sample_time_ms: 15135.994\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1631982451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1236.84</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">   -0.76</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            948.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-27-40\n",
      "  done: false\n",
      "  episode_len_mean: 948.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.64\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 101\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.854869700802697\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005535760776732799\n",
      "          policy_loss: -0.11080258120265273\n",
      "          total_loss: -0.12464827253586716\n",
      "          vf_explained_var: 0.1853426843881607\n",
      "          vf_loss: 0.0019005242521719387\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.58571428571429\n",
      "    ram_util_percent: 56.48571428571429\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03961449044759228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.051234256269284\n",
      "    mean_inference_ms: 1.352835401679944\n",
      "    mean_raw_obs_processing_ms: 3.9714210565234147\n",
      "  time_since_restore: 1246.2952196598053\n",
      "  time_this_iter_s: 9.453019142150879\n",
      "  time_total_s: 1246.2952196598053\n",
      "  timers:\n",
      "    learn_throughput: 1500.791\n",
      "    learn_time_ms: 666.315\n",
      "    load_throughput: 159700.878\n",
      "    load_time_ms: 6.262\n",
      "    sample_throughput: 66.13\n",
      "    sample_time_ms: 15121.643\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1631982460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">          1246.3</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">   -0.64</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            948.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-27-50\n",
      "  done: false\n",
      "  episode_len_mean: 948.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.55\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 102\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6262897080845302\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007376009181398994\n",
      "          policy_loss: -0.08015532460477617\n",
      "          total_loss: -0.09139372689856423\n",
      "          vf_explained_var: -0.09918142855167389\n",
      "          vf_loss: 0.001290392722391213\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.72307692307693\n",
      "    ram_util_percent: 55.976923076923065\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0395797544435671\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.861743670723998\n",
      "    mean_inference_ms: 1.35126246250508\n",
      "    mean_raw_obs_processing_ms: 4.0010163557922285\n",
      "  time_since_restore: 1255.8255496025085\n",
      "  time_this_iter_s: 9.530329942703247\n",
      "  time_total_s: 1255.8255496025085\n",
      "  timers:\n",
      "    learn_throughput: 1499.126\n",
      "    learn_time_ms: 667.055\n",
      "    load_throughput: 159340.498\n",
      "    load_time_ms: 6.276\n",
      "    sample_throughput: 66.033\n",
      "    sample_time_ms: 15143.86\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1631982470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1255.83</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">   -0.55</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            948.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-27-59\n",
      "  done: false\n",
      "  episode_len_mean: 948.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.47\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 103\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6481426530414158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005920376141791106\n",
      "          policy_loss: -0.17538051820463604\n",
      "          total_loss: -0.18753289646572538\n",
      "          vf_explained_var: 0.3180719017982483\n",
      "          vf_loss: 0.0013318552939583445\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.307142857142864\n",
      "    ram_util_percent: 55.65\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039547880648149264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.735810431330338\n",
      "    mean_inference_ms: 1.3497842187542204\n",
      "    mean_raw_obs_processing_ms: 4.0304712618020515\n",
      "  time_since_restore: 1265.261680841446\n",
      "  time_this_iter_s: 9.436131238937378\n",
      "  time_total_s: 1265.261680841446\n",
      "  timers:\n",
      "    learn_throughput: 1517.097\n",
      "    learn_time_ms: 659.153\n",
      "    load_throughput: 158911.268\n",
      "    load_time_ms: 6.293\n",
      "    sample_throughput: 65.948\n",
      "    sample_time_ms: 15163.52\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1631982479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1265.26</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">   -0.47</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            948.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 944.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.3\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 105\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1018926355573866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012215229894619463\n",
      "          policy_loss: -0.07517559429009756\n",
      "          total_loss: 0.1618111740383837\n",
      "          vf_explained_var: 0.42503446340560913\n",
      "          vf_loss: 0.2518217334316836\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.77333333333333\n",
      "    ram_util_percent: 55.55619047619047\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039502412186424675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.56753992899932\n",
      "    mean_inference_ms: 1.3475461138430946\n",
      "    mean_raw_obs_processing_ms: 4.035640110098476\n",
      "  time_since_restore: 1338.8418803215027\n",
      "  time_this_iter_s: 73.58019948005676\n",
      "  time_total_s: 1338.8418803215027\n",
      "  timers:\n",
      "    learn_throughput: 1534.022\n",
      "    learn_time_ms: 651.881\n",
      "    load_throughput: 132508.893\n",
      "    load_time_ms: 7.547\n",
      "    sample_throughput: 46.492\n",
      "    sample_time_ms: 21509.044\n",
      "    update_time_ms: 1.71\n",
      "  timestamp: 1631982553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1338.84</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">    -0.3</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            944.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 944.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.21\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 106\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.181885247760349\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011698166113473105\n",
      "          policy_loss: 0.026909055622915425\n",
      "          total_loss: 0.2645635020194782\n",
      "          vf_explained_var: 0.4689904451370239\n",
      "          vf_loss: 0.2535511041680972\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.38823529411765\n",
      "    ram_util_percent: 56.3235294117647\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03948495439591519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.505564061926043\n",
      "    mean_inference_ms: 1.3466335213187204\n",
      "    mean_raw_obs_processing_ms: 4.015841148736794\n",
      "  time_since_restore: 1350.7481398582458\n",
      "  time_this_iter_s: 11.906259536743164\n",
      "  time_total_s: 1350.7481398582458\n",
      "  timers:\n",
      "    learn_throughput: 1547.905\n",
      "    learn_time_ms: 646.034\n",
      "    load_throughput: 134942.748\n",
      "    load_time_ms: 7.411\n",
      "    sample_throughput: 46.165\n",
      "    sample_time_ms: 21661.258\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1631982565\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1350.75</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   -0.21</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            944.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-29-35\n",
      "  done: false\n",
      "  episode_len_mean: 944.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.17\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 107\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9883912391132779\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007825574744892963\n",
      "          policy_loss: -0.017567840963602067\n",
      "          total_loss: 0.10974200338953072\n",
      "          vf_explained_var: 0.0886550098657608\n",
      "          vf_loss: 0.14323206121722856\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.550000000000004\n",
      "    ram_util_percent: 57.085714285714296\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039468492184869794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.452972334015117\n",
      "    mean_inference_ms: 1.3458071993926322\n",
      "    mean_raw_obs_processing_ms: 4.003595748636052\n",
      "  time_since_restore: 1360.7386028766632\n",
      "  time_this_iter_s: 9.990463018417358\n",
      "  time_total_s: 1360.7386028766632\n",
      "  timers:\n",
      "    learn_throughput: 1549.482\n",
      "    learn_time_ms: 645.377\n",
      "    load_throughput: 164608.387\n",
      "    load_time_ms: 6.075\n",
      "    sample_throughput: 53.326\n",
      "    sample_time_ms: 18752.567\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1631982575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         1360.74</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">   -0.17</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">            944.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-30-44\n",
      "  done: false\n",
      "  episode_len_mean: 945.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.04\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 109\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0217105322413973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009397688952599025\n",
      "          policy_loss: 0.0017522010538313123\n",
      "          total_loss: 0.0686892402668794\n",
      "          vf_explained_var: 0.03761327639222145\n",
      "          vf_loss: 0.08239656447743376\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.265656565656567\n",
      "    ram_util_percent: 57.74242424242424\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039435990873549535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.361062682202038\n",
      "    mean_inference_ms: 1.3442194590169985\n",
      "    mean_raw_obs_processing_ms: 3.9005839395580857\n",
      "  time_since_restore: 1430.0652344226837\n",
      "  time_this_iter_s: 69.32663154602051\n",
      "  time_total_s: 1430.0652344226837\n",
      "  timers:\n",
      "    learn_throughput: 1543.487\n",
      "    learn_time_ms: 647.884\n",
      "    load_throughput: 167331.075\n",
      "    load_time_ms: 5.976\n",
      "    sample_throughput: 46.096\n",
      "    sample_time_ms: 21693.797\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1631982644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         1430.07</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">    0.04</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            945.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-31-17\n",
      "  done: false\n",
      "  episode_len_mean: 941.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.16\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 110\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9323501414722866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00710800979137842\n",
      "          policy_loss: -0.16508741438802746\n",
      "          total_loss: -0.0802989593707025\n",
      "          vf_explained_var: 0.09184782952070236\n",
      "          vf_loss: 0.10051352497086757\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.693617021276598\n",
      "    ram_util_percent: 57.855319148936175\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03941914553017776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.318579531510867\n",
      "    mean_inference_ms: 1.3434301867873935\n",
      "    mean_raw_obs_processing_ms: 3.861095548500081\n",
      "  time_since_restore: 1463.0220234394073\n",
      "  time_this_iter_s: 32.95678901672363\n",
      "  time_total_s: 1463.0220234394073\n",
      "  timers:\n",
      "    learn_throughput: 1532.552\n",
      "    learn_time_ms: 652.506\n",
      "    load_throughput: 130891.207\n",
      "    load_time_ms: 7.64\n",
      "    sample_throughput: 41.926\n",
      "    sample_time_ms: 23851.585\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1631982677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1463.02</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">    0.16</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            941.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-31-29\n",
      "  done: false\n",
      "  episode_len_mean: 941.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.17\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 111\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.509049317571852\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019906055578333356\n",
      "          policy_loss: 0.04922690904802746\n",
      "          total_loss: 0.10282159397999445\n",
      "          vf_explained_var: 0.10371164977550507\n",
      "          vf_loss: 0.058607738483179773\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.34705882352941\n",
      "    ram_util_percent: 57.91764705882353\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0394030979316088\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.280843343005799\n",
      "    mean_inference_ms: 1.3426924954950663\n",
      "    mean_raw_obs_processing_ms: 3.829398686767931\n",
      "  time_since_restore: 1474.8943350315094\n",
      "  time_this_iter_s: 11.87231159210205\n",
      "  time_total_s: 1474.8943350315094\n",
      "  timers:\n",
      "    learn_throughput: 1529.359\n",
      "    learn_time_ms: 653.869\n",
      "    load_throughput: 130843.841\n",
      "    load_time_ms: 7.643\n",
      "    sample_throughput: 41.516\n",
      "    sample_time_ms: 24087.352\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1631982689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1474.89</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">    0.17</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            941.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 940.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.26\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 112\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.005447930759854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007886442824742402\n",
      "          policy_loss: -0.028747622999880047\n",
      "          total_loss: 0.08693937808275223\n",
      "          vf_explained_var: 0.18945012986660004\n",
      "          vf_loss: 0.13174896927602175\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.505882352941175\n",
      "    ram_util_percent: 57.68823529411765\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039388935438625185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.247216131004318\n",
      "    mean_inference_ms: 1.3420440066530546\n",
      "    mean_raw_obs_processing_ms: 3.770752765066797\n",
      "  time_since_restore: 1522.7559819221497\n",
      "  time_this_iter_s: 47.86164689064026\n",
      "  time_total_s: 1522.7559819221497\n",
      "  timers:\n",
      "    learn_throughput: 1527.441\n",
      "    learn_time_ms: 654.69\n",
      "    load_throughput: 111062.671\n",
      "    load_time_ms: 9.004\n",
      "    sample_throughput: 35.813\n",
      "    sample_time_ms: 27923.097\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1631982737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         1522.76</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">    0.26</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            940.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-33-03\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.3\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 114\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9812973923153347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009240582904874723\n",
      "          policy_loss: 0.027288515037960478\n",
      "          total_loss: 0.0769810708032714\n",
      "          vf_explained_var: -0.06651411205530167\n",
      "          vf_loss: 0.06482748487550351\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.06567164179104\n",
      "    ram_util_percent: 57.44925373134329\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039361574004305934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.181821441186973\n",
      "    mean_inference_ms: 1.3407902442801605\n",
      "    mean_raw_obs_processing_ms: 3.6678726842743083\n",
      "  time_since_restore: 1569.259598493576\n",
      "  time_this_iter_s: 46.50361657142639\n",
      "  time_total_s: 1569.259598493576\n",
      "  timers:\n",
      "    learn_throughput: 1524.042\n",
      "    learn_time_ms: 656.15\n",
      "    load_throughput: 96751.293\n",
      "    load_time_ms: 10.336\n",
      "    sample_throughput: 31.62\n",
      "    sample_time_ms: 31625.343\n",
      "    update_time_ms: 1.721\n",
      "  timestamp: 1631982783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1569.26</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">     0.3</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-33-15\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.28\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 115\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8289337635040284\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010282987925444149\n",
      "          policy_loss: -0.0066660661664274005\n",
      "          total_loss: -0.01651849862602022\n",
      "          vf_explained_var: -0.5083476901054382\n",
      "          vf_loss: 0.00323114072283109\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.518750000000004\n",
      "    ram_util_percent: 57.8125\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03934965127915809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.15322990437712\n",
      "    mean_inference_ms: 1.34023836511062\n",
      "    mean_raw_obs_processing_ms: 3.62764120848232\n",
      "  time_since_restore: 1580.8036999702454\n",
      "  time_this_iter_s: 11.544101476669312\n",
      "  time_total_s: 1580.8036999702454\n",
      "  timers:\n",
      "    learn_throughput: 1526.625\n",
      "    learn_time_ms: 655.04\n",
      "    load_throughput: 96583.308\n",
      "    load_time_ms: 10.354\n",
      "    sample_throughput: 31.419\n",
      "    sample_time_ms: 31827.857\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1631982795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">          1580.8</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">    0.28</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-33-25\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.35\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 116\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.997600213686625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005063767123713071\n",
      "          policy_loss: -0.18088943080769646\n",
      "          total_loss: -0.1530528362426493\n",
      "          vf_explained_var: 0.038420699536800385\n",
      "          vf_loss: 0.04524906406230811\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.74666666666667\n",
      "    ram_util_percent: 57.87333333333333\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03933891295665179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.12735307248096\n",
      "    mean_inference_ms: 1.3397354371437877\n",
      "    mean_raw_obs_processing_ms: 3.5932491608250614\n",
      "  time_since_restore: 1591.195007801056\n",
      "  time_this_iter_s: 10.391307830810547\n",
      "  time_total_s: 1591.195007801056\n",
      "  timers:\n",
      "    learn_throughput: 1528.034\n",
      "    learn_time_ms: 654.436\n",
      "    load_throughput: 96864.354\n",
      "    load_time_ms: 10.324\n",
      "    sample_throughput: 31.324\n",
      "    sample_time_ms: 31923.992\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1631982805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">          1591.2</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">    0.35</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-33-36\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.35\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 117\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.106818121009403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018194247254637894\n",
      "          policy_loss: 0.019617316292391884\n",
      "          total_loss: 0.01360184070136812\n",
      "          vf_explained_var: 0.717519998550415\n",
      "          vf_loss: 0.005841862958105695\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.94666666666667\n",
      "    ram_util_percent: 58.11333333333334\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0393291186808375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.103716291993122\n",
      "    mean_inference_ms: 1.3392701986623659\n",
      "    mean_raw_obs_processing_ms: 3.563873994384192\n",
      "  time_since_restore: 1601.7670276165009\n",
      "  time_this_iter_s: 10.572019815444946\n",
      "  time_total_s: 1601.7670276165009\n",
      "  timers:\n",
      "    learn_throughput: 1529.264\n",
      "    learn_time_ms: 653.909\n",
      "    load_throughput: 110908.49\n",
      "    load_time_ms: 9.016\n",
      "    sample_throughput: 39.024\n",
      "    sample_time_ms: 25625.038\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1631982816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         1601.77</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">    0.35</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-33-46\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.34\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 118\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0948668241500856\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01350827739090449\n",
      "          policy_loss: -0.15086931867731943\n",
      "          total_loss: -0.14410900043116676\n",
      "          vf_explained_var: 0.5744604468345642\n",
      "          vf_loss: 0.02087041762812684\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.271428571428565\n",
      "    ram_util_percent: 57.65714285714286\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03931999585526779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.0818464667326\n",
      "    mean_inference_ms: 1.3388383439846923\n",
      "    mean_raw_obs_processing_ms: 3.53884439705262\n",
      "  time_since_restore: 1611.41091299057\n",
      "  time_this_iter_s: 9.643885374069214\n",
      "  time_total_s: 1611.41091299057\n",
      "  timers:\n",
      "    learn_throughput: 1511.811\n",
      "    learn_time_ms: 661.458\n",
      "    load_throughput: 109934.945\n",
      "    load_time_ms: 9.096\n",
      "    sample_throughput: 39.384\n",
      "    sample_time_ms: 25391.113\n",
      "    update_time_ms: 1.721\n",
      "  timestamp: 1631982826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         1611.41</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">    0.34</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-33-56\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.3\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 119\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9383375936084324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012653227449403762\n",
      "          policy_loss: -0.1262852641960813\n",
      "          total_loss: -0.04016966164732973\n",
      "          vf_explained_var: 0.6163173317909241\n",
      "          vf_loss: 0.09909328073263168\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.792857142857144\n",
      "    ram_util_percent: 57.33571428571429\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03931108411407122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.061457380617549\n",
      "    mean_inference_ms: 1.338422511869237\n",
      "    mean_raw_obs_processing_ms: 3.517602783505472\n",
      "  time_since_restore: 1621.4718737602234\n",
      "  time_this_iter_s: 10.06096076965332\n",
      "  time_total_s: 1621.4718737602234\n",
      "  timers:\n",
      "    learn_throughput: 1504.652\n",
      "    learn_time_ms: 664.606\n",
      "    load_throughput: 109947.625\n",
      "    load_time_ms: 9.095\n",
      "    sample_throughput: 39.378\n",
      "    sample_time_ms: 25395.004\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1631982836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         1621.47</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">     0.3</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-34-06\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.32\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 120\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9026051839192708\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011899059517918826\n",
      "          policy_loss: -0.11951694471968545\n",
      "          total_loss: -0.003020386066701677\n",
      "          vf_explained_var: 0.41021475195884705\n",
      "          vf_loss: 0.1294987139887073\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.660000000000004\n",
      "    ram_util_percent: 57.24000000000001\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03930150820271263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04214923176369\n",
      "    mean_inference_ms: 1.3380056921419126\n",
      "    mean_raw_obs_processing_ms: 3.4996868125430036\n",
      "  time_since_restore: 1631.7867324352264\n",
      "  time_this_iter_s: 10.314858675003052\n",
      "  time_total_s: 1631.7867324352264\n",
      "  timers:\n",
      "    learn_throughput: 1498.212\n",
      "    learn_time_ms: 667.462\n",
      "    load_throughput: 130098.699\n",
      "    load_time_ms: 7.686\n",
      "    sample_throughput: 51.302\n",
      "    sample_time_ms: 19492.395\n",
      "    update_time_ms: 1.739\n",
      "  timestamp: 1631982846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         1631.79</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">    0.32</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-34-16\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.32\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 121\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0072214669651456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00852238077684054\n",
      "          policy_loss: -0.1716554654141267\n",
      "          total_loss: -0.1734151591029432\n",
      "          vf_explained_var: 0.43708187341690063\n",
      "          vf_loss: 0.013998066362303992\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.478571428571435\n",
      "    ram_util_percent: 57.09285714285715\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03929151489219933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.023876216180176\n",
      "    mean_inference_ms: 1.3375842783233631\n",
      "    mean_raw_obs_processing_ms: 3.484710805519332\n",
      "  time_since_restore: 1641.6339130401611\n",
      "  time_this_iter_s: 9.847180604934692\n",
      "  time_total_s: 1641.6339130401611\n",
      "  timers:\n",
      "    learn_throughput: 1504.993\n",
      "    learn_time_ms: 664.455\n",
      "    load_throughput: 166104.471\n",
      "    load_time_ms: 6.02\n",
      "    sample_throughput: 58.187\n",
      "    sample_time_ms: 17186.093\n",
      "    update_time_ms: 1.733\n",
      "  timestamp: 1631982856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         1641.63</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">    0.32</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-34-27\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.32\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 122\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8978182077407837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013585599397847462\n",
      "          policy_loss: 0.006227764238913854\n",
      "          total_loss: 0.005272349135743247\n",
      "          vf_explained_var: 0.6134947538375854\n",
      "          vf_loss: 0.011145058248399033\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.943749999999994\n",
      "    ram_util_percent: 57.1375\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03928254549309109\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.006887216313874\n",
      "    mean_inference_ms: 1.3372013923829829\n",
      "    mean_raw_obs_processing_ms: 3.472351284586709\n",
      "  time_since_restore: 1652.3617715835571\n",
      "  time_this_iter_s: 10.727858543395996\n",
      "  time_total_s: 1652.3617715835571\n",
      "  timers:\n",
      "    learn_throughput: 1498.281\n",
      "    learn_time_ms: 667.432\n",
      "    load_throughput: 164474.125\n",
      "    load_time_ms: 6.08\n",
      "    sample_throughput: 58.587\n",
      "    sample_time_ms: 17068.588\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1631982867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         1652.36</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">    0.32</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-34-37\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.31\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 123\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9228509426116944\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010655998274529389\n",
      "          policy_loss: -0.044775815597838824\n",
      "          total_loss: 0.04259123139911228\n",
      "          vf_explained_var: 0.6160501837730408\n",
      "          vf_loss: 0.1012009564269748\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.15714285714285\n",
      "    ram_util_percent: 57.2\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0392746759816153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.990833961997189\n",
      "    mean_inference_ms: 1.3368521529116904\n",
      "    mean_raw_obs_processing_ms: 3.4623285973973195\n",
      "  time_since_restore: 1662.6805167198181\n",
      "  time_this_iter_s: 10.318745136260986\n",
      "  time_total_s: 1662.6805167198181\n",
      "  timers:\n",
      "    learn_throughput: 1499.907\n",
      "    learn_time_ms: 666.708\n",
      "    load_throughput: 211110.641\n",
      "    load_time_ms: 4.737\n",
      "    sample_throughput: 75.096\n",
      "    sample_time_ms: 13316.371\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1631982877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         1662.68</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">    0.31</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-34-47\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.31\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 124\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0224955042203265\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008992939272482422\n",
      "          policy_loss: 0.01354437122742335\n",
      "          total_loss: 0.023661233815881942\n",
      "          vf_explained_var: 0.5407137274742126\n",
      "          vf_loss: 0.025789138472949464\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.02857142857142\n",
      "    ram_util_percent: 57.200000000000024\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03926762840024607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.975724808236575\n",
      "    mean_inference_ms: 1.3365350899258492\n",
      "    mean_raw_obs_processing_ms: 3.4544002341134483\n",
      "  time_since_restore: 1672.5126864910126\n",
      "  time_this_iter_s: 9.832169771194458\n",
      "  time_total_s: 1672.5126864910126\n",
      "  timers:\n",
      "    learn_throughput: 1501.086\n",
      "    learn_time_ms: 666.184\n",
      "    load_throughput: 291982.819\n",
      "    load_time_ms: 3.425\n",
      "    sample_throughput: 103.616\n",
      "    sample_time_ms: 9651.048\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1631982887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1672.51</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">    0.31</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-34-57\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.31\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 125\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8773762543996175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008621158410224724\n",
      "          policy_loss: -0.13999290896786584\n",
      "          total_loss: -0.12004967398113675\n",
      "          vf_explained_var: -0.0018536223797127604\n",
      "          vf_loss: 0.034352538113792734\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.02666666666666\n",
      "    ram_util_percent: 57.20000000000002\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03926156477153847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.961468348676079\n",
      "    mean_inference_ms: 1.3362553449366983\n",
      "    mean_raw_obs_processing_ms: 3.448358519598383\n",
      "  time_since_restore: 1682.3204367160797\n",
      "  time_this_iter_s: 9.807750225067139\n",
      "  time_total_s: 1682.3204367160797\n",
      "  timers:\n",
      "    learn_throughput: 1499.752\n",
      "    learn_time_ms: 666.777\n",
      "    load_throughput: 285398.638\n",
      "    load_time_ms: 3.504\n",
      "    sample_throughput: 105.522\n",
      "    sample_time_ms: 9476.719\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1631982897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         1682.32</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">    0.31</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.31\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 126\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7209895451863606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009004613437113207\n",
      "          policy_loss: -0.04432893577549193\n",
      "          total_loss: -0.029004624237616856\n",
      "          vf_explained_var: 0.4794319272041321\n",
      "          vf_loss: 0.027975616702396008\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.26923076923077\n",
      "    ram_util_percent: 57.14615384615385\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03925605625973406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.947851415590926\n",
      "    mean_inference_ms: 1.3359918980190688\n",
      "    mean_raw_obs_processing_ms: 3.444019495447227\n",
      "  time_since_restore: 1691.6963136196136\n",
      "  time_this_iter_s: 9.375876903533936\n",
      "  time_total_s: 1691.6963136196136\n",
      "  timers:\n",
      "    learn_throughput: 1499.009\n",
      "    learn_time_ms: 667.107\n",
      "    load_throughput: 285394.754\n",
      "    load_time_ms: 3.504\n",
      "    sample_throughput: 106.668\n",
      "    sample_time_ms: 9374.84\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1631982906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">          1691.7</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">    0.31</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-35-16\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.31\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 127\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8992585327890183\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007096368785606641\n",
      "          policy_loss: -0.14509714874956342\n",
      "          total_loss: -0.11478069006568856\n",
      "          vf_explained_var: 0.3857593834400177\n",
      "          vf_loss: 0.04571651048026979\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.90714285714285\n",
      "    ram_util_percent: 56.8642857142857\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03925096067795808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.934848344437615\n",
      "    mean_inference_ms: 1.3357465973917235\n",
      "    mean_raw_obs_processing_ms: 3.441223610794878\n",
      "  time_since_restore: 1701.221105337143\n",
      "  time_this_iter_s: 9.524791717529297\n",
      "  time_total_s: 1701.221105337143\n",
      "  timers:\n",
      "    learn_throughput: 1491.078\n",
      "    learn_time_ms: 670.656\n",
      "    load_throughput: 284919.774\n",
      "    load_time_ms: 3.51\n",
      "    sample_throughput: 107.916\n",
      "    sample_time_ms: 9266.494\n",
      "    update_time_ms: 1.774\n",
      "  timestamp: 1631982916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1701.22</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">    0.31</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-35-25\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.32\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 128\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1763391243086923\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010275055309578403\n",
      "          policy_loss: -0.06383742325835758\n",
      "          total_loss: -0.06162283271551132\n",
      "          vf_explained_var: 0.6102808713912964\n",
      "          vf_loss: 0.018776239992843734\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.52307692307693\n",
      "    ram_util_percent: 56.93076923076923\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03924588741870798\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.922395496006732\n",
      "    mean_inference_ms: 1.335506053523978\n",
      "    mean_raw_obs_processing_ms: 3.4398275447038866\n",
      "  time_since_restore: 1710.5488135814667\n",
      "  time_this_iter_s: 9.32770824432373\n",
      "  time_total_s: 1710.5488135814667\n",
      "  timers:\n",
      "    learn_throughput: 1505.477\n",
      "    learn_time_ms: 664.241\n",
      "    load_throughput: 292216.757\n",
      "    load_time_ms: 3.422\n",
      "    sample_throughput: 108.208\n",
      "    sample_time_ms: 9241.429\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1631982925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         1710.55</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">    0.32</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-35-34\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.33\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 129\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9388151208559672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01329713528469858\n",
      "          policy_loss: -0.05536364706026183\n",
      "          total_loss: -0.015633273869752884\n",
      "          vf_explained_var: 0.6063057780265808\n",
      "          vf_loss: 0.052386847042685585\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.83846153846153\n",
      "    ram_util_percent: 57.053846153846166\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03924145448741875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.910567814336307\n",
      "    mean_inference_ms: 1.335282564766365\n",
      "    mean_raw_obs_processing_ms: 3.439709093476742\n",
      "  time_since_restore: 1719.9341924190521\n",
      "  time_this_iter_s: 9.38537883758545\n",
      "  time_total_s: 1719.9341924190521\n",
      "  timers:\n",
      "    learn_throughput: 1509.465\n",
      "    learn_time_ms: 662.487\n",
      "    load_throughput: 290653.472\n",
      "    load_time_ms: 3.441\n",
      "    sample_throughput: 108.984\n",
      "    sample_time_ms: 9175.646\n",
      "    update_time_ms: 1.738\n",
      "  timestamp: 1631982934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         1719.93</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">    0.33</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-35-44\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.33\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 130\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9263737983173794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011002212609494914\n",
      "          policy_loss: -0.03492831736803055\n",
      "          total_loss: -0.0360133182671335\n",
      "          vf_explained_var: 0.5430781245231628\n",
      "          vf_loss: 0.012608863030456835\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.26428571428571\n",
      "    ram_util_percent: 57.10000000000001\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0392374642294103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.899345307070942\n",
      "    mean_inference_ms: 1.3350766821061117\n",
      "    mean_raw_obs_processing_ms: 3.440759503558555\n",
      "  time_since_restore: 1729.4043850898743\n",
      "  time_this_iter_s: 9.470192670822144\n",
      "  time_total_s: 1729.4043850898743\n",
      "  timers:\n",
      "    learn_throughput: 1518.146\n",
      "    learn_time_ms: 658.698\n",
      "    load_throughput: 289789.962\n",
      "    load_time_ms: 3.451\n",
      "    sample_throughput: 109.951\n",
      "    sample_time_ms: 9094.951\n",
      "    update_time_ms: 1.73\n",
      "  timestamp: 1631982944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">          1729.4</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">    0.33</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.35\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 131\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6543938345379299\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008113184794988845\n",
      "          policy_loss: -0.19065508792797725\n",
      "          total_loss: -0.17894472579161327\n",
      "          vf_explained_var: 0.7369613647460938\n",
      "          vf_loss: 0.024146996267760793\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.95384615384615\n",
      "    ram_util_percent: 57.11538461538463\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03923384046240096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.888712056543816\n",
      "    mean_inference_ms: 1.3348876168918995\n",
      "    mean_raw_obs_processing_ms: 3.442880302825448\n",
      "  time_since_restore: 1738.55215549469\n",
      "  time_this_iter_s: 9.147770404815674\n",
      "  time_total_s: 1738.55215549469\n",
      "  timers:\n",
      "    learn_throughput: 1525.383\n",
      "    learn_time_ms: 655.573\n",
      "    load_throughput: 290114.682\n",
      "    load_time_ms: 3.447\n",
      "    sample_throughput: 110.765\n",
      "    sample_time_ms: 9028.156\n",
      "    update_time_ms: 1.738\n",
      "  timestamp: 1631982953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         1738.55</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">    0.35</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-36-03\n",
      "  done: false\n",
      "  episode_len_mean: 946.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.36\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 132\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5859987987412347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014328515869793258\n",
      "          policy_loss: -0.0565927106473181\n",
      "          total_loss: 0.08529146574437618\n",
      "          vf_explained_var: 0.4078081250190735\n",
      "          vf_loss: 0.15049035453961956\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.13571428571429\n",
      "    ram_util_percent: 57.200000000000024\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03923039986301686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.878533994822712\n",
      "    mean_inference_ms: 1.334708361867545\n",
      "    mean_raw_obs_processing_ms: 3.445982442592763\n",
      "  time_since_restore: 1748.2197153568268\n",
      "  time_this_iter_s: 9.66755986213684\n",
      "  time_total_s: 1748.2197153568268\n",
      "  timers:\n",
      "    learn_throughput: 1533.825\n",
      "    learn_time_ms: 651.965\n",
      "    load_throughput: 294601.748\n",
      "    load_time_ms: 3.394\n",
      "    sample_throughput: 112.035\n",
      "    sample_time_ms: 8925.811\n",
      "    update_time_ms: 1.716\n",
      "  timestamp: 1631982963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         1748.22</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">    0.36</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             946.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-36-54\n",
      "  done: false\n",
      "  episode_len_mean: 938.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.42\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 133\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.641596081521776\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010936568625881075\n",
      "          policy_loss: -0.04295312662919362\n",
      "          total_loss: 0.19173872247338294\n",
      "          vf_explained_var: 0.1320132464170456\n",
      "          vf_loss: 0.24557117232018047\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.587500000000002\n",
      "    ram_util_percent: 57.53194444444446\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03922727634052045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.8690988428553315\n",
      "    mean_inference_ms: 1.3345441205157753\n",
      "    mean_raw_obs_processing_ms: 3.4532779120989052\n",
      "  time_since_restore: 1799.1152787208557\n",
      "  time_this_iter_s: 50.89556336402893\n",
      "  time_total_s: 1799.1152787208557\n",
      "  timers:\n",
      "    learn_throughput: 1532.675\n",
      "    learn_time_ms: 652.454\n",
      "    load_throughput: 215185.208\n",
      "    load_time_ms: 4.647\n",
      "    sample_throughput: 77.031\n",
      "    sample_time_ms: 12981.746\n",
      "    update_time_ms: 1.71\n",
      "  timestamp: 1631983014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         1799.12</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">    0.42</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            938.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-37-59\n",
      "  done: false\n",
      "  episode_len_mean: 924.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.54\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 136\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5817578236262004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014568184449450822\n",
      "          policy_loss: -0.001188394675652186\n",
      "          total_loss: 0.13637544421686065\n",
      "          vf_explained_var: 0.5331628322601318\n",
      "          vf_loss: 0.14600627302295632\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.964516129032262\n",
      "    ram_util_percent: 58.47634408602148\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039220041361585564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.844781758431813\n",
      "    mean_inference_ms: 1.3341506681311706\n",
      "    mean_raw_obs_processing_ms: 3.4933911634126473\n",
      "  time_since_restore: 1863.939337015152\n",
      "  time_this_iter_s: 64.82405829429626\n",
      "  time_total_s: 1863.939337015152\n",
      "  timers:\n",
      "    learn_throughput: 1535.376\n",
      "    learn_time_ms: 651.306\n",
      "    load_throughput: 168436.473\n",
      "    load_time_ms: 5.937\n",
      "    sample_throughput: 54.11\n",
      "    sample_time_ms: 18480.798\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1631983079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         1863.94</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">    0.54</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             924.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-38-10\n",
      "  done: false\n",
      "  episode_len_mean: 924.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.58\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 137\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4974589308102926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009372407347914901\n",
      "          policy_loss: -0.04975844385723273\n",
      "          total_loss: 0.024677059344119495\n",
      "          vf_explained_var: 0.02506692335009575\n",
      "          vf_loss: 0.08466531107616093\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.93125\n",
      "    ram_util_percent: 58.35625\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039218271594716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.838070253480821\n",
      "    mean_inference_ms: 1.3340490141264283\n",
      "    mean_raw_obs_processing_ms: 3.5083854904669844\n",
      "  time_since_restore: 1875.4796268939972\n",
      "  time_this_iter_s: 11.540289878845215\n",
      "  time_total_s: 1875.4796268939972\n",
      "  timers:\n",
      "    learn_throughput: 1535.127\n",
      "    learn_time_ms: 651.412\n",
      "    load_throughput: 170201.273\n",
      "    load_time_ms: 5.875\n",
      "    sample_throughput: 53.608\n",
      "    sample_time_ms: 18654.011\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1631983090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         1875.48</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">    0.58</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             924.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-38-20\n",
      "  done: false\n",
      "  episode_len_mean: 924.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.61\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 138\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8129900733629862\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013265159937992306\n",
      "          policy_loss: 0.010647150956922107\n",
      "          total_loss: 0.13142382721934054\n",
      "          vf_explained_var: 0.20879007875919342\n",
      "          vf_loss: 0.13219109118605654\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.46\n",
      "    ram_util_percent: 58.526666666666664\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921688168312412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.831983288170249\n",
      "    mean_inference_ms: 1.3339665244452308\n",
      "    mean_raw_obs_processing_ms: 3.5239171008054946\n",
      "  time_since_restore: 1885.4119431972504\n",
      "  time_this_iter_s: 9.932316303253174\n",
      "  time_total_s: 1885.4119431972504\n",
      "  timers:\n",
      "    learn_throughput: 1519.827\n",
      "    learn_time_ms: 657.97\n",
      "    load_throughput: 168776.719\n",
      "    load_time_ms: 5.925\n",
      "    sample_throughput: 53.467\n",
      "    sample_time_ms: 18703.058\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1631983100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         1885.41</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">    0.61</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             924.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-39-07\n",
      "  done: false\n",
      "  episode_len_mean: 922.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.64\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 139\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2847236818737453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024015112341394012\n",
      "          policy_loss: 0.07942587104108599\n",
      "          total_loss: 0.2950616168479125\n",
      "          vf_explained_var: 0.7917361855506897\n",
      "          vf_loss: 0.2163253312309583\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.914925373134334\n",
      "    ram_util_percent: 58.32388059701492\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921591820768657\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.826599064452912\n",
      "    mean_inference_ms: 1.3339024947758424\n",
      "    mean_raw_obs_processing_ms: 3.542808393563312\n",
      "  time_since_restore: 1932.6920704841614\n",
      "  time_this_iter_s: 47.28012728691101\n",
      "  time_total_s: 1932.6920704841614\n",
      "  timers:\n",
      "    learn_throughput: 1529.733\n",
      "    learn_time_ms: 653.709\n",
      "    load_throughput: 139603.52\n",
      "    load_time_ms: 7.163\n",
      "    sample_throughput: 44.481\n",
      "    sample_time_ms: 22481.667\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1631983147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         1932.69</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">    0.64</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            922.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-39-19\n",
      "  done: false\n",
      "  episode_len_mean: 922.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.63\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 140\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.653649870554606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008522189384490541\n",
      "          policy_loss: 0.017539826780557634\n",
      "          total_loss: 0.03710671621892187\n",
      "          vf_explained_var: 0.597208559513092\n",
      "          vf_loss: 0.029631847908927336\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.976470588235294\n",
      "    ram_util_percent: 58.152941176470584\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921518318718082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.821921368356644\n",
      "    mean_inference_ms: 1.3338499394770067\n",
      "    mean_raw_obs_processing_ms: 3.5621321912054236\n",
      "  time_since_restore: 1944.4517543315887\n",
      "  time_this_iter_s: 11.759683847427368\n",
      "  time_total_s: 1944.4517543315887\n",
      "  timers:\n",
      "    learn_throughput: 1530.096\n",
      "    learn_time_ms: 653.554\n",
      "    load_throughput: 139079.964\n",
      "    load_time_ms: 7.19\n",
      "    sample_throughput: 44.004\n",
      "    sample_time_ms: 22725.015\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1631983159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         1944.45</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">    0.63</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            922.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 911.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.72\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 142\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8242120583852133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012047659213971966\n",
      "          policy_loss: -0.03314260809371869\n",
      "          total_loss: 0.06477348618209362\n",
      "          vf_explained_var: 0.7036176323890686\n",
      "          vf_loss: 0.10700952261168924\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.684375\n",
      "    ram_util_percent: 58.333333333333336\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921446382212529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.8141582360356905\n",
      "    mean_inference_ms: 1.3337834020255093\n",
      "    mean_raw_obs_processing_ms: 3.610933172305667\n",
      "  time_since_restore: 2011.7446646690369\n",
      "  time_this_iter_s: 67.29291033744812\n",
      "  time_total_s: 2011.7446646690369\n",
      "  timers:\n",
      "    learn_throughput: 1528.233\n",
      "    learn_time_ms: 654.35\n",
      "    load_throughput: 117456.251\n",
      "    load_time_ms: 8.514\n",
      "    sample_throughput: 35.071\n",
      "    sample_time_ms: 28513.615\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1631983226\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         2011.74</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">    0.72</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            911.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-41-11\n",
      "  done: false\n",
      "  episode_len_mean: 907.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.69\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 144\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7046727273199294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011783159944370543\n",
      "          policy_loss: 0.0914755734304587\n",
      "          total_loss: 0.1948854914969868\n",
      "          vf_explained_var: -0.1992635577917099\n",
      "          vf_loss: 0.11150880557898846\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.43125\n",
      "    ram_util_percent: 57.93281249999999\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921435886331994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.808069977935354\n",
      "    mean_inference_ms: 1.3337500121233339\n",
      "    mean_raw_obs_processing_ms: 3.6582964895539045\n",
      "  time_since_restore: 2056.6183943748474\n",
      "  time_this_iter_s: 44.87372970581055\n",
      "  time_total_s: 2056.6183943748474\n",
      "  timers:\n",
      "    learn_throughput: 1526.36\n",
      "    learn_time_ms: 655.153\n",
      "    load_throughput: 100515.337\n",
      "    load_time_ms: 9.949\n",
      "    sample_throughput: 31.2\n",
      "    sample_time_ms: 32051.741\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1631983271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         2056.62</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">    0.69</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            907.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-41-23\n",
      "  done: false\n",
      "  episode_len_mean: 907.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.69\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 145\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.104853139983283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010195084565808444\n",
      "          policy_loss: -0.06318613522582583\n",
      "          total_loss: -0.068148492442237\n",
      "          vf_explained_var: 0.23834927380084991\n",
      "          vf_loss: 0.00834428178301702\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.48125\n",
      "    ram_util_percent: 57.975\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039214426407826956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.80555623129151\n",
      "    mean_inference_ms: 1.3337428791313712\n",
      "    mean_raw_obs_processing_ms: 3.6826722327105834\n",
      "  time_since_restore: 2067.8515906333923\n",
      "  time_this_iter_s: 11.233196258544922\n",
      "  time_total_s: 2067.8515906333923\n",
      "  timers:\n",
      "    learn_throughput: 1521.364\n",
      "    learn_time_ms: 657.305\n",
      "    load_throughput: 100011.302\n",
      "    load_time_ms: 9.999\n",
      "    sample_throughput: 31.0\n",
      "    sample_time_ms: 32258.099\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1631983283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         2067.85</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">    0.69</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            907.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-41-33\n",
      "  done: false\n",
      "  episode_len_mean: 907.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.68\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 146\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9170289582676359\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008139772007632532\n",
      "          policy_loss: -0.015164795600705676\n",
      "          total_loss: 0.004289004703362783\n",
      "          vf_explained_var: 0.6024371385574341\n",
      "          vf_loss: 0.03244294571793742\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.80714285714286\n",
      "    ram_util_percent: 58.228571428571435\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921465445934104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.803499045841612\n",
      "    mean_inference_ms: 1.3337476066843488\n",
      "    mean_raw_obs_processing_ms: 3.707335840038703\n",
      "  time_since_restore: 2077.799797296524\n",
      "  time_this_iter_s: 9.948206663131714\n",
      "  time_total_s: 2077.799797296524\n",
      "  timers:\n",
      "    learn_throughput: 1522.081\n",
      "    learn_time_ms: 656.995\n",
      "    load_throughput: 99656.052\n",
      "    load_time_ms: 10.035\n",
      "    sample_throughput: 30.973\n",
      "    sample_time_ms: 32286.447\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1631983293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">          2077.8</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">    0.68</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            907.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-41-43\n",
      "  done: false\n",
      "  episode_len_mean: 907.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.67\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 147\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9393754151132372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007340012267430663\n",
      "          policy_loss: -0.13654586780402395\n",
      "          total_loss: -0.07257251764337222\n",
      "          vf_explained_var: 0.7434049844741821\n",
      "          vf_loss: 0.07779327867190457\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.08666666666667\n",
      "    ram_util_percent: 58.19333333333335\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921498537712384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.8016359083602245\n",
      "    mean_inference_ms: 1.3337591654907863\n",
      "    mean_raw_obs_processing_ms: 3.732258257929796\n",
      "  time_since_restore: 2088.047605037689\n",
      "  time_this_iter_s: 10.247807741165161\n",
      "  time_total_s: 2088.047605037689\n",
      "  timers:\n",
      "    learn_throughput: 1520.373\n",
      "    learn_time_ms: 657.734\n",
      "    load_throughput: 114129.168\n",
      "    load_time_ms: 8.762\n",
      "    sample_throughput: 35.433\n",
      "    sample_time_ms: 28222.203\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1631983303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         2088.05</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">    0.67</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            907.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-41-53\n",
      "  done: false\n",
      "  episode_len_mean: 907.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.63\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 148\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8458625819947985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016236432263620395\n",
      "          policy_loss: -0.06700662225484848\n",
      "          total_loss: 0.24612035769969226\n",
      "          vf_explained_var: 0.4609915018081665\n",
      "          vf_loss: 0.3192560656772306\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.973333333333336\n",
      "    ram_util_percent: 57.826666666666675\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039215367470788395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.8000832290192905\n",
      "    mean_inference_ms: 1.333777871424625\n",
      "    mean_raw_obs_processing_ms: 3.7574205960060625\n",
      "  time_since_restore: 2098.3691012859344\n",
      "  time_this_iter_s: 10.32149624824524\n",
      "  time_total_s: 2098.3691012859344\n",
      "  timers:\n",
      "    learn_throughput: 1520.508\n",
      "    learn_time_ms: 657.675\n",
      "    load_throughput: 133518.731\n",
      "    load_time_ms: 7.49\n",
      "    sample_throughput: 43.911\n",
      "    sample_time_ms: 22773.308\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1631983313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         2098.37</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">    0.63</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            907.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-42-03\n",
      "  done: false\n",
      "  episode_len_mean: 907.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.6\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 149\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5311259819401635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006364067225342791\n",
      "          policy_loss: 0.06388703098313676\n",
      "          total_loss: 0.13341363378696972\n",
      "          vf_explained_var: 0.19139918684959412\n",
      "          vf_loss: 0.0800051473495033\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.628571428571426\n",
      "    ram_util_percent: 57.378571428571426\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921594469912802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.798793436309039\n",
      "    mean_inference_ms: 1.3338054660296754\n",
      "    mean_raw_obs_processing_ms: 3.7827999901452087\n",
      "  time_since_restore: 2108.362986803055\n",
      "  time_this_iter_s: 9.993885517120361\n",
      "  time_total_s: 2108.362986803055\n",
      "  timers:\n",
      "    learn_throughput: 1523.17\n",
      "    learn_time_ms: 656.526\n",
      "    load_throughput: 133887.828\n",
      "    load_time_ms: 7.469\n",
      "    sample_throughput: 44.209\n",
      "    sample_time_ms: 22619.854\n",
      "    update_time_ms: 1.691\n",
      "  timestamp: 1631983323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         2108.36</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">     0.6</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            907.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-42-13\n",
      "  done: false\n",
      "  episode_len_mean: 907.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.58\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 150\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0159215794669256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009394990771441581\n",
      "          policy_loss: 0.07040197965171602\n",
      "          total_loss: 0.08695591588815053\n",
      "          vf_explained_var: 0.12081994116306305\n",
      "          vf_loss: 0.02957882615737617\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.88461538461537\n",
      "    ram_util_percent: 57.05384615384615\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921651176090601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.797616847214837\n",
      "    mean_inference_ms: 1.3338367293701003\n",
      "    mean_raw_obs_processing_ms: 3.7987865943865655\n",
      "  time_since_restore: 2117.7653152942657\n",
      "  time_this_iter_s: 9.402328491210938\n",
      "  time_total_s: 2117.7653152942657\n",
      "  timers:\n",
      "    learn_throughput: 1539.407\n",
      "    learn_time_ms: 649.601\n",
      "    load_throughput: 134931.461\n",
      "    load_time_ms: 7.411\n",
      "    sample_throughput: 44.299\n",
      "    sample_time_ms: 22573.843\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1631983333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         2117.77</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">    0.58</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            907.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-42-23\n",
      "  done: false\n",
      "  episode_len_mean: 912.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.46\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 151\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8498979912863838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01764073583056292\n",
      "          policy_loss: 0.031304090138938694\n",
      "          total_loss: 0.27627317214177716\n",
      "          vf_explained_var: 0.4437655210494995\n",
      "          vf_loss: 0.25007212791177963\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.36666666666667\n",
      "    ram_util_percent: 56.926666666666655\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921704473492318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.796455077383399\n",
      "    mean_inference_ms: 1.333868132772198\n",
      "    mean_raw_obs_processing_ms: 3.8143921417006674\n",
      "  time_since_restore: 2127.99392414093\n",
      "  time_this_iter_s: 10.228608846664429\n",
      "  time_total_s: 2127.99392414093\n",
      "  timers:\n",
      "    learn_throughput: 1539.852\n",
      "    learn_time_ms: 649.413\n",
      "    load_throughput: 162239.156\n",
      "    load_time_ms: 6.164\n",
      "    sample_throughput: 52.994\n",
      "    sample_time_ms: 18870.132\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1631983343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         2127.99</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">    0.46</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            912.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-42-32\n",
      "  done: false\n",
      "  episode_len_mean: 912.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.46\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 152\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.148477296034495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008840707787794186\n",
      "          policy_loss: -0.04570097434851858\n",
      "          total_loss: -0.05253276833229595\n",
      "          vf_explained_var: 0.6849467754364014\n",
      "          vf_loss: 0.007939571970685696\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.028571428571425\n",
      "    ram_util_percent: 56.89285714285713\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921713806634022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.795371963208833\n",
      "    mean_inference_ms: 1.3338899249618321\n",
      "    mean_raw_obs_processing_ms: 3.830378895968348\n",
      "  time_since_restore: 2137.5630853176117\n",
      "  time_this_iter_s: 9.569161176681519\n",
      "  time_total_s: 2137.5630853176117\n",
      "  timers:\n",
      "    learn_throughput: 1540.202\n",
      "    learn_time_ms: 649.266\n",
      "    load_throughput: 163018.539\n",
      "    load_time_ms: 6.134\n",
      "    sample_throughput: 53.616\n",
      "    sample_time_ms: 18651.221\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1631983352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         2137.56</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">    0.46</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            912.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-43-44\n",
      "  done: false\n",
      "  episode_len_mean: 903.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.54\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 154\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9560490793652006\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006405399726378678\n",
      "          policy_loss: 0.049374497102366556\n",
      "          total_loss: 0.11586013320419523\n",
      "          vf_explained_var: -0.08218669891357422\n",
      "          vf_loss: 0.08118202802207734\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.870588235294115\n",
      "    ram_util_percent: 57.369607843137274\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039216767925823674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.793827208421301\n",
      "    mean_inference_ms: 1.3339301393020795\n",
      "    mean_raw_obs_processing_ms: 3.8724170195869574\n",
      "  time_since_restore: 2208.9004623889923\n",
      "  time_this_iter_s: 71.33737707138062\n",
      "  time_total_s: 2208.9004623889923\n",
      "  timers:\n",
      "    learn_throughput: 1526.079\n",
      "    learn_time_ms: 655.274\n",
      "    load_throughput: 173798.087\n",
      "    load_time_ms: 5.754\n",
      "    sample_throughput: 52.493\n",
      "    sample_time_ms: 19050.018\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1631983424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">          2208.9</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">    0.54</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            903.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 903.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.54\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 155\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1014335605833265\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0070524621650273015\n",
      "          policy_loss: -0.13529233146044944\n",
      "          total_loss: -0.14575523589220313\n",
      "          vf_explained_var: -0.23663999140262604\n",
      "          vf_loss: 0.005195966518173615\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.905882352941184\n",
      "    ram_util_percent: 58.758823529411764\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921669809177467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.793580698645277\n",
      "    mean_inference_ms: 1.3339573595656609\n",
      "    mean_raw_obs_processing_ms: 3.894060580638377\n",
      "  time_since_restore: 2221.0878851413727\n",
      "  time_this_iter_s: 12.187422752380371\n",
      "  time_total_s: 2221.0878851413727\n",
      "  timers:\n",
      "    learn_throughput: 1516.048\n",
      "    learn_time_ms: 659.61\n",
      "    load_throughput: 222864.187\n",
      "    load_time_ms: 4.487\n",
      "    sample_throughput: 63.378\n",
      "    sample_time_ms: 15778.303\n",
      "    update_time_ms: 1.711\n",
      "  timestamp: 1631983436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         2221.09</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">    0.54</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            903.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-44-44\n",
      "  done: false\n",
      "  episode_len_mean: 901.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.53\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 156\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9582120378812153\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006238018935883257\n",
      "          policy_loss: 0.017758348749743566\n",
      "          total_loss: 0.170860593020916\n",
      "          vf_explained_var: -0.32372573018074036\n",
      "          vf_loss: 0.1679473689524457\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.508695652173916\n",
      "    ram_util_percent: 58.582608695652155\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921634556567283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.7934080258912255\n",
      "    mean_inference_ms: 1.3339796250824312\n",
      "    mean_raw_obs_processing_ms: 3.9185935774243266\n",
      "  time_since_restore: 2269.240757226944\n",
      "  time_this_iter_s: 48.15287208557129\n",
      "  time_total_s: 2269.240757226944\n",
      "  timers:\n",
      "    learn_throughput: 1515.692\n",
      "    learn_time_ms: 659.765\n",
      "    load_throughput: 190559.233\n",
      "    load_time_ms: 5.248\n",
      "    sample_throughput: 51.363\n",
      "    sample_time_ms: 19469.331\n",
      "    update_time_ms: 1.718\n",
      "  timestamp: 1631983484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         2269.24</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">    0.53</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            901.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-45-20\n",
      "  done: false\n",
      "  episode_len_mean: 900.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.53\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 157\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0202280865775215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00895012431251619\n",
      "          policy_loss: -0.047660395916965276\n",
      "          total_loss: 0.03329586833715439\n",
      "          vf_explained_var: 0.44870513677597046\n",
      "          vf_loss: 0.09436204409123294\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.40392156862745\n",
      "    ram_util_percent: 58.59803921568626\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921605014418558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.793471910017718\n",
      "    mean_inference_ms: 1.3340048073387074\n",
      "    mean_raw_obs_processing_ms: 3.9451459013146373\n",
      "  time_since_restore: 2305.4552097320557\n",
      "  time_this_iter_s: 36.214452505111694\n",
      "  time_total_s: 2305.4552097320557\n",
      "  timers:\n",
      "    learn_throughput: 1514.462\n",
      "    learn_time_ms: 660.301\n",
      "    load_throughput: 151101.44\n",
      "    load_time_ms: 6.618\n",
      "    sample_throughput: 45.261\n",
      "    sample_time_ms: 22094.059\n",
      "    update_time_ms: 1.712\n",
      "  timestamp: 1631983520\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         2305.46</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">    0.53</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            900.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 900.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.53\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 158\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.064853477478027\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008623892380053672\n",
      "          policy_loss: -0.09905536874300903\n",
      "          total_loss: -0.10917085044913821\n",
      "          vf_explained_var: -0.2424619048833847\n",
      "          vf_loss: 0.003984285250771791\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.11764705882353\n",
      "    ram_util_percent: 58.64705882352942\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039215719472732814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.793757450434639\n",
      "    mean_inference_ms: 1.3340289131779246\n",
      "    mean_raw_obs_processing_ms: 3.9718865284282447\n",
      "  time_since_restore: 2317.2218647003174\n",
      "  time_this_iter_s: 11.766654968261719\n",
      "  time_total_s: 2317.2218647003174\n",
      "  timers:\n",
      "    learn_throughput: 1514.638\n",
      "    learn_time_ms: 660.224\n",
      "    load_throughput: 151261.649\n",
      "    load_time_ms: 6.611\n",
      "    sample_throughput: 44.952\n",
      "    sample_time_ms: 22246.03\n",
      "    update_time_ms: 1.708\n",
      "  timestamp: 1631983532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         2317.22</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\">    0.53</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            900.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-46-16\n",
      "  done: false\n",
      "  episode_len_mean: 895.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.59\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 159\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.159372443623013\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010492082123510767\n",
      "          policy_loss: -0.1093989630540212\n",
      "          total_loss: -0.05748350839647982\n",
      "          vf_explained_var: 0.45623689889907837\n",
      "          vf_loss: 0.06554175354928399\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.476190476190478\n",
      "    ram_util_percent: 58.55873015873015\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039215205322143044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.794252294999777\n",
      "    mean_inference_ms: 1.3340487577357534\n",
      "    mean_raw_obs_processing_ms: 4.001076548403702\n",
      "  time_since_restore: 2361.1171169281006\n",
      "  time_this_iter_s: 43.8952522277832\n",
      "  time_total_s: 2361.1171169281006\n",
      "  timers:\n",
      "    learn_throughput: 1513.904\n",
      "    learn_time_ms: 660.544\n",
      "    load_throughput: 126144.481\n",
      "    load_time_ms: 7.927\n",
      "    sample_throughput: 39.06\n",
      "    sample_time_ms: 25601.755\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1631983576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         2361.12</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\">    0.59</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            895.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 888.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.65\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 161\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.070094048976898\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005859959351575405\n",
      "          policy_loss: -0.2660208213660452\n",
      "          total_loss: -0.15454463155733214\n",
      "          vf_explained_var: 0.7149320244789124\n",
      "          vf_loss: 0.1277272221373601\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.217543859649126\n",
      "    ram_util_percent: 58.39122807017544\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039213329454585874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.795165811031472\n",
      "    mean_inference_ms: 1.3340708551400626\n",
      "    mean_raw_obs_processing_ms: 4.064387838372225\n",
      "  time_since_restore: 2401.2565636634827\n",
      "  time_this_iter_s: 40.13944673538208\n",
      "  time_total_s: 2401.2565636634827\n",
      "  timers:\n",
      "    learn_throughput: 1514.925\n",
      "    learn_time_ms: 660.099\n",
      "    load_throughput: 105884.147\n",
      "    load_time_ms: 9.444\n",
      "    sample_throughput: 34.946\n",
      "    sample_time_ms: 28615.238\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1631983616\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         2401.26</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\">    0.65</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            888.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-47-08\n",
      "  done: false\n",
      "  episode_len_mean: 888.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.62\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 162\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593750000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.793657214111752\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022114263113768354\n",
      "          policy_loss: 0.032344404525227015\n",
      "          total_loss: 0.13784670424130227\n",
      "          vf_explained_var: 0.7331334352493286\n",
      "          vf_loss: 0.10664585631909884\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.32941176470588\n",
      "    ram_util_percent: 58.258823529411764\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921182986716883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.795613201257132\n",
      "    mean_inference_ms: 1.3340696987359297\n",
      "    mean_raw_obs_processing_ms: 4.096410286861777\n",
      "  time_since_restore: 2412.752002954483\n",
      "  time_this_iter_s: 11.495439291000366\n",
      "  time_total_s: 2412.752002954483\n",
      "  timers:\n",
      "    learn_throughput: 1512.149\n",
      "    learn_time_ms: 661.31\n",
      "    load_throughput: 105832.049\n",
      "    load_time_ms: 9.449\n",
      "    sample_throughput: 34.694\n",
      "    sample_time_ms: 28823.328\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1631983628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         2412.75</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">    0.62</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            888.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-47-18\n",
      "  done: false\n",
      "  episode_len_mean: 888.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.62\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 163\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8842369940545824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006099822591892007\n",
      "          policy_loss: -0.0847491910888089\n",
      "          total_loss: -0.016011113342311648\n",
      "          vf_explained_var: 0.22199414670467377\n",
      "          vf_loss: 0.08063236840276254\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.24285714285714\n",
      "    ram_util_percent: 58.27857142857141\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921019124399228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.796171496113074\n",
      "    mean_inference_ms: 1.3340653400040337\n",
      "    mean_raw_obs_processing_ms: 4.128517591202708\n",
      "  time_since_restore: 2422.9731504917145\n",
      "  time_this_iter_s: 10.221147537231445\n",
      "  time_total_s: 2422.9731504917145\n",
      "  timers:\n",
      "    learn_throughput: 1507.743\n",
      "    learn_time_ms: 663.243\n",
      "    load_throughput: 105523.188\n",
      "    load_time_ms: 9.477\n",
      "    sample_throughput: 34.697\n",
      "    sample_time_ms: 28820.619\n",
      "    update_time_ms: 1.706\n",
      "  timestamp: 1631983638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         2422.97</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\">    0.62</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            888.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-47-28\n",
      "  done: false\n",
      "  episode_len_mean: 888.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.56\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 164\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7990270177523295\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0075250516579167485\n",
      "          policy_loss: -0.1237764057599836\n",
      "          total_loss: 0.006170694147133165\n",
      "          vf_explained_var: 0.8076222538948059\n",
      "          vf_loss: 0.13936586258932948\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.51428571428571\n",
      "    ram_util_percent: 58.07857142857142\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03920863571182501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.796886589078295\n",
      "    mean_inference_ms: 1.3340624035163982\n",
      "    mean_raw_obs_processing_ms: 4.1607001791166365\n",
      "  time_since_restore: 2432.888605117798\n",
      "  time_this_iter_s: 9.915454626083374\n",
      "  time_total_s: 2432.888605117798\n",
      "  timers:\n",
      "    learn_throughput: 1505.761\n",
      "    learn_time_ms: 664.116\n",
      "    load_throughput: 105435.916\n",
      "    load_time_ms: 9.484\n",
      "    sample_throughput: 34.657\n",
      "    sample_time_ms: 28854.365\n",
      "    update_time_ms: 1.727\n",
      "  timestamp: 1631983648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         2432.89</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\">    0.56</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            888.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-47-38\n",
      "  done: false\n",
      "  episode_len_mean: 888.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.57\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 165\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8749460524982877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006071861527260363\n",
      "          policy_loss: -0.03626537521680196\n",
      "          total_loss: -0.01067571027411355\n",
      "          vf_explained_var: 0.8743882179260254\n",
      "          vf_loss: 0.03742289523490601\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.89999999999999\n",
      "    ram_util_percent: 57.59333333333333\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03920685055086197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.797586986530202\n",
      "    mean_inference_ms: 1.3340518731422446\n",
      "    mean_raw_obs_processing_ms: 4.192946505491517\n",
      "  time_since_restore: 2443.062397480011\n",
      "  time_this_iter_s: 10.173792362213135\n",
      "  time_total_s: 2443.062397480011\n",
      "  timers:\n",
      "    learn_throughput: 1523.456\n",
      "    learn_time_ms: 656.402\n",
      "    load_throughput: 117159.657\n",
      "    load_time_ms: 8.535\n",
      "    sample_throughput: 43.962\n",
      "    sample_time_ms: 22746.707\n",
      "    update_time_ms: 1.712\n",
      "  timestamp: 1631983658\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         2443.06</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">    0.57</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            888.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-48-26\n",
      "  done: false\n",
      "  episode_len_mean: 883.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.61\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 167\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.75989691151513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008386945768045292\n",
      "          policy_loss: -0.15538976076576444\n",
      "          total_loss: -0.018320794155200323\n",
      "          vf_explained_var: 0.6711099147796631\n",
      "          vf_loss: 0.1451146756609281\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.194202898550724\n",
      "    ram_util_percent: 57.16521739130435\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039203579213626966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.799180584384207\n",
      "    mean_inference_ms: 1.3340366767976066\n",
      "    mean_raw_obs_processing_ms: 4.262992352292319\n",
      "  time_since_restore: 2491.346985578537\n",
      "  time_this_iter_s: 48.284588098526\n",
      "  time_total_s: 2491.346985578537\n",
      "  timers:\n",
      "    learn_throughput: 1535.873\n",
      "    learn_time_ms: 651.096\n",
      "    load_throughput: 104230.912\n",
      "    load_time_ms: 9.594\n",
      "    sample_throughput: 37.935\n",
      "    sample_time_ms: 26360.666\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1631983706\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         2491.35</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">    0.61</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            883.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-49-23\n",
      "  done: false\n",
      "  episode_len_mean: 871.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.7\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 169\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8834109160635206\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007408183007859471\n",
      "          policy_loss: -0.07139522226320373\n",
      "          total_loss: 0.09120729371077485\n",
      "          vf_explained_var: 0.8207893967628479\n",
      "          vf_loss: 0.17299824169216058\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.17037037037037\n",
      "    ram_util_percent: 57.63456790123456\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03920078174668421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.801121539692023\n",
      "    mean_inference_ms: 1.334037190207821\n",
      "    mean_raw_obs_processing_ms: 4.339990680405698\n",
      "  time_since_restore: 2548.009910106659\n",
      "  time_this_iter_s: 56.66292452812195\n",
      "  time_total_s: 2548.009910106659\n",
      "  timers:\n",
      "    learn_throughput: 1534.126\n",
      "    learn_time_ms: 651.837\n",
      "    load_throughput: 100394.8\n",
      "    load_time_ms: 9.961\n",
      "    sample_throughput: 36.75\n",
      "    sample_time_ms: 27210.577\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1631983763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         2548.01</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\">     0.7</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            871.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-50-16\n",
      "  done: false\n",
      "  episode_len_mean: 859.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.8\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 171\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8798991243044536\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0060261885783590005\n",
      "          policy_loss: -0.02189245865576797\n",
      "          total_loss: 0.12231150538557106\n",
      "          vf_explained_var: 0.8825876712799072\n",
      "          vf_loss: 0.1561387496482995\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.569736842105264\n",
      "    ram_util_percent: 58.686842105263146\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919834542564266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.803478730668036\n",
      "    mean_inference_ms: 1.334046368738393\n",
      "    mean_raw_obs_processing_ms: 4.423281531174845\n",
      "  time_since_restore: 2601.0700075626373\n",
      "  time_this_iter_s: 53.060097455978394\n",
      "  time_total_s: 2601.0700075626373\n",
      "  timers:\n",
      "    learn_throughput: 1533.483\n",
      "    learn_time_ms: 652.11\n",
      "    load_throughput: 100450.582\n",
      "    load_time_ms: 9.955\n",
      "    sample_throughput: 34.608\n",
      "    sample_time_ms: 28894.838\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1631983816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         2601.07</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">     0.8</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            859.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-51-13\n",
      "  done: false\n",
      "  episode_len_mean: 852.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.91\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 173\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7617005056805082\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008118649407628473\n",
      "          policy_loss: 0.030389835975236364\n",
      "          total_loss: 0.12924490177796946\n",
      "          vf_explained_var: 0.5927925705909729\n",
      "          vf_loss: 0.10722442016833358\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.11\n",
      "    ram_util_percent: 58.43000000000001\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039196401867528936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.806009312472216\n",
      "    mean_inference_ms: 1.3340590093836784\n",
      "    mean_raw_obs_processing_ms: 4.513153776986464\n",
      "  time_since_restore: 2657.368110179901\n",
      "  time_this_iter_s: 56.298102617263794\n",
      "  time_total_s: 2657.368110179901\n",
      "  timers:\n",
      "    learn_throughput: 1533.915\n",
      "    learn_time_ms: 651.927\n",
      "    load_throughput: 87594.297\n",
      "    load_time_ms: 11.416\n",
      "    sample_throughput: 29.988\n",
      "    sample_time_ms: 33346.712\n",
      "    update_time_ms: 1.712\n",
      "  timestamp: 1631983873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         2657.37</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\">    0.91</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            852.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-51-43\n",
      "  done: false\n",
      "  episode_len_mean: 845.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.04\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 174\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8401538875367907\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008183155886859122\n",
      "          policy_loss: -0.19643741771578788\n",
      "          total_loss: -0.050157892372873096\n",
      "          vf_explained_var: 0.7532452940940857\n",
      "          vf_loss: 0.1553599375817511\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.11590909090909\n",
      "    ram_util_percent: 58.63863636363636\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919558564288223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.807312038895369\n",
      "    mean_inference_ms: 1.3340679048077424\n",
      "    mean_raw_obs_processing_ms: 4.559519682442857\n",
      "  time_since_restore: 2687.9524912834167\n",
      "  time_this_iter_s: 30.584381103515625\n",
      "  time_total_s: 2687.9524912834167\n",
      "  timers:\n",
      "    learn_throughput: 1537.345\n",
      "    learn_time_ms: 650.472\n",
      "    load_throughput: 86360.125\n",
      "    load_time_ms: 11.579\n",
      "    sample_throughput: 31.233\n",
      "    sample_time_ms: 32016.92\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1631983903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         2687.95</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">    1.04</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             845.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-52-23\n",
      "  done: false\n",
      "  episode_len_mean: 839.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.12\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 176\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8810569723447164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013687265136887098\n",
      "          policy_loss: -0.08840791814857059\n",
      "          total_loss: -0.010946833259529538\n",
      "          vf_explained_var: 0.929923415184021\n",
      "          vf_loss: 0.08068100506853726\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.933928571428574\n",
      "    ram_util_percent: 58.64464285714286\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039194200509102196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.809974428270333\n",
      "    mean_inference_ms: 1.3340908845304815\n",
      "    mean_raw_obs_processing_ms: 4.6561747268564355\n",
      "  time_since_restore: 2727.578197479248\n",
      "  time_this_iter_s: 39.6257061958313\n",
      "  time_total_s: 2727.578197479248\n",
      "  timers:\n",
      "    learn_throughput: 1534.082\n",
      "    learn_time_ms: 651.856\n",
      "    load_throughput: 86694.819\n",
      "    load_time_ms: 11.535\n",
      "    sample_throughput: 31.285\n",
      "    sample_time_ms: 31964.2\n",
      "    update_time_ms: 1.711\n",
      "  timestamp: 1631983943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         2727.58</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\">    1.12</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            839.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-53-25\n",
      "  done: false\n",
      "  episode_len_mean: 833.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 178\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7138471338484023\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006734503890565533\n",
      "          policy_loss: 0.12482104918195142\n",
      "          total_loss: 0.19894519589013523\n",
      "          vf_explained_var: 0.8533993363380432\n",
      "          vf_loss: 0.08359159684429567\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.76404494382023\n",
      "    ram_util_percent: 58.50898876404494\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919330062450683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.813047455038256\n",
      "    mean_inference_ms: 1.3341231460532705\n",
      "    mean_raw_obs_processing_ms: 4.759617026179411\n",
      "  time_since_restore: 2789.5978529453278\n",
      "  time_this_iter_s: 62.01965546607971\n",
      "  time_total_s: 2789.5978529453278\n",
      "  timers:\n",
      "    learn_throughput: 1533.436\n",
      "    learn_time_ms: 652.13\n",
      "    load_throughput: 76453.012\n",
      "    load_time_ms: 13.08\n",
      "    sample_throughput: 27.016\n",
      "    sample_time_ms: 37014.816\n",
      "    update_time_ms: 1.705\n",
      "  timestamp: 1631984005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">          2789.6</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\">    1.22</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            833.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-53-35\n",
      "  done: false\n",
      "  episode_len_mean: 833.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.24\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 179\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.794273070494334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005893443899210836\n",
      "          policy_loss: 0.007525887423091465\n",
      "          total_loss: 0.10130260437726975\n",
      "          vf_explained_var: 0.783203125\n",
      "          vf_loss: 0.10500644704120027\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.98571428571428\n",
      "    ram_util_percent: 58.62142857142858\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919284713249843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.814622243983381\n",
      "    mean_inference_ms: 1.3341388977010322\n",
      "    mean_raw_obs_processing_ms: 4.811336278769868\n",
      "  time_since_restore: 2799.792044878006\n",
      "  time_this_iter_s: 10.194191932678223\n",
      "  time_total_s: 2799.792044878006\n",
      "  timers:\n",
      "    learn_throughput: 1534.586\n",
      "    learn_time_ms: 651.642\n",
      "    load_throughput: 76558.931\n",
      "    load_time_ms: 13.062\n",
      "    sample_throughput: 27.018\n",
      "    sample_time_ms: 37012.626\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1631984015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         2799.79</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\">    1.24</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            833.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-54-21\n",
      "  done: false\n",
      "  episode_len_mean: 832.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.3\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 180\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1390624999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6342926621437073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003739820904205102\n",
      "          policy_loss: 0.1809181135561731\n",
      "          total_loss: 0.18751655634906556\n",
      "          vf_explained_var: 0.9270432591438293\n",
      "          vf_loss: 0.0186814796179533\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.77727272727273\n",
      "    ram_util_percent: 58.4969696969697\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919248769378536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.816243721860855\n",
      "    mean_inference_ms: 1.3341572295496165\n",
      "    mean_raw_obs_processing_ms: 4.865142503450127\n",
      "  time_since_restore: 2845.7693526744843\n",
      "  time_this_iter_s: 45.97730779647827\n",
      "  time_total_s: 2845.7693526744843\n",
      "  timers:\n",
      "    learn_throughput: 1538.904\n",
      "    learn_time_ms: 649.813\n",
      "    load_throughput: 68563.059\n",
      "    load_time_ms: 14.585\n",
      "    sample_throughput: 24.619\n",
      "    sample_time_ms: 40619.129\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1631984061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         2845.77</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">     1.3</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            832.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-55-36\n",
      "  done: false\n",
      "  episode_len_mean: 814.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.44\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 183\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9244172043270535\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007875726779798666\n",
      "          policy_loss: -0.035176515248086716\n",
      "          total_loss: 0.0820302085330089\n",
      "          vf_explained_var: 0.8855911493301392\n",
      "          vf_loss: 0.13196542635560035\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.489719626168224\n",
      "    ram_util_percent: 58.166355140186916\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919189947548584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.821051132630458\n",
      "    mean_inference_ms: 1.3342241534516066\n",
      "    mean_raw_obs_processing_ms: 5.032621298445689\n",
      "  time_since_restore: 2921.0803558826447\n",
      "  time_this_iter_s: 75.3110032081604\n",
      "  time_total_s: 2921.0803558826447\n",
      "  timers:\n",
      "    learn_throughput: 1533.679\n",
      "    learn_time_ms: 652.027\n",
      "    load_throughput: 61937.618\n",
      "    load_time_ms: 16.145\n",
      "    sample_throughput: 21.218\n",
      "    sample_time_ms: 47129.051\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1631984136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         2921.08</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\">    1.44</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">             814.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-56-33\n",
      "  done: false\n",
      "  episode_len_mean: 803.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.54\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 185\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6303341680102879\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011162457059279932\n",
      "          policy_loss: 0.0010445382859971789\n",
      "          total_loss: 0.41250074967328043\n",
      "          vf_explained_var: 0.5511208176612854\n",
      "          vf_loss: 0.42140218048459954\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.387654320987654\n",
      "    ram_util_percent: 57.7962962962963\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0391918374909129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.824448585333662\n",
      "    mean_inference_ms: 1.3342765842304742\n",
      "    mean_raw_obs_processing_ms: 5.150506253093303\n",
      "  time_since_restore: 2977.8042616844177\n",
      "  time_this_iter_s: 56.72390580177307\n",
      "  time_total_s: 2977.8042616844177\n",
      "  timers:\n",
      "    learn_throughput: 1536.33\n",
      "    learn_time_ms: 650.902\n",
      "    load_throughput: 60461.835\n",
      "    load_time_ms: 16.539\n",
      "    sample_throughput: 20.845\n",
      "    sample_time_ms: 47973.721\n",
      "    update_time_ms: 1.704\n",
      "  timestamp: 1631984193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">          2977.8</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\">    1.54</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            803.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 790.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.63\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 188\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.692093887594011\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014095249437662084\n",
      "          policy_loss: -0.07403871284590827\n",
      "          total_loss: 0.11390207496782144\n",
      "          vf_explained_var: 0.871947169303894\n",
      "          vf_loss: 0.1968340398122867\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.014285714285716\n",
      "    ram_util_percent: 57.860000000000014\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919116414190688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.829697352448914\n",
      "    mean_inference_ms: 1.3343359248609423\n",
      "    mean_raw_obs_processing_ms: 5.329894453355498\n",
      "  time_since_restore: 3051.1241779327393\n",
      "  time_this_iter_s: 73.31991624832153\n",
      "  time_total_s: 3051.1241779327393\n",
      "  timers:\n",
      "    learn_throughput: 1541.65\n",
      "    learn_time_ms: 648.655\n",
      "    load_throughput: 59328.144\n",
      "    load_time_ms: 16.855\n",
      "    sample_throughput: 20.145\n",
      "    sample_time_ms: 49641.341\n",
      "    update_time_ms: 1.691\n",
      "  timestamp: 1631984266\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         3051.12</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\">    1.63</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            790.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-57-58\n",
      "  done: false\n",
      "  episode_len_mean: 790.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.65\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 189\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.838579601711697\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016392065719166727\n",
      "          policy_loss: 0.021407435172133976\n",
      "          total_loss: 0.08327972247368759\n",
      "          vf_explained_var: 0.8261588215827942\n",
      "          vf_loss: 0.07092228877461619\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.925\n",
      "    ram_util_percent: 57.9\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919086320878271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.831375112066811\n",
      "    mean_inference_ms: 1.3343520031178486\n",
      "    mean_raw_obs_processing_ms: 5.388026989158988\n",
      "  time_since_restore: 3062.3442661762238\n",
      "  time_this_iter_s: 11.220088243484497\n",
      "  time_total_s: 3062.3442661762238\n",
      "  timers:\n",
      "    learn_throughput: 1542.997\n",
      "    learn_time_ms: 648.089\n",
      "    load_throughput: 64711.135\n",
      "    load_time_ms: 15.453\n",
      "    sample_throughput: 21.998\n",
      "    sample_time_ms: 45459.337\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1631984278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         3062.34</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">    1.65</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            790.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-58-07\n",
      "  done: false\n",
      "  episode_len_mean: 790.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.65\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 190\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9795056382815044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013890278851710312\n",
      "          policy_loss: 0.11365845211678081\n",
      "          total_loss: 0.16644094495309725\n",
      "          vf_explained_var: 0.2699771523475647\n",
      "          vf_loss: 0.06466660211897558\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.76153846153846\n",
      "    ram_util_percent: 58.0846153846154\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919052125327942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.833045745860496\n",
      "    mean_inference_ms: 1.3343661538094107\n",
      "    mean_raw_obs_processing_ms: 5.445959750815377\n",
      "  time_since_restore: 3071.6195442676544\n",
      "  time_this_iter_s: 9.275278091430664\n",
      "  time_total_s: 3071.6195442676544\n",
      "  timers:\n",
      "    learn_throughput: 1549.814\n",
      "    learn_time_ms: 645.239\n",
      "    load_throughput: 71345.412\n",
      "    load_time_ms: 14.016\n",
      "    sample_throughput: 24.533\n",
      "    sample_time_ms: 40761.339\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1631984287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         3071.62</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\">    1.65</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            790.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_16-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 770.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.78\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 193\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8461644728978475\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005590380375256032\n",
      "          policy_loss: -0.09726635076933438\n",
      "          total_loss: 0.041361833405163556\n",
      "          vf_explained_var: 0.9254485964775085\n",
      "          vf_loss: 0.15390593463348018\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.018110236220465\n",
      "    ram_util_percent: 58.15984251968502\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03918905378348767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.838437636202697\n",
      "    mean_inference_ms: 1.3344009555666354\n",
      "    mean_raw_obs_processing_ms: 5.634284870958716\n",
      "  time_since_restore: 3160.5285732746124\n",
      "  time_this_iter_s: 88.90902900695801\n",
      "  time_total_s: 3160.5285732746124\n",
      "  timers:\n",
      "    learn_throughput: 1548.272\n",
      "    learn_time_ms: 645.881\n",
      "    load_throughput: 71954.943\n",
      "    load_time_ms: 13.898\n",
      "    sample_throughput: 21.462\n",
      "    sample_time_ms: 46593.281\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1631984376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         3160.53</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\">    1.78</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            770.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-00-09\n",
      "  done: false\n",
      "  episode_len_mean: 763.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.85\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 194\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.758978980117374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01028616144408166\n",
      "          policy_loss: -0.15613119983010823\n",
      "          total_loss: -0.03953653987911013\n",
      "          vf_explained_var: 0.8627466559410095\n",
      "          vf_loss: 0.12832616290284526\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.82340425531915\n",
      "    ram_util_percent: 57.93404255319149\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039188223523367934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.840264396679576\n",
      "    mean_inference_ms: 1.3344059325783837\n",
      "    mean_raw_obs_processing_ms: 5.698397792760744\n",
      "  time_since_restore: 3193.1154284477234\n",
      "  time_this_iter_s: 32.58685517311096\n",
      "  time_total_s: 3193.1154284477234\n",
      "  timers:\n",
      "    learn_throughput: 1546.813\n",
      "    learn_time_ms: 646.491\n",
      "    load_throughput: 71412.586\n",
      "    load_time_ms: 14.003\n",
      "    sample_throughput: 21.792\n",
      "    sample_time_ms: 45888.681\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1631984409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         3193.12</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\">    1.85</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            763.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 758.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 1.84\n",
      "  episode_reward_min: -13.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 197\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8349338889122009\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01853024752671995\n",
      "          policy_loss: 0.0400211734076341\n",
      "          total_loss: 0.15806392634080516\n",
      "          vf_explained_var: 0.8826656937599182\n",
      "          vf_loss: 0.12583853668636746\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.976470588235294\n",
      "    ram_util_percent: 58.09264705882352\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03918572037401467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.845751989516973\n",
      "    mean_inference_ms: 1.3344199860942265\n",
      "    mean_raw_obs_processing_ms: 5.881202184160649\n",
      "  time_since_restore: 3240.90362739563\n",
      "  time_this_iter_s: 47.788198947906494\n",
      "  time_total_s: 3240.90362739563\n",
      "  timers:\n",
      "    learn_throughput: 1552.359\n",
      "    learn_time_ms: 644.181\n",
      "    load_throughput: 71707.678\n",
      "    load_time_ms: 13.946\n",
      "    sample_throughput: 22.488\n",
      "    sample_time_ms: 44467.897\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1631984456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">          3240.9</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">    1.84</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -13</td><td style=\"text-align: right;\">            758.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-02-31\n",
      "  done: false\n",
      "  episode_len_mean: 731.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.23\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 201\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.915089217821757\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0067748796043572805\n",
      "          policy_loss: -0.15368190198722814\n",
      "          total_loss: -0.06533215484685367\n",
      "          vf_explained_var: 0.9522221088409424\n",
      "          vf_loss: 0.10364213209185336\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.380000000000003\n",
      "    ram_util_percent: 58.11851851851852\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03918274334669275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.852814627944017\n",
      "    mean_inference_ms: 1.334447708093352\n",
      "    mean_raw_obs_processing_ms: 6.142106596394922\n",
      "  time_since_restore: 3335.460496902466\n",
      "  time_this_iter_s: 94.55686950683594\n",
      "  time_total_s: 3335.460496902466\n",
      "  timers:\n",
      "    learn_throughput: 1555.6\n",
      "    learn_time_ms: 642.839\n",
      "    load_throughput: 63848.601\n",
      "    load_time_ms: 15.662\n",
      "    sample_throughput: 18.902\n",
      "    sample_time_ms: 52903.795\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1631984551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         3335.46</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\">    2.23</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            731.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-03-00\n",
      "  done: false\n",
      "  episode_len_mean: 725.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.29\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 202\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.544854720433553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008589480259011623\n",
      "          policy_loss: -0.05817587441868252\n",
      "          total_loss: 0.1222751607083612\n",
      "          vf_explained_var: 0.8872463703155518\n",
      "          vf_loss: 0.19100760037286413\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.44146341463415\n",
      "    ram_util_percent: 57.617073170731715\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03918209627042504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.854682107867884\n",
      "    mean_inference_ms: 1.3344588219934546\n",
      "    mean_raw_obs_processing_ms: 6.208670232414397\n",
      "  time_since_restore: 3364.2046625614166\n",
      "  time_this_iter_s: 28.744165658950806\n",
      "  time_total_s: 3364.2046625614166\n",
      "  timers:\n",
      "    learn_throughput: 1550.753\n",
      "    learn_time_ms: 644.848\n",
      "    load_throughput: 63503.299\n",
      "    load_time_ms: 15.747\n",
      "    sample_throughput: 19.539\n",
      "    sample_time_ms: 51178.408\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1631984580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">          3364.2</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\">    2.29</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            725.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 722.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.28\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 204\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5747882604599\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015770064533701837\n",
      "          policy_loss: -0.1064097781976064\n",
      "          total_loss: -0.023952368771036466\n",
      "          vf_explained_var: 0.6319507956504822\n",
      "          vf_loss: 0.08922375316421191\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.33666666666667\n",
      "    ram_util_percent: 57.81833333333332\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0391809379799636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.858681326267969\n",
      "    mean_inference_ms: 1.334487884016251\n",
      "    mean_raw_obs_processing_ms: 6.338701781684168\n",
      "  time_since_restore: 3406.1415576934814\n",
      "  time_this_iter_s: 41.93689513206482\n",
      "  time_total_s: 3406.1415576934814\n",
      "  timers:\n",
      "    learn_throughput: 1555.446\n",
      "    learn_time_ms: 642.902\n",
      "    load_throughput: 64185.202\n",
      "    load_time_ms: 15.58\n",
      "    sample_throughput: 20.902\n",
      "    sample_time_ms: 47843.141\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1631984622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         3406.14</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\">    2.28</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            722.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-04-21\n",
      "  done: false\n",
      "  episode_len_mean: 723.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.33\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 206\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9157707889874775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004450683330219757\n",
      "          policy_loss: -0.0977612154972222\n",
      "          total_loss: -0.0905511924996972\n",
      "          vf_explained_var: 0.7008118629455566\n",
      "          vf_loss: 0.02383292725102769\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.85357142857143\n",
      "    ram_util_percent: 58.10892857142858\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039179738361830724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.862405914146654\n",
      "    mean_inference_ms: 1.334517082597749\n",
      "    mean_raw_obs_processing_ms: 6.465314980719914\n",
      "  time_since_restore: 3445.6231122016907\n",
      "  time_this_iter_s: 39.48155450820923\n",
      "  time_total_s: 3445.6231122016907\n",
      "  timers:\n",
      "    learn_throughput: 1555.757\n",
      "    learn_time_ms: 642.774\n",
      "    load_throughput: 64829.661\n",
      "    load_time_ms: 15.425\n",
      "    sample_throughput: 21.683\n",
      "    sample_time_ms: 46119.215\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1631984661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         3445.62</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">    2.33</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            723.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-04-51\n",
      "  done: false\n",
      "  episode_len_mean: 716.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.4\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 207\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7972085224257575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017422025236148532\n",
      "          policy_loss: -0.0691113336218728\n",
      "          total_loss: 0.004741486575868394\n",
      "          vf_explained_var: 0.8218594193458557\n",
      "          vf_loss: 0.08686370932393604\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.483333333333338\n",
      "    ram_util_percent: 58.35238095238095\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0391791788165065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.864140481507267\n",
      "    mean_inference_ms: 1.3345321227177116\n",
      "    mean_raw_obs_processing_ms: 6.529724970691342\n",
      "  time_since_restore: 3475.197901248932\n",
      "  time_this_iter_s: 29.57478904724121\n",
      "  time_total_s: 3475.197901248932\n",
      "  timers:\n",
      "    learn_throughput: 1553.234\n",
      "    learn_time_ms: 643.818\n",
      "    load_throughput: 63999.414\n",
      "    load_time_ms: 15.625\n",
      "    sample_throughput: 23.956\n",
      "    sample_time_ms: 41743.461\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1631984691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">          3475.2</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\">     2.4</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            716.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 721.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.36\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 209\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5070011602507698\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024836788024234824\n",
      "          policy_loss: -0.03943040370941162\n",
      "          total_loss: 0.24264829386439588\n",
      "          vf_explained_var: 0.5102092623710632\n",
      "          vf_loss: 0.29007604662328956\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.817741935483873\n",
      "    ram_util_percent: 58.47096774193548\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917794177004348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.8675583833851945\n",
      "    mean_inference_ms: 1.3345582616541896\n",
      "    mean_raw_obs_processing_ms: 6.650284290842595\n",
      "  time_since_restore: 3518.1814601421356\n",
      "  time_this_iter_s: 42.983558893203735\n",
      "  time_total_s: 3518.1814601421356\n",
      "  timers:\n",
      "    learn_throughput: 1557.641\n",
      "    learn_time_ms: 641.996\n",
      "    load_throughput: 58230.687\n",
      "    load_time_ms: 17.173\n",
      "    sample_throughput: 22.262\n",
      "    sample_time_ms: 44920.075\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1631984734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         3518.18</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\">    2.36</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">             721.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 720.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.38\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 210\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7433408564991422\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011898856446415746\n",
      "          policy_loss: -0.05768286453353034\n",
      "          total_loss: 0.1880754515528679\n",
      "          vf_explained_var: 0.7081406712532043\n",
      "          vf_loss: 0.2581091456943088\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.780851063829783\n",
      "    ram_util_percent: 58.640425531914914\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039177220599597565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.869186808375998\n",
      "    mean_inference_ms: 1.3345693783338666\n",
      "    mean_raw_obs_processing_ms: 6.709535739381149\n",
      "  time_since_restore: 3551.0430471897125\n",
      "  time_this_iter_s: 32.861587047576904\n",
      "  time_total_s: 3551.0430471897125\n",
      "  timers:\n",
      "    learn_throughput: 1555.135\n",
      "    learn_time_ms: 643.031\n",
      "    load_throughput: 52977.247\n",
      "    load_time_ms: 18.876\n",
      "    sample_throughput: 21.152\n",
      "    sample_time_ms: 47275.977\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1631984767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         3551.04</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\">    2.38</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            720.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 712.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.4\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 212\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9400378651089139\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010762229588081595\n",
      "          policy_loss: -0.13504240156875716\n",
      "          total_loss: -0.07853739427195655\n",
      "          vf_explained_var: 0.6878012418746948\n",
      "          vf_loss: 0.07130831362058719\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.652830188679243\n",
      "    ram_util_percent: 58.73962264150944\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917572796358589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.871949829129996\n",
      "    mean_inference_ms: 1.3345826215685357\n",
      "    mean_raw_obs_processing_ms: 6.8277447197698065\n",
      "  time_since_restore: 3588.670239686966\n",
      "  time_this_iter_s: 37.62719249725342\n",
      "  time_total_s: 3588.670239686966\n",
      "  timers:\n",
      "    learn_throughput: 1556.753\n",
      "    learn_time_ms: 642.363\n",
      "    load_throughput: 52641.538\n",
      "    load_time_ms: 18.996\n",
      "    sample_throughput: 23.726\n",
      "    sample_time_ms: 42148.346\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1631984804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         3588.67</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">     2.4</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            712.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-06-55\n",
      "  done: false\n",
      "  episode_len_mean: 712.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.4\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 213\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.075808718469408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006186561377982063\n",
      "          policy_loss: -0.008801175819502936\n",
      "          total_loss: 0.006829591364496284\n",
      "          vf_explained_var: 0.5639796853065491\n",
      "          vf_loss: 0.03374627354658312\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.618750000000006\n",
      "    ram_util_percent: 58.75\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917498734942795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.873209999244857\n",
      "    mean_inference_ms: 1.334586780461876\n",
      "    mean_raw_obs_processing_ms: 6.881820531648368\n",
      "  time_since_restore: 3599.6469197273254\n",
      "  time_this_iter_s: 10.976680040359497\n",
      "  time_total_s: 3599.6469197273254\n",
      "  timers:\n",
      "    learn_throughput: 1556.879\n",
      "    learn_time_ms: 642.311\n",
      "    load_throughput: 57373.068\n",
      "    load_time_ms: 17.43\n",
      "    sample_throughput: 25.007\n",
      "    sample_time_ms: 39988.96\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1631984815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         3599.65</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\">     2.4</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            712.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-07-05\n",
      "  done: false\n",
      "  episode_len_mean: 716.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.39\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 214\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8662616464826796\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008820844982727137\n",
      "          policy_loss: 0.025326057233744197\n",
      "          total_loss: 0.08818759491874112\n",
      "          vf_explained_var: 0.1400085687637329\n",
      "          vf_loss: 0.07775633947716819\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.82142857142858\n",
      "    ram_util_percent: 58.828571428571415\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917421724821209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.874435120072196\n",
      "    mean_inference_ms: 1.3345897619669314\n",
      "    mean_raw_obs_processing_ms: 6.935321771127366\n",
      "  time_since_restore: 3609.088511943817\n",
      "  time_this_iter_s: 9.4415922164917\n",
      "  time_total_s: 3609.088511943817\n",
      "  timers:\n",
      "    learn_throughput: 1554.835\n",
      "    learn_time_ms: 643.155\n",
      "    load_throughput: 62668.336\n",
      "    load_time_ms: 15.957\n",
      "    sample_throughput: 27.659\n",
      "    sample_time_ms: 36154.912\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1631984825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         3609.09</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\">    2.39</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            716.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-07-14\n",
      "  done: false\n",
      "  episode_len_mean: 716.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.42\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 215\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0988161166508994\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007009349464907929\n",
      "          policy_loss: -0.03347792625427246\n",
      "          total_loss: -0.023767783637675975\n",
      "          vf_explained_var: 0.2672085762023926\n",
      "          vf_loss: 0.027704269526940252\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.54615384615385\n",
      "    ram_util_percent: 58.73076923076924\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917347888855751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.875461117695142\n",
      "    mean_inference_ms: 1.334591081975884\n",
      "    mean_raw_obs_processing_ms: 6.988705098894769\n",
      "  time_since_restore: 3618.2268357276917\n",
      "  time_this_iter_s: 9.138323783874512\n",
      "  time_total_s: 3618.2268357276917\n",
      "  timers:\n",
      "    learn_throughput: 1551.761\n",
      "    learn_time_ms: 644.429\n",
      "    load_throughput: 70267.883\n",
      "    load_time_ms: 14.231\n",
      "    sample_throughput: 36.214\n",
      "    sample_time_ms: 27613.521\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1631984834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         3618.23</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\">    2.42</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            716.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-08-18\n",
      "  done: false\n",
      "  episode_len_mean: 699.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.56\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 218\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0208683292071026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011599847719199531\n",
      "          policy_loss: 0.03979698005649779\n",
      "          total_loss: 0.0651574685341782\n",
      "          vf_explained_var: 0.00904154684394598\n",
      "          vf_loss: 0.04061431679615958\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.9967032967033\n",
      "    ram_util_percent: 58.189010989010974\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917099323003112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.87826344966324\n",
      "    mean_inference_ms: 1.3345776178400877\n",
      "    mean_raw_obs_processing_ms: 7.158937145294107\n",
      "  time_since_restore: 3682.563183784485\n",
      "  time_this_iter_s: 64.33634805679321\n",
      "  time_total_s: 3682.563183784485\n",
      "  timers:\n",
      "    learn_throughput: 1553.299\n",
      "    learn_time_ms: 643.791\n",
      "    load_throughput: 70089.519\n",
      "    load_time_ms: 14.267\n",
      "    sample_throughput: 32.079\n",
      "    sample_time_ms: 31173.339\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1631984898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         3682.56</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">    2.56</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            699.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-08-29\n",
      "  done: false\n",
      "  episode_len_mean: 699.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.6\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 219\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0346039123005335\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013050374171296708\n",
      "          policy_loss: -0.03269350628058116\n",
      "          total_loss: -0.002158361714747217\n",
      "          vf_explained_var: 0.49769675731658936\n",
      "          vf_loss: 0.045306736996604334\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.61875\n",
      "    ram_util_percent: 58.618750000000006\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03916993426063708\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.879250285220722\n",
      "    mean_inference_ms: 1.3345676261544686\n",
      "    mean_raw_obs_processing_ms: 7.215952037125909\n",
      "  time_since_restore: 3693.374106168747\n",
      "  time_this_iter_s: 10.810922384262085\n",
      "  time_total_s: 3693.374106168747\n",
      "  timers:\n",
      "    learn_throughput: 1551.914\n",
      "    learn_time_ms: 644.366\n",
      "    load_throughput: 77682.798\n",
      "    load_time_ms: 12.873\n",
      "    sample_throughput: 35.636\n",
      "    sample_time_ms: 28061.563\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1631984909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         3693.37</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\">     2.6</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            699.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-08-39\n",
      "  done: false\n",
      "  episode_len_mean: 699.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.6\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 220\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.49249134759108226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0057653664357538836\n",
      "          policy_loss: -0.015202438665760888\n",
      "          total_loss: 0.3116461666093932\n",
      "          vf_explained_var: 0.5596858263015747\n",
      "          vf_loss: 0.3293108511302206\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.43333333333333\n",
      "    ram_util_percent: 58.77333333333332\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039168775725676286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.88022667253916\n",
      "    mean_inference_ms: 1.3345539761980962\n",
      "    mean_raw_obs_processing_ms: 7.272809721412618\n",
      "  time_since_restore: 3703.6255843639374\n",
      "  time_this_iter_s: 10.25147819519043\n",
      "  time_total_s: 3703.6255843639374\n",
      "  timers:\n",
      "    learn_throughput: 1550.477\n",
      "    learn_time_ms: 644.963\n",
      "    load_throughput: 87680.543\n",
      "    load_time_ms: 11.405\n",
      "    sample_throughput: 39.778\n",
      "    sample_time_ms: 25139.399\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1631984919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         3703.63</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\">     2.6</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            699.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-09-54\n",
      "  done: false\n",
      "  episode_len_mean: 687.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.68\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 222\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4008800321155124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017840069433194668\n",
      "          policy_loss: 0.019143868486086527\n",
      "          total_loss: 0.223934918973181\n",
      "          vf_explained_var: 0.6665905714035034\n",
      "          vf_loss: 0.21117949436108271\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.04433962264151\n",
      "    ram_util_percent: 58.92358490566038\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039165934641217814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.882230794184777\n",
      "    mean_inference_ms: 1.334514015776058\n",
      "    mean_raw_obs_processing_ms: 7.393570518765083\n",
      "  time_since_restore: 3778.0770301818848\n",
      "  time_this_iter_s: 74.45144581794739\n",
      "  time_total_s: 3778.0770301818848\n",
      "  timers:\n",
      "    learn_throughput: 1544.525\n",
      "    learn_time_ms: 647.448\n",
      "    load_throughput: 94434.403\n",
      "    load_time_ms: 10.589\n",
      "    sample_throughput: 33.755\n",
      "    sample_time_ms: 29625.4\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1631984994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         3778.08</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\">    2.68</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            687.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-10-35\n",
      "  done: false\n",
      "  episode_len_mean: 679.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.74\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 224\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7689861893653869\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008134382154812828\n",
      "          policy_loss: 0.19643797626097997\n",
      "          total_loss: 0.23242118656635286\n",
      "          vf_explained_var: 0.34810808300971985\n",
      "          vf_loss: 0.05019848552635974\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.899999999999995\n",
      "    ram_util_percent: 59.07457627118645\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03916207491519337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.884313975445193\n",
      "    mean_inference_ms: 1.3344490501466606\n",
      "    mean_raw_obs_processing_ms: 7.517988679511924\n",
      "  time_since_restore: 3819.439049720764\n",
      "  time_this_iter_s: 41.362019538879395\n",
      "  time_total_s: 3819.439049720764\n",
      "  timers:\n",
      "    learn_throughput: 1539.516\n",
      "    learn_time_ms: 649.555\n",
      "    load_throughput: 95987.184\n",
      "    load_time_ms: 10.418\n",
      "    sample_throughput: 33.943\n",
      "    sample_time_ms: 29461.307\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1631985035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         3819.44</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">    2.74</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">             679.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-12-10\n",
      "  done: false\n",
      "  episode_len_mean: 649.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.95\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 228\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2379867706033918\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0069014641965834496\n",
      "          policy_loss: -0.046531095852454504\n",
      "          total_loss: 0.12407240428858334\n",
      "          vf_explained_var: 0.891657829284668\n",
      "          vf_loss: 0.1800354147122966\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.662962962962965\n",
      "    ram_util_percent: 58.58444444444444\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03915324227575543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.888968164916305\n",
      "    mean_inference_ms: 1.3342802542142804\n",
      "    mean_raw_obs_processing_ms: 7.787238096341703\n",
      "  time_since_restore: 3914.2438685894012\n",
      "  time_this_iter_s: 94.80481886863708\n",
      "  time_total_s: 3914.2438685894012\n",
      "  timers:\n",
      "    learn_throughput: 1525.822\n",
      "    learn_time_ms: 655.384\n",
      "    load_throughput: 97722.399\n",
      "    load_time_ms: 10.233\n",
      "    sample_throughput: 28.051\n",
      "    sample_time_ms: 35649.974\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1631985130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         3914.24</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\">    2.95</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            649.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 633.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.05\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 230\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0790785498089261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009672157183915307\n",
      "          policy_loss: -0.08380910985999637\n",
      "          total_loss: 0.1503519655101829\n",
      "          vf_explained_var: 0.6803658604621887\n",
      "          vf_loss: 0.24082041788432335\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.21733333333333\n",
      "    ram_util_percent: 58.83466666666666\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03914911156134107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.891652377733631\n",
      "    mean_inference_ms: 1.3342018031901148\n",
      "    mean_raw_obs_processing_ms: 7.9272939517503005\n",
      "  time_since_restore: 3966.781414747238\n",
      "  time_this_iter_s: 52.537546157836914\n",
      "  time_total_s: 3966.781414747238\n",
      "  timers:\n",
      "    learn_throughput: 1509.799\n",
      "    learn_time_ms: 662.34\n",
      "    load_throughput: 100049.711\n",
      "    load_time_ms: 9.995\n",
      "    sample_throughput: 26.929\n",
      "    sample_time_ms: 37134.282\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1631985183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         3966.78</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\">    3.05</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">             633.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-13-13\n",
      "  done: false\n",
      "  episode_len_mean: 633.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.06\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 231\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.649879519144694\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00963422105832449\n",
      "          policy_loss: 0.030712714087631966\n",
      "          total_loss: 0.14195777278186547\n",
      "          vf_explained_var: 0.6937512159347534\n",
      "          vf_loss: 0.12362860788901647\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.8\n",
      "    ram_util_percent: 59.43333333333332\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03914730005570924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.893081519802779\n",
      "    mean_inference_ms: 1.3341693634544316\n",
      "    mean_raw_obs_processing_ms: 7.997235421447263\n",
      "  time_since_restore: 3977.0022959709167\n",
      "  time_this_iter_s: 10.220881223678589\n",
      "  time_total_s: 3977.0022959709167\n",
      "  timers:\n",
      "    learn_throughput: 1513.926\n",
      "    learn_time_ms: 660.534\n",
      "    load_throughput: 99812.336\n",
      "    load_time_ms: 10.019\n",
      "    sample_throughput: 26.983\n",
      "    sample_time_ms: 37060.479\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1631985193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">            3977</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\">    3.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">             633.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-14-01\n",
      "  done: false\n",
      "  episode_len_mean: 633.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.04\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 233\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6269322090678744\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012198981513686835\n",
      "          policy_loss: -0.12727104210191303\n",
      "          total_loss: -0.0417993475165632\n",
      "          vf_explained_var: 0.7045525312423706\n",
      "          vf_loss: 0.09653024209870233\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.407246376811596\n",
      "    ram_util_percent: 59.16956521739129\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03914407010334763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.895987737121443\n",
      "    mean_inference_ms: 1.3341145276884046\n",
      "    mean_raw_obs_processing_ms: 8.137729075478754\n",
      "  time_since_restore: 4025.5223758220673\n",
      "  time_this_iter_s: 48.52007985115051\n",
      "  time_total_s: 4025.5223758220673\n",
      "  timers:\n",
      "    learn_throughput: 1503.207\n",
      "    learn_time_ms: 665.245\n",
      "    load_throughput: 88484.066\n",
      "    load_time_ms: 11.301\n",
      "    sample_throughput: 24.413\n",
      "    sample_time_ms: 40962.336\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1631985241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         4025.52</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">    3.04</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            633.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-14-13\n",
      "  done: false\n",
      "  episode_len_mean: 633.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.04\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 234\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4969156410959032\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01579447844483795\n",
      "          policy_loss: -0.018381385091278287\n",
      "          total_loss: -0.00461319817437066\n",
      "          vf_explained_var: 0.10751919448375702\n",
      "          vf_loss: 0.021990757665803863\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.26875\n",
      "    ram_util_percent: 58.91875\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039142550165251366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.897371702693452\n",
      "    mean_inference_ms: 1.3340908202411728\n",
      "    mean_raw_obs_processing_ms: 8.202036786390002\n",
      "  time_since_restore: 4036.9274773597717\n",
      "  time_this_iter_s: 11.405101537704468\n",
      "  time_total_s: 4036.9274773597717\n",
      "  timers:\n",
      "    learn_throughput: 1498.154\n",
      "    learn_time_ms: 667.488\n",
      "    load_throughput: 88435.932\n",
      "    load_time_ms: 11.308\n",
      "    sample_throughput: 24.28\n",
      "    sample_time_ms: 41186.734\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1631985253\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         4036.93</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">    3.04</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            633.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-14-22\n",
      "  done: false\n",
      "  episode_len_mean: 640.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.97\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 235\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7821719633208382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02060451635627795\n",
      "          policy_loss: -0.03940156002839406\n",
      "          total_loss: -0.023951968053976695\n",
      "          vf_explained_var: 0.6233278512954712\n",
      "          vf_loss: 0.024470126809966235\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.35\n",
      "    ram_util_percent: 59.37857142857142\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039141070753583605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.898725131808207\n",
      "    mean_inference_ms: 1.3340686067345067\n",
      "    mean_raw_obs_processing_ms: 8.265753489409736\n",
      "  time_since_restore: 4046.4990150928497\n",
      "  time_this_iter_s: 9.571537733078003\n",
      "  time_total_s: 4046.4990150928497\n",
      "  timers:\n",
      "    learn_throughput: 1497.066\n",
      "    learn_time_ms: 667.973\n",
      "    load_throughput: 103503.004\n",
      "    load_time_ms: 9.662\n",
      "    sample_throughput: 28.002\n",
      "    sample_time_ms: 35711.403\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1631985262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">          4046.5</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\">    2.97</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            640.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 640.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.89\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 237\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6578647202915615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0037328081627151337\n",
      "          policy_loss: 0.09494381739447515\n",
      "          total_loss: 0.12942702358381616\n",
      "          vf_explained_var: 0.22548183798789978\n",
      "          vf_loss: 0.04867015728862801\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.71875\n",
      "    ram_util_percent: 59.7953125\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913823908757104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.901263379107177\n",
      "    mean_inference_ms: 1.3340290440396207\n",
      "    mean_raw_obs_processing_ms: 8.396079576076193\n",
      "  time_since_restore: 4091.44393658638\n",
      "  time_this_iter_s: 44.94492149353027\n",
      "  time_total_s: 4091.44393658638\n",
      "  timers:\n",
      "    learn_throughput: 1491.664\n",
      "    learn_time_ms: 670.392\n",
      "    load_throughput: 95160.722\n",
      "    load_time_ms: 10.509\n",
      "    sample_throughput: 25.561\n",
      "    sample_time_ms: 39121.527\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1631985307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         4091.44</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\">    2.89</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            640.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-15-19\n",
      "  done: false\n",
      "  episode_len_mean: 640.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.86\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 238\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7405540148417156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004684777227810086\n",
      "          policy_loss: -0.19435215294361113\n",
      "          total_loss: -0.20735148357020483\n",
      "          vf_explained_var: -0.07796776294708252\n",
      "          vf_loss: 0.002905387455312949\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.05\n",
      "    ram_util_percent: 60.5375\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913690923781596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.902534606782433\n",
      "    mean_inference_ms: 1.3340110959321212\n",
      "    mean_raw_obs_processing_ms: 8.461215362680154\n",
      "  time_since_restore: 4102.678423166275\n",
      "  time_this_iter_s: 11.23448657989502\n",
      "  time_total_s: 4102.678423166275\n",
      "  timers:\n",
      "    learn_throughput: 1480.63\n",
      "    learn_time_ms: 675.388\n",
      "    load_throughput: 94412.296\n",
      "    load_time_ms: 10.592\n",
      "    sample_throughput: 25.501\n",
      "    sample_time_ms: 39214.721\n",
      "    update_time_ms: 1.734\n",
      "  timestamp: 1631985319\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         4102.68</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">    2.86</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            640.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-15-28\n",
      "  done: false\n",
      "  episode_len_mean: 642.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.83\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 239\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018066406250003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7893300109439425\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005882607722316714\n",
      "          policy_loss: -0.06432106474207508\n",
      "          total_loss: -0.0792608506563637\n",
      "          vf_explained_var: -0.02352118492126465\n",
      "          vf_loss: 0.002011234706798051\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.82857142857143\n",
      "    ram_util_percent: 61.28571428571429\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913555909199322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.903754506616333\n",
      "    mean_inference_ms: 1.3339917019929657\n",
      "    mean_raw_obs_processing_ms: 8.523269712666991\n",
      "  time_since_restore: 4112.268622159958\n",
      "  time_this_iter_s: 9.590198993682861\n",
      "  time_total_s: 4112.268622159958\n",
      "  timers:\n",
      "    learn_throughput: 1485.581\n",
      "    learn_time_ms: 673.137\n",
      "    load_throughput: 102903.211\n",
      "    load_time_ms: 9.718\n",
      "    sample_throughput: 30.551\n",
      "    sample_time_ms: 32731.733\n",
      "    update_time_ms: 1.735\n",
      "  timestamp: 1631985328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         4112.27</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\">    2.83</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            642.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-15-56\n",
      "  done: false\n",
      "  episode_len_mean: 634.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.9\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 240\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018066406250003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9282676299413046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010421831647104243\n",
      "          policy_loss: -0.051400652424328855\n",
      "          total_loss: 0.020517359218663638\n",
      "          vf_explained_var: 0.38880980014801025\n",
      "          vf_loss: 0.08953131292429235\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.07\n",
      "    ram_util_percent: 60.87250000000002\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039134261199544974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.904882572517024\n",
      "    mean_inference_ms: 1.3339736068331476\n",
      "    mean_raw_obs_processing_ms: 8.585996714365578\n",
      "  time_since_restore: 4140.129736423492\n",
      "  time_this_iter_s: 27.861114263534546\n",
      "  time_total_s: 4140.129736423492\n",
      "  timers:\n",
      "    learn_throughput: 1476.016\n",
      "    learn_time_ms: 677.499\n",
      "    load_throughput: 102909.018\n",
      "    load_time_ms: 9.717\n",
      "    sample_throughput: 31.87\n",
      "    sample_time_ms: 31377.269\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1631985356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         4140.13</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\">     2.9</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            634.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-16-05\n",
      "  done: false\n",
      "  episode_len_mean: 639.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.86\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 241\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018066406250003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9660815411143833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010881198282703864\n",
      "          policy_loss: -0.02189102123181025\n",
      "          total_loss: -0.027002530131075116\n",
      "          vf_explained_var: 0.40650150179862976\n",
      "          vf_loss: 0.01280634422113912\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.96923076923076\n",
      "    ram_util_percent: 61.16923076923078\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913301302401093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.905913631091137\n",
      "    mean_inference_ms: 1.3339555136010859\n",
      "    mean_raw_obs_processing_ms: 8.644209821663871\n",
      "  time_since_restore: 4149.431897163391\n",
      "  time_this_iter_s: 9.302160739898682\n",
      "  time_total_s: 4149.431897163391\n",
      "  timers:\n",
      "    learn_throughput: 1487.538\n",
      "    learn_time_ms: 672.252\n",
      "    load_throughput: 122176.412\n",
      "    load_time_ms: 8.185\n",
      "    sample_throughput: 43.795\n",
      "    sample_time_ms: 22833.767\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1631985365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         4149.43</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\">    2.86</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            639.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-16-32\n",
      "  done: false\n",
      "  episode_len_mean: 640.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.86\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 243\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018066406250003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.619124382072025\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01565568062296268\n",
      "          policy_loss: -0.04904334396123886\n",
      "          total_loss: 0.19642961911029286\n",
      "          vf_explained_var: 0.23048150539398193\n",
      "          vf_loss: 0.2591564692556858\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.29473684210527\n",
      "    ram_util_percent: 61.1236842105263\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913052743154948\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.907832794878626\n",
      "    mean_inference_ms: 1.3339217227876483\n",
      "    mean_raw_obs_processing_ms: 8.75907915299331\n",
      "  time_since_restore: 4176.021856546402\n",
      "  time_this_iter_s: 26.589959383010864\n",
      "  time_total_s: 4176.021856546402\n",
      "  timers:\n",
      "    learn_throughput: 1491.629\n",
      "    learn_time_ms: 670.408\n",
      "    load_throughput: 120760.21\n",
      "    load_time_ms: 8.281\n",
      "    sample_throughput: 49.405\n",
      "    sample_time_ms: 20240.75\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1631985392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         4176.02</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">    2.86</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            640.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 645.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.88\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 244\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018066406250003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.61179246240192\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014119965968140047\n",
      "          policy_loss: 0.006219960418012407\n",
      "          total_loss: 0.00839064617951711\n",
      "          vf_explained_var: 0.8183921575546265\n",
      "          vf_loss: 0.016026866829229727\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.14666666666666\n",
      "    ram_util_percent: 60.93333333333332\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03912929304516146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.908747196681286\n",
      "    mean_inference_ms: 1.3339079237704694\n",
      "    mean_raw_obs_processing_ms: 8.814893290345317\n",
      "  time_since_restore: 4186.572465896606\n",
      "  time_this_iter_s: 10.550609350204468\n",
      "  time_total_s: 4186.572465896606\n",
      "  timers:\n",
      "    learn_throughput: 1490.0\n",
      "    learn_time_ms: 671.141\n",
      "    load_throughput: 121639.609\n",
      "    load_time_ms: 8.221\n",
      "    sample_throughput: 49.327\n",
      "    sample_time_ms: 20273.054\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1631985403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         4186.57</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\">    2.88</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">             645.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-16-52\n",
      "  done: false\n",
      "  episode_len_mean: 645.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.9\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 245\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018066406250003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7320823404524062\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011521203077803103\n",
      "          policy_loss: -0.02966531291604042\n",
      "          total_loss: -0.01225369738207923\n",
      "          vf_explained_var: 0.6073890924453735\n",
      "          vf_loss: 0.032886964399626274\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.89230769230768\n",
      "    ram_util_percent: 60.93846153846153\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039128120876547115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9095227904500165\n",
      "    mean_inference_ms: 1.3338953054810239\n",
      "    mean_raw_obs_processing_ms: 8.870562318255855\n",
      "  time_since_restore: 4195.741544246674\n",
      "  time_this_iter_s: 9.169078350067139\n",
      "  time_total_s: 4195.741544246674\n",
      "  timers:\n",
      "    learn_throughput: 1500.357\n",
      "    learn_time_ms: 666.508\n",
      "    load_throughput: 144615.215\n",
      "    load_time_ms: 6.915\n",
      "    sample_throughput: 61.185\n",
      "    sample_time_ms: 16343.902\n",
      "    update_time_ms: 1.733\n",
      "  timestamp: 1631985412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         4195.74</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\">     2.9</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">             645.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-17-01\n",
      "  done: false\n",
      "  episode_len_mean: 645.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 2.95\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 246\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018066406250003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7770652068985833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010651902790463394\n",
      "          policy_loss: -0.052710253579749004\n",
      "          total_loss: -0.025014198819796243\n",
      "          vf_explained_var: 0.6572611331939697\n",
      "          vf_loss: 0.04376048035143564\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.107142857142854\n",
      "    ram_util_percent: 60.87142857142856\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03912696141331159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.910263873641076\n",
      "    mean_inference_ms: 1.3338825439003807\n",
      "    mean_raw_obs_processing_ms: 8.926085848981948\n",
      "  time_since_restore: 4205.052453517914\n",
      "  time_this_iter_s: 9.310909271240234\n",
      "  time_total_s: 4205.052453517914\n",
      "  timers:\n",
      "    learn_throughput: 1503.551\n",
      "    learn_time_ms: 665.092\n",
      "    load_throughput: 143774.475\n",
      "    load_time_ms: 6.955\n",
      "    sample_throughput: 61.974\n",
      "    sample_time_ms: 16135.881\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1631985421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         4205.05</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\">    2.95</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">             645.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-18-03\n",
      "  done: false\n",
      "  episode_len_mean: 630.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.18\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 249\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018066406250003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5940816150771246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022055570853249544\n",
      "          policy_loss: 0.14354713981350262\n",
      "          total_loss: 0.21246946433352099\n",
      "          vf_explained_var: 0.30215269327163696\n",
      "          vf_loss: 0.08133026512676023\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.77241379310345\n",
      "    ram_util_percent: 60.4137931034483\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039123678942503085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.912302041511246\n",
      "    mean_inference_ms: 1.3338444651947492\n",
      "    mean_raw_obs_processing_ms: 9.1012004210597\n",
      "  time_since_restore: 4266.4572677612305\n",
      "  time_this_iter_s: 61.40481424331665\n",
      "  time_total_s: 4266.4572677612305\n",
      "  timers:\n",
      "    learn_throughput: 1507.532\n",
      "    learn_time_ms: 663.336\n",
      "    load_throughput: 118898.637\n",
      "    load_time_ms: 8.411\n",
      "    sample_throughput: 46.905\n",
      "    sample_time_ms: 21319.499\n",
      "    update_time_ms: 1.715\n",
      "  timestamp: 1631985483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         4266.46</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">    3.18</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            630.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-18-33\n",
      "  done: false\n",
      "  episode_len_mean: 622.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.25\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 250\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6753383941120572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016799043069577652\n",
      "          policy_loss: -0.026868377543158\n",
      "          total_loss: 0.18969121442900763\n",
      "          vf_explained_var: 0.5132055282592773\n",
      "          vf_loss: 0.22927665549019974\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.967441860465115\n",
      "    ram_util_percent: 60.58604651162789\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039122637134286015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.912988871022435\n",
      "    mean_inference_ms: 1.3338328024861084\n",
      "    mean_raw_obs_processing_ms: 9.160818172184303\n",
      "  time_since_restore: 4296.50199007988\n",
      "  time_this_iter_s: 30.044722318649292\n",
      "  time_total_s: 4296.50199007988\n",
      "  timers:\n",
      "    learn_throughput: 1513.459\n",
      "    learn_time_ms: 660.738\n",
      "    load_throughput: 110752.104\n",
      "    load_time_ms: 9.029\n",
      "    sample_throughput: 50.425\n",
      "    sample_time_ms: 19831.467\n",
      "    update_time_ms: 1.72\n",
      "  timestamp: 1631985513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">          4296.5</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\">    3.25</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -9</td><td style=\"text-align: right;\">            622.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-19-14\n",
      "  done: false\n",
      "  episode_len_mean: 614.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.38\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 252\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5903939829932319\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024218770579579836\n",
      "          policy_loss: 0.03402204646004571\n",
      "          total_loss: 0.195796297573381\n",
      "          vf_explained_var: 0.6170183420181274\n",
      "          vf_loss: 0.1718591243856483\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.637288135593217\n",
      "    ram_util_percent: 61.110169491525426\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039120745523065104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9144928907615055\n",
      "    mean_inference_ms: 1.333812424637039\n",
      "    mean_raw_obs_processing_ms: 9.283051026665953\n",
      "  time_since_restore: 4338.018041610718\n",
      "  time_this_iter_s: 41.51605153083801\n",
      "  time_total_s: 4338.018041610718\n",
      "  timers:\n",
      "    learn_throughput: 1513.14\n",
      "    learn_time_ms: 660.877\n",
      "    load_throughput: 96818.293\n",
      "    load_time_ms: 10.329\n",
      "    sample_throughput: 43.748\n",
      "    sample_time_ms: 22858.205\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1631985554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         4338.02</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\">    3.38</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            614.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-19-26\n",
      "  done: false\n",
      "  episode_len_mean: 619.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.33\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 253\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9618438243865968\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009801452746670868\n",
      "          policy_loss: -0.000606101109749741\n",
      "          total_loss: 0.07367879344771305\n",
      "          vf_explained_var: 0.4964483976364136\n",
      "          vf_loss: 0.09037082351278514\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.56470588235294\n",
      "    ram_util_percent: 60.9470588235294\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911994267953736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9153102719334045\n",
      "    mean_inference_ms: 1.3338045688257762\n",
      "    mean_raw_obs_processing_ms: 9.339838889711796\n",
      "  time_since_restore: 4349.459142208099\n",
      "  time_this_iter_s: 11.441100597381592\n",
      "  time_total_s: 4349.459142208099\n",
      "  timers:\n",
      "    learn_throughput: 1515.365\n",
      "    learn_time_ms: 659.907\n",
      "    load_throughput: 96681.265\n",
      "    load_time_ms: 10.343\n",
      "    sample_throughput: 43.395\n",
      "    sample_time_ms: 23044.253\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1631985566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         4349.46</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\">    3.33</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            619.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-20-14\n",
      "  done: false\n",
      "  episode_len_mean: 614.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.35\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 255\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7570990019374424\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010735574484949693\n",
      "          policy_loss: 0.03095068521797657\n",
      "          total_loss: 0.17167925246887736\n",
      "          vf_explained_var: 0.7827915549278259\n",
      "          vf_loss: 0.15443038727518998\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.27941176470588\n",
      "    ram_util_percent: 61.17941176470588\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911824576102478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.916774359566172\n",
      "    mean_inference_ms: 1.333785220847525\n",
      "    mean_raw_obs_processing_ms: 9.456447428858693\n",
      "  time_since_restore: 4397.52023267746\n",
      "  time_this_iter_s: 48.06109046936035\n",
      "  time_total_s: 4397.52023267746\n",
      "  timers:\n",
      "    learn_throughput: 1526.045\n",
      "    learn_time_ms: 655.288\n",
      "    load_throughput: 97106.555\n",
      "    load_time_ms: 10.298\n",
      "    sample_throughput: 39.89\n",
      "    sample_time_ms: 25068.936\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1631985614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         4397.52</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">    3.35</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            614.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-21-57\n",
      "  done: false\n",
      "  episode_len_mean: 585.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.51\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 261\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.785584114657508\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00799766156627266\n",
      "          policy_loss: -0.020339138474729325\n",
      "          total_loss: 0.2270138586974806\n",
      "          vf_explained_var: 0.7872968912124634\n",
      "          vf_loss: 0.2623264300585207\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.363513513513514\n",
      "    ram_util_percent: 61.00608108108109\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911288373736676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9203672568329715\n",
      "    mean_inference_ms: 1.33371654388616\n",
      "    mean_raw_obs_processing_ms: 9.803218329111438\n",
      "  time_since_restore: 4500.81888127327\n",
      "  time_this_iter_s: 103.29864859580994\n",
      "  time_total_s: 4500.81888127327\n",
      "  timers:\n",
      "    learn_throughput: 1523.658\n",
      "    learn_time_ms: 656.315\n",
      "    load_throughput: 86185.155\n",
      "    load_time_ms: 11.603\n",
      "    sample_throughput: 29.014\n",
      "    sample_time_ms: 34466.276\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1631985717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         4500.82</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\">    3.51</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            585.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-22-07\n",
      "  done: false\n",
      "  episode_len_mean: 585.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.51\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 262\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.129924609926012\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02179474124076049\n",
      "          policy_loss: 0.04383539110422134\n",
      "          total_loss: 0.1337899688217375\n",
      "          vf_explained_var: -0.24238643050193787\n",
      "          vf_loss: 0.10339885934841328\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.75714285714286\n",
      "    ram_util_percent: 61.00714285714287\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911216543777644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.920802405180826\n",
      "    mean_inference_ms: 1.3337092677851055\n",
      "    mean_raw_obs_processing_ms: 9.858738125919663\n",
      "  time_since_restore: 4510.84879732132\n",
      "  time_this_iter_s: 10.029916048049927\n",
      "  time_total_s: 4510.84879732132\n",
      "  timers:\n",
      "    learn_throughput: 1527.927\n",
      "    learn_time_ms: 654.481\n",
      "    load_throughput: 97689.396\n",
      "    load_time_ms: 10.237\n",
      "    sample_throughput: 30.475\n",
      "    sample_time_ms: 32813.473\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1631985727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         4510.85</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\">    3.51</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            585.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-22-16\n",
      "  done: false\n",
      "  episode_len_mean: 585.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.5\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 263\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.207993619971805\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00819208688011318\n",
      "          policy_loss: 0.18092907042139106\n",
      "          total_loss: 0.20946293990645143\n",
      "          vf_explained_var: 0.5497071743011475\n",
      "          vf_loss: 0.04618508404948645\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.66923076923077\n",
      "    ram_util_percent: 60.96153846153845\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911151696846303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9211828083953755\n",
      "    mean_inference_ms: 1.3337033628760222\n",
      "    mean_raw_obs_processing_ms: 9.914107314238167\n",
      "  time_since_restore: 4520.077937364578\n",
      "  time_this_iter_s: 9.229140043258667\n",
      "  time_total_s: 4520.077937364578\n",
      "  timers:\n",
      "    learn_throughput: 1525.374\n",
      "    learn_time_ms: 655.577\n",
      "    load_throughput: 97747.906\n",
      "    load_time_ms: 10.23\n",
      "    sample_throughput: 30.6\n",
      "    sample_time_ms: 32680.229\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1631985736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         4520.08</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\">     3.5</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            585.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-22-44\n",
      "  done: false\n",
      "  episode_len_mean: 575.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.64\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 265\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.077219041188558\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006925587311254983\n",
      "          policy_loss: 0.09331646172536744\n",
      "          total_loss: 0.11170255665977796\n",
      "          vf_explained_var: 0.5169402956962585\n",
      "          vf_loss: 0.03541424489683575\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.289743589743594\n",
      "    ram_util_percent: 60.64615384615385\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911039456552066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.921888579945209\n",
      "    mean_inference_ms: 1.333696807198846\n",
      "    mean_raw_obs_processing_ms: 10.026675649674946\n",
      "  time_since_restore: 4547.51030421257\n",
      "  time_this_iter_s: 27.432366847991943\n",
      "  time_total_s: 4547.51030421257\n",
      "  timers:\n",
      "    learn_throughput: 1519.118\n",
      "    learn_time_ms: 658.277\n",
      "    load_throughput: 86873.665\n",
      "    load_time_ms: 11.511\n",
      "    sample_throughput: 28.983\n",
      "    sample_time_ms: 34502.573\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1631985764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         4547.51</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">    3.64</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            575.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 575.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.61\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 266\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9864775737126668\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008373558479764023\n",
      "          policy_loss: 0.022558186741338835\n",
      "          total_loss: 0.08094161413609982\n",
      "          vf_explained_var: 0.29159313440322876\n",
      "          vf_loss: 0.07372137146174079\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.857142857142854\n",
      "    ram_util_percent: 61.321428571428555\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910996538128423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.922197249646397\n",
      "    mean_inference_ms: 1.3336959764408625\n",
      "    mean_raw_obs_processing_ms: 10.080502205062029\n",
      "  time_since_restore: 4557.137285709381\n",
      "  time_this_iter_s: 9.626981496810913\n",
      "  time_total_s: 4557.137285709381\n",
      "  timers:\n",
      "    learn_throughput: 1513.847\n",
      "    learn_time_ms: 660.569\n",
      "    load_throughput: 86902.104\n",
      "    load_time_ms: 11.507\n",
      "    sample_throughput: 28.959\n",
      "    sample_time_ms: 34531.898\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1631985773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         4557.14</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\">    3.61</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            575.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-24-08\n",
      "  done: false\n",
      "  episode_len_mean: 564.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.65\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 269\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2119108504719205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012475185451601808\n",
      "          policy_loss: -0.12273476057582432\n",
      "          total_loss: 0.14312263909313414\n",
      "          vf_explained_var: 0.28918394446372986\n",
      "          vf_loss: 0.281232296447787\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.561320754716977\n",
      "    ram_util_percent: 59.73867924528301\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910879586237666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9230537942365995\n",
      "    mean_inference_ms: 1.3336975951004695\n",
      "    mean_raw_obs_processing_ms: 10.244290826574602\n",
      "  time_since_restore: 4631.492479562759\n",
      "  time_this_iter_s: 74.3551938533783\n",
      "  time_total_s: 4631.492479562759\n",
      "  timers:\n",
      "    learn_throughput: 1509.581\n",
      "    learn_time_ms: 662.436\n",
      "    load_throughput: 87504.751\n",
      "    load_time_ms: 11.428\n",
      "    sample_throughput: 27.913\n",
      "    sample_time_ms: 35825.15\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1631985848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         4631.49</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\">    3.65</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            564.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-25-02\n",
      "  done: false\n",
      "  episode_len_mean: 562.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.66\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 272\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2783604939778646\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013672894119294288\n",
      "          policy_loss: -0.07458805955118603\n",
      "          total_loss: 0.26015892773866656\n",
      "          vf_explained_var: 0.5558165907859802\n",
      "          vf_loss: 0.3501388981938362\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.544871794871796\n",
      "    ram_util_percent: 58.47564102564105\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910783998891994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.923741801630476\n",
      "    mean_inference_ms: 1.3337017149211916\n",
      "    mean_raw_obs_processing_ms: 10.400599196571761\n",
      "  time_since_restore: 4686.044385671616\n",
      "  time_this_iter_s: 54.5519061088562\n",
      "  time_total_s: 4686.044385671616\n",
      "  timers:\n",
      "    learn_throughput: 1505.779\n",
      "    learn_time_ms: 664.108\n",
      "    load_throughput: 88395.86\n",
      "    load_time_ms: 11.313\n",
      "    sample_throughput: 26.127\n",
      "    sample_time_ms: 38274.304\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1631985902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         4686.04</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\">    3.66</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            562.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-25-14\n",
      "  done: false\n",
      "  episode_len_mean: 564.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.67\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 273\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.130074764622582\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010504879534900249\n",
      "          policy_loss: -0.012322091932098071\n",
      "          total_loss: 0.05519568705931306\n",
      "          vf_explained_var: 0.6274265050888062\n",
      "          vf_loss: 0.08313948707655072\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.7125\n",
      "    ram_util_percent: 58.4375\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910760410887257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9240062630297325\n",
      "    mean_inference_ms: 1.3337057082049393\n",
      "    mean_raw_obs_processing_ms: 10.450441752427883\n",
      "  time_since_restore: 4697.088217258453\n",
      "  time_this_iter_s: 11.043831586837769\n",
      "  time_total_s: 4697.088217258453\n",
      "  timers:\n",
      "    learn_throughput: 1507.566\n",
      "    learn_time_ms: 663.321\n",
      "    load_throughput: 99866.997\n",
      "    load_time_ms: 10.013\n",
      "    sample_throughput: 28.386\n",
      "    sample_time_ms: 35229.18\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1631985914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         4697.09</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">    3.67</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            564.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-26-02\n",
      "  done: false\n",
      "  episode_len_mean: 561.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.66\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 275\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.049455401632521\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017188288833770424\n",
      "          policy_loss: -0.007448598038819101\n",
      "          total_loss: 0.37887141381700834\n",
      "          vf_explained_var: 0.12696726620197296\n",
      "          vf_loss: 0.3975224108952615\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.900000000000002\n",
      "    ram_util_percent: 58.471014492753625\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910744133571445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.924605830696666\n",
      "    mean_inference_ms: 1.3337244897936398\n",
      "    mean_raw_obs_processing_ms: 10.549338932545586\n",
      "  time_since_restore: 4745.3329067230225\n",
      "  time_this_iter_s: 48.24468946456909\n",
      "  time_total_s: 4745.3329067230225\n",
      "  timers:\n",
      "    learn_throughput: 1495.67\n",
      "    learn_time_ms: 668.597\n",
      "    load_throughput: 88244.475\n",
      "    load_time_ms: 11.332\n",
      "    sample_throughput: 25.705\n",
      "    sample_time_ms: 38902.921\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1631985962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         4745.33</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\">    3.66</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            561.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 527.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.66\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 284\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.004732468393114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009575118247439472\n",
      "          policy_loss: -0.10403082304530674\n",
      "          total_loss: 0.16442310098144744\n",
      "          vf_explained_var: 0.20316599309444427\n",
      "          vf_loss: 0.28332484771187105\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.185777777777776\n",
      "    ram_util_percent: 59.46355555555555\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039108696790805524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.926807179343607\n",
      "    mean_inference_ms: 1.3338650325216117\n",
      "    mean_raw_obs_processing_ms: 11.000150391932811\n",
      "  time_since_restore: 4903.347083330154\n",
      "  time_this_iter_s: 158.01417660713196\n",
      "  time_total_s: 4903.347083330154\n",
      "  timers:\n",
      "    learn_throughput: 1495.857\n",
      "    learn_time_ms: 668.513\n",
      "    load_throughput: 87710.247\n",
      "    load_time_ms: 11.401\n",
      "    sample_throughput: 20.041\n",
      "    sample_time_ms: 49898.215\n",
      "    update_time_ms: 1.723\n",
      "  timestamp: 1631986120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         4903.35</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\">    3.66</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            527.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-30-06\n",
      "  done: false\n",
      "  episode_len_mean: 516.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.67\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 288\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5891621165805392\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009573723536428493\n",
      "          policy_loss: -0.0534142404794693\n",
      "          total_loss: 0.6710453248686261\n",
      "          vf_explained_var: 0.42722854018211365\n",
      "          vf_loss: 0.7351755420366923\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.969354838709677\n",
      "    ram_util_percent: 59.87258064516128\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039110160590517785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.927869212286907\n",
      "    mean_inference_ms: 1.3339508858740492\n",
      "    mean_raw_obs_processing_ms: 11.181932297318637\n",
      "  time_since_restore: 4989.920974254608\n",
      "  time_this_iter_s: 86.57389092445374\n",
      "  time_total_s: 4989.920974254608\n",
      "  timers:\n",
      "    learn_throughput: 1486.197\n",
      "    learn_time_ms: 672.858\n",
      "    load_throughput: 91615.896\n",
      "    load_time_ms: 10.915\n",
      "    sample_throughput: 20.737\n",
      "    sample_time_ms: 48221.868\n",
      "    update_time_ms: 1.726\n",
      "  timestamp: 1631986206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         4989.92</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\">    3.67</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            516.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-32-38\n",
      "  done: false\n",
      "  episode_len_mean: 485.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.73\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 296\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7568458689583673\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009470004092806643\n",
      "          policy_loss: 0.11183282261093458\n",
      "          total_loss: 0.4269035812881258\n",
      "          vf_explained_var: 0.22673867642879486\n",
      "          vf_loss: 0.32751963867081535\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.16805555555555\n",
      "    ram_util_percent: 60.24490740740742\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039114193106833124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.929775241890031\n",
      "    mean_inference_ms: 1.3341637651875988\n",
      "    mean_raw_obs_processing_ms: 11.562760629214955\n",
      "  time_since_restore: 5141.294044971466\n",
      "  time_this_iter_s: 151.3730707168579\n",
      "  time_total_s: 5141.294044971466\n",
      "  timers:\n",
      "    learn_throughput: 1489.243\n",
      "    learn_time_ms: 671.482\n",
      "    load_throughput: 81127.737\n",
      "    load_time_ms: 12.326\n",
      "    sample_throughput: 16.037\n",
      "    sample_time_ms: 62356.138\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1631986358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         5141.29</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">    3.73</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            485.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-34-04\n",
      "  done: false\n",
      "  episode_len_mean: 476.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.73\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 300\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.599537558025784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008066461301945773\n",
      "          policy_loss: -0.031399761140346524\n",
      "          total_loss: 0.3779536017941104\n",
      "          vf_explained_var: 0.5836997628211975\n",
      "          vf_loss: 0.42098793149408364\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.14552845528455\n",
      "    ram_util_percent: 59.890243902439025\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911696192183886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.930652126751482\n",
      "    mean_inference_ms: 1.334289453368596\n",
      "    mean_raw_obs_processing_ms: 11.740228695520214\n",
      "  time_since_restore: 5227.311024904251\n",
      "  time_this_iter_s: 86.01697993278503\n",
      "  time_total_s: 5227.311024904251\n",
      "  timers:\n",
      "    learn_throughput: 1480.123\n",
      "    learn_time_ms: 675.619\n",
      "    load_throughput: 74847.63\n",
      "    load_time_ms: 13.36\n",
      "    sample_throughput: 14.28\n",
      "    sample_time_ms: 70029.744\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1631986444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         5227.31</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">    3.73</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            476.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-36-25\n",
      "  done: false\n",
      "  episode_len_mean: 447.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.74\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 308\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.889686772558424\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011468235414933206\n",
      "          policy_loss: 0.018775958567857742\n",
      "          total_loss: 0.9072711825370788\n",
      "          vf_explained_var: 0.542256772518158\n",
      "          vf_loss: 0.9011922455496258\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.742574257425744\n",
      "    ram_util_percent: 60.153960396039615\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03912409217861715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.932572103331029\n",
      "    mean_inference_ms: 1.334579119564032\n",
      "    mean_raw_obs_processing_ms: 12.110703621423658\n",
      "  time_since_restore: 5368.854599952698\n",
      "  time_this_iter_s: 141.54357504844666\n",
      "  time_total_s: 5368.854599952698\n",
      "  timers:\n",
      "    learn_throughput: 1484.221\n",
      "    learn_time_ms: 673.754\n",
      "    load_throughput: 74350.877\n",
      "    load_time_ms: 13.45\n",
      "    sample_throughput: 12.279\n",
      "    sample_time_ms: 81442.604\n",
      "    update_time_ms: 1.714\n",
      "  timestamp: 1631986585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         5368.85</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\">    3.74</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            447.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-36-37\n",
      "  done: false\n",
      "  episode_len_mean: 450.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.7\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 309\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9913723429044088\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01132313459614543\n",
      "          policy_loss: 0.030882071952025095\n",
      "          total_loss: 0.15351998938454522\n",
      "          vf_explained_var: 0.16388773918151855\n",
      "          vf_loss: 0.13643024609320695\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.893750000000004\n",
      "    ram_util_percent: 60.987500000000004\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039125101468038846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.932898956147057\n",
      "    mean_inference_ms: 1.3346182410516831\n",
      "    mean_raw_obs_processing_ms: 12.153804839384026\n",
      "  time_since_restore: 5380.471367120743\n",
      "  time_this_iter_s: 11.616767168045044\n",
      "  time_total_s: 5380.471367120743\n",
      "  timers:\n",
      "    learn_throughput: 1490.337\n",
      "    learn_time_ms: 670.989\n",
      "    load_throughput: 74508.315\n",
      "    load_time_ms: 13.421\n",
      "    sample_throughput: 12.248\n",
      "    sample_time_ms: 81644.359\n",
      "    update_time_ms: 1.723\n",
      "  timestamp: 1631986597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         5380.47</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\">     3.7</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             450.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 420.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.78\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 314\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.028445283571879\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02290706198909834\n",
      "          policy_loss: -0.035162400123145844\n",
      "          total_loss: 0.7311061292886734\n",
      "          vf_explained_var: 0.3798457384109497\n",
      "          vf_loss: 0.7741691980097029\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.53518518518519\n",
      "    ram_util_percent: 62.02592592592592\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0391307186789772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.934956958939044\n",
      "    mean_inference_ms: 1.3348366560721205\n",
      "    mean_raw_obs_processing_ms: 12.383140558041418\n",
      "  time_since_restore: 5493.679281949997\n",
      "  time_this_iter_s: 113.20791482925415\n",
      "  time_total_s: 5493.679281949997\n",
      "  timers:\n",
      "    learn_throughput: 1492.567\n",
      "    learn_time_ms: 669.987\n",
      "    load_throughput: 73395.384\n",
      "    load_time_ms: 13.625\n",
      "    sample_throughput: 11.692\n",
      "    sample_time_ms: 85530.428\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1631986710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         5493.68</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">    3.78</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            420.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-39-38\n",
      "  done: false\n",
      "  episode_len_mean: 410.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.77\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 318\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9485778609911601\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0101652905012615\n",
      "          policy_loss: 0.12355755037731594\n",
      "          total_loss: 0.4367978495028284\n",
      "          vf_explained_var: 0.32721036672592163\n",
      "          vf_loss: 0.32448289969729055\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.706185567010316\n",
      "    ram_util_percent: 64.45463917525773\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913579236685427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9372665700922935\n",
      "    mean_inference_ms: 1.335031296789342\n",
      "    mean_raw_obs_processing_ms: 12.571262939601091\n",
      "  time_since_restore: 5561.587950706482\n",
      "  time_this_iter_s: 67.90866875648499\n",
      "  time_total_s: 5561.587950706482\n",
      "  timers:\n",
      "    learn_throughput: 1482.715\n",
      "    learn_time_ms: 674.438\n",
      "    load_throughput: 73796.867\n",
      "    load_time_ms: 13.551\n",
      "    sample_throughput: 11.513\n",
      "    sample_time_ms: 86861.667\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1631986778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         5561.59</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\">    3.77</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            410.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-43-21\n",
      "  done: false\n",
      "  episode_len_mean: 368.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 3.88\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 329\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9212625331348843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016550959875603885\n",
      "          policy_loss: 0.04711655076179239\n",
      "          total_loss: 0.7407066984309091\n",
      "          vf_explained_var: 0.46857982873916626\n",
      "          vf_loss: 0.6993813518020842\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.105660377358486\n",
      "    ram_util_percent: 65.5059748427673\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039150708983626804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9428203967587026\n",
      "    mean_inference_ms: 1.33559753731907\n",
      "    mean_raw_obs_processing_ms: 13.135787151471229\n",
      "  time_since_restore: 5784.346904993057\n",
      "  time_this_iter_s: 222.75895428657532\n",
      "  time_total_s: 5784.346904993057\n",
      "  timers:\n",
      "    learn_throughput: 1486.794\n",
      "    learn_time_ms: 672.588\n",
      "    load_throughput: 67490.482\n",
      "    load_time_ms: 14.817\n",
      "    sample_throughput: 9.256\n",
      "    sample_time_ms: 108033.756\n",
      "    update_time_ms: 1.748\n",
      "  timestamp: 1631987001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         5784.35</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\">    3.88</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            368.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 309.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 4.18\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 338\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9074920641051398\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008588387973760724\n",
      "          policy_loss: -0.05981851791342099\n",
      "          total_loss: 0.6974038812849257\n",
      "          vf_explained_var: 0.5798134803771973\n",
      "          vf_loss: 0.7693328685230679\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.6\n",
      "    ram_util_percent: 66.3234126984127\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03916104112325582\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.947108425465488\n",
      "    mean_inference_ms: 1.3359977846116804\n",
      "    mean_raw_obs_processing_ms: 13.613945069376056\n",
      "  time_since_restore: 5961.239267587662\n",
      "  time_this_iter_s: 176.8923625946045\n",
      "  time_total_s: 5961.239267587662\n",
      "  timers:\n",
      "    learn_throughput: 1500.106\n",
      "    learn_time_ms: 666.62\n",
      "    load_throughput: 66411.385\n",
      "    load_time_ms: 15.058\n",
      "    sample_throughput: 8.271\n",
      "    sample_time_ms: 120904.229\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1631987178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         5961.24</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\">    4.18</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             309.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 309.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 4.21\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 340\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.355715587404039\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005836548301087064\n",
      "          policy_loss: -0.01735704607433743\n",
      "          total_loss: 0.025700867941810025\n",
      "          vf_explained_var: 0.3180958926677704\n",
      "          vf_loss: 0.0618821293529537\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.183333333333334\n",
      "    ram_util_percent: 65.44047619047619\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039163139556767305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.948116864148428\n",
      "    mean_inference_ms: 1.336079640816067\n",
      "    mean_raw_obs_processing_ms: 13.721224741773746\n",
      "  time_since_restore: 5990.668121814728\n",
      "  time_this_iter_s: 29.42885422706604\n",
      "  time_total_s: 5990.668121814728\n",
      "  timers:\n",
      "    learn_throughput: 1501.647\n",
      "    learn_time_ms: 665.935\n",
      "    load_throughput: 66774.245\n",
      "    load_time_ms: 14.976\n",
      "    sample_throughput: 9.255\n",
      "    sample_time_ms: 108046.479\n",
      "    update_time_ms: 1.734\n",
      "  timestamp: 1631987207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         5990.67</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">    4.21</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            309.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-51-01\n",
      "  done: false\n",
      "  episode_len_mean: 225.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 4.61\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 353\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8499625113275315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009661372075146963\n",
      "          policy_loss: -0.09751475320922004\n",
      "          total_loss: 0.6662868464986483\n",
      "          vf_explained_var: 0.5030145645141602\n",
      "          vf_loss: 0.7744666821426816\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.313812154696134\n",
      "    ram_util_percent: 66.70524861878454\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0391765509800101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.956382157459829\n",
      "    mean_inference_ms: 1.336584564852293\n",
      "    mean_raw_obs_processing_ms: 14.547680837864231\n",
      "  time_since_restore: 6243.88362288475\n",
      "  time_this_iter_s: 253.21550107002258\n",
      "  time_total_s: 6243.88362288475\n",
      "  timers:\n",
      "    learn_throughput: 1508.542\n",
      "    learn_time_ms: 662.892\n",
      "    load_throughput: 64321.628\n",
      "    load_time_ms: 15.547\n",
      "    sample_throughput: 8.018\n",
      "    sample_time_ms: 124713.12\n",
      "    update_time_ms: 1.73\n",
      "  timestamp: 1631987461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         6243.88</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\">    4.61</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            225.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-54-22\n",
      "  done: false\n",
      "  episode_len_mean: 189.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 4.78\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 363\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8700449758105808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009782067041735914\n",
      "          policy_loss: -0.03558750715520647\n",
      "          total_loss: 0.649059945013788\n",
      "          vf_explained_var: 0.6287696957588196\n",
      "          vf_loss: 0.6954154802693261\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.007665505226477\n",
      "    ram_util_percent: 68.38257839721255\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039185709627891356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9623777463721375\n",
      "    mean_inference_ms: 1.336949027991218\n",
      "    mean_raw_obs_processing_ms: 15.207199949374331\n",
      "  time_since_restore: 6445.118587017059\n",
      "  time_this_iter_s: 201.23496413230896\n",
      "  time_total_s: 6445.118587017059\n",
      "  timers:\n",
      "    learn_throughput: 1503.562\n",
      "    learn_time_ms: 665.087\n",
      "    load_throughput: 66877.521\n",
      "    load_time_ms: 14.953\n",
      "    sample_throughput: 7.71\n",
      "    sample_time_ms: 129697.72\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1631987662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         6445.12</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\">    4.78</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            189.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_17-58-39\n",
      "  done: false\n",
      "  episode_len_mean: 135.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 4.96\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 376\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8517949355973138\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007263908439354339\n",
      "          policy_loss: -0.10228131744596693\n",
      "          total_loss: 0.7450110935502582\n",
      "          vf_explained_var: 0.5960050821304321\n",
      "          vf_loss: 0.8599199480480618\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.424523160762945\n",
      "    ram_util_percent: 68.37438692098092\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039196683398922245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.972342407876413\n",
      "    mean_inference_ms: 1.3373939828431478\n",
      "    mean_raw_obs_processing_ms: 16.15552115632123\n",
      "  time_since_restore: 6702.44019651413\n",
      "  time_this_iter_s: 257.3216094970703\n",
      "  time_total_s: 6702.44019651413\n",
      "  timers:\n",
      "    learn_throughput: 1504.32\n",
      "    learn_time_ms: 664.752\n",
      "    load_throughput: 65386.632\n",
      "    load_time_ms: 15.294\n",
      "    sample_throughput: 6.811\n",
      "    sample_time_ms: 146828.194\n",
      "    update_time_ms: 1.726\n",
      "  timestamp: 1631987919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         6702.44</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\">    4.96</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            135.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_18-01-40\n",
      "  done: false\n",
      "  episode_len_mean: 138.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 4.85\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 385\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8913516892327202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009724669212258293\n",
      "          policy_loss: 0.005155025463965204\n",
      "          total_loss: 0.6634866942962011\n",
      "          vf_explained_var: 0.5824788212776184\n",
      "          vf_loss: 0.6693593174219131\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.825868725868727\n",
      "    ram_util_percent: 68.76949806949807\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039203122765518156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.979426608657331\n",
      "    mean_inference_ms: 1.337660653813532\n",
      "    mean_raw_obs_processing_ms: 16.800173930721677\n",
      "  time_since_restore: 6883.633941888809\n",
      "  time_this_iter_s: 181.19374537467957\n",
      "  time_total_s: 6883.633941888809\n",
      "  timers:\n",
      "    learn_throughput: 1506.773\n",
      "    learn_time_ms: 663.67\n",
      "    load_throughput: 65078.417\n",
      "    load_time_ms: 15.366\n",
      "    sample_throughput: 6.632\n",
      "    sample_time_ms: 150794.226\n",
      "    update_time_ms: 1.715\n",
      "  timestamp: 1631988100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         6883.63</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">    4.85</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             138.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_6a34a_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_18-02-26\n",
      "  done: false\n",
      "  episode_len_mean: 145.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 4.79\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 388\n",
      "  experiment_id: 5962629f4db34f84b72a94ad1dbae226\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.078606551223331\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01039004403833337\n",
      "          policy_loss: 0.06413943337069618\n",
      "          total_loss: 0.19062571502808068\n",
      "          vf_explained_var: 0.3345493674278259\n",
      "          vf_loss: 0.13884690875808398\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.795384615384616\n",
      "    ram_util_percent: 69.10461538461539\n",
      "  pid: 168620\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03920495106170476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 7.9816611175014405\n",
      "    mean_inference_ms: 1.3377415196516085\n",
      "    mean_raw_obs_processing_ms: 17.009363962425788\n",
      "  time_since_restore: 6929.534131765366\n",
      "  time_this_iter_s: 45.9001898765564\n",
      "  time_total_s: 6929.534131765366\n",
      "  timers:\n",
      "    learn_throughput: 1510.426\n",
      "    learn_time_ms: 662.065\n",
      "    load_throughput: 58912.984\n",
      "    load_time_ms: 16.974\n",
      "    sample_throughput: 6.484\n",
      "    sample_time_ms: 154222.561\n",
      "    update_time_ms: 1.705\n",
      "  timestamp: 1631988146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: 6a34a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/7.23 GiB heap, 0.0/3.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_16-06-42<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_6a34a_00000</td><td>RUNNING </td><td>192.168.3.5:168620</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         6929.53</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\">    4.79</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            145.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=168619)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C17 pretrained (AnnaCNN)\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
