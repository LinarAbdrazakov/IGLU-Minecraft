{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C17']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-09-20 09:47:46,202\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-09-20 09:47:46,218\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id c2935_00000 but id cede2_00000 is set.\n",
      "\u001b[2m\u001b[36m(pid=526121)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526121)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C17 pretrained (AnnaCNN)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/cede2_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/cede2_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20210920_094746-cede2_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526121)\u001b[0m 2021-09-20 09:47:50,205\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=526121)\u001b[0m 2021-09-20 09:47:50,206\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=526121)\u001b[0m 2021-09-20 09:47:57,161\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-48-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5858139713605246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004197614454803953\n",
      "          policy_loss: -0.15425816666748787\n",
      "          total_loss: -0.14535930887278584\n",
      "          vf_explained_var: -0.21579505503177643\n",
      "          vf_loss: 0.023917475492796963\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.642857142857146\n",
      "    ram_util_percent: 75.05357142857142\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0437773191011869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 56.134488080050446\n",
      "    mean_inference_ms: 1.5534123221596519\n",
      "    mean_raw_obs_processing_ms: 0.15953370741197279\n",
      "  time_since_restore: 58.651225090026855\n",
      "  time_this_iter_s: 58.651225090026855\n",
      "  time_total_s: 58.651225090026855\n",
      "  timers:\n",
      "    learn_throughput: 1625.185\n",
      "    learn_time_ms: 615.314\n",
      "    load_throughput: 87259.534\n",
      "    load_time_ms: 11.46\n",
      "    sample_throughput: 17.236\n",
      "    sample_time_ms: 58017.891\n",
      "    update_time_ms: 2.578\n",
      "  timestamp: 1632131335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         58.6512</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-49-06\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6194965256585014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005532206874099662\n",
      "          policy_loss: -0.17034249471293555\n",
      "          total_loss: -0.17103961118393474\n",
      "          vf_explained_var: 0.03682316839694977\n",
      "          vf_loss: 0.014944628553671969\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.131249999999994\n",
      "    ram_util_percent: 82.03125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04339756604705225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.257245374292324\n",
      "    mean_inference_ms: 1.5341558506751833\n",
      "    mean_raw_obs_processing_ms: 0.15309826051234884\n",
      "  time_since_restore: 69.5630292892456\n",
      "  time_this_iter_s: 10.91180419921875\n",
      "  time_total_s: 69.5630292892456\n",
      "  timers:\n",
      "    learn_throughput: 1653.843\n",
      "    learn_time_ms: 604.652\n",
      "    load_throughput: 137218.982\n",
      "    load_time_ms: 7.288\n",
      "    sample_throughput: 29.271\n",
      "    sample_time_ms: 34163.664\n",
      "    update_time_ms: 2.38\n",
      "  timestamp: 1632131346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          69.563</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-49-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5953680872917175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014964428368584571\n",
      "          policy_loss: -0.17718921684556538\n",
      "          total_loss: -0.18015459477901458\n",
      "          vf_explained_var: -0.05769778788089752\n",
      "          vf_loss: 0.011491860014696915\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.65333333333332\n",
      "    ram_util_percent: 82.13333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04307234496042317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.627950517515536\n",
      "    mean_inference_ms: 1.5190291073354383\n",
      "    mean_raw_obs_processing_ms: 0.14829718138561296\n",
      "  time_since_restore: 80.13741540908813\n",
      "  time_this_iter_s: 10.57438611984253\n",
      "  time_total_s: 80.13741540908813\n",
      "  timers:\n",
      "    learn_throughput: 1660.369\n",
      "    learn_time_ms: 602.276\n",
      "    load_throughput: 169175.186\n",
      "    load_time_ms: 5.911\n",
      "    sample_throughput: 38.316\n",
      "    sample_time_ms: 26098.82\n",
      "    update_time_ms: 2.14\n",
      "  timestamp: 1632131357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         80.1374</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-49-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5541441228654649\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00809386123389992\n",
      "          policy_loss: -0.17818717277712293\n",
      "          total_loss: -0.18522295413745773\n",
      "          vf_explained_var: -0.12410946190357208\n",
      "          vf_loss: 0.007696273977247378\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.08\n",
      "    ram_util_percent: 81.89999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04280787532982162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.30389168880296\n",
      "    mean_inference_ms: 1.5068241183167328\n",
      "    mean_raw_obs_processing_ms: 0.14474153791711092\n",
      "  time_since_restore: 90.55908060073853\n",
      "  time_this_iter_s: 10.42166519165039\n",
      "  time_total_s: 90.55908060073853\n",
      "  timers:\n",
      "    learn_throughput: 1665.981\n",
      "    learn_time_ms: 600.247\n",
      "    load_throughput: 191332.892\n",
      "    load_time_ms: 5.226\n",
      "    sample_throughput: 45.395\n",
      "    sample_time_ms: 22029.007\n",
      "    update_time_ms: 2.028\n",
      "  timestamp: 1632131367\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         90.5591</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-49-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.558564551671346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009473096462409892\n",
      "          policy_loss: -0.1829755362537172\n",
      "          total_loss: -0.1923202234837744\n",
      "          vf_explained_var: 0.08727315068244934\n",
      "          vf_loss: 0.005293649841203458\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.553333333333335\n",
      "    ram_util_percent: 81.70666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04258900542582783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.22145049428267\n",
      "    mean_inference_ms: 1.4971528175210784\n",
      "    mean_raw_obs_processing_ms: 0.1421396894811903\n",
      "  time_since_restore: 100.87978982925415\n",
      "  time_this_iter_s: 10.320709228515625\n",
      "  time_total_s: 100.87978982925415\n",
      "  timers:\n",
      "    learn_throughput: 1676.851\n",
      "    learn_time_ms: 596.356\n",
      "    load_throughput: 207499.11\n",
      "    load_time_ms: 4.819\n",
      "    sample_throughput: 51.1\n",
      "    sample_time_ms: 19569.623\n",
      "    update_time_ms: 1.963\n",
      "  timestamp: 1632131378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          100.88</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-49-48\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5372863544358149\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013453825122942121\n",
      "          policy_loss: -0.18222500185171764\n",
      "          total_loss: -0.1923919356531567\n",
      "          vf_explained_var: -0.5111744999885559\n",
      "          vf_loss: 0.0038605467028295\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.77333333333333\n",
      "    ram_util_percent: 81.55333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0424094793603996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.89838572811753\n",
      "    mean_inference_ms: 1.4893403889493755\n",
      "    mean_raw_obs_processing_ms: 0.14017054387327751\n",
      "  time_since_restore: 111.32244229316711\n",
      "  time_this_iter_s: 10.442652463912964\n",
      "  time_total_s: 111.32244229316711\n",
      "  timers:\n",
      "    learn_throughput: 1677.704\n",
      "    learn_time_ms: 596.053\n",
      "    load_throughput: 219950.217\n",
      "    load_time_ms: 4.546\n",
      "    sample_throughput: 55.716\n",
      "    sample_time_ms: 17948.074\n",
      "    update_time_ms: 1.918\n",
      "  timestamp: 1632131388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         111.322</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5287842843267652\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006990541210717617\n",
      "          policy_loss: -0.1392275402115451\n",
      "          total_loss: -0.1504151001572609\n",
      "          vf_explained_var: -0.24355006217956543\n",
      "          vf_loss: 0.003401228506118059\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.185714285714276\n",
      "    ram_util_percent: 81.45000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0422575232340878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.073697866633193\n",
      "    mean_inference_ms: 1.4828461659470802\n",
      "    mean_raw_obs_processing_ms: 0.1385703365370382\n",
      "  time_since_restore: 121.73963165283203\n",
      "  time_this_iter_s: 10.417189359664917\n",
      "  time_total_s: 121.73963165283203\n",
      "  timers:\n",
      "    learn_throughput: 1677.21\n",
      "    learn_time_ms: 596.228\n",
      "    load_throughput: 230252.037\n",
      "    load_time_ms: 4.343\n",
      "    sample_throughput: 59.574\n",
      "    sample_time_ms: 16785.793\n",
      "    update_time_ms: 1.902\n",
      "  timestamp: 1632131399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">          121.74</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-50-09\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5058212187555102\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006159087489985361\n",
      "          policy_loss: -0.17018505103058285\n",
      "          total_loss: -0.18209755222002666\n",
      "          vf_explained_var: 0.13511773943901062\n",
      "          vf_loss: 0.0025298025853569724\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.43333333333332\n",
      "    ram_util_percent: 81.40000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04212806907217383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.595533323801313\n",
      "    mean_inference_ms: 1.4773740880079047\n",
      "    mean_raw_obs_processing_ms: 0.13721678314150385\n",
      "  time_since_restore: 132.0371241569519\n",
      "  time_this_iter_s: 10.297492504119873\n",
      "  time_total_s: 132.0371241569519\n",
      "  timers:\n",
      "    learn_throughput: 1679.905\n",
      "    learn_time_ms: 595.272\n",
      "    load_throughput: 238884.489\n",
      "    load_time_ms: 4.186\n",
      "    sample_throughput: 62.892\n",
      "    sample_time_ms: 15900.218\n",
      "    update_time_ms: 1.879\n",
      "  timestamp: 1632131409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         132.037</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-50-19\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.484717889626821\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008099556977441925\n",
      "          policy_loss: -0.16359848495986726\n",
      "          total_loss: -0.1758522735701667\n",
      "          vf_explained_var: -0.7904955744743347\n",
      "          vf_loss: 0.0017834352219425556\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.393333333333324\n",
      "    ram_util_percent: 81.30666666666669\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04201443240222834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.37013483299252\n",
      "    mean_inference_ms: 1.4726699381441946\n",
      "    mean_raw_obs_processing_ms: 0.13607281704594992\n",
      "  time_since_restore: 142.3505003452301\n",
      "  time_this_iter_s: 10.313376188278198\n",
      "  time_total_s: 142.3505003452301\n",
      "  timers:\n",
      "    learn_throughput: 1678.977\n",
      "    learn_time_ms: 595.601\n",
      "    load_throughput: 244679.967\n",
      "    load_time_ms: 4.087\n",
      "    sample_throughput: 65.737\n",
      "    sample_time_ms: 15212.093\n",
      "    update_time_ms: 1.864\n",
      "  timestamp: 1632131419\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         142.351</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-50-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.377723713715871\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007246564249931511\n",
      "          policy_loss: -0.15296834210554758\n",
      "          total_loss: -0.16443105770481958\n",
      "          vf_explained_var: -0.3074095547199249\n",
      "          vf_loss: 0.0015898620877932343\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.39333333333333\n",
      "    ram_util_percent: 81.33999999999997\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04191386500886906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.334934167774403\n",
      "    mean_inference_ms: 1.4685534070392414\n",
      "    mean_raw_obs_processing_ms: 0.1350751294844797\n",
      "  time_since_restore: 152.61859846115112\n",
      "  time_this_iter_s: 10.26809811592102\n",
      "  time_total_s: 152.61859846115112\n",
      "  timers:\n",
      "    learn_throughput: 1679.196\n",
      "    learn_time_ms: 595.523\n",
      "    load_throughput: 250708.556\n",
      "    load_time_ms: 3.989\n",
      "    sample_throughput: 68.225\n",
      "    sample_time_ms: 14657.421\n",
      "    update_time_ms: 1.867\n",
      "  timestamp: 1632131429\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         152.619</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-50-40\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3611518541971843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0077571270873462855\n",
      "          policy_loss: -0.1761245269742277\n",
      "          total_loss: -0.18774964983264605\n",
      "          vf_explained_var: -0.7157691717147827\n",
      "          vf_loss: 0.0012106834471018778\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.971428571428575\n",
      "    ram_util_percent: 81.27142857142857\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04182431208216469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.446994995895334\n",
      "    mean_inference_ms: 1.4649237938768542\n",
      "    mean_raw_obs_processing_ms: 0.1342469341760347\n",
      "  time_since_restore: 162.89225220680237\n",
      "  time_this_iter_s: 10.273653745651245\n",
      "  time_total_s: 162.89225220680237\n",
      "  timers:\n",
      "    learn_throughput: 1687.049\n",
      "    learn_time_ms: 592.751\n",
      "    load_throughput: 314611.34\n",
      "    load_time_ms: 3.179\n",
      "    sample_throughput: 101.797\n",
      "    sample_time_ms: 9823.441\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1632131440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         162.892</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-50-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3046043621169197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007648460003856725\n",
      "          policy_loss: -0.15459945239126682\n",
      "          total_loss: -0.16573272819320362\n",
      "          vf_explained_var: -0.6591548323631287\n",
      "          vf_loss: 0.0011479204831024012\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17333333333333\n",
      "    ram_util_percent: 81.26666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041759345159250984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.67610608636532\n",
      "    mean_inference_ms: 1.4617112883222576\n",
      "    mean_raw_obs_processing_ms: 0.13350687419753302\n",
      "  time_since_restore: 173.19957542419434\n",
      "  time_this_iter_s: 10.307323217391968\n",
      "  time_total_s: 173.19957542419434\n",
      "  timers:\n",
      "    learn_throughput: 1686.435\n",
      "    learn_time_ms: 592.967\n",
      "    load_throughput: 313618.615\n",
      "    load_time_ms: 3.189\n",
      "    sample_throughput: 102.429\n",
      "    sample_time_ms: 9762.823\n",
      "    update_time_ms: 1.742\n",
      "  timestamp: 1632131450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">           173.2</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3304601073265077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008105187429128548\n",
      "          policy_loss: -0.14746565032336448\n",
      "          total_loss: -0.1583812117576599\n",
      "          vf_explained_var: -0.17959171533584595\n",
      "          vf_loss: 0.0015785192103875388\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.36666666666667\n",
      "    ram_util_percent: 81.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04169997334109747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.99975450784356\n",
      "    mean_inference_ms: 1.4588608263535752\n",
      "    mean_raw_obs_processing_ms: 0.13285235637710993\n",
      "  time_since_restore: 183.55167150497437\n",
      "  time_this_iter_s: 10.35209608078003\n",
      "  time_total_s: 183.55167150497437\n",
      "  timers:\n",
      "    learn_throughput: 1686.726\n",
      "    learn_time_ms: 592.864\n",
      "    load_throughput: 312933.031\n",
      "    load_time_ms: 3.196\n",
      "    sample_throughput: 102.662\n",
      "    sample_time_ms: 9740.663\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632131460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         183.552</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-51-11\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3100615488158331\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00881421043609943\n",
      "          policy_loss: -0.1216636001235909\n",
      "          total_loss: -0.13288540807035235\n",
      "          vf_explained_var: -0.9628700017929077\n",
      "          vf_loss: 0.000997386726602498\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.13333333333334\n",
      "    ram_util_percent: 81.39333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0416448940538342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.400808658519246\n",
      "    mean_inference_ms: 1.456292544577145\n",
      "    mean_raw_obs_processing_ms: 0.13226247723170365\n",
      "  time_since_restore: 193.8845238685608\n",
      "  time_this_iter_s: 10.332852363586426\n",
      "  time_total_s: 193.8845238685608\n",
      "  timers:\n",
      "    learn_throughput: 1686.603\n",
      "    learn_time_ms: 592.908\n",
      "    load_throughput: 312979.733\n",
      "    load_time_ms: 3.195\n",
      "    sample_throughput: 102.783\n",
      "    sample_time_ms: 9729.23\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632131471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         193.885</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-51-21\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2972268144289651\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007344561188831338\n",
      "          policy_loss: -0.10320278315080536\n",
      "          total_loss: -0.11443943857318825\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0010011569525684334\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.628571428571426\n",
      "    ram_util_percent: 81.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159434859774881\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.86597811197509\n",
      "    mean_inference_ms: 1.45395885585814\n",
      "    mean_raw_obs_processing_ms: 0.1317365330315778\n",
      "  time_since_restore: 204.1348466873169\n",
      "  time_this_iter_s: 10.250322818756104\n",
      "  time_total_s: 204.1348466873169\n",
      "  timers:\n",
      "    learn_throughput: 1686.223\n",
      "    learn_time_ms: 593.041\n",
      "    load_throughput: 312415.57\n",
      "    load_time_ms: 3.201\n",
      "    sample_throughput: 102.859\n",
      "    sample_time_ms: 9722.066\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632131481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         204.135</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-51-31\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3224179718229505\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073289659866219775\n",
      "          policy_loss: -0.11039118203851912\n",
      "          total_loss: -0.12232004660699103\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005624193791946811\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.153333333333336\n",
      "    ram_util_percent: 81.51333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04154759079845978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.385108039853634\n",
      "    mean_inference_ms: 1.4518283544301789\n",
      "    mean_raw_obs_processing_ms: 0.13126455556277117\n",
      "  time_since_restore: 214.40530943870544\n",
      "  time_this_iter_s: 10.27046275138855\n",
      "  time_total_s: 214.40530943870544\n",
      "  timers:\n",
      "    learn_throughput: 1685.964\n",
      "    learn_time_ms: 593.132\n",
      "    load_throughput: 313091.875\n",
      "    load_time_ms: 3.194\n",
      "    sample_throughput: 103.042\n",
      "    sample_time_ms: 9704.741\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632131491\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         214.405</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-51-42\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2379758384492663\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018535920275143968\n",
      "          policy_loss: -0.054843286797404286\n",
      "          total_loss: -0.06397066466096375\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0013987879213851152\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.180000000000014\n",
      "    ram_util_percent: 81.51333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041504220877016974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.950040213544863\n",
      "    mean_inference_ms: 1.4498694687771185\n",
      "    mean_raw_obs_processing_ms: 0.13083524404646404\n",
      "  time_since_restore: 224.65296387672424\n",
      "  time_this_iter_s: 10.247654438018799\n",
      "  time_total_s: 224.65296387672424\n",
      "  timers:\n",
      "    learn_throughput: 1687.74\n",
      "    learn_time_ms: 592.508\n",
      "    load_throughput: 312069.225\n",
      "    load_time_ms: 3.204\n",
      "    sample_throughput: 103.216\n",
      "    sample_time_ms: 9688.381\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1632131502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         224.653</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-51-52\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3695458081033496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014309020405686202\n",
      "          policy_loss: -0.11366067880557643\n",
      "          total_loss: -0.12525681315196885\n",
      "          vf_explained_var: -0.7251760959625244\n",
      "          vf_loss: 0.0006684204256291398\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.392857142857146\n",
      "    ram_util_percent: 81.6\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04146400922964602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.554280844781875\n",
      "    mean_inference_ms: 1.4480654756145364\n",
      "    mean_raw_obs_processing_ms: 0.13043628085992443\n",
      "  time_since_restore: 234.90109419822693\n",
      "  time_this_iter_s: 10.248130321502686\n",
      "  time_total_s: 234.90109419822693\n",
      "  timers:\n",
      "    learn_throughput: 1687.262\n",
      "    learn_time_ms: 592.676\n",
      "    load_throughput: 311946.213\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 103.274\n",
      "    sample_time_ms: 9683.024\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632131512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         234.901</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-52-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4799692114194234\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04336783929839088\n",
      "          policy_loss: -0.013127398449513647\n",
      "          total_loss: -0.02151659114493264\n",
      "          vf_explained_var: -0.6003594398498535\n",
      "          vf_loss: 0.0020737138017365296\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.76666666666667\n",
      "    ram_util_percent: 81.59999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04142739148424557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.192645945350286\n",
      "    mean_inference_ms: 1.4464872328754792\n",
      "    mean_raw_obs_processing_ms: 0.13007425042701423\n",
      "  time_since_restore: 245.23926258087158\n",
      "  time_this_iter_s: 10.338168382644653\n",
      "  time_total_s: 245.23926258087158\n",
      "  timers:\n",
      "    learn_throughput: 1686.126\n",
      "    learn_time_ms: 593.075\n",
      "    load_throughput: 311587.017\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 103.251\n",
      "    sample_time_ms: 9685.097\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1632131522\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         245.239</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-52-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6129278169737922\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011256609710478097\n",
      "          policy_loss: -0.014552241341314382\n",
      "          total_loss: -0.027912587672472\n",
      "          vf_explained_var: -0.8331255316734314\n",
      "          vf_loss: 0.0010804388284062346\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.379999999999995\n",
      "    ram_util_percent: 81.58666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04139352508210064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.860777420073312\n",
      "    mean_inference_ms: 1.445044289242714\n",
      "    mean_raw_obs_processing_ms: 0.12973740115998159\n",
      "  time_since_restore: 255.5611870288849\n",
      "  time_this_iter_s: 10.321924448013306\n",
      "  time_total_s: 255.5611870288849\n",
      "  timers:\n",
      "    learn_throughput: 1687.116\n",
      "    learn_time_ms: 592.728\n",
      "    load_throughput: 309430.833\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 103.19\n",
      "    sample_time_ms: 9690.819\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1632131533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         255.561</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-52-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6792548060417176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019762085718053966\n",
      "          policy_loss: 0.10429486487474707\n",
      "          total_loss: 0.09195923660364416\n",
      "          vf_explained_var: -0.8810697793960571\n",
      "          vf_loss: 0.0014926063026198082\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.614285714285714\n",
      "    ram_util_percent: 81.47857142857143\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04136190763392971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.55481504476479\n",
      "    mean_inference_ms: 1.4437017928243838\n",
      "    mean_raw_obs_processing_ms: 0.1294263633767446\n",
      "  time_since_restore: 265.79865860939026\n",
      "  time_this_iter_s: 10.237471580505371\n",
      "  time_total_s: 265.79865860939026\n",
      "  timers:\n",
      "    learn_throughput: 1684.537\n",
      "    learn_time_ms: 593.635\n",
      "    load_throughput: 311044.006\n",
      "    load_time_ms: 3.215\n",
      "    sample_throughput: 103.239\n",
      "    sample_time_ms: 9686.289\n",
      "    update_time_ms: 1.74\n",
      "  timestamp: 1632131543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         265.799</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-52-33\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7282009522120159\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0097559818691694\n",
      "          policy_loss: -0.07785210692220264\n",
      "          total_loss: -0.09244877863675356\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00122194182250597\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.34\n",
      "    ram_util_percent: 81.48\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04133213148889915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.271680621782856\n",
      "    mean_inference_ms: 1.442450659619799\n",
      "    mean_raw_obs_processing_ms: 0.12913845935320004\n",
      "  time_since_restore: 276.0311670303345\n",
      "  time_this_iter_s: 10.232508420944214\n",
      "  time_total_s: 276.0311670303345\n",
      "  timers:\n",
      "    learn_throughput: 1684.367\n",
      "    learn_time_ms: 593.695\n",
      "    load_throughput: 312229.517\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 103.319\n",
      "    sample_time_ms: 9678.735\n",
      "    update_time_ms: 1.742\n",
      "  timestamp: 1632131553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         276.031</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-52-44\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.558189825216929\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02685017134655191\n",
      "          policy_loss: -0.07684699048598607\n",
      "          total_loss: -0.08119069147441123\n",
      "          vf_explained_var: 0.11566727608442307\n",
      "          vf_loss: 0.007210669889011317\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.19333333333334\n",
      "    ram_util_percent: 81.47333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130396217422958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.00931197037966\n",
      "    mean_inference_ms: 1.4412789665265149\n",
      "    mean_raw_obs_processing_ms: 0.12887032959071235\n",
      "  time_since_restore: 286.4996585845947\n",
      "  time_this_iter_s: 10.468491554260254\n",
      "  time_total_s: 286.4996585845947\n",
      "  timers:\n",
      "    learn_throughput: 1687.751\n",
      "    learn_time_ms: 592.504\n",
      "    load_throughput: 312751.025\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 103.183\n",
      "    sample_time_ms: 9691.565\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1632131564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">           286.5</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-52-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1652720285786522\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006185225843367764\n",
      "          policy_loss: -0.020026194718148975\n",
      "          total_loss: -0.026667617426978218\n",
      "          vf_explained_var: 0.2713870406150818\n",
      "          vf_loss: 0.003619621383647124\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.346666666666664\n",
      "    ram_util_percent: 81.42000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041277333165410234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.765456435119994\n",
      "    mean_inference_ms: 1.4401809720381935\n",
      "    mean_raw_obs_processing_ms: 0.1286223321766776\n",
      "  time_since_restore: 297.00254583358765\n",
      "  time_this_iter_s: 10.50288724899292\n",
      "  time_total_s: 297.00254583358765\n",
      "  timers:\n",
      "    learn_throughput: 1688.131\n",
      "    learn_time_ms: 592.371\n",
      "    load_throughput: 313707.751\n",
      "    load_time_ms: 3.188\n",
      "    sample_throughput: 102.974\n",
      "    sample_time_ms: 9711.206\n",
      "    update_time_ms: 1.741\n",
      "  timestamp: 1632131574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         297.003</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-53-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1434491879410213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018276799798108218\n",
      "          policy_loss: -0.16050905519061617\n",
      "          total_loss: -0.16571457493636343\n",
      "          vf_explained_var: -0.43983814120292664\n",
      "          vf_loss: 0.0021166889475555057\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.67333333333333\n",
      "    ram_util_percent: 81.40000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125261063601272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.537835204806765\n",
      "    mean_inference_ms: 1.4391488100888912\n",
      "    mean_raw_obs_processing_ms: 0.12839158138946224\n",
      "  time_since_restore: 307.3005425930023\n",
      "  time_this_iter_s: 10.297996759414673\n",
      "  time_total_s: 307.3005425930023\n",
      "  timers:\n",
      "    learn_throughput: 1686.45\n",
      "    learn_time_ms: 592.962\n",
      "    load_throughput: 313243.863\n",
      "    load_time_ms: 3.192\n",
      "    sample_throughput: 102.93\n",
      "    sample_time_ms: 9715.359\n",
      "    update_time_ms: 1.741\n",
      "  timestamp: 1632131584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         307.301</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-53-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.558248237768809\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008950635552895865\n",
      "          policy_loss: -0.12198018175032403\n",
      "          total_loss: -0.13236192560030355\n",
      "          vf_explained_var: -0.09826100617647171\n",
      "          vf_loss: 0.0031868456203180055\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.278571428571425\n",
      "    ram_util_percent: 81.30714285714285\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041229038871693165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.324702170550836\n",
      "    mean_inference_ms: 1.4381722953373801\n",
      "    mean_raw_obs_processing_ms: 0.12818367924249283\n",
      "  time_since_restore: 317.5252220630646\n",
      "  time_this_iter_s: 10.224679470062256\n",
      "  time_total_s: 317.5252220630646\n",
      "  timers:\n",
      "    learn_throughput: 1687.758\n",
      "    learn_time_ms: 592.502\n",
      "    load_throughput: 313403.023\n",
      "    load_time_ms: 3.191\n",
      "    sample_throughput: 102.973\n",
      "    sample_time_ms: 9711.249\n",
      "    update_time_ms: 1.741\n",
      "  timestamp: 1632131595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         317.525</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-53-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8574481328328452\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019499372812585097\n",
      "          policy_loss: -0.04181600643528832\n",
      "          total_loss: -0.05343319045172797\n",
      "          vf_explained_var: -0.47997915744781494\n",
      "          vf_loss: 0.0025699409720901815\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.186666666666675\n",
      "    ram_util_percent: 81.05999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04120652068209976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.124740046121541\n",
      "    mean_inference_ms: 1.4372462140782392\n",
      "    mean_raw_obs_processing_ms: 0.12799166447434146\n",
      "  time_since_restore: 327.808308839798\n",
      "  time_this_iter_s: 10.283086776733398\n",
      "  time_total_s: 327.808308839798\n",
      "  timers:\n",
      "    learn_throughput: 1686.287\n",
      "    learn_time_ms: 593.019\n",
      "    load_throughput: 312336.471\n",
      "    load_time_ms: 3.202\n",
      "    sample_throughput: 102.941\n",
      "    sample_time_ms: 9714.269\n",
      "    update_time_ms: 1.739\n",
      "  timestamp: 1632131605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         327.808</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-53-36\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8990823533799914\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013380723162871967\n",
      "          policy_loss: -0.11116128423147731\n",
      "          total_loss: -0.12414450504713588\n",
      "          vf_explained_var: -0.39647725224494934\n",
      "          vf_loss: 0.002996941313095805\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.086666666666666\n",
      "    ram_util_percent: 81.03333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041185327815652185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.936980515009912\n",
      "    mean_inference_ms: 1.4363753542652995\n",
      "    mean_raw_obs_processing_ms: 0.12780754517301135\n",
      "  time_since_restore: 338.292818069458\n",
      "  time_this_iter_s: 10.484509229660034\n",
      "  time_total_s: 338.292818069458\n",
      "  timers:\n",
      "    learn_throughput: 1686.562\n",
      "    learn_time_ms: 592.922\n",
      "    load_throughput: 309364.646\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 102.688\n",
      "    sample_time_ms: 9738.217\n",
      "    update_time_ms: 1.736\n",
      "  timestamp: 1632131616\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         338.293</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-53-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7276380817095438\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006425732589198368\n",
      "          policy_loss: -0.1215537486390935\n",
      "          total_loss: -0.13505873940885066\n",
      "          vf_explained_var: -0.9283248782157898\n",
      "          vf_loss: 0.0023255990190793655\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.14666666666666\n",
      "    ram_util_percent: 80.98\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04116542564935098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.760241628813441\n",
      "    mean_inference_ms: 1.435549976041479\n",
      "    mean_raw_obs_processing_ms: 0.1276346031768812\n",
      "  time_since_restore: 348.71980023384094\n",
      "  time_this_iter_s: 10.426982164382935\n",
      "  time_total_s: 348.71980023384094\n",
      "  timers:\n",
      "    learn_throughput: 1692.225\n",
      "    learn_time_ms: 590.938\n",
      "    load_throughput: 311103.991\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 102.574\n",
      "    sample_time_ms: 9749.094\n",
      "    update_time_ms: 1.733\n",
      "  timestamp: 1632131626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">          348.72</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-54-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8726647324032253\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012440836422607592\n",
      "          policy_loss: -0.1212104214148389\n",
      "          total_loss: -0.13203075886186627\n",
      "          vf_explained_var: -0.855684757232666\n",
      "          vf_loss: 0.005107120186504391\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.357142857142854\n",
      "    ram_util_percent: 79.59285714285713\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04114699466796963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.593894166284723\n",
      "    mean_inference_ms: 1.4347854224598113\n",
      "    mean_raw_obs_processing_ms: 0.14796355694317798\n",
      "  time_since_restore: 377.97214245796204\n",
      "  time_this_iter_s: 29.252342224121094\n",
      "  time_total_s: 377.97214245796204\n",
      "  timers:\n",
      "    learn_throughput: 1689.988\n",
      "    learn_time_ms: 591.72\n",
      "    load_throughput: 219045.441\n",
      "    load_time_ms: 4.565\n",
      "    sample_throughput: 85.911\n",
      "    sample_time_ms: 11639.959\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1632131655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         377.972</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-54-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.2258064516129\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4500676737891303\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010757307374219715\n",
      "          policy_loss: -0.12722025006595586\n",
      "          total_loss: -0.13363154840966066\n",
      "          vf_explained_var: 0.1989409178495407\n",
      "          vf_loss: 0.005668982360253317\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.11333333333333\n",
      "    ram_util_percent: 80.85999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041130792473118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.43712639140615\n",
      "    mean_inference_ms: 1.4340888170382533\n",
      "    mean_raw_obs_processing_ms: 0.16635514200989768\n",
      "  time_since_restore: 388.8860592842102\n",
      "  time_this_iter_s: 10.913916826248169\n",
      "  time_total_s: 388.8860592842102\n",
      "  timers:\n",
      "    learn_throughput: 1687.705\n",
      "    learn_time_ms: 592.521\n",
      "    load_throughput: 218967.679\n",
      "    load_time_ms: 4.567\n",
      "    sample_throughput: 85.421\n",
      "    sample_time_ms: 11706.788\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632131666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         388.886</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.226</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-54-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.34375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6833488676283095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010310340574552839\n",
      "          policy_loss: -0.07916929523150126\n",
      "          total_loss: -0.08943000170919631\n",
      "          vf_explained_var: -0.30081331729888916\n",
      "          vf_loss: 0.004252955030339459\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.85\n",
      "    ram_util_percent: 80.8875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04111640305032292\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.28897184563663\n",
      "    mean_inference_ms: 1.4334491206844504\n",
      "    mean_raw_obs_processing_ms: 0.18303995558769726\n",
      "  time_since_restore: 399.65978813171387\n",
      "  time_this_iter_s: 10.773728847503662\n",
      "  time_total_s: 399.65978813171387\n",
      "  timers:\n",
      "    learn_throughput: 1686.849\n",
      "    learn_time_ms: 592.821\n",
      "    load_throughput: 218927.677\n",
      "    load_time_ms: 4.568\n",
      "    sample_throughput: 85.03\n",
      "    sample_time_ms: 11760.584\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632131677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">          399.66</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.344</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-54-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.4545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.088177247842153\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012264918436711427\n",
      "          policy_loss: -0.05935015686684185\n",
      "          total_loss: -0.07388836507582003\n",
      "          vf_explained_var: -0.7646636366844177\n",
      "          vf_loss: 0.0035839594732452597\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.99333333333333\n",
      "    ram_util_percent: 80.82\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04110322642268299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.148881951395705\n",
      "    mean_inference_ms: 1.4328612946374475\n",
      "    mean_raw_obs_processing_ms: 0.19819076534212376\n",
      "  time_since_restore: 410.6057667732239\n",
      "  time_this_iter_s: 10.94597864151001\n",
      "  time_total_s: 410.6057667732239\n",
      "  timers:\n",
      "    learn_throughput: 1685.838\n",
      "    learn_time_ms: 593.177\n",
      "    load_throughput: 218283.936\n",
      "    load_time_ms: 4.581\n",
      "    sample_throughput: 84.689\n",
      "    sample_time_ms: 11807.968\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632131688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         410.606</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-54-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.5588235294117\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.651310067706638\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007644685650080716\n",
      "          policy_loss: -0.09203054126765993\n",
      "          total_loss: -0.10355404168367385\n",
      "          vf_explained_var: 0.33500999212265015\n",
      "          vf_loss: 0.0032695420319214462\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.74375\n",
      "    ram_util_percent: 80.6875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041091036514056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.015913951907276\n",
      "    mean_inference_ms: 1.4323015818072045\n",
      "    mean_raw_obs_processing_ms: 0.21199858508365968\n",
      "  time_since_restore: 421.21451020240784\n",
      "  time_this_iter_s: 10.60874342918396\n",
      "  time_total_s: 421.21451020240784\n",
      "  timers:\n",
      "    learn_throughput: 1686.821\n",
      "    learn_time_ms: 592.831\n",
      "    load_throughput: 216811.438\n",
      "    load_time_ms: 4.612\n",
      "    sample_throughput: 84.61\n",
      "    sample_time_ms: 11818.874\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632131699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         421.215</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.559</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 996.6571428571428\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5658040894402399\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006939078440723634\n",
      "          policy_loss: -0.057929689519935185\n",
      "          total_loss: -0.07066390038364463\n",
      "          vf_explained_var: -0.9416953325271606\n",
      "          vf_loss: 0.001362538119752167\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.0\n",
      "    ram_util_percent: 80.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0410795836395928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.889582114830361\n",
      "    mean_inference_ms: 1.4317733196413291\n",
      "    mean_raw_obs_processing_ms: 0.22457264255846285\n",
      "  time_since_restore: 431.87595534324646\n",
      "  time_this_iter_s: 10.661445140838623\n",
      "  time_total_s: 431.87595534324646\n",
      "  timers:\n",
      "    learn_throughput: 1684.121\n",
      "    learn_time_ms: 593.782\n",
      "    load_throughput: 217077.379\n",
      "    load_time_ms: 4.607\n",
      "    sample_throughput: 84.358\n",
      "    sample_time_ms: 11854.243\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1632131709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         431.876</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.657</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-55-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8291881216896906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01162053367630745\n",
      "          policy_loss: -0.06804240312841203\n",
      "          total_loss: -0.08133806517968575\n",
      "          vf_explained_var: -0.9954347610473633\n",
      "          vf_loss: 0.0023815969664913914\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.533333333333324\n",
      "    ram_util_percent: 68.67333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041068690322928376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.769139521195616\n",
      "    mean_inference_ms: 1.43126414813516\n",
      "    mean_raw_obs_processing_ms: 0.23605053964152173\n",
      "  time_since_restore: 442.21326756477356\n",
      "  time_this_iter_s: 10.3373122215271\n",
      "  time_total_s: 442.21326756477356\n",
      "  timers:\n",
      "    learn_throughput: 1683.862\n",
      "    learn_time_ms: 593.873\n",
      "    load_throughput: 216849.55\n",
      "    load_time_ms: 4.611\n",
      "    sample_throughput: 84.279\n",
      "    sample_time_ms: 11865.396\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1632131720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         442.213</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-55-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.8378378378378\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7358187556266784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017256798903585346\n",
      "          policy_loss: 0.0011323141554991404\n",
      "          total_loss: -0.011267151228255695\n",
      "          vf_explained_var: -0.22925998270511627\n",
      "          vf_loss: 0.0010759391019948654\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26428571428572\n",
      "    ram_util_percent: 64.38571428571427\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04105823146255376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.654177518089682\n",
      "    mean_inference_ms: 1.4307740133623947\n",
      "    mean_raw_obs_processing_ms: 0.24653340947013633\n",
      "  time_since_restore: 452.55801463127136\n",
      "  time_this_iter_s: 10.344747066497803\n",
      "  time_total_s: 452.55801463127136\n",
      "  timers:\n",
      "    learn_throughput: 1684.352\n",
      "    learn_time_ms: 593.7\n",
      "    load_throughput: 217224.656\n",
      "    load_time_ms: 4.604\n",
      "    sample_throughput: 84.234\n",
      "    sample_time_ms: 11871.751\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632131730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         452.558</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.838</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-55-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.921052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5386323942078484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01655774851273893\n",
      "          policy_loss: 0.011972203022903867\n",
      "          total_loss: 0.0023925891353024377\n",
      "          vf_explained_var: -0.9611326456069946\n",
      "          vf_loss: 0.0020812161137453386\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.56875\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041048259920719636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.544568317845316\n",
      "    mean_inference_ms: 1.4303041805628642\n",
      "    mean_raw_obs_processing_ms: 0.25611956514895473\n",
      "  time_since_restore: 463.26365399360657\n",
      "  time_this_iter_s: 10.705639362335205\n",
      "  time_total_s: 463.26365399360657\n",
      "  timers:\n",
      "    learn_throughput: 1684.191\n",
      "    learn_time_ms: 593.757\n",
      "    load_throughput: 218387.362\n",
      "    load_time_ms: 4.579\n",
      "    sample_throughput: 84.077\n",
      "    sample_time_ms: 11893.845\n",
      "    update_time_ms: 1.774\n",
      "  timestamp: 1632131741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         463.264</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.921</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 997.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3510720100667741\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010692685461348637\n",
      "          policy_loss: -0.1342867899272177\n",
      "          total_loss: -0.14313185132212108\n",
      "          vf_explained_var: -0.43866315484046936\n",
      "          vf_loss: 0.0022598041758303427\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.106666666666676\n",
      "    ram_util_percent: 64.56000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04103856377205862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.43992597856491\n",
      "    mean_inference_ms: 1.429847933806393\n",
      "    mean_raw_obs_processing_ms: 0.2649043297530977\n",
      "  time_since_restore: 473.96490240097046\n",
      "  time_this_iter_s: 10.701248407363892\n",
      "  time_total_s: 473.96490240097046\n",
      "  timers:\n",
      "    learn_throughput: 1681.272\n",
      "    learn_time_ms: 594.788\n",
      "    load_throughput: 217961.784\n",
      "    load_time_ms: 4.588\n",
      "    sample_throughput: 83.891\n",
      "    sample_time_ms: 11920.249\n",
      "    update_time_ms: 1.768\n",
      "  timestamp: 1632131751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         473.965</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               997</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-56-02\n",
      "  done: false\n",
      "  episode_len_mean: 997.075\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 40\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8190964539845784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01126062118919382\n",
      "          policy_loss: -0.1528777528968122\n",
      "          total_loss: -0.16682377929488817\n",
      "          vf_explained_var: -0.9992848634719849\n",
      "          vf_loss: 0.0017112986870213515\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.03333333333334\n",
      "    ram_util_percent: 64.68000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041029230720358925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.339902085361379\n",
      "    mean_inference_ms: 1.4294062423124636\n",
      "    mean_raw_obs_processing_ms: 0.272952462420353\n",
      "  time_since_restore: 484.6340012550354\n",
      "  time_this_iter_s: 10.669098854064941\n",
      "  time_total_s: 484.6340012550354\n",
      "  timers:\n",
      "    learn_throughput: 1684.644\n",
      "    learn_time_ms: 593.597\n",
      "    load_throughput: 305945.891\n",
      "    load_time_ms: 3.269\n",
      "    sample_throughput: 99.359\n",
      "    sample_time_ms: 10064.472\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632131762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         484.634</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.075</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-56-13\n",
      "  done: false\n",
      "  episode_len_mean: 997.1463414634146\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 41\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.493008542060852\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01071625280037512\n",
      "          policy_loss: -0.11689675086074405\n",
      "          total_loss: -0.12698317699962192\n",
      "          vf_explained_var: -0.6311197280883789\n",
      "          vf_loss: 0.0024325005425554183\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.88125\n",
      "    ram_util_percent: 64.8\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041020203398089844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.244223442614626\n",
      "    mean_inference_ms: 1.4289801226158698\n",
      "    mean_raw_obs_processing_ms: 0.2803416581981264\n",
      "  time_since_restore: 495.38916420936584\n",
      "  time_this_iter_s: 10.755162954330444\n",
      "  time_total_s: 495.38916420936584\n",
      "  timers:\n",
      "    learn_throughput: 1687.324\n",
      "    learn_time_ms: 592.654\n",
      "    load_throughput: 305304.518\n",
      "    load_time_ms: 3.275\n",
      "    sample_throughput: 99.507\n",
      "    sample_time_ms: 10049.531\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632131773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         495.389</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.146</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 997.2142857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 42\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6632765942149692\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01118372453921926\n",
      "          policy_loss: -0.08368489423559772\n",
      "          total_loss: -0.09526577707793978\n",
      "          vf_explained_var: -0.13871583342552185\n",
      "          vf_loss: 0.0025355432486523772\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.18666666666666\n",
      "    ram_util_percent: 65.25333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041011590179832115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.152438924741075\n",
      "    mean_inference_ms: 1.428569342802552\n",
      "    mean_raw_obs_processing_ms: 0.2871279854232274\n",
      "  time_since_restore: 505.845810174942\n",
      "  time_this_iter_s: 10.456645965576172\n",
      "  time_total_s: 505.845810174942\n",
      "  timers:\n",
      "    learn_throughput: 1689.768\n",
      "    learn_time_ms: 591.797\n",
      "    load_throughput: 305097.982\n",
      "    load_time_ms: 3.278\n",
      "    sample_throughput: 99.813\n",
      "    sample_time_ms: 10018.714\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632131783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         505.846</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.214</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-56-34\n",
      "  done: false\n",
      "  episode_len_mean: 997.2790697674419\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 43\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5611995140711465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00844206152735984\n",
      "          policy_loss: -0.15036085955798625\n",
      "          total_loss: -0.16230771210458544\n",
      "          vf_explained_var: -0.3387606739997864\n",
      "          vf_loss: 0.0017656793578579607\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.94666666666668\n",
      "    ram_util_percent: 65.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04100337720347147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.064450677913111\n",
      "    mean_inference_ms: 1.4281753751872\n",
      "    mean_raw_obs_processing_ms: 0.2933675163180569\n",
      "  time_since_restore: 516.5828566551208\n",
      "  time_this_iter_s: 10.737046480178833\n",
      "  time_total_s: 516.5828566551208\n",
      "  timers:\n",
      "    learn_throughput: 1687.196\n",
      "    learn_time_ms: 592.699\n",
      "    load_throughput: 305284.519\n",
      "    load_time_ms: 3.276\n",
      "    sample_throughput: 100.031\n",
      "    sample_time_ms: 9996.925\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632131794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         516.583</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-56-45\n",
      "  done: false\n",
      "  episode_len_mean: 997.3409090909091\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9088896407021416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0088023972533425\n",
      "          policy_loss: -0.09697954104178481\n",
      "          total_loss: -0.1128353880925311\n",
      "          vf_explained_var: -0.6585257053375244\n",
      "          vf_loss: 0.0012525076466974698\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.10000000000001\n",
      "    ram_util_percent: 65.39333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04099559643020057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.98001908102821\n",
      "    mean_inference_ms: 1.4277995681289681\n",
      "    mean_raw_obs_processing_ms: 0.29910028359872975\n",
      "  time_since_restore: 527.29829621315\n",
      "  time_this_iter_s: 10.715439558029175\n",
      "  time_total_s: 527.29829621315\n",
      "  timers:\n",
      "    learn_throughput: 1687.286\n",
      "    learn_time_ms: 592.668\n",
      "    load_throughput: 307642.386\n",
      "    load_time_ms: 3.251\n",
      "    sample_throughput: 99.924\n",
      "    sample_time_ms: 10007.637\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632131805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         527.298</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.341</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-56-56\n",
      "  done: false\n",
      "  episode_len_mean: 997.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 45\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1412836644384596\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02195658371301747\n",
      "          policy_loss: -0.08099097307357524\n",
      "          total_loss: -0.08023439678880903\n",
      "          vf_explained_var: 0.17112188041210175\n",
      "          vf_loss: 0.007229181495495141\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.34375\n",
      "    ram_util_percent: 65.44375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04098834069791723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.898913515614893\n",
      "    mean_inference_ms: 1.4274412917860209\n",
      "    mean_raw_obs_processing_ms: 0.30436958623997956\n",
      "  time_since_restore: 538.0063529014587\n",
      "  time_this_iter_s: 10.708056688308716\n",
      "  time_total_s: 538.0063529014587\n",
      "  timers:\n",
      "    learn_throughput: 1685.192\n",
      "    learn_time_ms: 593.404\n",
      "    load_throughput: 307692.037\n",
      "    load_time_ms: 3.25\n",
      "    sample_throughput: 99.886\n",
      "    sample_time_ms: 10011.441\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632131816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         538.006</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             997.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-57-06\n",
      "  done: false\n",
      "  episode_len_mean: 997.4565217391304\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 46\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4814237674077353\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009000798484940929\n",
      "          policy_loss: -0.04398535564541817\n",
      "          total_loss: -0.05182894219954808\n",
      "          vf_explained_var: -0.39603081345558167\n",
      "          vf_loss: 0.003932884417331984\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.306666666666665\n",
      "    ram_util_percent: 65.48666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04098139304994888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.820917320572937\n",
      "    mean_inference_ms: 1.4270984749284101\n",
      "    mean_raw_obs_processing_ms: 0.30922015039207235\n",
      "  time_since_restore: 548.6819877624512\n",
      "  time_this_iter_s: 10.675634860992432\n",
      "  time_total_s: 548.6819877624512\n",
      "  timers:\n",
      "    learn_throughput: 1682.376\n",
      "    learn_time_ms: 594.398\n",
      "    load_throughput: 307414.65\n",
      "    load_time_ms: 3.253\n",
      "    sample_throughput: 99.559\n",
      "    sample_time_ms: 10044.288\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632131826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         548.682</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.457</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-57-17\n",
      "  done: false\n",
      "  episode_len_mean: 997.5106382978723\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 47\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.272872503598531\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011289457521159108\n",
      "          policy_loss: -0.1954679869943195\n",
      "          total_loss: -0.2006102225018872\n",
      "          vf_explained_var: -0.3827425241470337\n",
      "          vf_loss: 0.0037762959791709564\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.72666666666668\n",
      "    ram_util_percent: 65.56000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0409747272213815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.74582482796497\n",
      "    mean_inference_ms: 1.426769117381226\n",
      "    mean_raw_obs_processing_ms: 0.31368661819473137\n",
      "  time_since_restore: 559.2823343276978\n",
      "  time_this_iter_s: 10.600346565246582\n",
      "  time_total_s: 559.2823343276978\n",
      "  timers:\n",
      "    learn_throughput: 1685.277\n",
      "    learn_time_ms: 593.374\n",
      "    load_throughput: 307924.706\n",
      "    load_time_ms: 3.248\n",
      "    sample_throughput: 99.296\n",
      "    sample_time_ms: 10070.873\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632131837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         559.282</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.511</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-57-28\n",
      "  done: false\n",
      "  episode_len_mean: 997.5625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 48\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6491299404038324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010271933615885784\n",
      "          policy_loss: -0.07210809071030881\n",
      "          total_loss: -0.08355754009551472\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001575072305665041\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.97333333333333\n",
      "    ram_util_percent: 65.68000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04096823944786629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.673546938766437\n",
      "    mean_inference_ms: 1.4264507879943522\n",
      "    mean_raw_obs_processing_ms: 0.3177948418874665\n",
      "  time_since_restore: 570.0493941307068\n",
      "  time_this_iter_s: 10.767059803009033\n",
      "  time_total_s: 570.0493941307068\n",
      "  timers:\n",
      "    learn_throughput: 1684.14\n",
      "    learn_time_ms: 593.775\n",
      "    load_throughput: 308717.964\n",
      "    load_time_ms: 3.239\n",
      "    sample_throughput: 99.24\n",
      "    sample_time_ms: 10076.582\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632131848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         570.049</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.562</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-57-38\n",
      "  done: false\n",
      "  episode_len_mean: 997.6122448979592\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 49\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0717610822783576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00771190025835406\n",
      "          policy_loss: -0.010155696877174908\n",
      "          total_loss: -0.0170777741405699\n",
      "          vf_explained_var: -0.4312187731266022\n",
      "          vf_loss: 0.0011927659487506995\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.512499999999996\n",
      "    ram_util_percent: 65.6875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04096194614879682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.603857269781095\n",
      "    mean_inference_ms: 1.4261411760600822\n",
      "    mean_raw_obs_processing_ms: 0.3215740705626849\n",
      "  time_since_restore: 580.6510667800903\n",
      "  time_this_iter_s: 10.601672649383545\n",
      "  time_total_s: 580.6510667800903\n",
      "  timers:\n",
      "    learn_throughput: 1684.36\n",
      "    learn_time_ms: 593.697\n",
      "    load_throughput: 308558.985\n",
      "    load_time_ms: 3.241\n",
      "    sample_throughput: 99.338\n",
      "    sample_time_ms: 10066.679\n",
      "    update_time_ms: 1.769\n",
      "  timestamp: 1632131858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         580.651</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.612</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-57-49\n",
      "  done: false\n",
      "  episode_len_mean: 997.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 50\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1692486021253798\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006320354063129941\n",
      "          policy_loss: -0.11434786450117826\n",
      "          total_loss: -0.123059374673499\n",
      "          vf_explained_var: -0.2909744381904602\n",
      "          vf_loss: 0.0008478555805696589\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.13999999999999\n",
      "    ram_util_percent: 65.75333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04095584737704984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.536607073026248\n",
      "    mean_inference_ms: 1.4258407769819932\n",
      "    mean_raw_obs_processing_ms: 0.32504997614773423\n",
      "  time_since_restore: 591.2344217300415\n",
      "  time_this_iter_s: 10.583354949951172\n",
      "  time_total_s: 591.2344217300415\n",
      "  timers:\n",
      "    learn_throughput: 1683.664\n",
      "    learn_time_ms: 593.943\n",
      "    load_throughput: 313944.91\n",
      "    load_time_ms: 3.185\n",
      "    sample_throughput: 99.424\n",
      "    sample_time_ms: 10057.888\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632131869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         591.234</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            997.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-58-00\n",
      "  done: false\n",
      "  episode_len_mean: 997.7058823529412\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 51\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5684882071283128\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014190776739632425\n",
      "          policy_loss: -0.057623132835659716\n",
      "          total_loss: -0.06498013658242094\n",
      "          vf_explained_var: -0.35665592551231384\n",
      "          vf_loss: 0.003538490979311367\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.07333333333334\n",
      "    ram_util_percent: 65.81333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040949953688209695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.471695378264302\n",
      "    mean_inference_ms: 1.4255480764061061\n",
      "    mean_raw_obs_processing_ms: 0.3282492358735145\n",
      "  time_since_restore: 601.8990828990936\n",
      "  time_this_iter_s: 10.664661169052124\n",
      "  time_total_s: 601.8990828990936\n",
      "  timers:\n",
      "    learn_throughput: 1685.051\n",
      "    learn_time_ms: 593.454\n",
      "    load_throughput: 314592.462\n",
      "    load_time_ms: 3.179\n",
      "    sample_throughput: 99.509\n",
      "    sample_time_ms: 10049.337\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632131880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         601.899</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.706</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-58-10\n",
      "  done: false\n",
      "  episode_len_mean: 997.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 52\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4207330968644885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008248601070263935\n",
      "          policy_loss: -0.12317343768146304\n",
      "          total_loss: -0.13234148286283015\n",
      "          vf_explained_var: -0.9148505330085754\n",
      "          vf_loss: 0.002255381479497171\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.07333333333334\n",
      "    ram_util_percent: 65.74000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040944274286553325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.409023692343293\n",
      "    mean_inference_ms: 1.4252633905113326\n",
      "    mean_raw_obs_processing_ms: 0.3311953965046662\n",
      "  time_since_restore: 612.6449890136719\n",
      "  time_this_iter_s: 10.745906114578247\n",
      "  time_total_s: 612.6449890136719\n",
      "  timers:\n",
      "    learn_throughput: 1683.399\n",
      "    learn_time_ms: 594.036\n",
      "    load_throughput: 314549.995\n",
      "    load_time_ms: 3.179\n",
      "    sample_throughput: 99.229\n",
      "    sample_time_ms: 10077.656\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632131890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         612.645</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            997.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-58-21\n",
      "  done: false\n",
      "  episode_len_mean: 997.7924528301887\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 53\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2525714092784457\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009055521230830976\n",
      "          policy_loss: -0.08268224365181392\n",
      "          total_loss: -0.08806738216016027\n",
      "          vf_explained_var: -0.8152797222137451\n",
      "          vf_loss: 0.004084336997604825\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.868750000000006\n",
      "    ram_util_percent: 65.67500000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0409388226244729\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.348540879916943\n",
      "    mean_inference_ms: 1.4249904552089343\n",
      "    mean_raw_obs_processing_ms: 0.33390525223955914\n",
      "  time_since_restore: 623.5847046375275\n",
      "  time_this_iter_s: 10.93971562385559\n",
      "  time_total_s: 623.5847046375275\n",
      "  timers:\n",
      "    learn_throughput: 1682.956\n",
      "    learn_time_ms: 594.193\n",
      "    load_throughput: 311906.777\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 99.032\n",
      "    sample_time_ms: 10097.719\n",
      "    update_time_ms: 1.778\n",
      "  timestamp: 1632131901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         623.585</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.792</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-58-32\n",
      "  done: false\n",
      "  episode_len_mean: 997.8333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 54\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2106985237863328\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007471635970953283\n",
      "          policy_loss: -0.027458333058489694\n",
      "          total_loss: -0.0360347958902518\n",
      "          vf_explained_var: -0.9381418824195862\n",
      "          vf_loss: 0.0010088448601891288\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.01333333333333\n",
      "    ram_util_percent: 65.52666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04093351279484245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.289997567474847\n",
      "    mean_inference_ms: 1.4247241536338242\n",
      "    mean_raw_obs_processing_ms: 0.3363938219167523\n",
      "  time_since_restore: 634.1036875247955\n",
      "  time_this_iter_s: 10.518982887268066\n",
      "  time_total_s: 634.1036875247955\n",
      "  timers:\n",
      "    learn_throughput: 1682.85\n",
      "    learn_time_ms: 594.23\n",
      "    load_throughput: 311781.575\n",
      "    load_time_ms: 3.207\n",
      "    sample_throughput: 99.226\n",
      "    sample_time_ms: 10078.046\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1632131912\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         634.104</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 997.8727272727273\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 55\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1929300122790867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009844739443192117\n",
      "          policy_loss: 0.011326003074645995\n",
      "          total_loss: 0.004179682417048349\n",
      "          vf_explained_var: -0.7260732650756836\n",
      "          vf_loss: 0.0014603788375906232\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.12\n",
      "    ram_util_percent: 65.36666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04092835158024308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.233327563582485\n",
      "    mean_inference_ms: 1.4244648075935957\n",
      "    mean_raw_obs_processing_ms: 0.3386775318001872\n",
      "  time_since_restore: 644.7083175182343\n",
      "  time_this_iter_s: 10.60462999343872\n",
      "  time_total_s: 644.7083175182343\n",
      "  timers:\n",
      "    learn_throughput: 1687.915\n",
      "    learn_time_ms: 592.447\n",
      "    load_throughput: 312041.364\n",
      "    load_time_ms: 3.205\n",
      "    sample_throughput: 99.309\n",
      "    sample_time_ms: 10069.606\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632131923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         644.708</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.873</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-58-54\n",
      "  done: false\n",
      "  episode_len_mean: 997.9107142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 56\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5042117264535693\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012252818901627431\n",
      "          policy_loss: -0.09062863199247254\n",
      "          total_loss: -0.09957182697123951\n",
      "          vf_explained_var: -0.5400875806808472\n",
      "          vf_loss: 0.0019635961481576994\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.775000000000006\n",
      "    ram_util_percent: 65.21875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040923337672642925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.178529963886701\n",
      "    mean_inference_ms: 1.4242148326634603\n",
      "    mean_raw_obs_processing_ms: 0.3407762091432086\n",
      "  time_since_restore: 655.6297769546509\n",
      "  time_this_iter_s: 10.921459436416626\n",
      "  time_total_s: 655.6297769546509\n",
      "  timers:\n",
      "    learn_throughput: 1689.352\n",
      "    learn_time_ms: 591.943\n",
      "    load_throughput: 310055.294\n",
      "    load_time_ms: 3.225\n",
      "    sample_throughput: 99.063\n",
      "    sample_time_ms: 10094.628\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632131934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">          655.63</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.911</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-59-04\n",
      "  done: false\n",
      "  episode_len_mean: 997.9473684210526\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 57\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7314734631114537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012278510374557901\n",
      "          policy_loss: 0.041452794935968186\n",
      "          total_loss: 0.029595153364870282\n",
      "          vf_explained_var: 0.0036270353011786938\n",
      "          vf_loss: 0.0013130915040771166\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.45333333333334\n",
      "    ram_util_percent: 65.15333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04091847068586155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.125502759237346\n",
      "    mean_inference_ms: 1.4239745229972571\n",
      "    mean_raw_obs_processing_ms: 0.34270347896865583\n",
      "  time_since_restore: 666.5177986621857\n",
      "  time_this_iter_s: 10.88802170753479\n",
      "  time_total_s: 666.5177986621857\n",
      "  timers:\n",
      "    learn_throughput: 1687.943\n",
      "    learn_time_ms: 592.437\n",
      "    load_throughput: 309702.725\n",
      "    load_time_ms: 3.229\n",
      "    sample_throughput: 98.786\n",
      "    sample_time_ms: 10122.906\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632131944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         666.518</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-59-15\n",
      "  done: false\n",
      "  episode_len_mean: 997.9827586206897\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 58\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7211189932293363\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007882407445900637\n",
      "          policy_loss: -0.038029252199663056\n",
      "          total_loss: -0.051425180832544964\n",
      "          vf_explained_var: -0.44550010561943054\n",
      "          vf_loss: 0.0011549471136984518\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.7125\n",
      "    ram_util_percent: 64.98124999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04091380474105743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.074097146920936\n",
      "    mean_inference_ms: 1.4237415342460908\n",
      "    mean_raw_obs_processing_ms: 0.3444711222781505\n",
      "  time_since_restore: 677.2019259929657\n",
      "  time_this_iter_s: 10.68412733078003\n",
      "  time_total_s: 677.2019259929657\n",
      "  timers:\n",
      "    learn_throughput: 1685.785\n",
      "    learn_time_ms: 593.196\n",
      "    load_throughput: 309257.438\n",
      "    load_time_ms: 3.234\n",
      "    sample_throughput: 98.874\n",
      "    sample_time_ms: 10113.854\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632131955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         677.202</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.983</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 998.0169491525423\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 59\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8325586239496867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012730181936953135\n",
      "          policy_loss: -0.12031067949202326\n",
      "          total_loss: -0.13222253322601318\n",
      "          vf_explained_var: 0.17852702736854553\n",
      "          vf_loss: 0.0021172919077798724\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.14666666666666\n",
      "    ram_util_percent: 64.80666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04090926358018142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.024280114520703\n",
      "    mean_inference_ms: 1.4235155120917877\n",
      "    mean_raw_obs_processing_ms: 0.34608868851375485\n",
      "  time_since_restore: 688.0242040157318\n",
      "  time_this_iter_s: 10.822278022766113\n",
      "  time_total_s: 688.0242040157318\n",
      "  timers:\n",
      "    learn_throughput: 1684.236\n",
      "    learn_time_ms: 593.741\n",
      "    load_throughput: 309732.456\n",
      "    load_time_ms: 3.229\n",
      "    sample_throughput: 98.664\n",
      "    sample_time_ms: 10135.367\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632131966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         688.024</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           998.017</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 60\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7515101313591004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009417216621032775\n",
      "          policy_loss: 0.0834349435236719\n",
      "          total_loss: 0.07107350511683358\n",
      "          vf_explained_var: -0.10774092376232147\n",
      "          vf_loss: 0.001975354368591474\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.9609756097561\n",
      "    ram_util_percent: 64.65853658536585\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040904883052329005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.975969921517466\n",
      "    mean_inference_ms: 1.4232969448370938\n",
      "    mean_raw_obs_processing_ms: 0.3524580320638118\n",
      "  time_since_restore: 716.446615934372\n",
      "  time_this_iter_s: 28.422411918640137\n",
      "  time_total_s: 716.446615934372\n",
      "  timers:\n",
      "    learn_throughput: 1682.509\n",
      "    learn_time_ms: 594.35\n",
      "    load_throughput: 211026.731\n",
      "    load_time_ms: 4.739\n",
      "    sample_throughput: 83.913\n",
      "    sample_time_ms: 11917.156\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632131994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         716.447</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             995.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-00-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.672131147541\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 61\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0048905849456786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019072052863035753\n",
      "          policy_loss: 0.11928532334665458\n",
      "          total_loss: 0.10922900508675311\n",
      "          vf_explained_var: -0.12283191829919815\n",
      "          vf_loss: 0.0035557697346020076\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.89444444444444\n",
      "    ram_util_percent: 64.96111111111112\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040900652139658246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.929583821134374\n",
      "    mean_inference_ms: 1.4230858816059249\n",
      "    mean_raw_obs_processing_ms: 0.3584579866559625\n",
      "  time_since_restore: 729.0594246387482\n",
      "  time_this_iter_s: 12.61280870437622\n",
      "  time_total_s: 729.0594246387482\n",
      "  timers:\n",
      "    learn_throughput: 1679.923\n",
      "    learn_time_ms: 595.265\n",
      "    load_throughput: 210229.21\n",
      "    load_time_ms: 4.757\n",
      "    sample_throughput: 82.569\n",
      "    sample_time_ms: 12111.019\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632132007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         729.059</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.672</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-00-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.741935483871\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 62\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0557602961858112\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013399730046987043\n",
      "          policy_loss: 0.12576284425126183\n",
      "          total_loss: 0.11139341071248055\n",
      "          vf_explained_var: -0.3330909013748169\n",
      "          vf_loss: 0.0016657595018235345\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.64666666666666\n",
      "    ram_util_percent: 65.99999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04089655944313278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.884591705262098\n",
      "    mean_inference_ms: 1.4228831026306923\n",
      "    mean_raw_obs_processing_ms: 0.36411280306952953\n",
      "  time_since_restore: 740.051545381546\n",
      "  time_this_iter_s: 10.992120742797852\n",
      "  time_total_s: 740.051545381546\n",
      "  timers:\n",
      "    learn_throughput: 1677.931\n",
      "    learn_time_ms: 595.972\n",
      "    load_throughput: 210094.42\n",
      "    load_time_ms: 4.76\n",
      "    sample_throughput: 82.407\n",
      "    sample_time_ms: 12134.94\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1632132018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         740.052</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.742</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-00-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.8095238095239\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 63\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4368997097015381\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008463435295373885\n",
      "          policy_loss: 0.007578576770093706\n",
      "          total_loss: -0.0024039429095056323\n",
      "          vf_explained_var: 0.06331554800271988\n",
      "          vf_loss: 0.0015300670373512226\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.173333333333325\n",
      "    ram_util_percent: 66.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04089250936428691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.840820932414207\n",
      "    mean_inference_ms: 1.4226855959940408\n",
      "    mean_raw_obs_processing_ms: 0.36944224931670416\n",
      "  time_since_restore: 750.5706088542938\n",
      "  time_this_iter_s: 10.519063472747803\n",
      "  time_total_s: 750.5706088542938\n",
      "  timers:\n",
      "    learn_throughput: 1681.471\n",
      "    learn_time_ms: 594.718\n",
      "    load_throughput: 211689.202\n",
      "    load_time_ms: 4.724\n",
      "    sample_throughput: 82.684\n",
      "    sample_time_ms: 12094.189\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632132029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         750.571</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-00-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 64\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.966723820898268\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009756126852642153\n",
      "          policy_loss: -0.10221681470672289\n",
      "          total_loss: -0.11713004526164797\n",
      "          vf_explained_var: -0.44134774804115295\n",
      "          vf_loss: 0.0014613140419694698\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.09375\n",
      "    ram_util_percent: 66.1625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0408885228471053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.79829610999473\n",
      "    mean_inference_ms: 1.4224988107102288\n",
      "    mean_raw_obs_processing_ms: 0.3744677166149477\n",
      "  time_since_restore: 761.4448580741882\n",
      "  time_this_iter_s: 10.87424921989441\n",
      "  time_total_s: 761.4448580741882\n",
      "  timers:\n",
      "    learn_throughput: 1678.235\n",
      "    learn_time_ms: 595.864\n",
      "    load_throughput: 211914.876\n",
      "    load_time_ms: 4.719\n",
      "    sample_throughput: 82.45\n",
      "    sample_time_ms: 12128.562\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632132040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         761.445</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.875</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-00-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.9384615384615\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 65\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.796142570177714\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011513616245660864\n",
      "          policy_loss: 0.014375921835501989\n",
      "          total_loss: 0.0020699054209722414\n",
      "          vf_explained_var: -0.14572401344776154\n",
      "          vf_loss: 0.0017695629050851697\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.68666666666666\n",
      "    ram_util_percent: 65.96666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04088459457803543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.756916869501353\n",
      "    mean_inference_ms: 1.4223154031832355\n",
      "    mean_raw_obs_processing_ms: 0.37920433390738806\n",
      "  time_since_restore: 772.0740346908569\n",
      "  time_this_iter_s: 10.629176616668701\n",
      "  time_total_s: 772.0740346908569\n",
      "  timers:\n",
      "    learn_throughput: 1675.951\n",
      "    learn_time_ms: 596.676\n",
      "    load_throughput: 210656.836\n",
      "    load_time_ms: 4.747\n",
      "    sample_throughput: 82.439\n",
      "    sample_time_ms: 12130.18\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632132050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         772.074</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.938</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 66\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7610543012619018\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009355671722882757\n",
      "          policy_loss: -0.031036658874816363\n",
      "          total_loss: -0.04401159170601103\n",
      "          vf_explained_var: -0.3644428253173828\n",
      "          vf_loss: 0.0014780717610847205\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.06000000000001\n",
      "    ram_util_percent: 65.99333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040880744849067285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.716637896947644\n",
      "    mean_inference_ms: 1.4221361336923029\n",
      "    mean_raw_obs_processing_ms: 0.38366907979860193\n",
      "  time_since_restore: 782.7122972011566\n",
      "  time_this_iter_s: 10.638262510299683\n",
      "  time_total_s: 782.7122972011566\n",
      "  timers:\n",
      "    learn_throughput: 1677.144\n",
      "    learn_time_ms: 596.252\n",
      "    load_throughput: 210829.434\n",
      "    load_time_ms: 4.743\n",
      "    sample_throughput: 82.629\n",
      "    sample_time_ms: 12102.338\n",
      "    update_time_ms: 1.714\n",
      "  timestamp: 1632132061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         782.712</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               996</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-01-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.0597014925373\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 67\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6602005726761289\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01059805362439966\n",
      "          policy_loss: -0.02388369043668111\n",
      "          total_loss: -0.031368890818622375\n",
      "          vf_explained_var: 0.2799522876739502\n",
      "          vf_loss: 0.0055399626040727725\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.160000000000004\n",
      "    ram_util_percent: 66.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040876951736387106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.677358064150493\n",
      "    mean_inference_ms: 1.4219598840612802\n",
      "    mean_raw_obs_processing_ms: 0.3878777493147409\n",
      "  time_since_restore: 793.0987541675568\n",
      "  time_this_iter_s: 10.386456966400146\n",
      "  time_total_s: 793.0987541675568\n",
      "  timers:\n",
      "    learn_throughput: 1674.438\n",
      "    learn_time_ms: 597.215\n",
      "    load_throughput: 210389.498\n",
      "    load_time_ms: 4.753\n",
      "    sample_throughput: 82.98\n",
      "    sample_time_ms: 12051.159\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1632132071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         793.099</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.1176470588235\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 68\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8596374697155422\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011344596041654607\n",
      "          policy_loss: -0.016750595801406438\n",
      "          total_loss: -0.029558310657739638\n",
      "          vf_explained_var: -0.5123997330665588\n",
      "          vf_loss: 0.0019598563531568894\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.68125\n",
      "    ram_util_percent: 66.025\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0408732131942626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.639108507027627\n",
      "    mean_inference_ms: 1.421786201870024\n",
      "    mean_raw_obs_processing_ms: 0.3918472186351669\n",
      "  time_since_restore: 803.8032495975494\n",
      "  time_this_iter_s: 10.704495429992676\n",
      "  time_total_s: 803.8032495975494\n",
      "  timers:\n",
      "    learn_throughput: 1677.853\n",
      "    learn_time_ms: 596.0\n",
      "    load_throughput: 209656.496\n",
      "    load_time_ms: 4.77\n",
      "    sample_throughput: 82.957\n",
      "    sample_time_ms: 12054.386\n",
      "    update_time_ms: 1.715\n",
      "  timestamp: 1632132082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         803.803</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.118</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-01-33\n",
      "  done: false\n",
      "  episode_len_mean: 996.1739130434783\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 69\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5600185645951166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011602718153156744\n",
      "          policy_loss: -0.035578123148944645\n",
      "          total_loss: -0.0447843697335985\n",
      "          vf_explained_var: 0.00827204342931509\n",
      "          vf_loss: 0.002478020189381722\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.11333333333333\n",
      "    ram_util_percent: 66.16666666666669\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04086954004574174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.601838120614588\n",
      "    mean_inference_ms: 1.4216163487160323\n",
      "    mean_raw_obs_processing_ms: 0.39558987491645087\n",
      "  time_since_restore: 814.4723525047302\n",
      "  time_this_iter_s: 10.669102907180786\n",
      "  time_total_s: 814.4723525047302\n",
      "  timers:\n",
      "    learn_throughput: 1676.656\n",
      "    learn_time_ms: 596.425\n",
      "    load_throughput: 208996.253\n",
      "    load_time_ms: 4.785\n",
      "    sample_throughput: 83.066\n",
      "    sample_time_ms: 12038.613\n",
      "    update_time_ms: 1.721\n",
      "  timestamp: 1632132093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         814.472</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.174</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-01-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.2285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 70\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1141416721873814\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010111412557520122\n",
      "          policy_loss: -0.10431660144693322\n",
      "          total_loss: -0.12122874421377977\n",
      "          vf_explained_var: -0.845201849937439\n",
      "          vf_loss: 0.0008166697804376276\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.831250000000004\n",
      "    ram_util_percent: 66.225\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04086595789719014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.565593542849276\n",
      "    mean_inference_ms: 1.421454001979383\n",
      "    mean_raw_obs_processing_ms: 0.3991219134364788\n",
      "  time_since_restore: 825.5864679813385\n",
      "  time_this_iter_s: 11.114115476608276\n",
      "  time_total_s: 825.5864679813385\n",
      "  timers:\n",
      "    learn_throughput: 1677.064\n",
      "    learn_time_ms: 596.28\n",
      "    load_throughput: 304814.174\n",
      "    load_time_ms: 3.281\n",
      "    sample_throughput: 96.998\n",
      "    sample_time_ms: 10309.441\n",
      "    update_time_ms: 1.721\n",
      "  timestamp: 1632132104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         825.586</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.229</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-01-55\n",
      "  done: false\n",
      "  episode_len_mean: 996.2816901408451\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 71\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.956453635957506\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014880328462822707\n",
      "          policy_loss: -0.0327380095091131\n",
      "          total_loss: -0.04505559154268768\n",
      "          vf_explained_var: -0.8026089072227478\n",
      "          vf_loss: 0.0022248443681746723\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.14\n",
      "    ram_util_percent: 66.35999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040862409137845904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.530265703667599\n",
      "    mean_inference_ms: 1.4212956957075398\n",
      "    mean_raw_obs_processing_ms: 0.40245162125027456\n",
      "  time_since_restore: 836.3299760818481\n",
      "  time_this_iter_s: 10.743508100509644\n",
      "  time_total_s: 836.3299760818481\n",
      "  timers:\n",
      "    learn_throughput: 1678.756\n",
      "    learn_time_ms: 595.679\n",
      "    load_throughput: 303752.381\n",
      "    load_time_ms: 3.292\n",
      "    sample_throughput: 98.784\n",
      "    sample_time_ms: 10123.097\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1632132115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">          836.33</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.282</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-02-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.3333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 72\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3199013498094347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006069360336309712\n",
      "          policy_loss: -0.048854100538624656\n",
      "          total_loss: -0.05872666835784912\n",
      "          vf_explained_var: -0.3132691979408264\n",
      "          vf_loss: 0.0012780370119596935\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.13333333333335\n",
      "    ram_util_percent: 66.45333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040858952449972494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.495793069708933\n",
      "    mean_inference_ms: 1.4211419023468335\n",
      "    mean_raw_obs_processing_ms: 0.40559019351081343\n",
      "  time_since_restore: 846.9427208900452\n",
      "  time_this_iter_s: 10.612744808197021\n",
      "  time_total_s: 846.9427208900452\n",
      "  timers:\n",
      "    learn_throughput: 1681.131\n",
      "    learn_time_ms: 594.838\n",
      "    load_throughput: 299625.246\n",
      "    load_time_ms: 3.338\n",
      "    sample_throughput: 99.148\n",
      "    sample_time_ms: 10085.952\n",
      "    update_time_ms: 1.725\n",
      "  timestamp: 1632132125\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         846.943</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-02-16\n",
      "  done: false\n",
      "  episode_len_mean: 996.3835616438356\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 73\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0474622805913287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013592834084764496\n",
      "          policy_loss: -0.15053810953266092\n",
      "          total_loss: -0.16426462564203476\n",
      "          vf_explained_var: 0.13212865591049194\n",
      "          vf_loss: 0.002160522281580294\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.04375\n",
      "    ram_util_percent: 66.50625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040855558545277705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.462226871382324\n",
      "    mean_inference_ms: 1.420993476856122\n",
      "    mean_raw_obs_processing_ms: 0.4085503043135642\n",
      "  time_since_restore: 858.0117809772491\n",
      "  time_this_iter_s: 11.06906008720398\n",
      "  time_total_s: 858.0117809772491\n",
      "  timers:\n",
      "    learn_throughput: 1679.242\n",
      "    learn_time_ms: 595.507\n",
      "    load_throughput: 299052.712\n",
      "    load_time_ms: 3.344\n",
      "    sample_throughput: 98.617\n",
      "    sample_time_ms: 10140.239\n",
      "    update_time_ms: 1.721\n",
      "  timestamp: 1632132136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         858.012</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.384</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-02-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.4324324324324\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 74\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.745006643401252\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010161105698174798\n",
      "          policy_loss: -0.034267088439729476\n",
      "          total_loss: -0.04633745044055912\n",
      "          vf_explained_var: -0.6941686272621155\n",
      "          vf_loss: 0.0019503311241149074\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.8125\n",
      "    ram_util_percent: 66.60624999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04085222120157047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.429492814374756\n",
      "    mean_inference_ms: 1.420848828950496\n",
      "    mean_raw_obs_processing_ms: 0.4113396068390356\n",
      "  time_since_restore: 868.8501913547516\n",
      "  time_this_iter_s: 10.838410377502441\n",
      "  time_total_s: 868.8501913547516\n",
      "  timers:\n",
      "    learn_throughput: 1682.147\n",
      "    learn_time_ms: 594.479\n",
      "    load_throughput: 298397.422\n",
      "    load_time_ms: 3.351\n",
      "    sample_throughput: 98.642\n",
      "    sample_time_ms: 10137.663\n",
      "    update_time_ms: 1.724\n",
      "  timestamp: 1632132147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">          868.85</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.432</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-02-38\n",
      "  done: false\n",
      "  episode_len_mean: 996.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 75\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5399217486381531\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0119964276988946\n",
      "          policy_loss: -0.01891687342690097\n",
      "          total_loss: -0.029223648769160112\n",
      "          vf_explained_var: -0.8177337050437927\n",
      "          vf_loss: 0.0010436481065375523\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.75333333333334\n",
      "    ram_util_percent: 66.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0408489306470528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.397504898088\n",
      "    mean_inference_ms: 1.4207071460331335\n",
      "    mean_raw_obs_processing_ms: 0.4139673916194434\n",
      "  time_since_restore: 879.3766508102417\n",
      "  time_this_iter_s: 10.526459455490112\n",
      "  time_total_s: 879.3766508102417\n",
      "  timers:\n",
      "    learn_throughput: 1683.152\n",
      "    learn_time_ms: 594.124\n",
      "    load_throughput: 300759.661\n",
      "    load_time_ms: 3.325\n",
      "    sample_throughput: 98.739\n",
      "    sample_time_ms: 10127.76\n",
      "    update_time_ms: 1.725\n",
      "  timestamp: 1632132158\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         879.377</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.5263157894736\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 76\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.006657995118035\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011085671820336805\n",
      "          policy_loss: -0.12964454458819497\n",
      "          total_loss: -0.14445659512033066\n",
      "          vf_explained_var: 0.14642255008220673\n",
      "          vf_loss: 0.0015131151575284699\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.973333333333336\n",
      "    ram_util_percent: 66.74000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04084570478801931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.366322747246176\n",
      "    mean_inference_ms: 1.420570679715005\n",
      "    mean_raw_obs_processing_ms: 0.41644632332399784\n",
      "  time_since_restore: 890.4345195293427\n",
      "  time_this_iter_s: 11.057868719100952\n",
      "  time_total_s: 890.4345195293427\n",
      "  timers:\n",
      "    learn_throughput: 1683.613\n",
      "    learn_time_ms: 593.961\n",
      "    load_throughput: 302551.666\n",
      "    load_time_ms: 3.305\n",
      "    sample_throughput: 98.33\n",
      "    sample_time_ms: 10169.855\n",
      "    update_time_ms: 1.748\n",
      "  timestamp: 1632132169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         890.435</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.526</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-02-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.5714285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 77\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.648264029290941\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011490203711383708\n",
      "          policy_loss: -0.040572264873319205\n",
      "          total_loss: -0.05222460851073265\n",
      "          vf_explained_var: -0.5701891779899597\n",
      "          vf_loss: 0.000952353332993678\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.1125\n",
      "    ram_util_percent: 66.82499999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04084253977530096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.335846308597551\n",
      "    mean_inference_ms: 1.4204377591148658\n",
      "    mean_raw_obs_processing_ms: 0.41878040701452784\n",
      "  time_since_restore: 901.0493659973145\n",
      "  time_this_iter_s: 10.614846467971802\n",
      "  time_total_s: 901.0493659973145\n",
      "  timers:\n",
      "    learn_throughput: 1685.838\n",
      "    learn_time_ms: 593.177\n",
      "    load_throughput: 303915.252\n",
      "    load_time_ms: 3.29\n",
      "    sample_throughput: 98.101\n",
      "    sample_time_ms: 10193.528\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1632132179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         901.049</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.571</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-03-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.6153846153846\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 78\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5817711220847235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007225866560374743\n",
      "          policy_loss: 0.0013294087515936957\n",
      "          total_loss: -0.011312809669309191\n",
      "          vf_explained_var: -0.7538763284683228\n",
      "          vf_loss: 0.0007367587632163325\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95333333333334\n",
      "    ram_util_percent: 66.83999999999997\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040839407593878094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.30605414480382\n",
      "    mean_inference_ms: 1.420307537560354\n",
      "    mean_raw_obs_processing_ms: 0.42097706470467966\n",
      "  time_since_restore: 911.6874108314514\n",
      "  time_this_iter_s: 10.638044834136963\n",
      "  time_total_s: 911.6874108314514\n",
      "  timers:\n",
      "    learn_throughput: 1682.199\n",
      "    learn_time_ms: 594.46\n",
      "    load_throughput: 305515.785\n",
      "    load_time_ms: 3.273\n",
      "    sample_throughput: 98.178\n",
      "    sample_time_ms: 10185.626\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632132190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         911.687</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.615</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-03-21\n",
      "  done: false\n",
      "  episode_len_mean: 996.6582278481013\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 79\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5734075890647041\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011861822986915785\n",
      "          policy_loss: 0.008032419615321688\n",
      "          total_loss: -0.0031187715629736584\n",
      "          vf_explained_var: -0.47761133313179016\n",
      "          vf_loss: 0.0005795217866155629\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.90666666666666\n",
      "    ram_util_percent: 66.81999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040836310382341215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.276932471116435\n",
      "    mean_inference_ms: 1.4201800497384633\n",
      "    mean_raw_obs_processing_ms: 0.4230434121888898\n",
      "  time_since_restore: 922.3791472911835\n",
      "  time_this_iter_s: 10.691736459732056\n",
      "  time_total_s: 922.3791472911835\n",
      "  timers:\n",
      "    learn_throughput: 1684.48\n",
      "    learn_time_ms: 593.655\n",
      "    load_throughput: 305215.651\n",
      "    load_time_ms: 3.276\n",
      "    sample_throughput: 98.148\n",
      "    sample_time_ms: 10188.685\n",
      "    update_time_ms: 1.746\n",
      "  timestamp: 1632132201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         922.379</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.658</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 80\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0740519205729167\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004747600712632375\n",
      "          policy_loss: 0.005381399020552635\n",
      "          total_loss: -0.0027157407874862354\n",
      "          vf_explained_var: -0.61545729637146\n",
      "          vf_loss: 0.0010410635819425806\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.98666666666667\n",
      "    ram_util_percent: 66.74666666666668\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04083327966267686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.248449570246809\n",
      "    mean_inference_ms: 1.4200554447412967\n",
      "    mean_raw_obs_processing_ms: 0.4249862555606635\n",
      "  time_since_restore: 933.0294001102448\n",
      "  time_this_iter_s: 10.65025281906128\n",
      "  time_total_s: 933.0294001102448\n",
      "  timers:\n",
      "    learn_throughput: 1681.267\n",
      "    learn_time_ms: 594.79\n",
      "    load_throughput: 305095.763\n",
      "    load_time_ms: 3.278\n",
      "    sample_throughput: 98.608\n",
      "    sample_time_ms: 10141.135\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632132211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         933.029</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.7407407407408\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 81\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.802864678700765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010602383718361155\n",
      "          policy_loss: 0.021347948991590076\n",
      "          total_loss: 0.0059313370535771055\n",
      "          vf_explained_var: -0.8866456747055054\n",
      "          vf_loss: 0.0008228833806545784\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.75625\n",
      "    ram_util_percent: 66.6875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04083030655272117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.22059565897762\n",
      "    mean_inference_ms: 1.4199332658327695\n",
      "    mean_raw_obs_processing_ms: 0.42681179433710525\n",
      "  time_since_restore: 943.7484793663025\n",
      "  time_this_iter_s: 10.71907925605774\n",
      "  time_total_s: 943.7484793663025\n",
      "  timers:\n",
      "    learn_throughput: 1679.424\n",
      "    learn_time_ms: 595.442\n",
      "    load_throughput: 307904.361\n",
      "    load_time_ms: 3.248\n",
      "    sample_throughput: 98.638\n",
      "    sample_time_ms: 10138.085\n",
      "    update_time_ms: 1.741\n",
      "  timestamp: 1632132222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         943.748</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.741</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-03-53\n",
      "  done: false\n",
      "  episode_len_mean: 996.780487804878\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 82\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.871284956402249\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01447422682447789\n",
      "          policy_loss: 0.02866053059697151\n",
      "          total_loss: 0.012935171524683634\n",
      "          vf_explained_var: -0.5090610980987549\n",
      "          vf_loss: 0.0005449645297226703\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.92666666666667\n",
      "    ram_util_percent: 66.64666666666668\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04082739825955581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.193336497040017\n",
      "    mean_inference_ms: 1.4198137138361415\n",
      "    mean_raw_obs_processing_ms: 0.4285264498392485\n",
      "  time_since_restore: 954.383416891098\n",
      "  time_this_iter_s: 10.634937524795532\n",
      "  time_total_s: 954.383416891098\n",
      "  timers:\n",
      "    learn_throughput: 1678.569\n",
      "    learn_time_ms: 595.746\n",
      "    load_throughput: 312569.231\n",
      "    load_time_ms: 3.199\n",
      "    sample_throughput: 98.619\n",
      "    sample_time_ms: 10140.035\n",
      "    update_time_ms: 1.731\n",
      "  timestamp: 1632132233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         954.383</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 996.8192771084338\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 83\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7855614238315158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011270293292614319\n",
      "          policy_loss: 0.03990416203935941\n",
      "          total_loss: 0.0248137762149175\n",
      "          vf_explained_var: -0.313468337059021\n",
      "          vf_loss: 0.0008633664615141849\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.98\n",
      "    ram_util_percent: 66.62000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04082452735969759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.166695786794268\n",
      "    mean_inference_ms: 1.419697121035809\n",
      "    mean_raw_obs_processing_ms: 0.43013551048037696\n",
      "  time_since_restore: 965.313791513443\n",
      "  time_this_iter_s: 10.93037462234497\n",
      "  time_total_s: 965.313791513443\n",
      "  timers:\n",
      "    learn_throughput: 1677.517\n",
      "    learn_time_ms: 596.119\n",
      "    load_throughput: 312793.008\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 98.758\n",
      "    sample_time_ms: 10125.796\n",
      "    update_time_ms: 1.73\n",
      "  timestamp: 1632132244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         965.314</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.819</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.8571428571429\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 84\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6633292661772834\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012555273884961337\n",
      "          policy_loss: 0.08176937699317932\n",
      "          total_loss: 0.06810723352763388\n",
      "          vf_explained_var: -0.7121371626853943\n",
      "          vf_loss: 0.000852447669280486\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.08\n",
      "    ram_util_percent: 66.60666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04082171896633653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.140560876456991\n",
      "    mean_inference_ms: 1.4195831610493082\n",
      "    mean_raw_obs_processing_ms: 0.4316442259043585\n",
      "  time_since_restore: 975.5931615829468\n",
      "  time_this_iter_s: 10.279370069503784\n",
      "  time_total_s: 975.5931615829468\n",
      "  timers:\n",
      "    learn_throughput: 1677.209\n",
      "    learn_time_ms: 596.228\n",
      "    load_throughput: 313470.95\n",
      "    load_time_ms: 3.19\n",
      "    sample_throughput: 99.307\n",
      "    sample_time_ms: 10069.817\n",
      "    update_time_ms: 1.734\n",
      "  timestamp: 1632132254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         975.593</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-04-25\n",
      "  done: false\n",
      "  episode_len_mean: 996.8941176470588\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 85\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7210422052277459\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015746788644799874\n",
      "          policy_loss: 0.0058237951248884205\n",
      "          total_loss: -0.007885645495520698\n",
      "          vf_explained_var: -0.7286867499351501\n",
      "          vf_loss: 0.0008437109341482735\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.80625\n",
      "    ram_util_percent: 66.59375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040818949395183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.115022663160651\n",
      "    mean_inference_ms: 1.4194729080166195\n",
      "    mean_raw_obs_processing_ms: 0.4330579540889419\n",
      "  time_since_restore: 986.6442844867706\n",
      "  time_this_iter_s: 11.051122903823853\n",
      "  time_total_s: 986.6442844867706\n",
      "  timers:\n",
      "    learn_throughput: 1678.017\n",
      "    learn_time_ms: 595.941\n",
      "    load_throughput: 309376.056\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 98.789\n",
      "    sample_time_ms: 10122.559\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1632132265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         986.644</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.894</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-04-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.9302325581396\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 86\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8338145428233676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012358811109130032\n",
      "          policy_loss: -0.010352836839026875\n",
      "          total_loss: -0.026151328616672093\n",
      "          vf_explained_var: -0.800819993019104\n",
      "          vf_loss: 0.0004541047358846602\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.731249999999996\n",
      "    ram_util_percent: 66.56875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04081623703431003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.090061077252418\n",
      "    mean_inference_ms: 1.4193656209114245\n",
      "    mean_raw_obs_processing_ms: 0.43438110216914044\n",
      "  time_since_restore: 997.7387099266052\n",
      "  time_this_iter_s: 11.094425439834595\n",
      "  time_total_s: 997.7387099266052\n",
      "  timers:\n",
      "    learn_throughput: 1675.92\n",
      "    learn_time_ms: 596.687\n",
      "    load_throughput: 309613.565\n",
      "    load_time_ms: 3.23\n",
      "    sample_throughput: 98.761\n",
      "    sample_time_ms: 10125.487\n",
      "    update_time_ms: 1.746\n",
      "  timestamp: 1632132276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         997.739</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-04-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.9655172413793\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 87\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.637742993566725\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009418358306510024\n",
      "          policy_loss: 0.017414523950881427\n",
      "          total_loss: 0.0033224558354251915\n",
      "          vf_explained_var: -0.5822107195854187\n",
      "          vf_loss: 0.0006960144146837087\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.81875\n",
      "    ram_util_percent: 66.54375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04081357994852877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.0656627427965\n",
      "    mean_inference_ms: 1.419260889496606\n",
      "    mean_raw_obs_processing_ms: 0.43561800879318907\n",
      "  time_since_restore: 1008.8349130153656\n",
      "  time_this_iter_s: 11.096203088760376\n",
      "  time_total_s: 1008.8349130153656\n",
      "  timers:\n",
      "    learn_throughput: 1676.518\n",
      "    learn_time_ms: 596.474\n",
      "    load_throughput: 309501.616\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 98.291\n",
      "    sample_time_ms: 10173.828\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632132287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1008.83</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           996.966</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-04-59\n",
      "  done: false\n",
      "  episode_len_mean: 997.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 88\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6999667432573107\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014146510506295269\n",
      "          policy_loss: -0.029673678138189847\n",
      "          total_loss: -0.04369761780318287\n",
      "          vf_explained_var: -0.9999801516532898\n",
      "          vf_loss: 0.0005885033064664134\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.25\n",
      "    ram_util_percent: 66.51875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040810986603050006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.041805072465689\n",
      "    mean_inference_ms: 1.4191589817706531\n",
      "    mean_raw_obs_processing_ms: 0.4367729676624188\n",
      "  time_since_restore: 1019.9066739082336\n",
      "  time_this_iter_s: 11.071760892868042\n",
      "  time_total_s: 1019.9066739082336\n",
      "  timers:\n",
      "    learn_throughput: 1678.799\n",
      "    learn_time_ms: 595.664\n",
      "    load_throughput: 309394.313\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 97.866\n",
      "    sample_time_ms: 10218.011\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632132299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1019.91</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               997</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-05-10\n",
      "  done: false\n",
      "  episode_len_mean: 997.0337078651686\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 89\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.055920895602968\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03708825004243614\n",
      "          policy_loss: -0.13196271024644374\n",
      "          total_loss: -0.1332754297595885\n",
      "          vf_explained_var: 0.08467429131269455\n",
      "          vf_loss: 0.0029878449031255313\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.02666666666666\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04080843716680367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.018469987614367\n",
      "    mean_inference_ms: 1.419059727264711\n",
      "    mean_raw_obs_processing_ms: 0.4378502464061868\n",
      "  time_since_restore: 1030.9827539920807\n",
      "  time_this_iter_s: 11.076080083847046\n",
      "  time_total_s: 1030.9827539920807\n",
      "  timers:\n",
      "    learn_throughput: 1677.332\n",
      "    learn_time_ms: 596.185\n",
      "    load_throughput: 311071.688\n",
      "    load_time_ms: 3.215\n",
      "    sample_throughput: 97.504\n",
      "    sample_time_ms: 10255.955\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632132310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1030.98</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           997.034</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-05-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.5444444444445\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 90\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5249440736240811\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012692299940261876\n",
      "          policy_loss: 0.008837371236748166\n",
      "          total_loss: -0.0006930389338069492\n",
      "          vf_explained_var: -0.3671059012413025\n",
      "          vf_loss: 0.0025062903157150786\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.83170731707317\n",
      "    ram_util_percent: 66.39756097560975\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04080594715157171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.995592705112877\n",
      "    mean_inference_ms: 1.4189623392994075\n",
      "    mean_raw_obs_processing_ms: 0.44104401466388693\n",
      "  time_since_restore: 1059.4270396232605\n",
      "  time_this_iter_s: 28.44428563117981\n",
      "  time_total_s: 1059.4270396232605\n",
      "  timers:\n",
      "    learn_throughput: 1679.142\n",
      "    learn_time_ms: 595.542\n",
      "    load_throughput: 205343.438\n",
      "    load_time_ms: 4.87\n",
      "    sample_throughput: 83.095\n",
      "    sample_time_ms: 12034.36\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1632132338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1059.43</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.544</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.5934065934066\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 91\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4278546465767754\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015926932414305463\n",
      "          policy_loss: -0.10152752569152249\n",
      "          total_loss: -0.11040433678362105\n",
      "          vf_explained_var: -0.27411922812461853\n",
      "          vf_loss: 0.0013702299132192922\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.023529411764706\n",
      "    ram_util_percent: 66.08823529411765\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04080351353060281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.973285282751155\n",
      "    mean_inference_ms: 1.4188664701901317\n",
      "    mean_raw_obs_processing_ms: 0.444094482361687\n",
      "  time_since_restore: 1071.1360805034637\n",
      "  time_this_iter_s: 11.709040880203247\n",
      "  time_total_s: 1071.1360805034637\n",
      "  timers:\n",
      "    learn_throughput: 1681.851\n",
      "    learn_time_ms: 594.583\n",
      "    load_throughput: 204902.051\n",
      "    load_time_ms: 4.88\n",
      "    sample_throughput: 82.411\n",
      "    sample_time_ms: 12134.32\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632132350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1071.14</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.593</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-06-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.6413043478261\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 92\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4737313879860772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014510054089447048\n",
      "          policy_loss: -0.0008005213406350877\n",
      "          total_loss: -0.011278209421369765\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005867677954180787\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.76428571428572\n",
      "    ram_util_percent: 66.60000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0408011365698595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.951320405943957\n",
      "    mean_inference_ms: 1.4187735287199048\n",
      "    mean_raw_obs_processing_ms: 0.4470078642129225\n",
      "  time_since_restore: 1081.104204416275\n",
      "  time_this_iter_s: 9.96812391281128\n",
      "  time_total_s: 1081.104204416275\n",
      "  timers:\n",
      "    learn_throughput: 1683.755\n",
      "    learn_time_ms: 593.911\n",
      "    load_throughput: 205151.603\n",
      "    load_time_ms: 4.874\n",
      "    sample_throughput: 82.862\n",
      "    sample_time_ms: 12068.326\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632132360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">          1081.1</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.641</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-06-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.6881720430108\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 93\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2313952856593662\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0040323658829963215\n",
      "          policy_loss: 0.03410986504620976\n",
      "          total_loss: 0.023185092252161768\n",
      "          vf_explained_var: -0.7881138920783997\n",
      "          vf_loss: 0.00036848666374377593\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.24285714285715\n",
      "    ram_util_percent: 66.62857142857145\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0407987730406679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.929684472089363\n",
      "    mean_inference_ms: 1.4186812639265722\n",
      "    mean_raw_obs_processing_ms: 0.4497900504901336\n",
      "  time_since_restore: 1091.0083150863647\n",
      "  time_this_iter_s: 9.904110670089722\n",
      "  time_total_s: 1091.0083150863647\n",
      "  timers:\n",
      "    learn_throughput: 1683.945\n",
      "    learn_time_ms: 593.844\n",
      "    load_throughput: 204607.182\n",
      "    load_time_ms: 4.887\n",
      "    sample_throughput: 83.572\n",
      "    sample_time_ms: 11965.779\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632132370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1091.01</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.688</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-06-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.7340425531914\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 94\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3582000388039484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007400783599944896\n",
      "          policy_loss: 0.00591578007572227\n",
      "          total_loss: -0.005813854353295432\n",
      "          vf_explained_var: -0.8675096035003662\n",
      "          vf_loss: 0.0009157065298899802\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.96\n",
      "    ram_util_percent: 66.39999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04079643374981964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.908411622681696\n",
      "    mean_inference_ms: 1.4185893981622497\n",
      "    mean_raw_obs_processing_ms: 0.45244665982253124\n",
      "  time_since_restore: 1101.27654504776\n",
      "  time_this_iter_s: 10.268229961395264\n",
      "  time_total_s: 1101.27654504776\n",
      "  timers:\n",
      "    learn_throughput: 1683.414\n",
      "    learn_time_ms: 594.031\n",
      "    load_throughput: 204526.366\n",
      "    load_time_ms: 4.889\n",
      "    sample_throughput: 83.581\n",
      "    sample_time_ms: 11964.481\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632132380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1101.28</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.734</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-06-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.7789473684211\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 95\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4851659999953375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012915107168606157\n",
      "          policy_loss: -0.02315259901806712\n",
      "          total_loss: -0.03562256990828448\n",
      "          vf_explained_var: -0.6930463910102844\n",
      "          vf_loss: 0.0007471215290327867\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.16\n",
      "    ram_util_percent: 66.23333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04079411681023619\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.88752053373876\n",
      "    mean_inference_ms: 1.4184982816226406\n",
      "    mean_raw_obs_processing_ms: 0.4549830937029842\n",
      "  time_since_restore: 1111.807379245758\n",
      "  time_this_iter_s: 10.530834197998047\n",
      "  time_total_s: 1111.807379245758\n",
      "  timers:\n",
      "    learn_throughput: 1680.058\n",
      "    learn_time_ms: 595.218\n",
      "    load_throughput: 206509.146\n",
      "    load_time_ms: 4.842\n",
      "    sample_throughput: 83.954\n",
      "    sample_time_ms: 11911.302\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632132391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1111.81</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.779</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-06-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.8229166666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 96\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2233661519156562\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012250307000302128\n",
      "          policy_loss: 0.005738155957725313\n",
      "          total_loss: -0.00379505240254932\n",
      "          vf_explained_var: -0.9102221131324768\n",
      "          vf_loss: 0.0011500218789377767\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85999999999999\n",
      "    ram_util_percent: 66.20000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04079181338569768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.866992915193878\n",
      "    mean_inference_ms: 1.418408044565787\n",
      "    mean_raw_obs_processing_ms: 0.45740442846031826\n",
      "  time_since_restore: 1122.272541999817\n",
      "  time_this_iter_s: 10.465162754058838\n",
      "  time_total_s: 1122.272541999817\n",
      "  timers:\n",
      "    learn_throughput: 1679.984\n",
      "    learn_time_ms: 595.244\n",
      "    load_throughput: 206391.269\n",
      "    load_time_ms: 4.845\n",
      "    sample_throughput: 84.4\n",
      "    sample_time_ms: 11848.338\n",
      "    update_time_ms: 1.748\n",
      "  timestamp: 1632132401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1122.27</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.823</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-06-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.8659793814433\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 97\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.647290711932712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025890152762918936\n",
      "          policy_loss: -0.06111644140134255\n",
      "          total_loss: -0.07249728383289443\n",
      "          vf_explained_var: -0.5092973709106445\n",
      "          vf_loss: 0.001815341927188759\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.84\n",
      "    ram_util_percent: 66.20000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04078951907966252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.846808592280876\n",
      "    mean_inference_ms: 1.4183181820069264\n",
      "    mean_raw_obs_processing_ms: 0.4597159074139695\n",
      "  time_since_restore: 1132.6324090957642\n",
      "  time_this_iter_s: 10.359867095947266\n",
      "  time_total_s: 1132.6324090957642\n",
      "  timers:\n",
      "    learn_throughput: 1677.914\n",
      "    learn_time_ms: 595.978\n",
      "    load_throughput: 206239.041\n",
      "    load_time_ms: 4.849\n",
      "    sample_throughput: 84.933\n",
      "    sample_time_ms: 11773.951\n",
      "    update_time_ms: 1.746\n",
      "  timestamp: 1632132411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1132.63</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.866</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-07-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.9081632653061\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 98\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5358658207787408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02330661193643727\n",
      "          policy_loss: 0.016847639448112912\n",
      "          total_loss: 0.007303644054465824\n",
      "          vf_explained_var: -0.6382951736450195\n",
      "          vf_loss: 0.0013900493614427331\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.45\n",
      "    ram_util_percent: 66.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04078726375573898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.826929099108623\n",
      "    mean_inference_ms: 1.4182291740152837\n",
      "    mean_raw_obs_processing_ms: 0.4619217166793221\n",
      "  time_since_restore: 1142.7030923366547\n",
      "  time_this_iter_s: 10.070683240890503\n",
      "  time_total_s: 1142.7030923366547\n",
      "  timers:\n",
      "    learn_throughput: 1678.626\n",
      "    learn_time_ms: 595.725\n",
      "    load_throughput: 206385.175\n",
      "    load_time_ms: 4.845\n",
      "    sample_throughput: 85.66\n",
      "    sample_time_ms: 11674.09\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632132422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">          1142.7</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.908</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-07-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.9494949494949\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 99\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8475914239883422\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01733684130771383\n",
      "          policy_loss: -0.10382630736049679\n",
      "          total_loss: -0.1134062730293307\n",
      "          vf_explained_var: -0.03402813896536827\n",
      "          vf_loss: 0.003959013841166679\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.164285714285725\n",
      "    ram_util_percent: 66.37142857142855\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0407850376565168\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.807324252628954\n",
      "    mean_inference_ms: 1.4181405175248494\n",
      "    mean_raw_obs_processing_ms: 0.46402616820707965\n",
      "  time_since_restore: 1152.5419886112213\n",
      "  time_this_iter_s: 9.83889627456665\n",
      "  time_total_s: 1152.5419886112213\n",
      "  timers:\n",
      "    learn_throughput: 1680.909\n",
      "    learn_time_ms: 594.916\n",
      "    load_throughput: 206284.686\n",
      "    load_time_ms: 4.848\n",
      "    sample_throughput: 86.571\n",
      "    sample_time_ms: 11551.153\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632132431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1152.54</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           995.949</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-07-22\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 100\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6626736137602065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011528256122660787\n",
      "          policy_loss: -0.05002714287903574\n",
      "          total_loss: -0.06192832158671485\n",
      "          vf_explained_var: -0.4401041567325592\n",
      "          vf_loss: 0.0014427024263164235\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79333333333333\n",
      "    ram_util_percent: 66.42666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04078284056251488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.788039827276405\n",
      "    mean_inference_ms: 1.418052423798574\n",
      "    mean_raw_obs_processing_ms: 0.4660350284807879\n",
      "  time_since_restore: 1162.9125020503998\n",
      "  time_this_iter_s: 10.370513439178467\n",
      "  time_total_s: 1162.9125020503998\n",
      "  timers:\n",
      "    learn_throughput: 1683.613\n",
      "    learn_time_ms: 593.961\n",
      "    load_throughput: 312800.006\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 102.602\n",
      "    sample_time_ms: 9746.369\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632132442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1162.91</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-07-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 101\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5615793188412985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007729148698227463\n",
      "          policy_loss: -0.02521112639353507\n",
      "          total_loss: -0.037740118806767795\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000885803909235013\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.74666666666666\n",
      "    ram_util_percent: 66.52666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04075069149367206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.315435502657277\n",
      "    mean_inference_ms: 1.4166107846683624\n",
      "    mean_raw_obs_processing_ms: 0.4710348094715248\n",
      "  time_since_restore: 1173.4949779510498\n",
      "  time_this_iter_s: 10.582475900650024\n",
      "  time_total_s: 1173.4949779510498\n",
      "  timers:\n",
      "    learn_throughput: 1682.735\n",
      "    learn_time_ms: 594.271\n",
      "    load_throughput: 313515.469\n",
      "    load_time_ms: 3.19\n",
      "    sample_throughput: 103.805\n",
      "    sample_time_ms: 9633.411\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632132452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         1173.49</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-07-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 102\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.639965017636617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012673230601339193\n",
      "          policy_loss: -0.03982460124211179\n",
      "          total_loss: -0.0512544303925501\n",
      "          vf_explained_var: -0.23795267939567566\n",
      "          vf_loss: 0.001360919308434758\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95333333333333\n",
      "    ram_util_percent: 66.60000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0407261145772009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.080301231301972\n",
      "    mean_inference_ms: 1.4155531725909893\n",
      "    mean_raw_obs_processing_ms: 0.4761103741031736\n",
      "  time_since_restore: 1183.8013758659363\n",
      "  time_this_iter_s: 10.306397914886475\n",
      "  time_total_s: 1183.8013758659363\n",
      "  timers:\n",
      "    learn_throughput: 1680.615\n",
      "    learn_time_ms: 595.02\n",
      "    load_throughput: 312755.689\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 103.451\n",
      "    sample_time_ms: 9666.456\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1632132463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">          1183.8</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-07-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 103\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7493347154723273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01504816379802156\n",
      "          policy_loss: 0.01967264697369602\n",
      "          total_loss: 0.00729214135143492\n",
      "          vf_explained_var: -0.8845155239105225\n",
      "          vf_loss: 0.0008276401530666691\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.314285714285724\n",
      "    ram_util_percent: 66.6857142857143\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04070748869646598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.925140436312494\n",
      "    mean_inference_ms: 1.4147555318338638\n",
      "    mean_raw_obs_processing_ms: 0.4812137369936241\n",
      "  time_since_restore: 1193.4873802661896\n",
      "  time_this_iter_s: 9.686004400253296\n",
      "  time_total_s: 1193.4873802661896\n",
      "  timers:\n",
      "    learn_throughput: 1680.913\n",
      "    learn_time_ms: 594.915\n",
      "    load_throughput: 314220.088\n",
      "    load_time_ms: 3.182\n",
      "    sample_throughput: 103.684\n",
      "    sample_time_ms: 9644.728\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632132472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1193.49</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-08-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 104\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7845824109183417\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01436718741332628\n",
      "          policy_loss: -0.05763891115784645\n",
      "          total_loss: -0.06949320654902193\n",
      "          vf_explained_var: -0.6137697100639343\n",
      "          vf_loss: 0.001900248765014112\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.07857142857142\n",
      "    ram_util_percent: 66.74285714285713\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04069293300473811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.810239368832294\n",
      "    mean_inference_ms: 1.4141425728592258\n",
      "    mean_raw_obs_processing_ms: 0.48631246668153794\n",
      "  time_since_restore: 1203.3139896392822\n",
      "  time_this_iter_s: 9.826609373092651\n",
      "  time_total_s: 1203.3139896392822\n",
      "  timers:\n",
      "    learn_throughput: 1682.073\n",
      "    learn_time_ms: 594.505\n",
      "    load_throughput: 314344.9\n",
      "    load_time_ms: 3.181\n",
      "    sample_throughput: 104.156\n",
      "    sample_time_ms: 9600.96\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632132482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1203.31</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-08-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 105\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7568753189510768\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009279128040414105\n",
      "          policy_loss: -0.08740552105009555\n",
      "          total_loss: -0.10129107928110494\n",
      "          vf_explained_var: -0.5958526730537415\n",
      "          vf_loss: 0.00104081969604724\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.09285714285714\n",
      "    ram_util_percent: 66.82857142857141\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040681386416275306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.719644933332097\n",
      "    mean_inference_ms: 1.4136458288277856\n",
      "    mean_raw_obs_processing_ms: 0.491384766707593\n",
      "  time_since_restore: 1213.354966878891\n",
      "  time_this_iter_s: 10.040977239608765\n",
      "  time_total_s: 1213.354966878891\n",
      "  timers:\n",
      "    learn_throughput: 1686.116\n",
      "    learn_time_ms: 593.079\n",
      "    load_throughput: 313367.9\n",
      "    load_time_ms: 3.191\n",
      "    sample_throughput: 104.675\n",
      "    sample_time_ms: 9553.374\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1632132492\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         1213.35</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-08-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 106\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7599756585227118\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013332402341523657\n",
      "          policy_loss: -0.08476591815965043\n",
      "          total_loss: -0.09773766187330087\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008314040382780756\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.786666666666676\n",
      "    ram_util_percent: 66.89999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04067183461588837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.645092217177442\n",
      "    mean_inference_ms: 1.4132300034459808\n",
      "    mean_raw_obs_processing_ms: 0.4964221075454462\n",
      "  time_since_restore: 1223.9059927463531\n",
      "  time_this_iter_s: 10.551025867462158\n",
      "  time_total_s: 1223.9059927463531\n",
      "  timers:\n",
      "    learn_throughput: 1687.376\n",
      "    learn_time_ms: 592.636\n",
      "    load_throughput: 312490.054\n",
      "    load_time_ms: 3.2\n",
      "    sample_throughput: 104.576\n",
      "    sample_time_ms: 9562.388\n",
      "    update_time_ms: 1.782\n",
      "  timestamp: 1632132503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1223.91</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-08-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 107\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.829172013865577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012283799545976038\n",
      "          policy_loss: 0.06862365293006102\n",
      "          total_loss: 0.05634690978460842\n",
      "          vf_explained_var: -0.5906594395637512\n",
      "          vf_loss: 0.0025169747571150464\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.22666666666666\n",
      "    ram_util_percent: 66.99333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04066393140886728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.5820575039693\n",
      "    mean_inference_ms: 1.4128770768756527\n",
      "    mean_raw_obs_processing_ms: 0.5014254632842146\n",
      "  time_since_restore: 1234.3151144981384\n",
      "  time_this_iter_s: 10.409121751785278\n",
      "  time_total_s: 1234.3151144981384\n",
      "  timers:\n",
      "    learn_throughput: 1689.367\n",
      "    learn_time_ms: 591.938\n",
      "    load_throughput: 312711.385\n",
      "    load_time_ms: 3.198\n",
      "    sample_throughput: 104.515\n",
      "    sample_time_ms: 9568.008\n",
      "    update_time_ms: 1.79\n",
      "  timestamp: 1632132513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         1234.32</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-08-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 108\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8973805970615811\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008465360107655704\n",
      "          policy_loss: -0.03721758425235748\n",
      "          total_loss: -0.05230071594317754\n",
      "          vf_explained_var: -0.10221656411886215\n",
      "          vf_loss: 0.0014800314008930906\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.00000000000001\n",
      "    ram_util_percent: 67.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04065726127409068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.527722048979648\n",
      "    mean_inference_ms: 1.4125714589245133\n",
      "    mean_raw_obs_processing_ms: 0.5063939169284751\n",
      "  time_since_restore: 1244.543496131897\n",
      "  time_this_iter_s: 10.228381633758545\n",
      "  time_total_s: 1244.543496131897\n",
      "  timers:\n",
      "    learn_throughput: 1688.911\n",
      "    learn_time_ms: 592.098\n",
      "    load_throughput: 312662.433\n",
      "    load_time_ms: 3.198\n",
      "    sample_throughput: 104.345\n",
      "    sample_time_ms: 9583.616\n",
      "    update_time_ms: 1.782\n",
      "  timestamp: 1632132524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         1244.54</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-08-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 109\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8564255926344129\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017238816821179902\n",
      "          policy_loss: 0.058320539279116526\n",
      "          total_loss: 0.04815281459854709\n",
      "          vf_explained_var: 0.4768558442592621\n",
      "          vf_loss: 0.0034875062158486497\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.15\n",
      "    ram_util_percent: 67.07857142857144\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04065174309833758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.480127903752733\n",
      "    mean_inference_ms: 1.4123052000575564\n",
      "    mean_raw_obs_processing_ms: 0.5113243580498468\n",
      "  time_since_restore: 1254.753321647644\n",
      "  time_this_iter_s: 10.20982551574707\n",
      "  time_total_s: 1254.753321647644\n",
      "  timers:\n",
      "    learn_throughput: 1687.78\n",
      "    learn_time_ms: 592.494\n",
      "    load_throughput: 312727.707\n",
      "    load_time_ms: 3.198\n",
      "    sample_throughput: 103.947\n",
      "    sample_time_ms: 9620.312\n",
      "    update_time_ms: 1.78\n",
      "  timestamp: 1632132534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         1254.75</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-09-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 110\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.921550084484948\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013719435933428049\n",
      "          policy_loss: -0.07562887788646751\n",
      "          total_loss: -0.08867176295154625\n",
      "          vf_explained_var: -0.1230088323354721\n",
      "          vf_loss: 0.0022657906688335868\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.36\n",
      "    ram_util_percent: 67.09333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04064720692335981\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.437926127152394\n",
      "    mean_inference_ms: 1.4120734553532361\n",
      "    mean_raw_obs_processing_ms: 0.5162176779262575\n",
      "  time_since_restore: 1264.705820798874\n",
      "  time_this_iter_s: 9.952499151229858\n",
      "  time_total_s: 1264.705820798874\n",
      "  timers:\n",
      "    learn_throughput: 1684.497\n",
      "    learn_time_ms: 593.649\n",
      "    load_throughput: 313609.236\n",
      "    load_time_ms: 3.189\n",
      "    sample_throughput: 104.413\n",
      "    sample_time_ms: 9577.388\n",
      "    update_time_ms: 1.777\n",
      "  timestamp: 1632132544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         1264.71</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-09-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 111\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.039230063226488\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013208286862128342\n",
      "          policy_loss: -0.047241618898179794\n",
      "          total_loss: -0.06152551248669624\n",
      "          vf_explained_var: -0.47115975618362427\n",
      "          vf_loss: 0.002347139451497545\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.057142857142864\n",
      "    ram_util_percent: 67.07142857142857\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04064345569324282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.400162833085272\n",
      "    mean_inference_ms: 1.4118693792900519\n",
      "    mean_raw_obs_processing_ms: 0.5210677106283428\n",
      "  time_since_restore: 1274.9479267597198\n",
      "  time_this_iter_s: 10.242105960845947\n",
      "  time_total_s: 1274.9479267597198\n",
      "  timers:\n",
      "    learn_throughput: 1686.008\n",
      "    learn_time_ms: 593.117\n",
      "    load_throughput: 313794.589\n",
      "    load_time_ms: 3.187\n",
      "    sample_throughput: 104.779\n",
      "    sample_time_ms: 9543.878\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632132554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         1274.95</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-09-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 112\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0753041174676685\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01307366332317613\n",
      "          policy_loss: -0.08352688054243723\n",
      "          total_loss: -0.0982394613739517\n",
      "          vf_explained_var: -0.26778584718704224\n",
      "          vf_loss: 0.0023175274642805257\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.14285714285713\n",
      "    ram_util_percent: 67.07857142857144\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04063854397979909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.36600910867331\n",
      "    mean_inference_ms: 1.4116868925511994\n",
      "    mean_raw_obs_processing_ms: 0.5258799320094434\n",
      "  time_since_restore: 1284.7728297710419\n",
      "  time_this_iter_s: 9.824903011322021\n",
      "  time_total_s: 1284.7728297710419\n",
      "  timers:\n",
      "    learn_throughput: 1688.076\n",
      "    learn_time_ms: 592.39\n",
      "    load_throughput: 313987.214\n",
      "    load_time_ms: 3.185\n",
      "    sample_throughput: 105.302\n",
      "    sample_time_ms: 9496.484\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632132564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         1284.77</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-09-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 113\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1569346878263684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01174071692463782\n",
      "          policy_loss: -0.08957970750828584\n",
      "          total_loss: -0.10583217384086716\n",
      "          vf_explained_var: -0.7868576645851135\n",
      "          vf_loss: 0.0019735254269714155\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.97333333333333\n",
      "    ram_util_percent: 67.05333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406342048348047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.334899522270293\n",
      "    mean_inference_ms: 1.4115208191056632\n",
      "    mean_raw_obs_processing_ms: 0.530652844688412\n",
      "  time_since_restore: 1294.798141002655\n",
      "  time_this_iter_s: 10.02531123161316\n",
      "  time_total_s: 1294.798141002655\n",
      "  timers:\n",
      "    learn_throughput: 1688.742\n",
      "    learn_time_ms: 592.157\n",
      "    load_throughput: 314255.402\n",
      "    load_time_ms: 3.182\n",
      "    sample_throughput: 104.924\n",
      "    sample_time_ms: 9530.687\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632132574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">          1294.8</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 114\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.090310666296217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012381255490338737\n",
      "          policy_loss: -0.1180449430934257\n",
      "          total_loss: -0.13401883095502853\n",
      "          vf_explained_var: -0.3954167366027832\n",
      "          vf_loss: 0.001403464477819701\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.82666666666666\n",
      "    ram_util_percent: 67.00666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04063043832457289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.306447767132484\n",
      "    mean_inference_ms: 1.4113714419405448\n",
      "    mean_raw_obs_processing_ms: 0.535387521463574\n",
      "  time_since_restore: 1305.3901529312134\n",
      "  time_this_iter_s: 10.59201192855835\n",
      "  time_total_s: 1305.3901529312134\n",
      "  timers:\n",
      "    learn_throughput: 1688.069\n",
      "    learn_time_ms: 592.393\n",
      "    load_throughput: 313869.732\n",
      "    load_time_ms: 3.186\n",
      "    sample_throughput: 104.091\n",
      "    sample_time_ms: 9606.972\n",
      "    update_time_ms: 1.769\n",
      "  timestamp: 1632132585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         1305.39</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-09-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 115\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0944940911398993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010167176758180465\n",
      "          policy_loss: -0.08693979494273663\n",
      "          total_loss: -0.10311116522385014\n",
      "          vf_explained_var: -0.9304311275482178\n",
      "          vf_loss: 0.001878307382705518\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.07142857142857\n",
      "    ram_util_percent: 67.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04062707022375739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.280289572084994\n",
      "    mean_inference_ms: 1.4112371757240934\n",
      "    mean_raw_obs_processing_ms: 0.5400828783858862\n",
      "  time_since_restore: 1315.5581953525543\n",
      "  time_this_iter_s: 10.168042421340942\n",
      "  time_total_s: 1315.5581953525543\n",
      "  timers:\n",
      "    learn_throughput: 1687.525\n",
      "    learn_time_ms: 592.584\n",
      "    load_throughput: 314394.381\n",
      "    load_time_ms: 3.181\n",
      "    sample_throughput: 103.955\n",
      "    sample_time_ms: 9619.51\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632132595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         1315.56</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-10-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 116\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1003246770964727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01207267765090863\n",
      "          policy_loss: -0.028623906812734076\n",
      "          total_loss: -0.04522075984213087\n",
      "          vf_explained_var: -0.4482373595237732\n",
      "          vf_loss: 0.0009685098672182195\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85333333333333\n",
      "    ram_util_percent: 67.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04062409492960268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.256121941492879\n",
      "    mean_inference_ms: 1.4111160491250723\n",
      "    mean_raw_obs_processing_ms: 0.5447392092760843\n",
      "  time_since_restore: 1325.669512271881\n",
      "  time_this_iter_s: 10.111316919326782\n",
      "  time_total_s: 1325.669512271881\n",
      "  timers:\n",
      "    learn_throughput: 1686.942\n",
      "    learn_time_ms: 592.788\n",
      "    load_throughput: 315513.029\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 104.435\n",
      "    sample_time_ms: 9575.37\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632132605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1325.67</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-10-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 117\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.163239622116089\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014402667099471205\n",
      "          policy_loss: -0.06417756229639053\n",
      "          total_loss: -0.07916433935364088\n",
      "          vf_explained_var: -0.2546488046646118\n",
      "          vf_loss: 0.0025442361165510696\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.357142857142854\n",
      "    ram_util_percent: 66.98571428571428\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04062146844845415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.23369507478992\n",
      "    mean_inference_ms: 1.4110076138148628\n",
      "    mean_raw_obs_processing_ms: 0.5493576181184681\n",
      "  time_since_restore: 1335.5569643974304\n",
      "  time_this_iter_s: 9.887452125549316\n",
      "  time_total_s: 1335.5569643974304\n",
      "  timers:\n",
      "    learn_throughput: 1686.914\n",
      "    learn_time_ms: 592.799\n",
      "    load_throughput: 315306.676\n",
      "    load_time_ms: 3.172\n",
      "    sample_throughput: 105.007\n",
      "    sample_time_ms: 9523.19\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632132615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         1335.56</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-10-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 118\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.234185700946384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011157882156489407\n",
      "          policy_loss: 0.03616954812573062\n",
      "          total_loss: 0.018818356614145968\n",
      "          vf_explained_var: -0.9593501687049866\n",
      "          vf_loss: 0.0018132844461231595\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.18571428571429\n",
      "    ram_util_percent: 66.95714285714284\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061914525705441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.212804679885581\n",
      "    mean_inference_ms: 1.4109099313400728\n",
      "    mean_raw_obs_processing_ms: 0.5539396325785431\n",
      "  time_since_restore: 1345.4485974311829\n",
      "  time_this_iter_s: 9.891633033752441\n",
      "  time_total_s: 1345.4485974311829\n",
      "  timers:\n",
      "    learn_throughput: 1685.017\n",
      "    learn_time_ms: 593.466\n",
      "    load_throughput: 314302.5\n",
      "    load_time_ms: 3.182\n",
      "    sample_throughput: 105.387\n",
      "    sample_time_ms: 9488.816\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632132625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         1345.45</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-10-35\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 119\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.220852178997464\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014939703039475057\n",
      "          policy_loss: -0.08807926128307979\n",
      "          total_loss: -0.10478469034036\n",
      "          vf_explained_var: -0.5743306279182434\n",
      "          vf_loss: 0.0012487770601486167\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.228571428571435\n",
      "    ram_util_percent: 66.92857142857142\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040616930774571375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.193259958577496\n",
      "    mean_inference_ms: 1.4108046065707855\n",
      "    mean_raw_obs_processing_ms: 0.5584837916303426\n",
      "  time_since_restore: 1355.3466703891754\n",
      "  time_this_iter_s: 9.898072957992554\n",
      "  time_total_s: 1355.3466703891754\n",
      "  timers:\n",
      "    learn_throughput: 1687.392\n",
      "    learn_time_ms: 592.631\n",
      "    load_throughput: 314196.55\n",
      "    load_time_ms: 3.183\n",
      "    sample_throughput: 105.725\n",
      "    sample_time_ms: 9458.49\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632132635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1355.35</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 120\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.204995244079166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012159428560475641\n",
      "          policy_loss: -0.09263535001211697\n",
      "          total_loss: -0.10949225864476628\n",
      "          vf_explained_var: -0.4757828116416931\n",
      "          vf_loss: 0.001730454324408331\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.20526315789474\n",
      "    ram_util_percent: 66.71578947368421\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040614888827668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.174917435590404\n",
      "    mean_inference_ms: 1.4107031094450107\n",
      "    mean_raw_obs_processing_ms: 0.5643695113453049\n",
      "  time_since_restore: 1381.8943107128143\n",
      "  time_this_iter_s: 26.547640323638916\n",
      "  time_total_s: 1381.8943107128143\n",
      "  timers:\n",
      "    learn_throughput: 1687.42\n",
      "    learn_time_ms: 592.621\n",
      "    load_throughput: 205783.702\n",
      "    load_time_ms: 4.859\n",
      "    sample_throughput: 89.958\n",
      "    sample_time_ms: 11116.316\n",
      "    update_time_ms: 1.757\n",
      "  timestamp: 1632132661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         1381.89</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-11-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 121\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0398449354701573\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015261283618736915\n",
      "          policy_loss: 0.018025765774978532\n",
      "          total_loss: 0.0029808600743611653\n",
      "          vf_explained_var: -0.5013144016265869\n",
      "          vf_loss: 0.0010076543788373885\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.088235294117645\n",
      "    ram_util_percent: 66.24705882352941\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061304773747382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.157834846031433\n",
      "    mean_inference_ms: 1.410609287204213\n",
      "    mean_raw_obs_processing_ms: 0.5702076283337976\n",
      "  time_since_restore: 1393.5731670856476\n",
      "  time_this_iter_s: 11.678856372833252\n",
      "  time_total_s: 1393.5731670856476\n",
      "  timers:\n",
      "    learn_throughput: 1685.842\n",
      "    learn_time_ms: 593.175\n",
      "    load_throughput: 205812.986\n",
      "    load_time_ms: 4.859\n",
      "    sample_throughput: 88.814\n",
      "    sample_time_ms: 11259.444\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632132673\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         1393.57</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 122\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.142591342661116\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014610785237965097\n",
      "          policy_loss: 0.06645174125830332\n",
      "          total_loss: 0.050112998651133646\n",
      "          vf_explained_var: -0.89571213722229\n",
      "          vf_loss: 0.0009265225771943936\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.57857142857143\n",
      "    ram_util_percent: 66.79285714285712\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040611429122817545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.141770324645801\n",
      "    mean_inference_ms: 1.4105220430992211\n",
      "    mean_raw_obs_processing_ms: 0.5759989864600108\n",
      "  time_since_restore: 1403.5355739593506\n",
      "  time_this_iter_s: 9.962406873703003\n",
      "  time_total_s: 1403.5355739593506\n",
      "  timers:\n",
      "    learn_throughput: 1685.891\n",
      "    learn_time_ms: 593.158\n",
      "    load_throughput: 206005.049\n",
      "    load_time_ms: 4.854\n",
      "    sample_throughput: 88.706\n",
      "    sample_time_ms: 11273.224\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632132683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         1403.54</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-11-33\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 123\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.071274687184228\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01315439358902112\n",
      "          policy_loss: -0.08507957112871939\n",
      "          total_loss: -0.1001588961109519\n",
      "          vf_explained_var: -0.6380782127380371\n",
      "          vf_loss: 0.0018875042468102443\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.300000000000004\n",
      "    ram_util_percent: 66.80714285714284\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061001645762145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.12651384805918\n",
      "    mean_inference_ms: 1.410440920752111\n",
      "    mean_raw_obs_processing_ms: 0.5817443290200969\n",
      "  time_since_restore: 1413.4892137050629\n",
      "  time_this_iter_s: 9.95363974571228\n",
      "  time_total_s: 1413.4892137050629\n",
      "  timers:\n",
      "    learn_throughput: 1683.358\n",
      "    learn_time_ms: 594.051\n",
      "    load_throughput: 205760.484\n",
      "    load_time_ms: 4.86\n",
      "    sample_throughput: 88.769\n",
      "    sample_time_ms: 11265.151\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632132693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         1413.49</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-11-43\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 124\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9564607249365913\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012605317570645803\n",
      "          policy_loss: -0.050273012204302685\n",
      "          total_loss: -0.06237065527174208\n",
      "          vf_explained_var: 0.18794861435890198\n",
      "          vf_loss: 0.0038773999109657275\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.04666666666666\n",
      "    ram_util_percent: 66.67333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060878580474209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.111992522269693\n",
      "    mean_inference_ms: 1.4103648780090603\n",
      "    mean_raw_obs_processing_ms: 0.5874437116430105\n",
      "  time_since_restore: 1423.5398366451263\n",
      "  time_this_iter_s: 10.050622940063477\n",
      "  time_total_s: 1423.5398366451263\n",
      "  timers:\n",
      "    learn_throughput: 1681.835\n",
      "    learn_time_ms: 594.589\n",
      "    load_throughput: 205850.36\n",
      "    load_time_ms: 4.858\n",
      "    sample_throughput: 89.202\n",
      "    sample_time_ms: 11210.479\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1632132703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         1423.54</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-11-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 125\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9589169436030918\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014654057370452694\n",
      "          policy_loss: -0.046131092806657156\n",
      "          total_loss: -0.047830865697728264\n",
      "          vf_explained_var: 0.4168159067630768\n",
      "          vf_loss: 0.013716423724933218\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.959999999999994\n",
      "    ram_util_percent: 66.53999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060763333954611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.098263502982055\n",
      "    mean_inference_ms: 1.4102953800319313\n",
      "    mean_raw_obs_processing_ms: 0.5930986743977837\n",
      "  time_since_restore: 1434.1420798301697\n",
      "  time_this_iter_s: 10.602243185043335\n",
      "  time_total_s: 1434.1420798301697\n",
      "  timers:\n",
      "    learn_throughput: 1679.51\n",
      "    learn_time_ms: 595.412\n",
      "    load_throughput: 205671.694\n",
      "    load_time_ms: 4.862\n",
      "    sample_throughput: 88.865\n",
      "    sample_time_ms: 11253.009\n",
      "    update_time_ms: 1.748\n",
      "  timestamp: 1632132714\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         1434.14</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-12-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 126\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.918297451072269\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018190640361403495\n",
      "          policy_loss: 0.06270775089247359\n",
      "          total_loss: 0.05309251909040742\n",
      "          vf_explained_var: 0.6278002858161926\n",
      "          vf_loss: 0.004387675190810114\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.160000000000004\n",
      "    ram_util_percent: 66.48666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060667158551264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.08528902682806\n",
      "    mean_inference_ms: 1.4102315972334625\n",
      "    mean_raw_obs_processing_ms: 0.5987072723110639\n",
      "  time_since_restore: 1444.643078327179\n",
      "  time_this_iter_s: 10.500998497009277\n",
      "  time_total_s: 1444.643078327179\n",
      "  timers:\n",
      "    learn_throughput: 1681.636\n",
      "    learn_time_ms: 594.659\n",
      "    load_throughput: 205777.645\n",
      "    load_time_ms: 4.86\n",
      "    sample_throughput: 88.553\n",
      "    sample_time_ms: 11292.734\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632132724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         1444.64</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-12-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 127\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1075443347295124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01063816718347829\n",
      "          policy_loss: -0.012346918135881424\n",
      "          total_loss: -0.018233144241902564\n",
      "          vf_explained_var: 0.4703611731529236\n",
      "          vf_loss: 0.012159836016750584\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.480000000000004\n",
      "    ram_util_percent: 66.48666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060589642509493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.072996256637758\n",
      "    mean_inference_ms: 1.4101737215600914\n",
      "    mean_raw_obs_processing_ms: 0.604275150884805\n",
      "  time_since_restore: 1455.2810378074646\n",
      "  time_this_iter_s: 10.637959480285645\n",
      "  time_total_s: 1455.2810378074646\n",
      "  timers:\n",
      "    learn_throughput: 1678.706\n",
      "    learn_time_ms: 595.697\n",
      "    load_throughput: 205776.635\n",
      "    load_time_ms: 4.86\n",
      "    sample_throughput: 87.976\n",
      "    sample_time_ms: 11366.758\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632132735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         1455.28</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-12-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 128\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.999619542227851\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017246855647761766\n",
      "          policy_loss: -0.0458010291111552\n",
      "          total_loss: -0.051677555890960826\n",
      "          vf_explained_var: 0.4962727427482605\n",
      "          vf_loss: 0.00920835956704751\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.93333333333332\n",
      "    ram_util_percent: 66.54666666666668\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060520249566462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.061228954217517\n",
      "    mean_inference_ms: 1.4101185121603013\n",
      "    mean_raw_obs_processing_ms: 0.6098020274151273\n",
      "  time_since_restore: 1465.456224679947\n",
      "  time_this_iter_s: 10.1751868724823\n",
      "  time_total_s: 1465.456224679947\n",
      "  timers:\n",
      "    learn_throughput: 1680.378\n",
      "    learn_time_ms: 595.104\n",
      "    load_throughput: 205845.308\n",
      "    load_time_ms: 4.858\n",
      "    sample_throughput: 87.752\n",
      "    sample_time_ms: 11395.727\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1632132745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         1465.46</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-12-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 129\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.107128384378221\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018973140607918954\n",
      "          policy_loss: -0.08814303527275721\n",
      "          total_loss: -0.10025665652420786\n",
      "          vf_explained_var: 0.5864740014076233\n",
      "          vf_loss: 0.0035547636384661825\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.32857142857143\n",
      "    ram_util_percent: 66.64285714285715\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040604563559410105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.049950436206219\n",
      "    mean_inference_ms: 1.4100673829059307\n",
      "    mean_raw_obs_processing_ms: 0.6152864157457509\n",
      "  time_since_restore: 1475.4372227191925\n",
      "  time_this_iter_s: 9.980998039245605\n",
      "  time_total_s: 1475.4372227191925\n",
      "  timers:\n",
      "    learn_throughput: 1678.692\n",
      "    learn_time_ms: 595.702\n",
      "    load_throughput: 205967.619\n",
      "    load_time_ms: 4.855\n",
      "    sample_throughput: 87.693\n",
      "    sample_time_ms: 11403.466\n",
      "    update_time_ms: 1.738\n",
      "  timestamp: 1632132755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         1475.44</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-12-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 130\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9384185274442036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01754838448153306\n",
      "          policy_loss: -0.00046533702148331536\n",
      "          total_loss: -0.011275985712806384\n",
      "          vf_explained_var: 0.5911391973495483\n",
      "          vf_loss: 0.0035763582033622597\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.08666666666666\n",
      "    ram_util_percent: 66.72\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060387353495964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.039045723817525\n",
      "    mean_inference_ms: 1.410013966730285\n",
      "    mean_raw_obs_processing_ms: 0.6145814602361629\n",
      "  time_since_restore: 1485.7466461658478\n",
      "  time_this_iter_s: 10.309423446655273\n",
      "  time_total_s: 1485.7466461658478\n",
      "  timers:\n",
      "    learn_throughput: 1681.234\n",
      "    learn_time_ms: 594.801\n",
      "    load_throughput: 314147.131\n",
      "    load_time_ms: 3.183\n",
      "    sample_throughput: 102.226\n",
      "    sample_time_ms: 9782.211\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632132765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         1485.75</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-12-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 131\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.961223730776045\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017813909334317576\n",
      "          policy_loss: 0.00750369421309895\n",
      "          total_loss: -0.005730564147233963\n",
      "          vf_explained_var: -0.046259477734565735\n",
      "          vf_loss: 0.001305192511386445\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.628571428571426\n",
      "    ram_util_percent: 66.76428571428572\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060284377571676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.028452853842458\n",
      "    mean_inference_ms: 1.4099542640170946\n",
      "    mean_raw_obs_processing_ms: 0.6140288556298268\n",
      "  time_since_restore: 1496.023652791977\n",
      "  time_this_iter_s: 10.27700662612915\n",
      "  time_total_s: 1496.023652791977\n",
      "  timers:\n",
      "    learn_throughput: 1681.467\n",
      "    learn_time_ms: 594.719\n",
      "    load_throughput: 313309.38\n",
      "    load_time_ms: 3.192\n",
      "    sample_throughput: 103.712\n",
      "    sample_time_ms: 9642.068\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632132776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         1496.02</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-13-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 132\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0824272248480056\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012768057127506557\n",
      "          policy_loss: -0.07288123418887456\n",
      "          total_loss: -0.08694125004112721\n",
      "          vf_explained_var: -0.020901784300804138\n",
      "          vf_loss: 0.0031283532547402297\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.01333333333333\n",
      "    ram_util_percent: 66.86666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060153594817009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.018197365917665\n",
      "    mean_inference_ms: 1.4098897801180783\n",
      "    mean_raw_obs_processing_ms: 0.6136135550724089\n",
      "  time_since_restore: 1506.3437900543213\n",
      "  time_this_iter_s: 10.32013726234436\n",
      "  time_total_s: 1506.3437900543213\n",
      "  timers:\n",
      "    learn_throughput: 1680.181\n",
      "    learn_time_ms: 595.174\n",
      "    load_throughput: 312015.83\n",
      "    load_time_ms: 3.205\n",
      "    sample_throughput: 103.334\n",
      "    sample_time_ms: 9677.339\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632132786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         1506.34</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-13-16\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 133\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0266743938128156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016599137974392913\n",
      "          policy_loss: -0.06782207807732953\n",
      "          total_loss: -0.0815213835487763\n",
      "          vf_explained_var: -0.3142562508583069\n",
      "          vf_loss: 0.0018405737633454718\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.98000000000001\n",
      "    ram_util_percent: 66.92666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406000983487741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.008194735552289\n",
      "    mean_inference_ms: 1.4098203523735533\n",
      "    mean_raw_obs_processing_ms: 0.6133306426821656\n",
      "  time_since_restore: 1516.563366651535\n",
      "  time_this_iter_s: 10.219576597213745\n",
      "  time_total_s: 1516.563366651535\n",
      "  timers:\n",
      "    learn_throughput: 1681.272\n",
      "    learn_time_ms: 594.788\n",
      "    load_throughput: 312345.775\n",
      "    load_time_ms: 3.202\n",
      "    sample_throughput: 103.047\n",
      "    sample_time_ms: 9704.317\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632132796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         1516.56</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-13-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 134\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9952035413848028\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01124789376692716\n",
      "          policy_loss: -0.08632878752218352\n",
      "          total_loss: -0.10149182453751564\n",
      "          vf_explained_var: -0.09631216526031494\n",
      "          vf_loss: 0.0015859837003517896\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.25714285714287\n",
      "    ram_util_percent: 67.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059855810722689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.998521711278006\n",
      "    mean_inference_ms: 1.409752472826793\n",
      "    mean_raw_obs_processing_ms: 0.6131615457780041\n",
      "  time_since_restore: 1526.7477731704712\n",
      "  time_this_iter_s: 10.184406518936157\n",
      "  time_total_s: 1526.7477731704712\n",
      "  timers:\n",
      "    learn_throughput: 1680.509\n",
      "    learn_time_ms: 595.058\n",
      "    load_throughput: 312769.683\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 102.908\n",
      "    sample_time_ms: 9717.428\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632132806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         1526.75</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-13-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 135\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0197070346938237\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008939235491094957\n",
      "          policy_loss: -0.0956906565775474\n",
      "          total_loss: -0.1119194186396069\n",
      "          vf_explained_var: -0.5435057878494263\n",
      "          vf_loss: 0.0014227220297066702\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.69333333333334\n",
      "    ram_util_percent: 67.07333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059697848190542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.989145464966876\n",
      "    mean_inference_ms: 1.4096840948146234\n",
      "    mean_raw_obs_processing_ms: 0.6131089053976412\n",
      "  time_since_restore: 1537.0612275600433\n",
      "  time_this_iter_s: 10.313454389572144\n",
      "  time_total_s: 1537.0612275600433\n",
      "  timers:\n",
      "    learn_throughput: 1681.115\n",
      "    learn_time_ms: 594.843\n",
      "    load_throughput: 312977.398\n",
      "    load_time_ms: 3.195\n",
      "    sample_throughput: 103.212\n",
      "    sample_time_ms: 9688.837\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632132817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         1537.06</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-13-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 136\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.518281598885854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013908289320419895\n",
      "          policy_loss: -0.015581458641423119\n",
      "          total_loss: -0.023220203402969573\n",
      "          vf_explained_var: 0.6369300484657288\n",
      "          vf_loss: 0.003583467142501225\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.23333333333333\n",
      "    ram_util_percent: 67.24666666666664\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040595430900339444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.980153698206834\n",
      "    mean_inference_ms: 1.4096199642495881\n",
      "    mean_raw_obs_processing_ms: 0.613160786616521\n",
      "  time_since_restore: 1547.6242110729218\n",
      "  time_this_iter_s: 10.562983512878418\n",
      "  time_total_s: 1547.6242110729218\n",
      "  timers:\n",
      "    learn_throughput: 1680.331\n",
      "    learn_time_ms: 595.121\n",
      "    load_throughput: 312804.672\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 103.149\n",
      "    sample_time_ms: 9694.745\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1632132827\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         1547.62</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-13-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 137\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1151990758048163\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013082562157746046\n",
      "          policy_loss: -0.15557213127613068\n",
      "          total_loss: -0.1714780141909917\n",
      "          vf_explained_var: 0.15885911881923676\n",
      "          vf_loss: 0.0015206430605353995\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.41428571428572\n",
      "    ram_util_percent: 67.3142857142857\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040593947235101464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.971479289606348\n",
      "    mean_inference_ms: 1.409558710065225\n",
      "    mean_raw_obs_processing_ms: 0.6133132222541436\n",
      "  time_since_restore: 1557.5996670722961\n",
      "  time_this_iter_s: 9.97545599937439\n",
      "  time_total_s: 1557.5996670722961\n",
      "  timers:\n",
      "    learn_throughput: 1682.862\n",
      "    learn_time_ms: 594.226\n",
      "    load_throughput: 313499.066\n",
      "    load_time_ms: 3.19\n",
      "    sample_throughput: 103.849\n",
      "    sample_time_ms: 9629.373\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632132837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">          1557.6</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-14-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 138\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.023938771088918\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01069645590795029\n",
      "          policy_loss: -0.07910092891090446\n",
      "          total_loss: -0.09388232255975405\n",
      "          vf_explained_var: 0.0022134457249194384\n",
      "          vf_loss: 0.002412013772926811\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.093333333333334\n",
      "    ram_util_percent: 67.42666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040592483295684066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.963036579274595\n",
      "    mean_inference_ms: 1.4095004571975116\n",
      "    mean_raw_obs_processing_ms: 0.6135595215163921\n",
      "  time_since_restore: 1568.0013360977173\n",
      "  time_this_iter_s: 10.401669025421143\n",
      "  time_total_s: 1568.0013360977173\n",
      "  timers:\n",
      "    learn_throughput: 1683.17\n",
      "    learn_time_ms: 594.117\n",
      "    load_throughput: 313778.157\n",
      "    load_time_ms: 3.187\n",
      "    sample_throughput: 103.604\n",
      "    sample_time_ms: 9652.143\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632132848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">            1568</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-14-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 139\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.142646763059828\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010191779241733891\n",
      "          policy_loss: 0.12596582795182865\n",
      "          total_loss: 0.10887968018651009\n",
      "          vf_explained_var: 0.14413216710090637\n",
      "          vf_loss: 0.0014380500322052588\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.326666666666675\n",
      "    ram_util_percent: 67.50666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059110338421105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.954830323516243\n",
      "    mean_inference_ms: 1.4094459067680918\n",
      "    mean_raw_obs_processing_ms: 0.6138906966463682\n",
      "  time_since_restore: 1578.5978782176971\n",
      "  time_this_iter_s: 10.596542119979858\n",
      "  time_total_s: 1578.5978782176971\n",
      "  timers:\n",
      "    learn_throughput: 1682.232\n",
      "    learn_time_ms: 594.448\n",
      "    load_throughput: 312176.068\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 102.951\n",
      "    sample_time_ms: 9713.319\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632132858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">          1578.6</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-14-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 140\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0570518838034735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012128039810856429\n",
      "          policy_loss: -0.018812795045475166\n",
      "          total_loss: -0.03430077750235796\n",
      "          vf_explained_var: -0.03364674746990204\n",
      "          vf_loss: 0.0016288879502099007\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85333333333333\n",
      "    ram_util_percent: 67.43999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058976839285647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.94684714482684\n",
      "    mean_inference_ms: 1.4093941800262875\n",
      "    mean_raw_obs_processing_ms: 0.6143050867853844\n",
      "  time_since_restore: 1589.1340305805206\n",
      "  time_this_iter_s: 10.536152362823486\n",
      "  time_total_s: 1589.1340305805206\n",
      "  timers:\n",
      "    learn_throughput: 1680.974\n",
      "    learn_time_ms: 594.893\n",
      "    load_throughput: 312604.175\n",
      "    load_time_ms: 3.199\n",
      "    sample_throughput: 102.716\n",
      "    sample_time_ms: 9735.556\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1632132869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         1589.13</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-14-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 141\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9585345811314052\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01939479334881514\n",
      "          policy_loss: -0.03358491907517115\n",
      "          total_loss: -0.045418596433268656\n",
      "          vf_explained_var: 0.40092167258262634\n",
      "          vf_loss: 0.0022287025983031426\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.09333333333334\n",
      "    ram_util_percent: 67.33999999999997\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058848657263197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.939018641533258\n",
      "    mean_inference_ms: 1.4093444565909314\n",
      "    mean_raw_obs_processing_ms: 0.6147936116557686\n",
      "  time_since_restore: 1599.0825774669647\n",
      "  time_this_iter_s: 9.948546886444092\n",
      "  time_total_s: 1599.0825774669647\n",
      "  timers:\n",
      "    learn_throughput: 1678.676\n",
      "    learn_time_ms: 595.708\n",
      "    load_throughput: 312832.668\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 103.073\n",
      "    sample_time_ms: 9701.906\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632132879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         1599.08</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-14-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 142\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.118538788954417\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015229909445114072\n",
      "          policy_loss: -0.04253272273474269\n",
      "          total_loss: -0.055753869687517485\n",
      "          vf_explained_var: 0.08356574922800064\n",
      "          vf_loss: 0.00362728802073333\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.04\n",
      "    ram_util_percent: 67.29333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058719409774228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.931434788083385\n",
      "    mean_inference_ms: 1.4092962790216765\n",
      "    mean_raw_obs_processing_ms: 0.6153521247618698\n",
      "  time_since_restore: 1609.431664466858\n",
      "  time_this_iter_s: 10.349086999893188\n",
      "  time_total_s: 1609.431664466858\n",
      "  timers:\n",
      "    learn_throughput: 1680.535\n",
      "    learn_time_ms: 595.049\n",
      "    load_throughput: 311670.37\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 103.034\n",
      "    sample_time_ms: 9705.494\n",
      "    update_time_ms: 1.748\n",
      "  timestamp: 1632132889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         1609.43</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 143\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0512934313880073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016766213115103786\n",
      "          policy_loss: -0.013363879463738865\n",
      "          total_loss: -0.026961080957618025\n",
      "          vf_explained_var: 0.2577970623970032\n",
      "          vf_loss: 0.0021412935062673773\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.8\n",
      "    ram_util_percent: 67.2214285714286\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058588479743437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.924024113325721\n",
      "    mean_inference_ms: 1.409248335312778\n",
      "    mean_raw_obs_processing_ms: 0.6159759890087316\n",
      "  time_since_restore: 1619.8293073177338\n",
      "  time_this_iter_s: 10.397642850875854\n",
      "  time_total_s: 1619.8293073177338\n",
      "  timers:\n",
      "    learn_throughput: 1682.784\n",
      "    learn_time_ms: 594.253\n",
      "    load_throughput: 311918.375\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 102.837\n",
      "    sample_time_ms: 9724.091\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1632132900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         1619.83</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-15-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 144\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.006518816947937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018491346460776804\n",
      "          policy_loss: -0.05266445693042543\n",
      "          total_loss: -0.06422830203341114\n",
      "          vf_explained_var: -0.3183142840862274\n",
      "          vf_loss: 0.00323564271028671\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.940000000000005\n",
      "    ram_util_percent: 67.14000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058454527154318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.916772390760672\n",
      "    mean_inference_ms: 1.409199636072066\n",
      "    mean_raw_obs_processing_ms: 0.616663624589899\n",
      "  time_since_restore: 1630.166677236557\n",
      "  time_this_iter_s: 10.337369918823242\n",
      "  time_total_s: 1630.166677236557\n",
      "  timers:\n",
      "    learn_throughput: 1685.572\n",
      "    learn_time_ms: 593.27\n",
      "    load_throughput: 311406.574\n",
      "    load_time_ms: 3.211\n",
      "    sample_throughput: 102.666\n",
      "    sample_time_ms: 9740.364\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632132910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         1630.17</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-15-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 145\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.094816345638699\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010580963758218095\n",
      "          policy_loss: -0.07160607404592964\n",
      "          total_loss: -0.08856123151878516\n",
      "          vf_explained_var: 0.012523566372692585\n",
      "          vf_loss: 0.0009799114839956424\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.13333333333333\n",
      "    ram_util_percent: 67.02666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058310848809623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.909682688136918\n",
      "    mean_inference_ms: 1.4091498113156466\n",
      "    mean_raw_obs_processing_ms: 0.6174114535765196\n",
      "  time_since_restore: 1640.5530896186829\n",
      "  time_this_iter_s: 10.386412382125854\n",
      "  time_total_s: 1640.5530896186829\n",
      "  timers:\n",
      "    learn_throughput: 1685.103\n",
      "    learn_time_ms: 593.436\n",
      "    load_throughput: 311573.13\n",
      "    load_time_ms: 3.21\n",
      "    sample_throughput: 102.591\n",
      "    sample_time_ms: 9747.453\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632132920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         1640.55</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-15-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 146\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.125182130601671\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01321493217609841\n",
      "          policy_loss: -0.12013750415709283\n",
      "          total_loss: -0.1361681520111031\n",
      "          vf_explained_var: -0.6297724843025208\n",
      "          vf_loss: 0.001458014116764793\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.986666666666665\n",
      "    ram_util_percent: 66.91333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058166989306225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.902751931693045\n",
      "    mean_inference_ms: 1.4090996739358843\n",
      "    mean_raw_obs_processing_ms: 0.6182130095500861\n",
      "  time_since_restore: 1650.9031994342804\n",
      "  time_this_iter_s: 10.350109815597534\n",
      "  time_total_s: 1650.9031994342804\n",
      "  timers:\n",
      "    learn_throughput: 1681.945\n",
      "    learn_time_ms: 594.55\n",
      "    load_throughput: 311656.475\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 102.831\n",
      "    sample_time_ms: 9724.657\n",
      "    update_time_ms: 1.742\n",
      "  timestamp: 1632132931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">          1650.9</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-15-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 147\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1663492229249743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015507453112076833\n",
      "          policy_loss: -0.11980101333724127\n",
      "          total_loss: -0.13502446942859225\n",
      "          vf_explained_var: -0.33907070755958557\n",
      "          vf_loss: 0.0020240468930246103\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.059999999999995\n",
      "    ram_util_percent: 66.75333333333336\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040580222473838676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.895979776081452\n",
      "    mean_inference_ms: 1.4090495526987132\n",
      "    mean_raw_obs_processing_ms: 0.6190662262313771\n",
      "  time_since_restore: 1661.194037437439\n",
      "  time_this_iter_s: 10.29083800315857\n",
      "  time_total_s: 1661.194037437439\n",
      "  timers:\n",
      "    learn_throughput: 1681.164\n",
      "    learn_time_ms: 594.826\n",
      "    load_throughput: 308865.733\n",
      "    load_time_ms: 3.238\n",
      "    sample_throughput: 102.502\n",
      "    sample_time_ms: 9755.91\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1632132941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         1661.19</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-15-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 148\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.108252353138394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01172945419841681\n",
      "          policy_loss: -0.05852850121963355\n",
      "          total_loss: -0.07480451543298032\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001466364792173004\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.42\n",
      "    ram_util_percent: 66.6\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057881611826858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.88933916497224\n",
      "    mean_inference_ms: 1.4090003693796056\n",
      "    mean_raw_obs_processing_ms: 0.6199699381252657\n",
      "  time_since_restore: 1671.7350687980652\n",
      "  time_this_iter_s: 10.54103136062622\n",
      "  time_total_s: 1671.7350687980652\n",
      "  timers:\n",
      "    learn_throughput: 1677.955\n",
      "    learn_time_ms: 595.964\n",
      "    load_throughput: 309186.767\n",
      "    load_time_ms: 3.234\n",
      "    sample_throughput: 102.368\n",
      "    sample_time_ms: 9768.714\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632132952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         1671.74</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-16-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 149\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9497286505169338\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008243039927758645\n",
      "          policy_loss: 0.014381177288790544\n",
      "          total_loss: -0.0022747294563386176\n",
      "          vf_explained_var: -0.5901203155517578\n",
      "          vf_loss: 0.0004940461984208216\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.06666666666667\n",
      "    ram_util_percent: 66.43999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057742820162643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.882850015919926\n",
      "    mean_inference_ms: 1.4089531216888458\n",
      "    mean_raw_obs_processing_ms: 0.6209215469985361\n",
      "  time_since_restore: 1682.1779868602753\n",
      "  time_this_iter_s: 10.442918062210083\n",
      "  time_total_s: 1682.1779868602753\n",
      "  timers:\n",
      "    learn_throughput: 1677.964\n",
      "    learn_time_ms: 595.96\n",
      "    load_throughput: 311133.992\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 102.529\n",
      "    sample_time_ms: 9753.368\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632132962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         1682.18</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-16-30\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 150\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0601072669029237\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012629532811485215\n",
      "          policy_loss: -0.06891398946754634\n",
      "          total_loss: -0.08479121920859647\n",
      "          vf_explained_var: -0.4468943774700165\n",
      "          vf_loss: 0.0011273853571765358\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.18205128205128\n",
      "    ram_util_percent: 66.25897435897434\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040576051566919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.876496932575105\n",
      "    mean_inference_ms: 1.4089073900573124\n",
      "    mean_raw_obs_processing_ms: 0.6230799379999908\n",
      "  time_since_restore: 1709.8846538066864\n",
      "  time_this_iter_s: 27.706666946411133\n",
      "  time_total_s: 1709.8846538066864\n",
      "  timers:\n",
      "    learn_throughput: 1679.484\n",
      "    learn_time_ms: 595.421\n",
      "    load_throughput: 200755.483\n",
      "    load_time_ms: 4.981\n",
      "    sample_throughput: 87.19\n",
      "    sample_time_ms: 11469.198\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1632132990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         1709.88</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 151\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.984659567144182\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011882041755736062\n",
      "          policy_loss: 0.10738337544931306\n",
      "          total_loss: 0.09255976097451316\n",
      "          vf_explained_var: -0.512764036655426\n",
      "          vf_loss: 0.0016393845728695548\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.138888888888886\n",
      "    ram_util_percent: 66.66666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057468837679418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.87041220164793\n",
      "    mean_inference_ms: 1.4088638304357353\n",
      "    mean_raw_obs_processing_ms: 0.6252717296144579\n",
      "  time_since_restore: 1722.4606847763062\n",
      "  time_this_iter_s: 12.576030969619751\n",
      "  time_total_s: 1722.4606847763062\n",
      "  timers:\n",
      "    learn_throughput: 1680.57\n",
      "    learn_time_ms: 595.036\n",
      "    load_throughput: 200848.733\n",
      "    load_time_ms: 4.979\n",
      "    sample_throughput: 85.235\n",
      "    sample_time_ms: 11732.314\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632133003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         1722.46</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-16-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 152\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9785224583413865\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01097998407386667\n",
      "          policy_loss: 0.018319577972094218\n",
      "          total_loss: 0.003481548610660765\n",
      "          vf_explained_var: 0.22416259348392487\n",
      "          vf_loss: 0.0018204740332698243\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.93125\n",
      "    ram_util_percent: 66.88749999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405733234622687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.864471353826065\n",
      "    mean_inference_ms: 1.4088220800074753\n",
      "    mean_raw_obs_processing_ms: 0.6274931217835049\n",
      "  time_since_restore: 1733.4460101127625\n",
      "  time_this_iter_s: 10.985325336456299\n",
      "  time_total_s: 1733.4460101127625\n",
      "  timers:\n",
      "    learn_throughput: 1679.465\n",
      "    learn_time_ms: 595.428\n",
      "    load_throughput: 201739.432\n",
      "    load_time_ms: 4.957\n",
      "    sample_throughput: 84.778\n",
      "    sample_time_ms: 11795.568\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632133014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         1733.45</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-17-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 153\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0943585488531324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013534741278356677\n",
      "          policy_loss: -0.028044678105248345\n",
      "          total_loss: -0.04398969908555349\n",
      "          vf_explained_var: -0.1459551453590393\n",
      "          vf_loss: 0.0011443353697864545\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.59333333333333\n",
      "    ram_util_percent: 66.75333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057194365223135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.858603592714513\n",
      "    mean_inference_ms: 1.4087795542375\n",
      "    mean_raw_obs_processing_ms: 0.629743723811128\n",
      "  time_since_restore: 1743.9714703559875\n",
      "  time_this_iter_s: 10.525460243225098\n",
      "  time_total_s: 1743.9714703559875\n",
      "  timers:\n",
      "    learn_throughput: 1677.183\n",
      "    learn_time_ms: 596.238\n",
      "    load_throughput: 201057.657\n",
      "    load_time_ms: 4.974\n",
      "    sample_throughput: 84.692\n",
      "    sample_time_ms: 11807.536\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632133024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         1743.97</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-17-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 154\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0045780181884765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016688902885772017\n",
      "          policy_loss: -0.09430966079235077\n",
      "          total_loss: -0.10741137468980419\n",
      "          vf_explained_var: 0.3694266378879547\n",
      "          vf_loss: 0.002191641688760784\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.12666666666666\n",
      "    ram_util_percent: 66.46\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040570586081896655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.85288009036491\n",
      "    mean_inference_ms: 1.4087385421592142\n",
      "    mean_raw_obs_processing_ms: 0.6320226577043646\n",
      "  time_since_restore: 1754.5112228393555\n",
      "  time_this_iter_s: 10.53975248336792\n",
      "  time_total_s: 1754.5112228393555\n",
      "  timers:\n",
      "    learn_throughput: 1676.826\n",
      "    learn_time_ms: 596.365\n",
      "    load_throughput: 200801.616\n",
      "    load_time_ms: 4.98\n",
      "    sample_throughput: 84.548\n",
      "    sample_time_ms: 11827.617\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632133035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         1754.51</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-17-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 155\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1136937061945598\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009593520580445977\n",
      "          policy_loss: -0.05266723177499241\n",
      "          total_loss: -0.06974239481820001\n",
      "          vf_explained_var: -0.5080550909042358\n",
      "          vf_loss: 0.0013298679078515205\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.32666666666666\n",
      "    ram_util_percent: 66.35333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056924074409113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.847286453243772\n",
      "    mean_inference_ms: 1.4086999769736621\n",
      "    mean_raw_obs_processing_ms: 0.6343274524276729\n",
      "  time_since_restore: 1765.1821458339691\n",
      "  time_this_iter_s: 10.670922994613647\n",
      "  time_total_s: 1765.1821458339691\n",
      "  timers:\n",
      "    learn_throughput: 1675.485\n",
      "    learn_time_ms: 596.842\n",
      "    load_throughput: 199666.01\n",
      "    load_time_ms: 5.008\n",
      "    sample_throughput: 84.349\n",
      "    sample_time_ms: 11855.57\n",
      "    update_time_ms: 1.78\n",
      "  timestamp: 1632133045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         1765.18</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-17-36\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 156\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0457192222277323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013201746677134136\n",
      "          policy_loss: 0.017523237152232064\n",
      "          total_loss: 0.00294706995288531\n",
      "          vf_explained_var: 0.24504230916500092\n",
      "          vf_loss: 0.002121623217762034\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.30666666666666\n",
      "    ram_util_percent: 66.33333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056791227665014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.84174060143702\n",
      "    mean_inference_ms: 1.4086611048268376\n",
      "    mean_raw_obs_processing_ms: 0.6366544141991685\n",
      "  time_since_restore: 1775.4322941303253\n",
      "  time_this_iter_s: 10.250148296356201\n",
      "  time_total_s: 1775.4322941303253\n",
      "  timers:\n",
      "    learn_throughput: 1676.342\n",
      "    learn_time_ms: 596.537\n",
      "    load_throughput: 199714.497\n",
      "    load_time_ms: 5.007\n",
      "    sample_throughput: 84.415\n",
      "    sample_time_ms: 11846.273\n",
      "    update_time_ms: 1.778\n",
      "  timestamp: 1632133056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         1775.43</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-17-46\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 157\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.159570418463813\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012907268227643638\n",
      "          policy_loss: -0.1409534513950348\n",
      "          total_loss: -0.15757562120755514\n",
      "          vf_explained_var: 0.23879840970039368\n",
      "          vf_loss: 0.0012979876203139105\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.233333333333334\n",
      "    ram_util_percent: 66.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040566605853764995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.836256171719477\n",
      "    mean_inference_ms: 1.4086215398843087\n",
      "    mean_raw_obs_processing_ms: 0.6390018181408039\n",
      "  time_since_restore: 1785.8514957427979\n",
      "  time_this_iter_s: 10.419201612472534\n",
      "  time_total_s: 1785.8514957427979\n",
      "  timers:\n",
      "    learn_throughput: 1676.951\n",
      "    learn_time_ms: 596.321\n",
      "    load_throughput: 199841.054\n",
      "    load_time_ms: 5.004\n",
      "    sample_throughput: 84.322\n",
      "    sample_time_ms: 11859.337\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632133066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         1785.85</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-17-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 158\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.092960646417406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013815407694522257\n",
      "          policy_loss: 0.032255915821426444\n",
      "          total_loss: 0.01571135947273837\n",
      "          vf_explained_var: -0.9935857057571411\n",
      "          vf_loss: 0.0004508972985301322\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.67999999999999\n",
      "    ram_util_percent: 66.33999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056528399318015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.830890567890503\n",
      "    mean_inference_ms: 1.40858281882455\n",
      "    mean_raw_obs_processing_ms: 0.6413684803374328\n",
      "  time_since_restore: 1796.6605398654938\n",
      "  time_this_iter_s: 10.809044122695923\n",
      "  time_total_s: 1796.6605398654938\n",
      "  timers:\n",
      "    learn_throughput: 1679.503\n",
      "    learn_time_ms: 595.414\n",
      "    load_throughput: 199664.109\n",
      "    load_time_ms: 5.008\n",
      "    sample_throughput: 84.125\n",
      "    sample_time_ms: 11887.043\n",
      "    update_time_ms: 1.768\n",
      "  timestamp: 1632133077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         1796.66</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-18-08\n",
      "  done: false\n",
      "  episode_len_mean: 994.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 159\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.103975146346622\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012720601196837183\n",
      "          policy_loss: 0.04019200363092952\n",
      "          total_loss: 0.024813906558685834\n",
      "          vf_explained_var: -0.3509613573551178\n",
      "          vf_loss: 0.00203926447445863\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.9125\n",
      "    ram_util_percent: 66.44375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040563986672378045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.825614253935345\n",
      "    mean_inference_ms: 1.4085449727370558\n",
      "    mean_raw_obs_processing_ms: 0.6437541004344146\n",
      "  time_since_restore: 1807.4670951366425\n",
      "  time_this_iter_s: 10.806555271148682\n",
      "  time_total_s: 1807.4670951366425\n",
      "  timers:\n",
      "    learn_throughput: 1679.66\n",
      "    learn_time_ms: 595.358\n",
      "    load_throughput: 199403.07\n",
      "    load_time_ms: 5.015\n",
      "    sample_throughput: 83.869\n",
      "    sample_time_ms: 11923.421\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632133088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         1807.47</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-18-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 160\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0884365518887837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010840109524679498\n",
      "          policy_loss: -0.10010338961664174\n",
      "          total_loss: -0.11658958457410336\n",
      "          vf_explained_var: -0.991135835647583\n",
      "          vf_loss: 0.0013112780851467201\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.94666666666667\n",
      "    ram_util_percent: 66.55333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040562689516608685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.820425271952233\n",
      "    mean_inference_ms: 1.408507068628279\n",
      "    mean_raw_obs_processing_ms: 0.6432215441171932\n",
      "  time_since_restore: 1818.2033557891846\n",
      "  time_this_iter_s: 10.736260652542114\n",
      "  time_total_s: 1818.2033557891846\n",
      "  timers:\n",
      "    learn_throughput: 1676.823\n",
      "    learn_time_ms: 596.366\n",
      "    load_throughput: 307430.423\n",
      "    load_time_ms: 3.253\n",
      "    sample_throughput: 97.779\n",
      "    sample_time_ms: 10227.153\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632133098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">          1818.2</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-18-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 161\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2288594749238757\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009114892185139478\n",
      "          policy_loss: 0.0076098176443742385\n",
      "          total_loss: -0.01087192134000361\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001211247861566436\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.833333333333336\n",
      "    ram_util_percent: 66.60000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056137042612831\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.815014237283894\n",
      "    mean_inference_ms: 1.408468495475022\n",
      "    mean_raw_obs_processing_ms: 0.642752527632148\n",
      "  time_since_restore: 1828.7651166915894\n",
      "  time_this_iter_s: 10.561760902404785\n",
      "  time_total_s: 1828.7651166915894\n",
      "  timers:\n",
      "    learn_throughput: 1678.398\n",
      "    learn_time_ms: 595.806\n",
      "    load_throughput: 307509.311\n",
      "    load_time_ms: 3.252\n",
      "    sample_throughput: 99.738\n",
      "    sample_time_ms: 10026.286\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632133109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         1828.77</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-18-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 162\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2579626030392115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01793606934573412\n",
      "          policy_loss: -0.14545745050741565\n",
      "          total_loss: -0.16181534594959682\n",
      "          vf_explained_var: -0.23274004459381104\n",
      "          vf_loss: 0.0011141531583335664\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.846666666666664\n",
      "    ram_util_percent: 66.69333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040560035987857804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.809661784361042\n",
      "    mean_inference_ms: 1.4084287886919755\n",
      "    mean_raw_obs_processing_ms: 0.6423436068517838\n",
      "  time_since_restore: 1839.490252494812\n",
      "  time_this_iter_s: 10.725135803222656\n",
      "  time_total_s: 1839.490252494812\n",
      "  timers:\n",
      "    learn_throughput: 1678.238\n",
      "    learn_time_ms: 595.863\n",
      "    load_throughput: 307561.174\n",
      "    load_time_ms: 3.251\n",
      "    sample_throughput: 99.998\n",
      "    sample_time_ms: 10000.21\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632133120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         1839.49</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-18-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 163\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1407726407051086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014194326096308331\n",
      "          policy_loss: -0.019560462195012306\n",
      "          total_loss: -0.03408653820450935\n",
      "          vf_explained_var: -0.3019522726535797\n",
      "          vf_loss: 0.0028395884896680297\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85625\n",
      "    ram_util_percent: 66.71875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040558755548463284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.8044362231417\n",
      "    mean_inference_ms: 1.408389581702632\n",
      "    mean_raw_obs_processing_ms: 0.6419931495282113\n",
      "  time_since_restore: 1850.2356088161469\n",
      "  time_this_iter_s: 10.745356321334839\n",
      "  time_total_s: 1850.2356088161469\n",
      "  timers:\n",
      "    learn_throughput: 1679.366\n",
      "    learn_time_ms: 595.463\n",
      "    load_throughput: 303352.548\n",
      "    load_time_ms: 3.296\n",
      "    sample_throughput: 99.775\n",
      "    sample_time_ms: 10022.574\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632133131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         1850.24</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-19-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 164\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1286831272972955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011526963718566355\n",
      "          policy_loss: -0.043944284402661855\n",
      "          total_loss: -0.06072215156422721\n",
      "          vf_explained_var: -0.08772361278533936\n",
      "          vf_loss: 0.001226480938364855\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.300000000000004\n",
      "    ram_util_percent: 66.81333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055752493369993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.799274038785734\n",
      "    mean_inference_ms: 1.4083472920824147\n",
      "    mean_raw_obs_processing_ms: 0.6416975937472823\n",
      "  time_since_restore: 1860.8048849105835\n",
      "  time_this_iter_s: 10.569276094436646\n",
      "  time_total_s: 1860.8048849105835\n",
      "  timers:\n",
      "    learn_throughput: 1676.853\n",
      "    learn_time_ms: 596.355\n",
      "    load_throughput: 302593.138\n",
      "    load_time_ms: 3.305\n",
      "    sample_throughput: 99.754\n",
      "    sample_time_ms: 10024.65\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1632133141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">          1860.8</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-19-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 165\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.18602155579461\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012947394451262974\n",
      "          policy_loss: -0.0787698013914956\n",
      "          total_loss: -0.09506290025181241\n",
      "          vf_explained_var: 0.14059647917747498\n",
      "          vf_loss: 0.0018801430718869798\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85333333333333\n",
      "    ram_util_percent: 66.89333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055633391458033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.794220072791255\n",
      "    mean_inference_ms: 1.4083067009560029\n",
      "    mean_raw_obs_processing_ms: 0.6414566377924181\n",
      "  time_since_restore: 1871.656222820282\n",
      "  time_this_iter_s: 10.851337909698486\n",
      "  time_total_s: 1871.656222820282\n",
      "  timers:\n",
      "    learn_throughput: 1677.439\n",
      "    learn_time_ms: 596.147\n",
      "    load_throughput: 304785.38\n",
      "    load_time_ms: 3.281\n",
      "    sample_throughput: 99.572\n",
      "    sample_time_ms: 10042.947\n",
      "    update_time_ms: 1.74\n",
      "  timestamp: 1632133152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         1871.66</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-19-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 166\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.106229323811001\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017213433332182563\n",
      "          policy_loss: 0.10911446306854486\n",
      "          total_loss: 0.09354303150127331\n",
      "          vf_explained_var: -0.5932773947715759\n",
      "          vf_loss: 0.0005890651572877283\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.90625\n",
      "    ram_util_percent: 67.0125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055516210730864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.789266362893045\n",
      "    mean_inference_ms: 1.4082670446939618\n",
      "    mean_raw_obs_processing_ms: 0.6412696746497617\n",
      "  time_since_restore: 1882.467401266098\n",
      "  time_this_iter_s: 10.81117844581604\n",
      "  time_total_s: 1882.467401266098\n",
      "  timers:\n",
      "    learn_throughput: 1678.76\n",
      "    learn_time_ms: 595.678\n",
      "    load_throughput: 304369.571\n",
      "    load_time_ms: 3.285\n",
      "    sample_throughput: 99.015\n",
      "    sample_time_ms: 10099.511\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632133163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         1882.47</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-19-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 167\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0743551426463656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013189757877997603\n",
      "          policy_loss: 0.019229946720103423\n",
      "          total_loss: 0.0032990467217233446\n",
      "          vf_explained_var: -0.4754064381122589\n",
      "          vf_loss: 0.001056660368016714\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.699999999999996\n",
      "    ram_util_percent: 67.1125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055403257063598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.7844540200504\n",
      "    mean_inference_ms: 1.408229356509544\n",
      "    mean_raw_obs_processing_ms: 0.6411330541054819\n",
      "  time_since_restore: 1893.3693163394928\n",
      "  time_this_iter_s: 10.901915073394775\n",
      "  time_total_s: 1893.3693163394928\n",
      "  timers:\n",
      "    learn_throughput: 1677.754\n",
      "    learn_time_ms: 596.035\n",
      "    load_throughput: 306209.454\n",
      "    load_time_ms: 3.266\n",
      "    sample_throughput: 98.547\n",
      "    sample_time_ms: 10147.447\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632133174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         1893.37</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-19-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 168\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.227729540401035\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011425200142175226\n",
      "          policy_loss: -0.04173272351423899\n",
      "          total_loss: -0.05987347753511535\n",
      "          vf_explained_var: -0.7710734605789185\n",
      "          vf_loss: 0.0008830370905343443\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.96666666666666\n",
      "    ram_util_percent: 67.25333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055294397975697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.77971970646175\n",
      "    mean_inference_ms: 1.408193584047326\n",
      "    mean_raw_obs_processing_ms: 0.6410434106487013\n",
      "  time_since_restore: 1904.0616631507874\n",
      "  time_this_iter_s: 10.692346811294556\n",
      "  time_total_s: 1904.0616631507874\n",
      "  timers:\n",
      "    learn_throughput: 1677.202\n",
      "    learn_time_ms: 596.231\n",
      "    load_throughput: 305631.549\n",
      "    load_time_ms: 3.272\n",
      "    sample_throughput: 98.662\n",
      "    sample_time_ms: 10135.567\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632133185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         1904.06</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-19-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 169\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1694641881518892\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01176121069999131\n",
      "          policy_loss: -0.0027824120389090645\n",
      "          total_loss: -0.020316467185815177\n",
      "          vf_explained_var: -0.554719865322113\n",
      "          vf_loss: 0.0008113969725349711\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79375\n",
      "    ram_util_percent: 67.25\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040551882393050916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.775084607521512\n",
      "    mean_inference_ms: 1.408159388753197\n",
      "    mean_raw_obs_processing_ms: 0.6409997145191545\n",
      "  time_since_restore: 1915.050276517868\n",
      "  time_this_iter_s: 10.988613367080688\n",
      "  time_total_s: 1915.050276517868\n",
      "  timers:\n",
      "    learn_throughput: 1676.303\n",
      "    learn_time_ms: 596.551\n",
      "    load_throughput: 304577.333\n",
      "    load_time_ms: 3.283\n",
      "    sample_throughput: 98.488\n",
      "    sample_time_ms: 10153.489\n",
      "    update_time_ms: 1.746\n",
      "  timestamp: 1632133196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         1915.05</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-20-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 170\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2075549337599014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009890217579172382\n",
      "          policy_loss: -0.04122819271352556\n",
      "          total_loss: -0.05981212312148677\n",
      "          vf_explained_var: -0.8649802207946777\n",
      "          vf_loss: 0.0006752248070875389\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.720000000000006\n",
      "    ram_util_percent: 67.29333333333331\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055082603401029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.770477820766363\n",
      "    mean_inference_ms: 1.4081237168338478\n",
      "    mean_raw_obs_processing_ms: 0.6409977792796233\n",
      "  time_since_restore: 1925.8783860206604\n",
      "  time_this_iter_s: 10.828109502792358\n",
      "  time_total_s: 1925.8783860206604\n",
      "  timers:\n",
      "    learn_throughput: 1678.179\n",
      "    learn_time_ms: 595.884\n",
      "    load_throughput: 304219.452\n",
      "    load_time_ms: 3.287\n",
      "    sample_throughput: 98.393\n",
      "    sample_time_ms: 10163.33\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632133206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         1925.88</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-20-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 171\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2497629854414196\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009798423902025268\n",
      "          policy_loss: -0.011379328618446986\n",
      "          total_loss: -0.030176353620158303\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009103492840141472\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79333333333332\n",
      "    ram_util_percent: 67.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054980897444881\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.765930992792264\n",
      "    mean_inference_ms: 1.4080883762438805\n",
      "    mean_raw_obs_processing_ms: 0.641038446055659\n",
      "  time_since_restore: 1936.4469721317291\n",
      "  time_this_iter_s: 10.568586111068726\n",
      "  time_total_s: 1936.4469721317291\n",
      "  timers:\n",
      "    learn_throughput: 1676.179\n",
      "    learn_time_ms: 596.595\n",
      "    load_throughput: 304062.867\n",
      "    load_time_ms: 3.289\n",
      "    sample_throughput: 98.393\n",
      "    sample_time_ms: 10163.328\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1632133217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         1936.45</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-20-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 172\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1450917296939425\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013431011947031715\n",
      "          policy_loss: -0.048555030963487095\n",
      "          total_loss: -0.06531579838030868\n",
      "          vf_explained_var: -0.6846693158149719\n",
      "          vf_loss: 0.0008654583203476957\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.913333333333334\n",
      "    ram_util_percent: 67.36666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054879603223423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.761469628030378\n",
      "    mean_inference_ms: 1.408053097080866\n",
      "    mean_raw_obs_processing_ms: 0.6411200961676089\n",
      "  time_since_restore: 1947.1599898338318\n",
      "  time_this_iter_s: 10.713017702102661\n",
      "  time_total_s: 1947.1599898338318\n",
      "  timers:\n",
      "    learn_throughput: 1676.738\n",
      "    learn_time_ms: 596.396\n",
      "    load_throughput: 303831.594\n",
      "    load_time_ms: 3.291\n",
      "    sample_throughput: 98.403\n",
      "    sample_time_ms: 10162.319\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632133228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         1947.16</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-20-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 173\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2625931792789036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01234159862038984\n",
      "          policy_loss: -0.05322128971003824\n",
      "          total_loss: -0.07139152460214164\n",
      "          vf_explained_var: -0.9090731143951416\n",
      "          vf_loss: 0.0009412323525692854\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.7\n",
      "    ram_util_percent: 67.39375000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054780446787067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.757030161766396\n",
      "    mean_inference_ms: 1.4080168632248362\n",
      "    mean_raw_obs_processing_ms: 0.641240570687117\n",
      "  time_since_restore: 1957.8798949718475\n",
      "  time_this_iter_s: 10.719905138015747\n",
      "  time_total_s: 1957.8798949718475\n",
      "  timers:\n",
      "    learn_throughput: 1676.296\n",
      "    learn_time_ms: 596.553\n",
      "    load_throughput: 308813.43\n",
      "    load_time_ms: 3.238\n",
      "    sample_throughput: 98.429\n",
      "    sample_time_ms: 10159.615\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632133238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         1957.88</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 174\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2010250727335614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01717230870337768\n",
      "          policy_loss: 0.030484720050460764\n",
      "          total_loss: 0.014139676321711805\n",
      "          vf_explained_var: -0.4226686358451843\n",
      "          vf_loss: 0.0007751229814150268\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.919999999999995\n",
      "    ram_util_percent: 67.47333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054683661333697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.75265108118259\n",
      "    mean_inference_ms: 1.4079809894444102\n",
      "    mean_raw_obs_processing_ms: 0.6413989943619014\n",
      "  time_since_restore: 1968.764940738678\n",
      "  time_this_iter_s: 10.885045766830444\n",
      "  time_total_s: 1968.764940738678\n",
      "  timers:\n",
      "    learn_throughput: 1680.234\n",
      "    learn_time_ms: 595.155\n",
      "    load_throughput: 309977.385\n",
      "    load_time_ms: 3.226\n",
      "    sample_throughput: 98.11\n",
      "    sample_time_ms: 10192.597\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632133249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         1968.76</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-21-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 175\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.928760215971205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01644486538426724\n",
      "          policy_loss: -0.08280162852671412\n",
      "          total_loss: -0.0935511692530579\n",
      "          vf_explained_var: 0.0693698599934578\n",
      "          vf_loss: 0.0038551237566732907\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.03125\n",
      "    ram_util_percent: 67.54374999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040545901206075693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.748368456468988\n",
      "    mean_inference_ms: 1.4079459686464795\n",
      "    mean_raw_obs_processing_ms: 0.6415937252278511\n",
      "  time_since_restore: 1979.5962884426117\n",
      "  time_this_iter_s: 10.831347703933716\n",
      "  time_total_s: 1979.5962884426117\n",
      "  timers:\n",
      "    learn_throughput: 1681.369\n",
      "    learn_time_ms: 594.754\n",
      "    load_throughput: 310417.857\n",
      "    load_time_ms: 3.221\n",
      "    sample_throughput: 98.126\n",
      "    sample_time_ms: 10190.994\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632133260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">          1979.6</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-21-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 176\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.099005369345347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010296983524815149\n",
      "          policy_loss: 0.04069431391027239\n",
      "          total_loss: 0.023413066607382564\n",
      "          vf_explained_var: -0.558141827583313\n",
      "          vf_loss: 0.0007765798541691361\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.846666666666664\n",
      "    ram_util_percent: 67.59333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054497592309653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.744119903897332\n",
      "    mean_inference_ms: 1.4079099637138535\n",
      "    mean_raw_obs_processing_ms: 0.6418207180041767\n",
      "  time_since_restore: 1990.5248022079468\n",
      "  time_this_iter_s: 10.928513765335083\n",
      "  time_total_s: 1990.5248022079468\n",
      "  timers:\n",
      "    learn_throughput: 1682.051\n",
      "    learn_time_ms: 594.512\n",
      "    load_throughput: 310103.434\n",
      "    load_time_ms: 3.225\n",
      "    sample_throughput: 98.011\n",
      "    sample_time_ms: 10202.976\n",
      "    update_time_ms: 1.769\n",
      "  timestamp: 1632133271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         1990.52</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-21-22\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 177\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9028687159220377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015212006455722597\n",
      "          policy_loss: -0.08038037286864387\n",
      "          total_loss: -0.09377484106355244\n",
      "          vf_explained_var: 0.6841613054275513\n",
      "          vf_loss: 0.001302361653910743\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.756249999999994\n",
      "    ram_util_percent: 67.68125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054406803026136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.739973844331105\n",
      "    mean_inference_ms: 1.4078747013876054\n",
      "    mean_raw_obs_processing_ms: 0.6420812833151605\n",
      "  time_since_restore: 2001.736709356308\n",
      "  time_this_iter_s: 11.211907148361206\n",
      "  time_total_s: 2001.736709356308\n",
      "  timers:\n",
      "    learn_throughput: 1682.512\n",
      "    learn_time_ms: 594.349\n",
      "    load_throughput: 310270.894\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 97.712\n",
      "    sample_time_ms: 10234.14\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632133282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         2001.74</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-21-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 178\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.851017591688368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010100563207811388\n",
      "          policy_loss: -0.10410048473212453\n",
      "          total_loss: -0.11862328184975518\n",
      "          vf_explained_var: -0.35334697365760803\n",
      "          vf_loss: 0.0011110836028819903\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.71176470588235\n",
      "    ram_util_percent: 67.74117647058823\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054319786505735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.735929056805634\n",
      "    mean_inference_ms: 1.4078409023827034\n",
      "    mean_raw_obs_processing_ms: 0.6423742995132659\n",
      "  time_since_restore: 2013.0175313949585\n",
      "  time_this_iter_s: 11.280822038650513\n",
      "  time_total_s: 2013.0175313949585\n",
      "  timers:\n",
      "    learn_throughput: 1682.009\n",
      "    learn_time_ms: 594.527\n",
      "    load_throughput: 311120.144\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 97.155\n",
      "    sample_time_ms: 10292.821\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1632133294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         2013.02</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-21-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 179\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7973589804437426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01578147286721937\n",
      "          policy_loss: -0.07450878769159316\n",
      "          total_loss: -0.07674949864546458\n",
      "          vf_explained_var: 0.4351363778114319\n",
      "          vf_loss: 0.01123885825476868\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.186666666666675\n",
      "    ram_util_percent: 67.76666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054235858368732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.731952407110727\n",
      "    mean_inference_ms: 1.4078080867337703\n",
      "    mean_raw_obs_processing_ms: 0.6426991222630923\n",
      "  time_since_restore: 2023.885239124298\n",
      "  time_this_iter_s: 10.8677077293396\n",
      "  time_total_s: 2023.885239124298\n",
      "  timers:\n",
      "    learn_throughput: 1682.546\n",
      "    learn_time_ms: 594.337\n",
      "    load_throughput: 312187.686\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 97.267\n",
      "    sample_time_ms: 10280.939\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632133305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         2023.89</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-22-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 180\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.035971314377255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013746625269142538\n",
      "          policy_loss: -0.03393654310041004\n",
      "          total_loss: -0.04871466995941268\n",
      "          vf_explained_var: -0.34330716729164124\n",
      "          vf_loss: 0.0016670200718282204\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.4325\n",
      "    ram_util_percent: 67.855\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054152707806364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.728027228166969\n",
      "    mean_inference_ms: 1.4077755100831786\n",
      "    mean_raw_obs_processing_ms: 0.6440405558671921\n",
      "  time_since_restore: 2052.1109459400177\n",
      "  time_this_iter_s: 28.225706815719604\n",
      "  time_total_s: 2052.1109459400177\n",
      "  timers:\n",
      "    learn_throughput: 1682.103\n",
      "    learn_time_ms: 594.494\n",
      "    load_throughput: 213192.368\n",
      "    load_time_ms: 4.691\n",
      "    sample_throughput: 83.201\n",
      "    sample_time_ms: 12019.057\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632133333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         2052.11</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-22-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 181\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9297977818383112\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00645078801165483\n",
      "          policy_loss: -0.17780905086547136\n",
      "          total_loss: -0.19385000550084644\n",
      "          vf_explained_var: -0.03796077147126198\n",
      "          vf_loss: 0.0014200593445113756\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.05\n",
      "    ram_util_percent: 68.0111111111111\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054071210208447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.724257329611977\n",
      "    mean_inference_ms: 1.4077435776766467\n",
      "    mean_raw_obs_processing_ms: 0.6454059128257866\n",
      "  time_since_restore: 2064.654490709305\n",
      "  time_this_iter_s: 12.54354476928711\n",
      "  time_total_s: 2064.654490709305\n",
      "  timers:\n",
      "    learn_throughput: 1682.25\n",
      "    learn_time_ms: 594.442\n",
      "    load_throughput: 213137.116\n",
      "    load_time_ms: 4.692\n",
      "    sample_throughput: 81.856\n",
      "    sample_time_ms: 12216.582\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632133345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         2064.65</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 182\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.106863778167301\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00993354679435754\n",
      "          policy_loss: -0.06158890053629875\n",
      "          total_loss: -0.07762789730396535\n",
      "          vf_explained_var: 0.12965896725654602\n",
      "          vf_loss: 0.00220090925706447\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.826666666666675\n",
      "    ram_util_percent: 68.10000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053989881059208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.72052473266286\n",
      "    mean_inference_ms: 1.4077119502394533\n",
      "    mean_raw_obs_processing_ms: 0.6467942643149912\n",
      "  time_since_restore: 2074.909545660019\n",
      "  time_this_iter_s: 10.255054950714111\n",
      "  time_total_s: 2074.909545660019\n",
      "  timers:\n",
      "    learn_throughput: 1680.881\n",
      "    learn_time_ms: 594.926\n",
      "    load_throughput: 213071.069\n",
      "    load_time_ms: 4.693\n",
      "    sample_throughput: 82.17\n",
      "    sample_time_ms: 12169.887\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632133356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         2074.91</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-22-46\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 183\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7346187207433912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01691128085902523\n",
      "          policy_loss: 0.06565035548475054\n",
      "          total_loss: 0.05519500159555012\n",
      "          vf_explained_var: 0.23796740174293518\n",
      "          vf_loss: 0.0020750810388967186\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95333333333334\n",
      "    ram_util_percent: 68.10000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040539105244018304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.716789955061534\n",
      "    mean_inference_ms: 1.4076801240601027\n",
      "    mean_raw_obs_processing_ms: 0.6482044516942012\n",
      "  time_since_restore: 2085.1111936569214\n",
      "  time_this_iter_s: 10.201647996902466\n",
      "  time_total_s: 2085.1111936569214\n",
      "  timers:\n",
      "    learn_throughput: 1679.437\n",
      "    learn_time_ms: 595.438\n",
      "    load_throughput: 212932.612\n",
      "    load_time_ms: 4.696\n",
      "    sample_throughput: 82.525\n",
      "    sample_time_ms: 12117.595\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632133366\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         2085.11</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-22-56\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 184\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5823887573348152\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00912328044911344\n",
      "          policy_loss: -0.03659738169776069\n",
      "          total_loss: -0.04379269464148416\n",
      "          vf_explained_var: 0.22052106261253357\n",
      "          vf_loss: 0.006030573002256763\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.65\n",
      "    ram_util_percent: 68.07142857142857\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040538311766757575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.713134262995995\n",
      "    mean_inference_ms: 1.407648450562345\n",
      "    mean_raw_obs_processing_ms: 0.6496355244630705\n",
      "  time_since_restore: 2095.3999803066254\n",
      "  time_this_iter_s: 10.28878664970398\n",
      "  time_total_s: 2095.3999803066254\n",
      "  timers:\n",
      "    learn_throughput: 1677.211\n",
      "    learn_time_ms: 596.228\n",
      "    load_throughput: 211301.014\n",
      "    load_time_ms: 4.733\n",
      "    sample_throughput: 82.939\n",
      "    sample_time_ms: 12057.049\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632133376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">          2095.4</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 185\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1440253416697184\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014853125206543026\n",
      "          policy_loss: 0.05261724883069595\n",
      "          total_loss: 0.037056992803182864\n",
      "          vf_explained_var: -0.05988286808133125\n",
      "          vf_loss: 0.001650334480089239\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.14666666666667\n",
      "    ram_util_percent: 68.03333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040537579383429966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.709468196678916\n",
      "    mean_inference_ms: 1.4076155220568236\n",
      "    mean_raw_obs_processing_ms: 0.6510864399670535\n",
      "  time_since_restore: 2105.7309448719025\n",
      "  time_this_iter_s: 10.3309645652771\n",
      "  time_total_s: 2105.7309448719025\n",
      "  timers:\n",
      "    learn_throughput: 1677.274\n",
      "    learn_time_ms: 596.206\n",
      "    load_throughput: 211602.696\n",
      "    load_time_ms: 4.726\n",
      "    sample_throughput: 83.284\n",
      "    sample_time_ms: 12007.039\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632133387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         2105.73</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-23-17\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 186\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.101758208539751\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016744239920348472\n",
      "          policy_loss: -0.05415340728229946\n",
      "          total_loss: -0.06912884778446621\n",
      "          vf_explained_var: 0.11857501417398453\n",
      "          vf_loss: 0.0012739624824866446\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.99333333333333\n",
      "    ram_util_percent: 68.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040536840563292306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.705806519417632\n",
      "    mean_inference_ms: 1.407581943250821\n",
      "    mean_raw_obs_processing_ms: 0.6525564481848675\n",
      "  time_since_restore: 2116.346868991852\n",
      "  time_this_iter_s: 10.61592411994934\n",
      "  time_total_s: 2116.346868991852\n",
      "  timers:\n",
      "    learn_throughput: 1675.435\n",
      "    learn_time_ms: 596.86\n",
      "    load_throughput: 211934.151\n",
      "    load_time_ms: 4.718\n",
      "    sample_throughput: 83.506\n",
      "    sample_time_ms: 11975.121\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632133397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         2116.35</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-23-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 187\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8916118992699518\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013226066999063925\n",
      "          policy_loss: -0.011202357398966948\n",
      "          total_loss: -0.025469023485978445\n",
      "          vf_explained_var: -0.49372321367263794\n",
      "          vf_loss: 0.000883122808429309\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.10666666666666\n",
      "    ram_util_percent: 68.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053608893191116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.702139843854269\n",
      "    mean_inference_ms: 1.4075479136912825\n",
      "    mean_raw_obs_processing_ms: 0.6540451712163534\n",
      "  time_since_restore: 2126.897920370102\n",
      "  time_this_iter_s: 10.551051378250122\n",
      "  time_total_s: 2126.897920370102\n",
      "  timers:\n",
      "    learn_throughput: 1673.84\n",
      "    learn_time_ms: 597.429\n",
      "    load_throughput: 211611.237\n",
      "    load_time_ms: 4.726\n",
      "    sample_throughput: 83.974\n",
      "    sample_time_ms: 11908.444\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632133408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">          2126.9</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-23-38\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 188\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.981636639436086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011229366756772387\n",
      "          policy_loss: -0.12737562925451332\n",
      "          total_loss: -0.14186259520550568\n",
      "          vf_explained_var: 0.1471191793680191\n",
      "          vf_loss: 0.0021316638714375183\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.12666666666667\n",
      "    ram_util_percent: 68.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053531870316235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.698467956478037\n",
      "    mean_inference_ms: 1.4075137059288365\n",
      "    mean_raw_obs_processing_ms: 0.655550998625642\n",
      "  time_since_restore: 2137.3786981105804\n",
      "  time_this_iter_s: 10.480777740478516\n",
      "  time_total_s: 2137.3786981105804\n",
      "  timers:\n",
      "    learn_throughput: 1674.435\n",
      "    learn_time_ms: 597.216\n",
      "    load_throughput: 210897.279\n",
      "    load_time_ms: 4.742\n",
      "    sample_throughput: 84.541\n",
      "    sample_time_ms: 11828.609\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632133418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         2137.38</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-23-49\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 189\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8746702684296501\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015754116694603148\n",
      "          policy_loss: 0.02845566483835379\n",
      "          total_loss: 0.021627549692574473\n",
      "          vf_explained_var: 0.3690478503704071\n",
      "          vf_loss: 0.007432356941798288\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.946666666666665\n",
      "    ram_util_percent: 68.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040534559088551435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.69477991866912\n",
      "    mean_inference_ms: 1.4074797982986167\n",
      "    mean_raw_obs_processing_ms: 0.6570729472289915\n",
      "  time_since_restore: 2147.6566298007965\n",
      "  time_this_iter_s: 10.277931690216064\n",
      "  time_total_s: 2147.6566298007965\n",
      "  timers:\n",
      "    learn_throughput: 1675.323\n",
      "    learn_time_ms: 596.9\n",
      "    load_throughput: 211162.72\n",
      "    load_time_ms: 4.736\n",
      "    sample_throughput: 84.962\n",
      "    sample_time_ms: 11769.932\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632133429\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         2147.66</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-23-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 190\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0619775070084465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01733050530875424\n",
      "          policy_loss: -0.09788672385944261\n",
      "          total_loss: -0.11166430132256613\n",
      "          vf_explained_var: 0.5145321488380432\n",
      "          vf_loss: 0.0019070662021274782\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.126666666666665\n",
      "    ram_util_percent: 68.04\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040533789412220037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.691122676687357\n",
      "    mean_inference_ms: 1.4074459999897087\n",
      "    mean_raw_obs_processing_ms: 0.6566392929971222\n",
      "  time_since_restore: 2158.000429868698\n",
      "  time_this_iter_s: 10.343800067901611\n",
      "  time_total_s: 2158.000429868698\n",
      "  timers:\n",
      "    learn_throughput: 1676.742\n",
      "    learn_time_ms: 596.395\n",
      "    load_throughput: 309072.849\n",
      "    load_time_ms: 3.235\n",
      "    sample_throughput: 100.163\n",
      "    sample_time_ms: 9983.756\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632133439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">            2158</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-24-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 191\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1917386876212226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01145200700181594\n",
      "          policy_loss: -0.053165502266751395\n",
      "          total_loss: -0.06829812261793348\n",
      "          vf_explained_var: 0.19415634870529175\n",
      "          vf_loss: 0.0035236275082247124\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.906666666666666\n",
      "    ram_util_percent: 68.10000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053300810382168\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.687375518509333\n",
      "    mean_inference_ms: 1.4074125369509296\n",
      "    mean_raw_obs_processing_ms: 0.6562432886738208\n",
      "  time_since_restore: 2168.2459001541138\n",
      "  time_this_iter_s: 10.24547028541565\n",
      "  time_total_s: 2168.2459001541138\n",
      "  timers:\n",
      "    learn_throughput: 1678.117\n",
      "    learn_time_ms: 595.906\n",
      "    load_throughput: 309316.735\n",
      "    load_time_ms: 3.233\n",
      "    sample_throughput: 102.517\n",
      "    sample_time_ms: 9754.48\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632133449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         2168.25</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-24-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 192\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8474455575148265\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.029169838974656415\n",
      "          policy_loss: -0.003760940002070533\n",
      "          total_loss: -0.011856946680280898\n",
      "          vf_explained_var: 0.43290722370147705\n",
      "          vf_loss: 0.0020718808948812593\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.20714285714286\n",
      "    ram_util_percent: 68.16428571428574\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405322164117496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.68373344063044\n",
      "    mean_inference_ms: 1.4073779554579047\n",
      "    mean_raw_obs_processing_ms: 0.6558834364511981\n",
      "  time_since_restore: 2178.533177137375\n",
      "  time_this_iter_s: 10.287276983261108\n",
      "  time_total_s: 2178.533177137375\n",
      "  timers:\n",
      "    learn_throughput: 1679.704\n",
      "    learn_time_ms: 595.343\n",
      "    load_throughput: 309757.618\n",
      "    load_time_ms: 3.228\n",
      "    sample_throughput: 102.473\n",
      "    sample_time_ms: 9758.672\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632133460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         2178.53</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-24-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 193\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1737987094455296\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012933760338568866\n",
      "          policy_loss: -0.01346700530913141\n",
      "          total_loss: -0.02865028033653895\n",
      "          vf_explained_var: 0.04767125844955444\n",
      "          vf_loss: 0.0010300761787220836\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.033333333333324\n",
      "    ram_util_percent: 68.24\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040531449637057196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.680215050465225\n",
      "    mean_inference_ms: 1.4073441935934483\n",
      "    mean_raw_obs_processing_ms: 0.6555586413798521\n",
      "  time_since_restore: 2189.132183790207\n",
      "  time_this_iter_s: 10.599006652832031\n",
      "  time_total_s: 2189.132183790207\n",
      "  timers:\n",
      "    learn_throughput: 1680.484\n",
      "    learn_time_ms: 595.067\n",
      "    load_throughput: 309129.797\n",
      "    load_time_ms: 3.235\n",
      "    sample_throughput: 102.055\n",
      "    sample_time_ms: 9798.644\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632133470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         2189.13</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-24-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 194\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.212952052222358\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013644264957834048\n",
      "          policy_loss: -0.04270955744302935\n",
      "          total_loss: -0.05779873981244034\n",
      "          vf_explained_var: -0.2294166386127472\n",
      "          vf_loss: 0.0012122047987456123\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.6875\n",
      "    ram_util_percent: 68.30625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053070247217449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.676777644332747\n",
      "    mean_inference_ms: 1.4073115141532286\n",
      "    mean_raw_obs_processing_ms: 0.6552678205229525\n",
      "  time_since_restore: 2199.715322494507\n",
      "  time_this_iter_s: 10.583138704299927\n",
      "  time_total_s: 2199.715322494507\n",
      "  timers:\n",
      "    learn_throughput: 1679.128\n",
      "    learn_time_ms: 595.547\n",
      "    load_throughput: 311455.134\n",
      "    load_time_ms: 3.211\n",
      "    sample_throughput: 101.753\n",
      "    sample_time_ms: 9827.703\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632133481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         2199.72</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-24-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 195\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1526113828023274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008999690065278069\n",
      "          policy_loss: -0.0893091779616144\n",
      "          total_loss: -0.10457011875179079\n",
      "          vf_explained_var: -0.28475940227508545\n",
      "          vf_loss: 0.0024209708602736806\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17333333333333\n",
      "    ram_util_percent: 68.38666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052996718068477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.673396486190079\n",
      "    mean_inference_ms: 1.407279659402499\n",
      "    mean_raw_obs_processing_ms: 0.6550103240825278\n",
      "  time_since_restore: 2210.381101369858\n",
      "  time_this_iter_s: 10.665778875350952\n",
      "  time_total_s: 2210.381101369858\n",
      "  timers:\n",
      "    learn_throughput: 1678.02\n",
      "    learn_time_ms: 595.941\n",
      "    load_throughput: 310284.666\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 101.412\n",
      "    sample_time_ms: 9860.778\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632133491\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         2210.38</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-25-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 196\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.151994428369734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010296460411519664\n",
      "          policy_loss: -0.03736620419141319\n",
      "          total_loss: -0.05226833290523953\n",
      "          vf_explained_var: -0.3574862480163574\n",
      "          vf_loss: 0.002219694418211778\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.60666666666666\n",
      "    ram_util_percent: 68.48666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052926264997115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.670057805735091\n",
      "    mean_inference_ms: 1.4072486455387498\n",
      "    mean_raw_obs_processing_ms: 0.6547854700410258\n",
      "  time_since_restore: 2220.6629960536957\n",
      "  time_this_iter_s: 10.28189468383789\n",
      "  time_total_s: 2220.6629960536957\n",
      "  timers:\n",
      "    learn_throughput: 1678.816\n",
      "    learn_time_ms: 595.658\n",
      "    load_throughput: 309787.36\n",
      "    load_time_ms: 3.228\n",
      "    sample_throughput: 101.754\n",
      "    sample_time_ms: 9827.633\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632133502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         2220.66</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 197\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8697673757870992\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013555787911057927\n",
      "          policy_loss: -0.09732058151728577\n",
      "          total_loss: -0.1087632001688083\n",
      "          vf_explained_var: -0.19859664142131805\n",
      "          vf_loss: 0.0014647211077519588\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17857142857142\n",
      "    ram_util_percent: 68.55\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040528588166745975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.666777522214389\n",
      "    mean_inference_ms: 1.4072188360594644\n",
      "    mean_raw_obs_processing_ms: 0.6545916399275613\n",
      "  time_since_restore: 2231.0863654613495\n",
      "  time_this_iter_s: 10.423369407653809\n",
      "  time_total_s: 2231.0863654613495\n",
      "  timers:\n",
      "    learn_throughput: 1680.215\n",
      "    learn_time_ms: 595.162\n",
      "    load_throughput: 309961.35\n",
      "    load_time_ms: 3.226\n",
      "    sample_throughput: 101.881\n",
      "    sample_time_ms: 9815.373\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632133512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         2231.09</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-25-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 198\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1631674978468154\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01110509823033467\n",
      "          policy_loss: -0.062080597173836495\n",
      "          total_loss: -0.07800847213301394\n",
      "          vf_explained_var: -0.8792492151260376\n",
      "          vf_loss: 0.0009602736992140611\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.940000000000005\n",
      "    ram_util_percent: 68.62000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040527917057180396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.663586004114688\n",
      "    mean_inference_ms: 1.4071896113637623\n",
      "    mean_raw_obs_processing_ms: 0.6544278454900383\n",
      "  time_since_restore: 2241.541985273361\n",
      "  time_this_iter_s: 10.455619812011719\n",
      "  time_total_s: 2241.541985273361\n",
      "  timers:\n",
      "    learn_throughput: 1680.228\n",
      "    learn_time_ms: 595.157\n",
      "    load_throughput: 311739.864\n",
      "    load_time_ms: 3.208\n",
      "    sample_throughput: 101.907\n",
      "    sample_time_ms: 9812.89\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632133523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         2241.54</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-25-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 199\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.005477754275004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01062353710526125\n",
      "          policy_loss: -0.08522236821138196\n",
      "          total_loss: -0.09995575623793734\n",
      "          vf_explained_var: -0.6045636534690857\n",
      "          vf_loss: 0.0007835591024356998\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.96666666666667\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052724588617136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.660504051407957\n",
      "    mean_inference_ms: 1.4071616777454061\n",
      "    mean_raw_obs_processing_ms: 0.6542933209022379\n",
      "  time_since_restore: 2251.9979264736176\n",
      "  time_this_iter_s: 10.455941200256348\n",
      "  time_total_s: 2251.9979264736176\n",
      "  timers:\n",
      "    learn_throughput: 1680.423\n",
      "    learn_time_ms: 595.088\n",
      "    load_throughput: 310142.416\n",
      "    load_time_ms: 3.224\n",
      "    sample_throughput: 101.722\n",
      "    sample_time_ms: 9830.751\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632133533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">            2252</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-25-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 200\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1352517512109546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010627477107078034\n",
      "          policy_loss: -0.03751291723714934\n",
      "          total_loss: -0.05293100525935491\n",
      "          vf_explained_var: -0.6487246155738831\n",
      "          vf_loss: 0.0013949211777394845\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.96666666666666\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052658059608225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.657473543257098\n",
      "    mean_inference_ms: 1.4071346278881334\n",
      "    mean_raw_obs_processing_ms: 0.6541855451192751\n",
      "  time_since_restore: 2262.3663659095764\n",
      "  time_this_iter_s: 10.368439435958862\n",
      "  time_total_s: 2262.3663659095764\n",
      "  timers:\n",
      "    learn_throughput: 1679.178\n",
      "    learn_time_ms: 595.529\n",
      "    load_throughput: 309508.468\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 101.701\n",
      "    sample_time_ms: 9832.759\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632133544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         2262.37</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-25-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 201\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.084183504846361\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01836736350056215\n",
      "          policy_loss: 0.007472958995236291\n",
      "          total_loss: -0.004693039630850156\n",
      "          vf_explained_var: -0.638526976108551\n",
      "          vf_loss: 0.0008302443411796251\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.826666666666675\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052593964732325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.654463163725689\n",
      "    mean_inference_ms: 1.4071081457302494\n",
      "    mean_raw_obs_processing_ms: 0.6541053278224301\n",
      "  time_since_restore: 2272.572194337845\n",
      "  time_this_iter_s: 10.205828428268433\n",
      "  time_total_s: 2272.572194337845\n",
      "  timers:\n",
      "    learn_throughput: 1679.0\n",
      "    learn_time_ms: 595.593\n",
      "    load_throughput: 309453.663\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 101.743\n",
      "    sample_time_ms: 9828.705\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632133554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         2272.57</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-26-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 202\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.076734533574846\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00978535598736193\n",
      "          policy_loss: -0.1305796946088473\n",
      "          total_loss: -0.14624757965405782\n",
      "          vf_explained_var: -0.3630428612232208\n",
      "          vf_loss: 0.0009196595618656526\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.00000000000001\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040525313271466315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.651512275009985\n",
      "    mean_inference_ms: 1.407082471269827\n",
      "    mean_raw_obs_processing_ms: 0.6540518023498417\n",
      "  time_since_restore: 2283.0283834934235\n",
      "  time_this_iter_s: 10.456189155578613\n",
      "  time_total_s: 2283.0283834934235\n",
      "  timers:\n",
      "    learn_throughput: 1678.972\n",
      "    learn_time_ms: 595.602\n",
      "    load_throughput: 309462.796\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 101.568\n",
      "    sample_time_ms: 9845.574\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632133564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         2283.03</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-26-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 203\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.01738123761283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008446461758627658\n",
      "          policy_loss: -0.0024021887944804298\n",
      "          total_loss: -0.018378776932756104\n",
      "          vf_explained_var: -0.3696616590023041\n",
      "          vf_loss: 0.0005893305065304351\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.91333333333334\n",
      "    ram_util_percent: 68.76\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052468166977133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.64868014585341\n",
      "    mean_inference_ms: 1.407057739552424\n",
      "    mean_raw_obs_processing_ms: 0.6540249162906707\n",
      "  time_since_restore: 2293.5261216163635\n",
      "  time_this_iter_s: 10.497738122940063\n",
      "  time_total_s: 2293.5261216163635\n",
      "  timers:\n",
      "    learn_throughput: 1678.126\n",
      "    learn_time_ms: 595.903\n",
      "    load_throughput: 310850.367\n",
      "    load_time_ms: 3.217\n",
      "    sample_throughput: 101.676\n",
      "    sample_time_ms: 9835.181\n",
      "    update_time_ms: 1.757\n",
      "  timestamp: 1632133575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         2293.53</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-26-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 204\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.179107575946384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009610078810516537\n",
      "          policy_loss: -0.06668769837253624\n",
      "          total_loss: -0.0836644244276815\n",
      "          vf_explained_var: -0.7242202758789062\n",
      "          vf_loss: 0.0007094190627362372\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79999999999999\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040524040823599256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.645949067980428\n",
      "    mean_inference_ms: 1.407033663641015\n",
      "    mean_raw_obs_processing_ms: 0.6540231900959362\n",
      "  time_since_restore: 2303.9717853069305\n",
      "  time_this_iter_s: 10.445663690567017\n",
      "  time_total_s: 2303.9717853069305\n",
      "  timers:\n",
      "    learn_throughput: 1679.485\n",
      "    learn_time_ms: 595.421\n",
      "    load_throughput: 311658.79\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 101.813\n",
      "    sample_time_ms: 9821.939\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632133585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         2303.97</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-26-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 205\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.003146349059211\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010731218155645629\n",
      "          policy_loss: -0.14476490670090747\n",
      "          total_loss: -0.15907214518843427\n",
      "          vf_explained_var: -0.4148727059364319\n",
      "          vf_loss: 0.001140402109336315\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.84\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405233900627494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.643304183367189\n",
      "    mean_inference_ms: 1.407010677630818\n",
      "    mean_raw_obs_processing_ms: 0.6540464461520331\n",
      "  time_since_restore: 2314.6017196178436\n",
      "  time_this_iter_s: 10.629934310913086\n",
      "  time_total_s: 2314.6017196178436\n",
      "  timers:\n",
      "    learn_throughput: 1679.757\n",
      "    learn_time_ms: 595.324\n",
      "    load_throughput: 311830.253\n",
      "    load_time_ms: 3.207\n",
      "    sample_throughput: 101.85\n",
      "    sample_time_ms: 9818.372\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632133596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">          2314.6</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-26-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 206\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0178769403033785\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008710903742620532\n",
      "          policy_loss: -0.11600301994217767\n",
      "          total_loss: -0.13178989713390668\n",
      "          vf_explained_var: -0.39402517676353455\n",
      "          vf_loss: 0.0006710415389130099\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.973333333333336\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040522748466432654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.640690299692693\n",
      "    mean_inference_ms: 1.406988349732249\n",
      "    mean_raw_obs_processing_ms: 0.6540938415113474\n",
      "  time_since_restore: 2325.0890021324158\n",
      "  time_this_iter_s: 10.487282514572144\n",
      "  time_total_s: 2325.0890021324158\n",
      "  timers:\n",
      "    learn_throughput: 1681.151\n",
      "    learn_time_ms: 594.831\n",
      "    load_throughput: 312401.609\n",
      "    load_time_ms: 3.201\n",
      "    sample_throughput: 101.632\n",
      "    sample_time_ms: 9839.407\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632133606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         2325.09</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 207\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.225390511088901\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010045889261423982\n",
      "          policy_loss: -0.07688166987564829\n",
      "          total_loss: -0.09397464642922083\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008698421058296744\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.54666666666666\n",
      "    ram_util_percent: 68.72000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052211071442113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.638130013485949\n",
      "    mean_inference_ms: 1.406967219268399\n",
      "    mean_raw_obs_processing_ms: 0.654163852374836\n",
      "  time_since_restore: 2335.79496049881\n",
      "  time_this_iter_s: 10.705958366394043\n",
      "  time_total_s: 2335.79496049881\n",
      "  timers:\n",
      "    learn_throughput: 1679.96\n",
      "    learn_time_ms: 595.252\n",
      "    load_throughput: 310845.759\n",
      "    load_time_ms: 3.217\n",
      "    sample_throughput: 101.346\n",
      "    sample_time_ms: 9867.232\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632133617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         2335.79</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-27-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 208\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6904793169763352\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010818759855835304\n",
      "          policy_loss: 0.0052369440595308936\n",
      "          total_loss: -0.0062653564330604344\n",
      "          vf_explained_var: 0.27492862939834595\n",
      "          vf_loss: 0.0007812758309430339\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.080000000000005\n",
      "    ram_util_percent: 68.77999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040521468178797246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.635627521412493\n",
      "    mean_inference_ms: 1.4069465998185426\n",
      "    mean_raw_obs_processing_ms: 0.6542566474189577\n",
      "  time_since_restore: 2346.252943754196\n",
      "  time_this_iter_s: 10.457983255386353\n",
      "  time_total_s: 2346.252943754196\n",
      "  timers:\n",
      "    learn_throughput: 1680.803\n",
      "    learn_time_ms: 594.954\n",
      "    load_throughput: 310811.207\n",
      "    load_time_ms: 3.217\n",
      "    sample_throughput: 101.34\n",
      "    sample_time_ms: 9867.773\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632133628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         2346.25</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-27-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 209\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9012049873669943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005963817996022098\n",
      "          policy_loss: -0.02372974654038747\n",
      "          total_loss: -0.03964514997270372\n",
      "          vf_explained_var: -0.4234120547771454\n",
      "          vf_loss: 0.0005492095849250391\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.873333333333335\n",
      "    ram_util_percent: 68.76666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040520821888093576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.633180239327501\n",
      "    mean_inference_ms: 1.406926450416547\n",
      "    mean_raw_obs_processing_ms: 0.6543711396024058\n",
      "  time_since_restore: 2356.627116918564\n",
      "  time_this_iter_s: 10.374173164367676\n",
      "  time_total_s: 2356.627116918564\n",
      "  timers:\n",
      "    learn_throughput: 1682.112\n",
      "    learn_time_ms: 594.491\n",
      "    load_throughput: 311728.279\n",
      "    load_time_ms: 3.208\n",
      "    sample_throughput: 101.419\n",
      "    sample_time_ms: 9860.083\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632133638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         2356.63</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 210\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7845515780978733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011753839997658035\n",
      "          policy_loss: -0.14665217914928994\n",
      "          total_loss: -0.1580428326709403\n",
      "          vf_explained_var: 0.14485818147659302\n",
      "          vf_loss: 0.0014342266768734488\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.426829268292686\n",
      "    ram_util_percent: 68.70731707317073\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052015143554133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.63088689992501\n",
      "    mean_inference_ms: 1.406906800392096\n",
      "    mean_raw_obs_processing_ms: 0.6553107680834678\n",
      "  time_since_restore: 2385.5059475898743\n",
      "  time_this_iter_s: 28.878830671310425\n",
      "  time_total_s: 2385.5059475898743\n",
      "  timers:\n",
      "    learn_throughput: 1682.593\n",
      "    learn_time_ms: 594.321\n",
      "    load_throughput: 205449.05\n",
      "    load_time_ms: 4.867\n",
      "    sample_throughput: 85.4\n",
      "    sample_time_ms: 11709.641\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632133667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         2385.51</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-27-58\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 211\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.200297274854448\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012361151771861185\n",
      "          policy_loss: -0.0035252097580167983\n",
      "          total_loss: -0.018919511801666684\n",
      "          vf_explained_var: -0.0630386620759964\n",
      "          vf_loss: 0.0013286243422448427\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.2625\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051950215191431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.628662971993412\n",
      "    mean_inference_ms: 1.4068884798150934\n",
      "    mean_raw_obs_processing_ms: 0.6562672151153716\n",
      "  time_since_restore: 2396.3435130119324\n",
      "  time_this_iter_s: 10.837565422058105\n",
      "  time_total_s: 2396.3435130119324\n",
      "  timers:\n",
      "    learn_throughput: 1680.639\n",
      "    learn_time_ms: 595.012\n",
      "    load_throughput: 205745.344\n",
      "    load_time_ms: 4.86\n",
      "    sample_throughput: 84.946\n",
      "    sample_time_ms: 11772.136\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632133678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         2396.34</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 212\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8359550356864929\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014767760494288674\n",
      "          policy_loss: 0.01960198043121232\n",
      "          total_loss: 0.008747032491697206\n",
      "          vf_explained_var: -0.13606545329093933\n",
      "          vf_loss: 0.00119657795213344\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.239999999999995\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051885753045931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.626538706688946\n",
      "    mean_inference_ms: 1.406871294242204\n",
      "    mean_raw_obs_processing_ms: 0.6572406035262053\n",
      "  time_since_restore: 2407.0549297332764\n",
      "  time_this_iter_s: 10.711416721343994\n",
      "  time_total_s: 2407.0549297332764\n",
      "  timers:\n",
      "    learn_throughput: 1680.665\n",
      "    learn_time_ms: 595.003\n",
      "    load_throughput: 205373.602\n",
      "    load_time_ms: 4.869\n",
      "    sample_throughput: 84.763\n",
      "    sample_time_ms: 11797.596\n",
      "    update_time_ms: 1.769\n",
      "  timestamp: 1632133689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         2407.05</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-28-20\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 213\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.080828314357334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01121583057188733\n",
      "          policy_loss: -0.08572609027226766\n",
      "          total_loss: -0.10087246480915281\n",
      "          vf_explained_var: -0.4099237322807312\n",
      "          vf_loss: 0.0008710831853224793\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.91875\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051821789535472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.624505894821391\n",
      "    mean_inference_ms: 1.4068554098084731\n",
      "    mean_raw_obs_processing_ms: 0.6582310130443909\n",
      "  time_since_restore: 2418.031405687332\n",
      "  time_this_iter_s: 10.976475954055786\n",
      "  time_total_s: 2418.031405687332\n",
      "  timers:\n",
      "    learn_throughput: 1681.323\n",
      "    learn_time_ms: 594.77\n",
      "    load_throughput: 204865.021\n",
      "    load_time_ms: 4.881\n",
      "    sample_throughput: 84.419\n",
      "    sample_time_ms: 11845.658\n",
      "    update_time_ms: 1.774\n",
      "  timestamp: 1632133700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         2418.03</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-28-31\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 214\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1732866684595744\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013595205066651772\n",
      "          policy_loss: -0.03470884184870455\n",
      "          total_loss: -0.04972539378537072\n",
      "          vf_explained_var: -0.05317068099975586\n",
      "          vf_loss: 0.000909145670529041\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.737500000000004\n",
      "    ram_util_percent: 68.70625000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051759869373266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.62252346886235\n",
      "    mean_inference_ms: 1.406841172841115\n",
      "    mean_raw_obs_processing_ms: 0.6592367572349096\n",
      "  time_since_restore: 2429.209161758423\n",
      "  time_this_iter_s: 11.177756071090698\n",
      "  time_total_s: 2429.209161758423\n",
      "  timers:\n",
      "    learn_throughput: 1681.044\n",
      "    learn_time_ms: 594.869\n",
      "    load_throughput: 204723.029\n",
      "    load_time_ms: 4.885\n",
      "    sample_throughput: 83.901\n",
      "    sample_time_ms: 11918.759\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632133711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         2429.21</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-28-42\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 215\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9623902996381124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013533033877618155\n",
      "          policy_loss: -0.07593944379025036\n",
      "          total_loss: -0.08891164975033866\n",
      "          vf_explained_var: 0.14123137295246124\n",
      "          vf_loss: 0.0008710784662980586\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.70625\n",
      "    ram_util_percent: 68.63125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051700589175003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.620625439904986\n",
      "    mean_inference_ms: 1.4068285091440622\n",
      "    mean_raw_obs_processing_ms: 0.6602581008705783\n",
      "  time_since_restore: 2440.361773252487\n",
      "  time_this_iter_s: 11.152611494064331\n",
      "  time_total_s: 2440.361773252487\n",
      "  timers:\n",
      "    learn_throughput: 1682.003\n",
      "    learn_time_ms: 594.529\n",
      "    load_throughput: 205066.346\n",
      "    load_time_ms: 4.876\n",
      "    sample_throughput: 83.532\n",
      "    sample_time_ms: 11971.445\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632133722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         2440.36</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-28-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 216\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7247387568155925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006493534568571846\n",
      "          policy_loss: -0.018123218748304578\n",
      "          total_loss: -0.03164293724629614\n",
      "          vf_explained_var: 0.45735085010528564\n",
      "          vf_loss: 0.0009539677797066462\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.779999999999994\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040516425222578124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.61880185932829\n",
      "    mean_inference_ms: 1.4068170934432587\n",
      "    mean_raw_obs_processing_ms: 0.6612945184867272\n",
      "  time_since_restore: 2451.2061836719513\n",
      "  time_this_iter_s: 10.844410419464111\n",
      "  time_total_s: 2451.2061836719513\n",
      "  timers:\n",
      "    learn_throughput: 1679.763\n",
      "    learn_time_ms: 595.322\n",
      "    load_throughput: 205034.268\n",
      "    load_time_ms: 4.877\n",
      "    sample_throughput: 83.289\n",
      "    sample_time_ms: 12006.41\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632133733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         2451.21</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-29-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 217\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1572826888826158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009798850230153672\n",
      "          policy_loss: -0.12099188081920147\n",
      "          total_loss: -0.13742713845438428\n",
      "          vf_explained_var: 0.09079993516206741\n",
      "          vf_loss: 0.0009520039955128191\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.1125\n",
      "    ram_util_percent: 68.6375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040515858011523916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.617083672696301\n",
      "    mean_inference_ms: 1.406807261473385\n",
      "    mean_raw_obs_processing_ms: 0.6623448725225921\n",
      "  time_since_restore: 2462.3250765800476\n",
      "  time_this_iter_s: 11.118892908096313\n",
      "  time_total_s: 2462.3250765800476\n",
      "  timers:\n",
      "    learn_throughput: 1682.523\n",
      "    learn_time_ms: 594.345\n",
      "    load_throughput: 205570.891\n",
      "    load_time_ms: 4.865\n",
      "    sample_throughput: 82.997\n",
      "    sample_time_ms: 12048.635\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632133744\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         2462.33</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-29-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 218\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9641286108228895\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008039645818122507\n",
      "          policy_loss: -0.08342239434520403\n",
      "          total_loss: -0.0986441146582365\n",
      "          vf_explained_var: -0.5111039280891418\n",
      "          vf_loss: 0.0009854405138563986\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.96875\n",
      "    ram_util_percent: 68.64375000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040515285126217354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.615464763063004\n",
      "    mean_inference_ms: 1.406798177745737\n",
      "    mean_raw_obs_processing_ms: 0.6634095237223506\n",
      "  time_since_restore: 2473.3351266384125\n",
      "  time_this_iter_s: 11.010050058364868\n",
      "  time_total_s: 2473.3351266384125\n",
      "  timers:\n",
      "    learn_throughput: 1683.459\n",
      "    learn_time_ms: 594.015\n",
      "    load_throughput: 205149.596\n",
      "    load_time_ms: 4.874\n",
      "    sample_throughput: 82.616\n",
      "    sample_time_ms: 12104.17\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632133755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         2473.34</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-29-26\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 219\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9442739619149103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011986064118752892\n",
      "          policy_loss: -0.08199113913708263\n",
      "          total_loss: -0.09482615039580398\n",
      "          vf_explained_var: -0.8493596315383911\n",
      "          vf_loss: 0.0014879002258466143\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.173333333333325\n",
      "    ram_util_percent: 68.64666666666669\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040514735911028346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.613935411068239\n",
      "    mean_inference_ms: 1.4067900411123497\n",
      "    mean_raw_obs_processing_ms: 0.664487705612536\n",
      "  time_since_restore: 2484.215631723404\n",
      "  time_this_iter_s: 10.880505084991455\n",
      "  time_total_s: 2484.215631723404\n",
      "  timers:\n",
      "    learn_throughput: 1679.758\n",
      "    learn_time_ms: 595.324\n",
      "    load_throughput: 205219.859\n",
      "    load_time_ms: 4.873\n",
      "    sample_throughput: 82.281\n",
      "    sample_time_ms: 12153.417\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1632133766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         2484.22</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-29-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 220\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0671769195132788\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008801913898683723\n",
      "          policy_loss: -0.10067625633544391\n",
      "          total_loss: -0.11550435775683986\n",
      "          vf_explained_var: -0.01367012970149517\n",
      "          vf_loss: 0.0020839431840512486\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.25\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051419349010924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.612498970817207\n",
      "    mean_inference_ms: 1.4067834797526388\n",
      "    mean_raw_obs_processing_ms: 0.6642018776400956\n",
      "  time_since_restore: 2495.3795971870422\n",
      "  time_this_iter_s: 11.163965463638306\n",
      "  time_total_s: 2495.3795971870422\n",
      "  timers:\n",
      "    learn_throughput: 1679.232\n",
      "    learn_time_ms: 595.51\n",
      "    load_throughput: 310023.209\n",
      "    load_time_ms: 3.226\n",
      "    sample_throughput: 96.308\n",
      "    sample_time_ms: 10383.383\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632133777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         2495.38</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-29-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 221\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.103529001606835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013431301445596658\n",
      "          policy_loss: 0.03876418024301529\n",
      "          total_loss: 0.02427817036708196\n",
      "          vf_explained_var: -0.49419620633125305\n",
      "          vf_loss: 0.0008121192261266212\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.873333333333335\n",
      "    ram_util_percent: 68.62000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040513653888030104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.610989098487726\n",
      "    mean_inference_ms: 1.406776778017389\n",
      "    mean_raw_obs_processing_ms: 0.6639404603628205\n",
      "  time_since_restore: 2505.935373544693\n",
      "  time_this_iter_s: 10.555776357650757\n",
      "  time_total_s: 2505.935373544693\n",
      "  timers:\n",
      "    learn_throughput: 1682.467\n",
      "    learn_time_ms: 594.365\n",
      "    load_throughput: 308416.045\n",
      "    load_time_ms: 3.242\n",
      "    sample_throughput: 96.559\n",
      "    sample_time_ms: 10356.328\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632133788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         2505.94</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-29-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 222\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0343871739175583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008394809898151716\n",
      "          policy_loss: -0.048820875212550165\n",
      "          total_loss: -0.06441418387823634\n",
      "          vf_explained_var: -0.5263194441795349\n",
      "          vf_loss: 0.0011647316494620302\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.86874999999999\n",
      "    ram_util_percent: 68.68125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051311778806482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.609565785204657\n",
      "    mean_inference_ms: 1.406770486438791\n",
      "    mean_raw_obs_processing_ms: 0.6637018597110459\n",
      "  time_since_restore: 2516.881740808487\n",
      "  time_this_iter_s: 10.946367263793945\n",
      "  time_total_s: 2516.881740808487\n",
      "  timers:\n",
      "    learn_throughput: 1683.188\n",
      "    learn_time_ms: 594.111\n",
      "    load_throughput: 309038.69\n",
      "    load_time_ms: 3.236\n",
      "    sample_throughput: 96.338\n",
      "    sample_time_ms: 10380.16\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632133799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         2516.88</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-30-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 223\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9952839824888442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0029239021920521263\n",
      "          policy_loss: -0.24832698752482732\n",
      "          total_loss: -0.26622952073812484\n",
      "          vf_explained_var: -0.23727107048034668\n",
      "          vf_loss: 0.0008013636510845067\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.8375\n",
      "    ram_util_percent: 68.73750000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051262007888343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.608237845328341\n",
      "    mean_inference_ms: 1.4067659491597357\n",
      "    mean_raw_obs_processing_ms: 0.6634864944300124\n",
      "  time_since_restore: 2528.0852932929993\n",
      "  time_this_iter_s: 11.203552484512329\n",
      "  time_total_s: 2528.0852932929993\n",
      "  timers:\n",
      "    learn_throughput: 1683.493\n",
      "    learn_time_ms: 594.003\n",
      "    load_throughput: 309460.512\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 96.126\n",
      "    sample_time_ms: 10403.026\n",
      "    update_time_ms: 1.741\n",
      "  timestamp: 1632133810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         2528.09</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-30-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 224\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0331083244747585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014569452918301156\n",
      "          policy_loss: -0.08174968130058713\n",
      "          total_loss: -0.0978364442785581\n",
      "          vf_explained_var: -0.442316472530365\n",
      "          vf_loss: 0.0011326560116786924\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.12\n",
      "    ram_util_percent: 68.79999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040512125578886365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.606961803387417\n",
      "    mean_inference_ms: 1.4067618478972843\n",
      "    mean_raw_obs_processing_ms: 0.6632939631811552\n",
      "  time_since_restore: 2538.4986743927\n",
      "  time_this_iter_s: 10.413381099700928\n",
      "  time_total_s: 2538.4986743927\n",
      "  timers:\n",
      "    learn_throughput: 1683.994\n",
      "    learn_time_ms: 593.826\n",
      "    load_throughput: 309732.456\n",
      "    load_time_ms: 3.229\n",
      "    sample_throughput: 96.836\n",
      "    sample_time_ms: 10326.778\n",
      "    update_time_ms: 1.746\n",
      "  timestamp: 1632133820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">          2538.5</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-30-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 225\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9779040588272943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01583567294886437\n",
      "          policy_loss: -0.04102839256326358\n",
      "          total_loss: -0.05645749494433403\n",
      "          vf_explained_var: 0.10102741420269012\n",
      "          vf_loss: 0.0009678477753671662\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.21333333333333\n",
      "    ram_util_percent: 68.79999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051160703713343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.605707984623644\n",
      "    mean_inference_ms: 1.4067567489173394\n",
      "    mean_raw_obs_processing_ms: 0.6631220991081836\n",
      "  time_since_restore: 2549.1962213516235\n",
      "  time_this_iter_s: 10.69754695892334\n",
      "  time_total_s: 2549.1962213516235\n",
      "  timers:\n",
      "    learn_throughput: 1682.224\n",
      "    learn_time_ms: 594.451\n",
      "    load_throughput: 309264.279\n",
      "    load_time_ms: 3.233\n",
      "    sample_throughput: 97.27\n",
      "    sample_time_ms: 10280.668\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632133831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">          2549.2</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-30-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 226\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6760393963919746\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015026235151964645\n",
      "          policy_loss: -0.05268843670686086\n",
      "          total_loss: -0.06502195878161324\n",
      "          vf_explained_var: 0.15493744611740112\n",
      "          vf_loss: 0.0012176591521387713\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.01875\n",
      "    ram_util_percent: 68.8375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040511091564581686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.604488483326683\n",
      "    mean_inference_ms: 1.4067522385608435\n",
      "    mean_raw_obs_processing_ms: 0.6629719004516598\n",
      "  time_since_restore: 2560.0753235816956\n",
      "  time_this_iter_s: 10.879102230072021\n",
      "  time_total_s: 2560.0753235816956\n",
      "  timers:\n",
      "    learn_throughput: 1683.53\n",
      "    learn_time_ms: 593.99\n",
      "    load_throughput: 308772.508\n",
      "    load_time_ms: 3.239\n",
      "    sample_throughput: 97.233\n",
      "    sample_time_ms: 10284.575\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632133842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         2560.08</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-30-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 227\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1000457723935444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015662187876605276\n",
      "          policy_loss: 0.05962922614481714\n",
      "          total_loss: 0.042986496910452844\n",
      "          vf_explained_var: -0.24586337804794312\n",
      "          vf_loss: 0.0010126863582020937\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.01875\n",
      "    ram_util_percent: 68.88125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040510577268323036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.60331831986596\n",
      "    mean_inference_ms: 1.4067486393559556\n",
      "    mean_raw_obs_processing_ms: 0.6628393362708807\n",
      "  time_since_restore: 2571.461769580841\n",
      "  time_this_iter_s: 11.386445999145508\n",
      "  time_total_s: 2571.461769580841\n",
      "  timers:\n",
      "    learn_throughput: 1680.91\n",
      "    learn_time_ms: 594.916\n",
      "    load_throughput: 307572.451\n",
      "    load_time_ms: 3.251\n",
      "    sample_throughput: 96.989\n",
      "    sample_time_ms: 10310.435\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632133853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         2571.46</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-31-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 228\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.989478161599901\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011941587308068335\n",
      "          policy_loss: -0.05515762919353114\n",
      "          total_loss: -0.07187500186264514\n",
      "          vf_explained_var: -0.35963940620422363\n",
      "          vf_loss: 0.0006269978004436578\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.33125\n",
      "    ram_util_percent: 68.89375000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040510067046082426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.602211676288398\n",
      "    mean_inference_ms: 1.4067456143584791\n",
      "    mean_raw_obs_processing_ms: 0.6627263880044469\n",
      "  time_since_restore: 2582.4189467430115\n",
      "  time_this_iter_s: 10.95717716217041\n",
      "  time_total_s: 2582.4189467430115\n",
      "  timers:\n",
      "    learn_throughput: 1678.327\n",
      "    learn_time_ms: 595.832\n",
      "    load_throughput: 308563.525\n",
      "    load_time_ms: 3.241\n",
      "    sample_throughput: 97.048\n",
      "    sample_time_ms: 10304.216\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632133864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         2582.42</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-31-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 229\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0394187516636317\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01726954104064485\n",
      "          policy_loss: 0.013302224708928003\n",
      "          total_loss: -0.002706345087952084\n",
      "          vf_explained_var: 0.2860275208950043\n",
      "          vf_loss: 0.0006972909258264634\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.975\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050955613544154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.601185621761353\n",
      "    mean_inference_ms: 1.406743514345676\n",
      "    mean_raw_obs_processing_ms: 0.662633446620823\n",
      "  time_since_restore: 2593.4732224941254\n",
      "  time_this_iter_s: 11.054275751113892\n",
      "  time_total_s: 2593.4732224941254\n",
      "  timers:\n",
      "    learn_throughput: 1678.716\n",
      "    learn_time_ms: 595.693\n",
      "    load_throughput: 308164.519\n",
      "    load_time_ms: 3.245\n",
      "    sample_throughput: 96.882\n",
      "    sample_time_ms: 10321.786\n",
      "    update_time_ms: 1.779\n",
      "  timestamp: 1632133875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         2593.47</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-31-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 230\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.203001884619395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01700193686337711\n",
      "          policy_loss: 0.09161169818705983\n",
      "          total_loss: 0.07383475229144096\n",
      "          vf_explained_var: -0.18102052807807922\n",
      "          vf_loss: 0.0006218988259206526\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85625\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040509054900846685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.600224044876361\n",
      "    mean_inference_ms: 1.4067430400975054\n",
      "    mean_raw_obs_processing_ms: 0.662559964625694\n",
      "  time_since_restore: 2604.7761931419373\n",
      "  time_this_iter_s: 11.30297064781189\n",
      "  time_total_s: 2604.7761931419373\n",
      "  timers:\n",
      "    learn_throughput: 1679.911\n",
      "    learn_time_ms: 595.27\n",
      "    load_throughput: 309748.468\n",
      "    load_time_ms: 3.228\n",
      "    sample_throughput: 96.748\n",
      "    sample_time_ms: 10336.082\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632133887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         2604.78</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 231\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.094878616597917\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012685826001738182\n",
      "          policy_loss: -0.03332464227245913\n",
      "          total_loss: -0.0507059791435798\n",
      "          vf_explained_var: -0.021936018019914627\n",
      "          vf_loss: 0.0008580843544526336\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.925\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050857152512154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.599319148639292\n",
      "    mean_inference_ms: 1.4067438969334234\n",
      "    mean_raw_obs_processing_ms: 0.6625054873816394\n",
      "  time_since_restore: 2615.866302251816\n",
      "  time_this_iter_s: 11.09010910987854\n",
      "  time_total_s: 2615.866302251816\n",
      "  timers:\n",
      "    learn_throughput: 1677.949\n",
      "    learn_time_ms: 595.966\n",
      "    load_throughput: 310473.004\n",
      "    load_time_ms: 3.221\n",
      "    sample_throughput: 96.257\n",
      "    sample_time_ms: 10388.843\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632133898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         2615.87</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-31-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 232\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.291078800625271\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01139309616227116\n",
      "          policy_loss: -0.16300978163878124\n",
      "          total_loss: -0.18216356510917345\n",
      "          vf_explained_var: -0.165721595287323\n",
      "          vf_loss: 0.0013237320203592794\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.8125\n",
      "    ram_util_percent: 68.8875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040508111225994764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.598483421372956\n",
      "    mean_inference_ms: 1.406746329457465\n",
      "    mean_raw_obs_processing_ms: 0.6624695662946144\n",
      "  time_since_restore: 2627.325621843338\n",
      "  time_this_iter_s: 11.459319591522217\n",
      "  time_total_s: 2627.325621843338\n",
      "  timers:\n",
      "    learn_throughput: 1676.879\n",
      "    learn_time_ms: 596.346\n",
      "    load_throughput: 310825.027\n",
      "    load_time_ms: 3.217\n",
      "    sample_throughput: 95.788\n",
      "    sample_time_ms: 10439.754\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1632133909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         2627.33</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-32-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 233\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.084522189034356\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023799500583323367\n",
      "          policy_loss: 0.007264517371853193\n",
      "          total_loss: -0.00762416852845086\n",
      "          vf_explained_var: 0.24631324410438538\n",
      "          vf_loss: 0.0008735740935662762\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.75\n",
      "    ram_util_percent: 68.8875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040507671097892134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.597707530373903\n",
      "    mean_inference_ms: 1.4067498337899216\n",
      "    mean_raw_obs_processing_ms: 0.6624518501353615\n",
      "  time_since_restore: 2638.4075224399567\n",
      "  time_this_iter_s: 11.081900596618652\n",
      "  time_total_s: 2638.4075224399567\n",
      "  timers:\n",
      "    learn_throughput: 1675.991\n",
      "    learn_time_ms: 596.662\n",
      "    load_throughput: 311746.815\n",
      "    load_time_ms: 3.208\n",
      "    sample_throughput: 95.902\n",
      "    sample_time_ms: 10427.259\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1632133920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         2638.41</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 234\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.239832940366533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013564574021321442\n",
      "          policy_loss: -0.11444414291116926\n",
      "          total_loss: -0.1317592671347989\n",
      "          vf_explained_var: -0.23201559484004974\n",
      "          vf_loss: 0.0007376381440230438\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.879999999999995\n",
      "    ram_util_percent: 68.88666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050726246174409\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.59698589719978\n",
      "    mean_inference_ms: 1.4067542783332359\n",
      "    mean_raw_obs_processing_ms: 0.6624521083811369\n",
      "  time_since_restore: 2649.3010635375977\n",
      "  time_this_iter_s: 10.893541097640991\n",
      "  time_total_s: 2649.3010635375977\n",
      "  timers:\n",
      "    learn_throughput: 1677.752\n",
      "    learn_time_ms: 596.035\n",
      "    load_throughput: 312527.309\n",
      "    load_time_ms: 3.2\n",
      "    sample_throughput: 95.457\n",
      "    sample_time_ms: 10475.921\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1632133931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">          2649.3</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-32-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 235\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8694673299789428\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011846861162226219\n",
      "          policy_loss: -0.035289325813452406\n",
      "          total_loss: -0.0495584901008341\n",
      "          vf_explained_var: -0.7645248174667358\n",
      "          vf_loss: 0.000630232955801249\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.976470588235294\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040506883421813515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.59632018416966\n",
      "    mean_inference_ms: 1.4067603770821875\n",
      "    mean_raw_obs_processing_ms: 0.6624700605213631\n",
      "  time_since_restore: 2660.523068666458\n",
      "  time_this_iter_s: 11.222005128860474\n",
      "  time_total_s: 2660.523068666458\n",
      "  timers:\n",
      "    learn_throughput: 1678.619\n",
      "    learn_time_ms: 595.728\n",
      "    load_throughput: 312753.357\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 94.985\n",
      "    sample_time_ms: 10527.954\n",
      "    update_time_ms: 2.369\n",
      "  timestamp: 1632133943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         2660.52</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-32-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 236\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8304040683640375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01111210360881683\n",
      "          policy_loss: -0.051946645064486395\n",
      "          total_loss: -0.06603096458646986\n",
      "          vf_explained_var: -0.60161292552948\n",
      "          vf_loss: 0.0006598312545166764\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.0\n",
      "    ram_util_percent: 68.88125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050649753541875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.595696668428984\n",
      "    mean_inference_ms: 1.4067667578636458\n",
      "    mean_raw_obs_processing_ms: 0.6625056489394633\n",
      "  time_since_restore: 2671.8502099514008\n",
      "  time_this_iter_s: 11.327141284942627\n",
      "  time_total_s: 2671.8502099514008\n",
      "  timers:\n",
      "    learn_throughput: 1676.371\n",
      "    learn_time_ms: 596.527\n",
      "    load_throughput: 311311.809\n",
      "    load_time_ms: 3.212\n",
      "    sample_throughput: 94.59\n",
      "    sample_time_ms: 10571.95\n",
      "    update_time_ms: 2.365\n",
      "  timestamp: 1632133954\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         2671.85</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-32-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 237\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1534587224324544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009583470513520827\n",
      "          policy_loss: -0.06121024481124348\n",
      "          total_loss: -0.07852937893735037\n",
      "          vf_explained_var: -0.9613648056983948\n",
      "          vf_loss: 0.00114527983047689\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.3625\n",
      "    ram_util_percent: 68.8875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050610428211439\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.595162426525413\n",
      "    mean_inference_ms: 1.406774328217408\n",
      "    mean_raw_obs_processing_ms: 0.6625581005553949\n",
      "  time_since_restore: 2683.302729845047\n",
      "  time_this_iter_s: 11.45251989364624\n",
      "  time_total_s: 2683.302729845047\n",
      "  timers:\n",
      "    learn_throughput: 1676.352\n",
      "    learn_time_ms: 596.533\n",
      "    load_throughput: 313302.359\n",
      "    load_time_ms: 3.192\n",
      "    sample_throughput: 94.531\n",
      "    sample_time_ms: 10578.576\n",
      "    update_time_ms: 2.362\n",
      "  timestamp: 1632133965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">          2683.3</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-32-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 238\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2678276459376017\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020032891013647636\n",
      "          policy_loss: 0.02902478819919957\n",
      "          total_loss: 0.013457170749704043\n",
      "          vf_explained_var: -0.3135180175304413\n",
      "          vf_loss: 0.0006928953422983695\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.41875\n",
      "    ram_util_percent: 68.80624999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050572138748772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.594665752401205\n",
      "    mean_inference_ms: 1.4067815342852013\n",
      "    mean_raw_obs_processing_ms: 0.6626267546942662\n",
      "  time_since_restore: 2694.2421910762787\n",
      "  time_this_iter_s: 10.93946123123169\n",
      "  time_total_s: 2694.2421910762787\n",
      "  timers:\n",
      "    learn_throughput: 1675.308\n",
      "    learn_time_ms: 596.905\n",
      "    load_throughput: 311563.872\n",
      "    load_time_ms: 3.21\n",
      "    sample_throughput: 94.55\n",
      "    sample_time_ms: 10576.443\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1632133976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         2694.24</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-33-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 240\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0678291903601753\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007335117720759854\n",
      "          policy_loss: -0.08550058282497856\n",
      "          total_loss: -0.10182305263976256\n",
      "          vf_explained_var: -0.1489935964345932\n",
      "          vf_loss: 0.0008309878622336935\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.739999999999995\n",
      "    ram_util_percent: 68.375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040504997052698506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.593743262705367\n",
      "    mean_inference_ms: 1.4067980726751828\n",
      "    mean_raw_obs_processing_ms: 0.6642968700930435\n",
      "  time_since_restore: 2722.752882003784\n",
      "  time_this_iter_s: 28.510690927505493\n",
      "  time_total_s: 2722.752882003784\n",
      "  timers:\n",
      "    learn_throughput: 1676.528\n",
      "    learn_time_ms: 596.471\n",
      "    load_throughput: 207403.686\n",
      "    load_time_ms: 4.822\n",
      "    sample_throughput: 81.163\n",
      "    sample_time_ms: 12320.914\n",
      "    update_time_ms: 2.358\n",
      "  timestamp: 1632134005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         2722.75</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-33-37\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 241\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0052854710155064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013926205437344623\n",
      "          policy_loss: -0.05369417816400528\n",
      "          total_loss: -0.06601131235559782\n",
      "          vf_explained_var: -0.5588250756263733\n",
      "          vf_loss: 0.0010435957125284605\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.227777777777774\n",
      "    ram_util_percent: 68.09444444444443\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040504645500888954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.593407514567978\n",
      "    mean_inference_ms: 1.406806781685247\n",
      "    mean_raw_obs_processing_ms: 0.6651617546707683\n",
      "  time_since_restore: 2734.7811300754547\n",
      "  time_this_iter_s: 12.028248071670532\n",
      "  time_total_s: 2734.7811300754547\n",
      "  timers:\n",
      "    learn_throughput: 1676.155\n",
      "    learn_time_ms: 596.603\n",
      "    load_throughput: 207224.363\n",
      "    load_time_ms: 4.826\n",
      "    sample_throughput: 80.688\n",
      "    sample_time_ms: 12393.356\n",
      "    update_time_ms: 2.356\n",
      "  timestamp: 1632134017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         2734.78</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 242\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.152091083261702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015215138742067246\n",
      "          policy_loss: -0.12167526288992829\n",
      "          total_loss: -0.1336351732412974\n",
      "          vf_explained_var: -0.0010658648097887635\n",
      "          vf_loss: 0.002249489986570552\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.513333333333335\n",
      "    ram_util_percent: 68.49333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050431782914942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.593101623769464\n",
      "    mean_inference_ms: 1.406816086902697\n",
      "    mean_raw_obs_processing_ms: 0.6660382077671789\n",
      "  time_since_restore: 2745.520179748535\n",
      "  time_this_iter_s: 10.739049673080444\n",
      "  time_total_s: 2745.520179748535\n",
      "  timers:\n",
      "    learn_throughput: 1675.144\n",
      "    learn_time_ms: 596.964\n",
      "    load_throughput: 206391.269\n",
      "    load_time_ms: 4.845\n",
      "    sample_throughput: 80.92\n",
      "    sample_time_ms: 12357.855\n",
      "    update_time_ms: 2.358\n",
      "  timestamp: 1632134028\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         2745.52</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-33-59\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 243\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9612744278377956\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009090884667483708\n",
      "          policy_loss: 0.04763343404564593\n",
      "          total_loss: 0.03325225448028909\n",
      "          vf_explained_var: -0.427175909280777\n",
      "          vf_loss: 0.0008630126496427693\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.46666666666667\n",
      "    ram_util_percent: 68.49333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040504015267569325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.592819916280346\n",
      "    mean_inference_ms: 1.4068268469427534\n",
      "    mean_raw_obs_processing_ms: 0.666927088255405\n",
      "  time_since_restore: 2756.259008169174\n",
      "  time_this_iter_s: 10.738828420639038\n",
      "  time_total_s: 2756.259008169174\n",
      "  timers:\n",
      "    learn_throughput: 1675.189\n",
      "    learn_time_ms: 596.947\n",
      "    load_throughput: 206372.989\n",
      "    load_time_ms: 4.846\n",
      "    sample_throughput: 81.395\n",
      "    sample_time_ms: 12285.785\n",
      "    update_time_ms: 2.358\n",
      "  timestamp: 1632134039\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         2756.26</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-34-09\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 244\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9107349236806235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008465707145276684\n",
      "          policy_loss: -0.0971608932233519\n",
      "          total_loss: -0.11181672389308611\n",
      "          vf_explained_var: -0.8093177676200867\n",
      "          vf_loss: 0.00038339027103372953\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.25625\n",
      "    ram_util_percent: 68.49375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040503709645895294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.592570809590413\n",
      "    mean_inference_ms: 1.4068380660695357\n",
      "    mean_raw_obs_processing_ms: 0.6678283021603546\n",
      "  time_since_restore: 2767.0226304531097\n",
      "  time_this_iter_s: 10.763622283935547\n",
      "  time_total_s: 2767.0226304531097\n",
      "  timers:\n",
      "    learn_throughput: 1677.696\n",
      "    learn_time_ms: 596.055\n",
      "    load_throughput: 205896.843\n",
      "    load_time_ms: 4.857\n",
      "    sample_throughput: 81.6\n",
      "    sample_time_ms: 12254.844\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1632134049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         2767.02</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-34-20\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 245\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9299603303273518\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009168946482434394\n",
      "          policy_loss: 0.036803304072883396\n",
      "          total_loss: 0.022530085469285647\n",
      "          vf_explained_var: -0.45537054538726807\n",
      "          vf_loss: 0.0006203212331860817\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.193749999999994\n",
      "    ram_util_percent: 68.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040503425673995286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.592358815775025\n",
      "    mean_inference_ms: 1.406850116871014\n",
      "    mean_raw_obs_processing_ms: 0.6687422227009407\n",
      "  time_since_restore: 2778.0754964351654\n",
      "  time_this_iter_s: 11.052865982055664\n",
      "  time_total_s: 2778.0754964351654\n",
      "  timers:\n",
      "    learn_throughput: 1675.269\n",
      "    learn_time_ms: 596.919\n",
      "    load_throughput: 205373.602\n",
      "    load_time_ms: 4.869\n",
      "    sample_throughput: 81.501\n",
      "    sample_time_ms: 12269.859\n",
      "    update_time_ms: 2.365\n",
      "  timestamp: 1632134060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         2778.08</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-34-31\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 246\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.747136127948761\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00817275686181631\n",
      "          policy_loss: -0.022168746590614317\n",
      "          total_loss: -0.0351184391313129\n",
      "          vf_explained_var: -0.6329759955406189\n",
      "          vf_loss: 0.0005943162816846679\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.279999999999994\n",
      "    ram_util_percent: 68.51333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050314591905534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.592189750974226\n",
      "    mean_inference_ms: 1.4068625852423962\n",
      "    mean_raw_obs_processing_ms: 0.6696670837158213\n",
      "  time_since_restore: 2789.152900695801\n",
      "  time_this_iter_s: 11.077404260635376\n",
      "  time_total_s: 2789.152900695801\n",
      "  timers:\n",
      "    learn_throughput: 1677.562\n",
      "    learn_time_ms: 596.103\n",
      "    load_throughput: 204917.067\n",
      "    load_time_ms: 4.88\n",
      "    sample_throughput: 81.587\n",
      "    sample_time_ms: 12256.929\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632134071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         2789.15</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 246000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-34-42\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 247\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.322450375556946\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013041474896298134\n",
      "          policy_loss: -0.10540606809986962\n",
      "          total_loss: -0.120964798082908\n",
      "          vf_explained_var: -0.2980819642543793\n",
      "          vf_loss: 0.0013987996518456689\n",
      "    num_agent_steps_sampled: 246000\n",
      "    num_agent_steps_trained: 246000\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.943749999999994\n",
      "    ram_util_percent: 68.5375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405028896042856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.592061979735135\n",
      "    mean_inference_ms: 1.4068758811143292\n",
      "    mean_raw_obs_processing_ms: 0.6706009473398872\n",
      "  time_since_restore: 2800.1011781692505\n",
      "  time_this_iter_s: 10.948277473449707\n",
      "  time_total_s: 2800.1011781692505\n",
      "  timers:\n",
      "    learn_throughput: 1678.613\n",
      "    learn_time_ms: 595.73\n",
      "    load_throughput: 205426.913\n",
      "    load_time_ms: 4.868\n",
      "    sample_throughput: 81.837\n",
      "    sample_time_ms: 12219.431\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632134082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 246\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">          2800.1</td><td style=\"text-align: right;\">246000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 247000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-34-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 248\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.715134506755405\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009405714370566809\n",
      "          policy_loss: -0.03473772257566452\n",
      "          total_loss: -0.047009866643283105\n",
      "          vf_explained_var: -0.48903125524520874\n",
      "          vf_loss: 0.0003593579696219725\n",
      "    num_agent_steps_sampled: 247000\n",
      "    num_agent_steps_trained: 247000\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.20625\n",
      "    ram_util_percent: 68.57499999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050264040638668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.591962411290162\n",
      "    mean_inference_ms: 1.4068898870548867\n",
      "    mean_raw_obs_processing_ms: 0.6715464935034334\n",
      "  time_since_restore: 2811.202941417694\n",
      "  time_this_iter_s: 11.101763248443604\n",
      "  time_total_s: 2811.202941417694\n",
      "  timers:\n",
      "    learn_throughput: 1679.832\n",
      "    learn_time_ms: 595.298\n",
      "    load_throughput: 205594.067\n",
      "    load_time_ms: 4.864\n",
      "    sample_throughput: 82.069\n",
      "    sample_time_ms: 12184.806\n",
      "    update_time_ms: 1.748\n",
      "  timestamp: 1632134094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">          2811.2</td><td style=\"text-align: right;\">247000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-35-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 249\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5676274471812779\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0064337199183215985\n",
      "          policy_loss: -0.0653656404879358\n",
      "          total_loss: -0.07760446336534288\n",
      "          vf_explained_var: -0.15447808802127838\n",
      "          vf_loss: 0.0003457798004092183\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.21333333333333\n",
      "    ram_util_percent: 68.68000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050241093464377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.59187908780835\n",
      "    mean_inference_ms: 1.406904097185351\n",
      "    mean_raw_obs_processing_ms: 0.6725001989419112\n",
      "  time_since_restore: 2821.773984670639\n",
      "  time_this_iter_s: 10.571043252944946\n",
      "  time_total_s: 2821.773984670639\n",
      "  timers:\n",
      "    learn_throughput: 1682.399\n",
      "    learn_time_ms: 594.389\n",
      "    load_throughput: 206041.48\n",
      "    load_time_ms: 4.853\n",
      "    sample_throughput: 82.312\n",
      "    sample_time_ms: 12148.861\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632134104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 248\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         2821.77</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 249000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-35-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 250\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3024398379855686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01310685202527032\n",
      "          policy_loss: 0.06274925726983283\n",
      "          total_loss: 0.046249191380209394\n",
      "          vf_explained_var: 0.26475757360458374\n",
      "          vf_loss: 0.00022594057131249834\n",
      "    num_agent_steps_sampled: 249000\n",
      "    num_agent_steps_trained: 249000\n",
      "    num_steps_sampled: 249000\n",
      "    num_steps_trained: 249000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.53076923076923\n",
      "    ram_util_percent: 68.76153846153845\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050219418283857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.591765723830505\n",
      "    mean_inference_ms: 1.4069176944405097\n",
      "    mean_raw_obs_processing_ms: 0.6723000986686514\n",
      "  time_since_restore: 2830.8568427562714\n",
      "  time_this_iter_s: 9.082858085632324\n",
      "  time_total_s: 2830.8568427562714\n",
      "  timers:\n",
      "    learn_throughput: 1684.354\n",
      "    learn_time_ms: 593.699\n",
      "    load_throughput: 307239.005\n",
      "    load_time_ms: 3.255\n",
      "    sample_throughput: 97.959\n",
      "    sample_time_ms: 10208.382\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1632134113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249000\n",
      "  training_iteration: 249\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         2830.86</td><td style=\"text-align: right;\">249000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 250000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-35-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 251\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2677425781885785\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005164207689687099\n",
      "          policy_loss: -0.07378325673441093\n",
      "          total_loss: -0.09359562293522888\n",
      "          vf_explained_var: 0.4930548369884491\n",
      "          vf_loss: 0.0003834397650886482\n",
      "    num_agent_steps_sampled: 250000\n",
      "    num_agent_steps_trained: 250000\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.76428571428571\n",
      "    ram_util_percent: 68.79999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050198681949921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.591487717054255\n",
      "    mean_inference_ms: 1.4069310819242955\n",
      "    mean_raw_obs_processing_ms: 0.6721191856628843\n",
      "  time_since_restore: 2840.534059047699\n",
      "  time_this_iter_s: 9.677216291427612\n",
      "  time_total_s: 2840.534059047699\n",
      "  timers:\n",
      "    learn_throughput: 1682.34\n",
      "    learn_time_ms: 594.41\n",
      "    load_throughput: 306193.806\n",
      "    load_time_ms: 3.266\n",
      "    sample_throughput: 100.275\n",
      "    sample_time_ms: 9972.56\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1632134123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         2840.53</td><td style=\"text-align: right;\">250000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 251000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-35-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 252\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.941793613963657\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01145341688472096\n",
      "          policy_loss: 0.0940865262515015\n",
      "          total_loss: 0.0808345483822955\n",
      "          vf_explained_var: -0.3506098687648773\n",
      "          vf_loss: 0.0006621125063651966\n",
      "    num_agent_steps_sampled: 251000\n",
      "    num_agent_steps_trained: 251000\n",
      "    num_steps_sampled: 251000\n",
      "    num_steps_trained: 251000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26875\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040501791407192955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.591206965765075\n",
      "    mean_inference_ms: 1.4069444843126937\n",
      "    mean_raw_obs_processing_ms: 0.671956450662621\n",
      "  time_since_restore: 2851.5941309928894\n",
      "  time_this_iter_s: 11.06007194519043\n",
      "  time_total_s: 2851.5941309928894\n",
      "  timers:\n",
      "    learn_throughput: 1683.514\n",
      "    learn_time_ms: 593.996\n",
      "    load_throughput: 308123.77\n",
      "    load_time_ms: 3.245\n",
      "    sample_throughput: 99.95\n",
      "    sample_time_ms: 10005.035\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632134134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 251000\n",
      "  training_iteration: 251\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         2851.59</td><td style=\"text-align: right;\">251000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-35-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 253\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8156281166606478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009860944408083267\n",
      "          policy_loss: 0.00043195860667361155\n",
      "          total_loss: -0.01237530294391844\n",
      "          vf_explained_var: -0.4735310971736908\n",
      "          vf_loss: 0.0006104206791658523\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.2\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050160825712873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.590958132947788\n",
      "    mean_inference_ms: 1.406958363612246\n",
      "    mean_raw_obs_processing_ms: 0.6718113893236619\n",
      "  time_since_restore: 2862.76483297348\n",
      "  time_this_iter_s: 11.17070198059082\n",
      "  time_total_s: 2862.76483297348\n",
      "  timers:\n",
      "    learn_throughput: 1682.336\n",
      "    learn_time_ms: 594.411\n",
      "    load_throughput: 308071.717\n",
      "    load_time_ms: 3.246\n",
      "    sample_throughput: 99.524\n",
      "    sample_time_ms: 10047.826\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632134145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 252\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         2862.76</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 253000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-35-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 254\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5900250845485264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010443023142859697\n",
      "          policy_loss: -0.035134351998567584\n",
      "          total_loss: -0.04501266703009606\n",
      "          vf_explained_var: -0.7638990879058838\n",
      "          vf_loss: 0.0010036264614932911\n",
      "    num_agent_steps_sampled: 253000\n",
      "    num_agent_steps_trained: 253000\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.71333333333333\n",
      "    ram_util_percent: 68.80666666666664\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050143271688693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.590734433313559\n",
      "    mean_inference_ms: 1.4069726383875973\n",
      "    mean_raw_obs_processing_ms: 0.6716809461215079\n",
      "  time_since_restore: 2873.747638940811\n",
      "  time_this_iter_s: 10.982805967330933\n",
      "  time_total_s: 2873.747638940811\n",
      "  timers:\n",
      "    learn_throughput: 1680.942\n",
      "    learn_time_ms: 594.904\n",
      "    load_throughput: 307297.531\n",
      "    load_time_ms: 3.254\n",
      "    sample_throughput: 99.313\n",
      "    sample_time_ms: 10069.225\n",
      "    update_time_ms: 1.757\n",
      "  timestamp: 1632134156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         2873.75</td><td style=\"text-align: right;\">253000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 254000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-36-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 255\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1860221174028185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008813471361005654\n",
      "          policy_loss: -0.089472117771705\n",
      "          total_loss: -0.10603078802426656\n",
      "          vf_explained_var: 0.3849965035915375\n",
      "          vf_loss: 0.0010663083347026258\n",
      "    num_agent_steps_sampled: 254000\n",
      "    num_agent_steps_trained: 254000\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.20714285714286\n",
      "    ram_util_percent: 68.83571428571426\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050126957892531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.590468380205747\n",
      "    mean_inference_ms: 1.4069855613683087\n",
      "    mean_raw_obs_processing_ms: 0.6715653159751122\n",
      "  time_since_restore: 2883.187199115753\n",
      "  time_this_iter_s: 9.439560174942017\n",
      "  time_total_s: 2883.187199115753\n",
      "  timers:\n",
      "    learn_throughput: 1680.173\n",
      "    learn_time_ms: 595.177\n",
      "    load_throughput: 308740.688\n",
      "    load_time_ms: 3.239\n",
      "    sample_throughput: 100.932\n",
      "    sample_time_ms: 9907.656\n",
      "    update_time_ms: 1.757\n",
      "  timestamp: 1632134166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 254\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         2883.19</td><td style=\"text-align: right;\">254000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-36-16\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 256\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9672450105349222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013629460325730097\n",
      "          policy_loss: 0.06029729230536355\n",
      "          total_loss: 0.048420866909954285\n",
      "          vf_explained_var: -0.28201034665107727\n",
      "          vf_loss: 0.001246496593780143\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.63571428571428\n",
      "    ram_util_percent: 68.85714285714285\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050111762125083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.590215495774755\n",
      "    mean_inference_ms: 1.4069994799345968\n",
      "    mean_raw_obs_processing_ms: 0.6714648202256158\n",
      "  time_since_restore: 2893.415725708008\n",
      "  time_this_iter_s: 10.228526592254639\n",
      "  time_total_s: 2893.415725708008\n",
      "  timers:\n",
      "    learn_throughput: 1679.568\n",
      "    learn_time_ms: 595.391\n",
      "    load_throughput: 309620.421\n",
      "    load_time_ms: 3.23\n",
      "    sample_throughput: 101.807\n",
      "    sample_time_ms: 9822.538\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632134176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 255\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         2893.42</td><td style=\"text-align: right;\">255000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-36-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 257\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9591962615648906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014667109054145379\n",
      "          policy_loss: 0.06424814114967982\n",
      "          total_loss: 0.0526653539803293\n",
      "          vf_explained_var: -0.5437403917312622\n",
      "          vf_loss: 0.0009610163921024651\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.40625\n",
      "    ram_util_percent: 68.8625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050096576497669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58999470498022\n",
      "    mean_inference_ms: 1.4070138076744474\n",
      "    mean_raw_obs_processing_ms: 0.6713809130512246\n",
      "  time_since_restore: 2904.4542870521545\n",
      "  time_this_iter_s: 11.038561344146729\n",
      "  time_total_s: 2904.4542870521545\n",
      "  timers:\n",
      "    learn_throughput: 1681.737\n",
      "    learn_time_ms: 594.623\n",
      "    load_throughput: 310032.376\n",
      "    load_time_ms: 3.225\n",
      "    sample_throughput: 101.705\n",
      "    sample_time_ms: 9832.345\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632134187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         2904.45</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 257000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 258\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9724995493888855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01260709494645277\n",
      "          policy_loss: -0.0171350184828043\n",
      "          total_loss: -0.028841014144321282\n",
      "          vf_explained_var: 0.07406521588563919\n",
      "          vf_loss: 0.0019607587885628972\n",
      "    num_agent_steps_sampled: 257000\n",
      "    num_agent_steps_trained: 257000\n",
      "    num_steps_sampled: 257000\n",
      "    num_steps_trained: 257000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.512499999999996\n",
      "    ram_util_percent: 68.7625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040500805260598204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.589774654185888\n",
      "    mean_inference_ms: 1.4070275419795208\n",
      "    mean_raw_obs_processing_ms: 0.6713102040198793\n",
      "  time_since_restore: 2915.222236394882\n",
      "  time_this_iter_s: 10.767949342727661\n",
      "  time_total_s: 2915.222236394882\n",
      "  timers:\n",
      "    learn_throughput: 1681.651\n",
      "    learn_time_ms: 594.654\n",
      "    load_throughput: 309261.998\n",
      "    load_time_ms: 3.234\n",
      "    sample_throughput: 102.052\n",
      "    sample_time_ms: 9798.915\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1632134198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257000\n",
      "  training_iteration: 257\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         2915.22</td><td style=\"text-align: right;\">257000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 258000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-36-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 259\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6878492487801446\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009240116120767174\n",
      "          policy_loss: -0.01597745360599624\n",
      "          total_loss: -0.027486726144949594\n",
      "          vf_explained_var: -0.3351427912712097\n",
      "          vf_loss: 0.000928956524714724\n",
      "    num_agent_steps_sampled: 258000\n",
      "    num_agent_steps_trained: 258000\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.64666666666667\n",
      "    ram_util_percent: 68.10000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050064136666721\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58956127109281\n",
      "    mean_inference_ms: 1.4070413008649054\n",
      "    mean_raw_obs_processing_ms: 0.6712523079148923\n",
      "  time_since_restore: 2926.178547143936\n",
      "  time_this_iter_s: 10.956310749053955\n",
      "  time_total_s: 2926.178547143936\n",
      "  timers:\n",
      "    learn_throughput: 1679.845\n",
      "    learn_time_ms: 595.293\n",
      "    load_throughput: 309663.854\n",
      "    load_time_ms: 3.229\n",
      "    sample_throughput: 101.659\n",
      "    sample_time_ms: 9836.821\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632134209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 258\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         2926.18</td><td style=\"text-align: right;\">258000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 259000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 260\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7294599374135335\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009070506400679178\n",
      "          policy_loss: -0.033440888424714404\n",
      "          total_loss: -0.045983101261986625\n",
      "          vf_explained_var: -0.4395371675491333\n",
      "          vf_loss: 0.00039362737491804487\n",
      "    num_agent_steps_sampled: 259000\n",
      "    num_agent_steps_trained: 259000\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.193333333333335\n",
      "    ram_util_percent: 68.12000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050046705378689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.589340909231613\n",
      "    mean_inference_ms: 1.4070543345083288\n",
      "    mean_raw_obs_processing_ms: 0.6712080184447679\n",
      "  time_since_restore: 2936.6399879455566\n",
      "  time_this_iter_s: 10.461440801620483\n",
      "  time_total_s: 2936.6399879455566\n",
      "  timers:\n",
      "    learn_throughput: 1678.433\n",
      "    learn_time_ms: 595.794\n",
      "    load_throughput: 311314.119\n",
      "    load_time_ms: 3.212\n",
      "    sample_throughput: 100.259\n",
      "    sample_time_ms: 9974.196\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1632134219\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         2936.64</td><td style=\"text-align: right;\">259000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-37-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 261\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2771596352259318\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005030382421367976\n",
      "          policy_loss: -0.029030212718579505\n",
      "          total_loss: -0.039194607569111715\n",
      "          vf_explained_var: -0.9830976724624634\n",
      "          vf_loss: 0.00018989296820816687\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.6875\n",
      "    ram_util_percent: 68.1375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040500308202878886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.589133023768241\n",
      "    mean_inference_ms: 1.4070676597277063\n",
      "    mean_raw_obs_processing_ms: 0.6711768329395684\n",
      "  time_since_restore: 2947.3675906658173\n",
      "  time_this_iter_s: 10.72760272026062\n",
      "  time_total_s: 2947.3675906658173\n",
      "  timers:\n",
      "    learn_throughput: 1678.72\n",
      "    learn_time_ms: 595.692\n",
      "    load_throughput: 312825.669\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 99.213\n",
      "    sample_time_ms: 10079.343\n",
      "    update_time_ms: 1.786\n",
      "  timestamp: 1632134230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 260\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         2947.37</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 261000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-37-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 262\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2534664975272283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008269674067908402\n",
      "          policy_loss: -0.008809134032991198\n",
      "          total_loss: -0.027284898857275645\n",
      "          vf_explained_var: -0.6381700038909912\n",
      "          vf_loss: 8.497909596674921e-05\n",
      "    num_agent_steps_sampled: 261000\n",
      "    num_agent_steps_trained: 261000\n",
      "    num_steps_sampled: 261000\n",
      "    num_steps_trained: 261000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.60833333333334\n",
      "    ram_util_percent: 68.15\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050015272501675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588860560312126\n",
      "    mean_inference_ms: 1.4070795821137363\n",
      "    mean_raw_obs_processing_ms: 0.6711583196948339\n",
      "  time_since_restore: 2956.2941567897797\n",
      "  time_this_iter_s: 8.926566123962402\n",
      "  time_total_s: 2956.2941567897797\n",
      "  timers:\n",
      "    learn_throughput: 1678.539\n",
      "    learn_time_ms: 595.756\n",
      "    load_throughput: 311610.166\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 101.358\n",
      "    sample_time_ms: 9865.981\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1632134239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 261000\n",
      "  training_iteration: 261\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         2956.29</td><td style=\"text-align: right;\">261000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 262000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-37-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 263\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2404077582889133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00652545211563162\n",
      "          policy_loss: -0.03540974744699067\n",
      "          total_loss: -0.054613781202998424\n",
      "          vf_explained_var: -0.5384316444396973\n",
      "          vf_loss: 6.429059261184497e-05\n",
      "    num_agent_steps_sampled: 262000\n",
      "    num_agent_steps_trained: 262000\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.653846153846146\n",
      "    ram_util_percent: 68.15384615384616\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049998473460788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588520759364386\n",
      "    mean_inference_ms: 1.4070902156314744\n",
      "    mean_raw_obs_processing_ms: 0.6711521849139495\n",
      "  time_since_restore: 2965.1705389022827\n",
      "  time_this_iter_s: 8.876382112503052\n",
      "  time_total_s: 2965.1705389022827\n",
      "  timers:\n",
      "    learn_throughput: 1679.147\n",
      "    learn_time_ms: 595.541\n",
      "    load_throughput: 311522.219\n",
      "    load_time_ms: 3.21\n",
      "    sample_throughput: 103.769\n",
      "    sample_time_ms: 9636.784\n",
      "    update_time_ms: 1.774\n",
      "  timestamp: 1632134248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         2965.17</td><td style=\"text-align: right;\">262000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 263000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-37-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 264\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3062881787618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032422940085756104\n",
      "          policy_loss: 0.18809778425428603\n",
      "          total_loss: 0.16664911011854808\n",
      "          vf_explained_var: -0.24816852807998657\n",
      "          vf_loss: 5.614735766559736e-05\n",
      "    num_agent_steps_sampled: 263000\n",
      "    num_agent_steps_trained: 263000\n",
      "    num_steps_sampled: 263000\n",
      "    num_steps_trained: 263000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.76923076923076\n",
      "    ram_util_percent: 68.16923076923078\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040499788359635705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588131285032993\n",
      "    mean_inference_ms: 1.407099525501228\n",
      "    mean_raw_obs_processing_ms: 0.6711614225071412\n",
      "  time_since_restore: 2974.2926304340363\n",
      "  time_this_iter_s: 9.12209153175354\n",
      "  time_total_s: 2974.2926304340363\n",
      "  timers:\n",
      "    learn_throughput: 1678.946\n",
      "    learn_time_ms: 595.612\n",
      "    load_throughput: 313529.53\n",
      "    load_time_ms: 3.189\n",
      "    sample_throughput: 105.813\n",
      "    sample_time_ms: 9450.649\n",
      "    update_time_ms: 1.817\n",
      "  timestamp: 1632134257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263000\n",
      "  training_iteration: 263\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         2974.29</td><td style=\"text-align: right;\">263000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-37-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 265\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.910600451628367\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01868825552680138\n",
      "          policy_loss: 0.10871809331907166\n",
      "          total_loss: 0.09466825301448505\n",
      "          vf_explained_var: -0.9307423233985901\n",
      "          vf_loss: 0.0005659197621247989\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.89375\n",
      "    ram_util_percent: 68.13125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049959396033818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587756919473907\n",
      "    mean_inference_ms: 1.4071085118158158\n",
      "    mean_raw_obs_processing_ms: 0.6711843972159958\n",
      "  time_since_restore: 2985.558198928833\n",
      "  time_this_iter_s: 11.265568494796753\n",
      "  time_total_s: 2985.558198928833\n",
      "  timers:\n",
      "    learn_throughput: 1679.563\n",
      "    learn_time_ms: 595.393\n",
      "    load_throughput: 313028.786\n",
      "    load_time_ms: 3.195\n",
      "    sample_throughput: 103.805\n",
      "    sample_time_ms: 9633.484\n",
      "    update_time_ms: 1.812\n",
      "  timestamp: 1632134268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 264\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         2985.56</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 265000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-38-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 266\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0312184744411046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016162834064480137\n",
      "          policy_loss: 0.11201443436245123\n",
      "          total_loss: 0.09757983821133773\n",
      "          vf_explained_var: -0.6450928449630737\n",
      "          vf_loss: 0.0019941291564868555\n",
      "    num_agent_steps_sampled: 265000\n",
      "    num_agent_steps_trained: 265000\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.27647058823529\n",
      "    ram_util_percent: 68.14117647058823\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0404993963192143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58741079699223\n",
      "    mean_inference_ms: 1.4071177567144968\n",
      "    mean_raw_obs_processing_ms: 0.6712186092102632\n",
      "  time_since_restore: 2997.0679359436035\n",
      "  time_this_iter_s: 11.509737014770508\n",
      "  time_total_s: 2997.0679359436035\n",
      "  timers:\n",
      "    learn_throughput: 1679.839\n",
      "    learn_time_ms: 595.295\n",
      "    load_throughput: 312618.155\n",
      "    load_time_ms: 3.199\n",
      "    sample_throughput: 102.441\n",
      "    sample_time_ms: 9761.726\n",
      "    update_time_ms: 1.812\n",
      "  timestamp: 1632134280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 265\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         2997.07</td><td style=\"text-align: right;\">265000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 266000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-38-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 267\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9743323630756802\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012367712306450665\n",
      "          policy_loss: 0.05395416520122025\n",
      "          total_loss: 0.037796618018506305\n",
      "          vf_explained_var: -0.9600528478622437\n",
      "          vf_loss: 0.0006141716345963586\n",
      "    num_agent_steps_sampled: 266000\n",
      "    num_agent_steps_trained: 266000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.086666666666666\n",
      "    ram_util_percent: 68.16666666666669\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0404991899359614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587054435257476\n",
      "    mean_inference_ms: 1.407125983041518\n",
      "    mean_raw_obs_processing_ms: 0.6712669410801834\n",
      "  time_since_restore: 3007.7524387836456\n",
      "  time_this_iter_s: 10.684502840042114\n",
      "  time_total_s: 3007.7524387836456\n",
      "  timers:\n",
      "    learn_throughput: 1679.094\n",
      "    learn_time_ms: 595.559\n",
      "    load_throughput: 309869.752\n",
      "    load_time_ms: 3.227\n",
      "    sample_throughput: 102.817\n",
      "    sample_time_ms: 9725.98\n",
      "    update_time_ms: 1.817\n",
      "  timestamp: 1632134291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         3007.75</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 267000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-38-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 268\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8282794144418504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021574264622784525\n",
      "          policy_loss: 0.03264879571894805\n",
      "          total_loss: 0.02095914036035538\n",
      "          vf_explained_var: -0.6433346271514893\n",
      "          vf_loss: 0.0014094692873994872\n",
      "    num_agent_steps_sampled: 267000\n",
      "    num_agent_steps_trained: 267000\n",
      "    num_steps_sampled: 267000\n",
      "    num_steps_trained: 267000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.88125\n",
      "    ram_util_percent: 68.2375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049897850364409\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.586706353531364\n",
      "    mean_inference_ms: 1.4071334692823214\n",
      "    mean_raw_obs_processing_ms: 0.6713292248555109\n",
      "  time_since_restore: 3018.6053142547607\n",
      "  time_this_iter_s: 10.852875471115112\n",
      "  time_total_s: 3018.6053142547607\n",
      "  timers:\n",
      "    learn_throughput: 1679.179\n",
      "    learn_time_ms: 595.529\n",
      "    load_throughput: 310188.289\n",
      "    load_time_ms: 3.224\n",
      "    sample_throughput: 102.728\n",
      "    sample_time_ms: 9734.491\n",
      "    update_time_ms: 1.804\n",
      "  timestamp: 1632134301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 267000\n",
      "  training_iteration: 267\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         3018.61</td><td style=\"text-align: right;\">267000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-38-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 269\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7881939040289985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01053387563817198\n",
      "          policy_loss: -0.0906694204443031\n",
      "          total_loss: -0.10400934211081929\n",
      "          vf_explained_var: -0.646027147769928\n",
      "          vf_loss: 0.0007455378583270228\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.66666666666667\n",
      "    ram_util_percent: 68.28666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049876515432879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.586355638710494\n",
      "    mean_inference_ms: 1.40713992302064\n",
      "    mean_raw_obs_processing_ms: 0.671403127701889\n",
      "  time_since_restore: 3029.578169822693\n",
      "  time_this_iter_s: 10.972855567932129\n",
      "  time_total_s: 3029.578169822693\n",
      "  timers:\n",
      "    learn_throughput: 1679.36\n",
      "    learn_time_ms: 595.465\n",
      "    load_throughput: 310525.872\n",
      "    load_time_ms: 3.22\n",
      "    sample_throughput: 102.709\n",
      "    sample_time_ms: 9736.206\n",
      "    update_time_ms: 1.797\n",
      "  timestamp: 1632134312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 268\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         3029.58</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 269000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-39-01\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 270\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7593079937828913\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010666970169841179\n",
      "          policy_loss: -0.009146047931992345\n",
      "          total_loss: -0.02253439367438356\n",
      "          vf_explained_var: -0.707702100276947\n",
      "          vf_loss: 0.0003602871862919225\n",
      "    num_agent_steps_sampled: 269000\n",
      "    num_agent_steps_trained: 269000\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.105\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049854879871984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.586000508973331\n",
      "    mean_inference_ms: 1.4071452847948998\n",
      "    mean_raw_obs_processing_ms: 0.6721375966155548\n",
      "  time_since_restore: 3057.71427154541\n",
      "  time_this_iter_s: 28.136101722717285\n",
      "  time_total_s: 3057.71427154541\n",
      "  timers:\n",
      "    learn_throughput: 1681.201\n",
      "    learn_time_ms: 594.813\n",
      "    load_throughput: 218230.556\n",
      "    load_time_ms: 4.582\n",
      "    sample_throughput: 86.934\n",
      "    sample_time_ms: 11502.946\n",
      "    update_time_ms: 1.793\n",
      "  timestamp: 1632134341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         3057.71</td><td style=\"text-align: right;\">269000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-39-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 271\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6960990322960747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007958505293995069\n",
      "          policy_loss: -0.015805207813779514\n",
      "          total_loss: -0.029336113565497927\n",
      "          vf_explained_var: 0.12186311185359955\n",
      "          vf_loss: 0.0005617883512362217\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.88333333333333\n",
      "    ram_util_percent: 68.25\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049833717960477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.585709624801927\n",
      "    mean_inference_ms: 1.4071501439629526\n",
      "    mean_raw_obs_processing_ms: 0.672882007936269\n",
      "  time_since_restore: 3069.8725316524506\n",
      "  time_this_iter_s: 12.158260107040405\n",
      "  time_total_s: 3069.8725316524506\n",
      "  timers:\n",
      "    learn_throughput: 1680.499\n",
      "    learn_time_ms: 595.061\n",
      "    load_throughput: 217838.394\n",
      "    load_time_ms: 4.591\n",
      "    sample_throughput: 85.868\n",
      "    sample_time_ms: 11645.753\n",
      "    update_time_ms: 1.786\n",
      "  timestamp: 1632134353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 270\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         3069.87</td><td style=\"text-align: right;\">270000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 271000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-39-23\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 272\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.102936953968472\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015551085638962698\n",
      "          policy_loss: 0.0017184935406678254\n",
      "          total_loss: -0.013132775430050161\n",
      "          vf_explained_var: -0.5887979865074158\n",
      "          vf_loss: 0.0005733861683337536\n",
      "    num_agent_steps_sampled: 271000\n",
      "    num_agent_steps_trained: 271000\n",
      "    num_steps_sampled: 271000\n",
      "    num_steps_trained: 271000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.96\n",
      "    ram_util_percent: 68.27333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049812511424984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.585412294607963\n",
      "    mean_inference_ms: 1.4071544669613212\n",
      "    mean_raw_obs_processing_ms: 0.6736345990858612\n",
      "  time_since_restore: 3080.309844017029\n",
      "  time_this_iter_s: 10.437312364578247\n",
      "  time_total_s: 3080.309844017029\n",
      "  timers:\n",
      "    learn_throughput: 1682.987\n",
      "    learn_time_ms: 594.182\n",
      "    load_throughput: 218538.7\n",
      "    load_time_ms: 4.576\n",
      "    sample_throughput: 84.762\n",
      "    sample_time_ms: 11797.72\n",
      "    update_time_ms: 1.786\n",
      "  timestamp: 1632134363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271000\n",
      "  training_iteration: 271\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         3080.31</td><td style=\"text-align: right;\">271000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-39-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 273\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.687683375676473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016004676253116162\n",
      "          policy_loss: 0.0017955397255718709\n",
      "          total_loss: -0.008596361490587394\n",
      "          vf_explained_var: -0.9943503737449646\n",
      "          vf_loss: 0.0007167430862965476\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.43333333333333\n",
      "    ram_util_percent: 68.20666666666669\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0404979121885384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.585112659376165\n",
      "    mean_inference_ms: 1.4071582280308648\n",
      "    mean_raw_obs_processing_ms: 0.6743941736785404\n",
      "  time_since_restore: 3090.8479673862457\n",
      "  time_this_iter_s: 10.538123369216919\n",
      "  time_total_s: 3090.8479673862457\n",
      "  timers:\n",
      "    learn_throughput: 1681.559\n",
      "    learn_time_ms: 594.686\n",
      "    load_throughput: 218394.185\n",
      "    load_time_ms: 4.579\n",
      "    sample_throughput: 83.589\n",
      "    sample_time_ms: 11963.357\n",
      "    update_time_ms: 1.777\n",
      "  timestamp: 1632134374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         3090.85</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 273000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 274\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.943103505505456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019855326589920164\n",
      "          policy_loss: -0.00888281421115001\n",
      "          total_loss: -0.018906293768021797\n",
      "          vf_explained_var: 0.40518859028816223\n",
      "          vf_loss: 0.002251566650940933\n",
      "    num_agent_steps_sampled: 273000\n",
      "    num_agent_steps_trained: 273000\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.60666666666667\n",
      "    ram_util_percent: 68.15333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049769893408262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.584796980865708\n",
      "    mean_inference_ms: 1.4071612801392517\n",
      "    mean_raw_obs_processing_ms: 0.6751614063799224\n",
      "  time_since_restore: 3101.3086009025574\n",
      "  time_this_iter_s: 10.460633516311646\n",
      "  time_total_s: 3101.3086009025574\n",
      "  timers:\n",
      "    learn_throughput: 1679.838\n",
      "    learn_time_ms: 595.296\n",
      "    load_throughput: 216505.908\n",
      "    load_time_ms: 4.619\n",
      "    sample_throughput: 82.668\n",
      "    sample_time_ms: 12096.606\n",
      "    update_time_ms: 1.735\n",
      "  timestamp: 1632134384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 273\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         3101.31</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 274000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-39-55\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 275\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7917432175742256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012362540655259657\n",
      "          policy_loss: 0.1505562533934911\n",
      "          total_loss: 0.13811403032806185\n",
      "          vf_explained_var: -0.9964312314987183\n",
      "          vf_loss: 0.0010196681147337787\n",
      "    num_agent_steps_sampled: 274000\n",
      "    num_agent_steps_trained: 274000\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.459999999999994\n",
      "    ram_util_percent: 68.32\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040497525049409504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.584479242690277\n",
      "    mean_inference_ms: 1.4071650996937317\n",
      "    mean_raw_obs_processing_ms: 0.6759365604862712\n",
      "  time_since_restore: 3112.147899866104\n",
      "  time_this_iter_s: 10.839298963546753\n",
      "  time_total_s: 3112.147899866104\n",
      "  timers:\n",
      "    learn_throughput: 1669.861\n",
      "    learn_time_ms: 598.852\n",
      "    load_throughput: 214740.119\n",
      "    load_time_ms: 4.657\n",
      "    sample_throughput: 82.985\n",
      "    sample_time_ms: 12050.349\n",
      "    update_time_ms: 1.736\n",
      "  timestamp: 1632134395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 274\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         3112.15</td><td style=\"text-align: right;\">274000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 275000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-40-07\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 276\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1637374771965874\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010752577388766503\n",
      "          policy_loss: -0.08254966255691316\n",
      "          total_loss: -0.09904369331068463\n",
      "          vf_explained_var: -0.5814963579177856\n",
      "          vf_loss: 0.0012680446635284978\n",
      "    num_agent_steps_sampled: 275000\n",
      "    num_agent_steps_trained: 275000\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.06470588235295\n",
      "    ram_util_percent: 68.44117647058822\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040497562529350085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58418757764579\n",
      "    mean_inference_ms: 1.4071729685985113\n",
      "    mean_raw_obs_processing_ms: 0.6767194203360645\n",
      "  time_since_restore: 3124.0672240257263\n",
      "  time_this_iter_s: 11.919324159622192\n",
      "  time_total_s: 3124.0672240257263\n",
      "  timers:\n",
      "    learn_throughput: 1641.789\n",
      "    learn_time_ms: 609.092\n",
      "    load_throughput: 211944.86\n",
      "    load_time_ms: 4.718\n",
      "    sample_throughput: 82.775\n",
      "    sample_time_ms: 12080.93\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632134407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         3124.07</td><td style=\"text-align: right;\">275000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-40-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 277\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8137380825148688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012368560181209846\n",
      "          policy_loss: -0.033246332448389794\n",
      "          total_loss: -0.04589683877097236\n",
      "          vf_explained_var: -0.543349027633667\n",
      "          vf_loss: 0.0010291644847408557\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.60000000000001\n",
      "    ram_util_percent: 68.125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049769116704547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583887409522038\n",
      "    mean_inference_ms: 1.4071826794049633\n",
      "    mean_raw_obs_processing_ms: 0.6775097131660712\n",
      "  time_since_restore: 3135.378978252411\n",
      "  time_this_iter_s: 11.31175422668457\n",
      "  time_total_s: 3135.378978252411\n",
      "  timers:\n",
      "    learn_throughput: 1622.117\n",
      "    learn_time_ms: 616.478\n",
      "    load_throughput: 212075.601\n",
      "    load_time_ms: 4.715\n",
      "    sample_throughput: 82.397\n",
      "    sample_time_ms: 12136.303\n",
      "    update_time_ms: 1.769\n",
      "  timestamp: 1632134418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 276\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         3135.38</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 277000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-40-30\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 278\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9410778919855753\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011715957045053344\n",
      "          policy_loss: -0.023086720787816577\n",
      "          total_loss: -0.03698825273248885\n",
      "          vf_explained_var: 0.2708693742752075\n",
      "          vf_loss: 0.0012867406492457828\n",
      "    num_agent_steps_sampled: 277000\n",
      "    num_agent_steps_trained: 277000\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.37058823529412\n",
      "    ram_util_percent: 67.95882352941177\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040497963867222035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583575242139668\n",
      "    mean_inference_ms: 1.4071932867107462\n",
      "    mean_raw_obs_processing_ms: 0.6783077064235491\n",
      "  time_since_restore: 3146.6852827072144\n",
      "  time_this_iter_s: 11.306304454803467\n",
      "  time_total_s: 3146.6852827072144\n",
      "  timers:\n",
      "    learn_throughput: 1601.723\n",
      "    learn_time_ms: 624.328\n",
      "    load_throughput: 210967.291\n",
      "    load_time_ms: 4.74\n",
      "    sample_throughput: 82.144\n",
      "    sample_time_ms: 12173.723\n",
      "    update_time_ms: 1.784\n",
      "  timestamp: 1632134430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         3146.69</td><td style=\"text-align: right;\">277000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 278000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-40-41\n",
      "  done: false\n",
      "  episode_len_mean: 994.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 279\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.931872186395857\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016709157067631906\n",
      "          policy_loss: -0.13065430654419793\n",
      "          total_loss: -0.14244303703308106\n",
      "          vf_explained_var: -0.06429778784513474\n",
      "          vf_loss: 0.0015079031905366315\n",
      "    num_agent_steps_sampled: 278000\n",
      "    num_agent_steps_trained: 278000\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.72666666666667\n",
      "    ram_util_percent: 67.83333333333331\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049827834420909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583268915104235\n",
      "    mean_inference_ms: 1.4072040078091188\n",
      "    mean_raw_obs_processing_ms: 0.6791122732778324\n",
      "  time_since_restore: 3157.743076324463\n",
      "  time_this_iter_s: 11.057793617248535\n",
      "  time_total_s: 3157.743076324463\n",
      "  timers:\n",
      "    learn_throughput: 1593.814\n",
      "    learn_time_ms: 627.426\n",
      "    load_throughput: 208718.569\n",
      "    load_time_ms: 4.791\n",
      "    sample_throughput: 82.108\n",
      "    sample_time_ms: 12179.077\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1632134441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 278\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         3157.74</td><td style=\"text-align: right;\">278000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 279000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 280\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9238452447785273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026848175502043287\n",
      "          policy_loss: -0.0024533579746882123\n",
      "          total_loss: -0.011444722198777728\n",
      "          vf_explained_var: -0.2887161672115326\n",
      "          vf_loss: 0.0005708344817523741\n",
      "    num_agent_steps_sampled: 279000\n",
      "    num_agent_steps_trained: 279000\n",
      "    num_steps_sampled: 279000\n",
      "    num_steps_trained: 279000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.974999999999994\n",
      "    ram_util_percent: 67.54374999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049860945560717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.582992268069138\n",
      "    mean_inference_ms: 1.407215666974691\n",
      "    mean_raw_obs_processing_ms: 0.6789377834553314\n",
      "  time_since_restore: 3168.8465297222137\n",
      "  time_this_iter_s: 11.103453397750854\n",
      "  time_total_s: 3168.8465297222137\n",
      "  timers:\n",
      "    learn_throughput: 1590.573\n",
      "    learn_time_ms: 628.704\n",
      "    load_throughput: 291822.331\n",
      "    load_time_ms: 3.427\n",
      "    sample_throughput: 95.457\n",
      "    sample_time_ms: 10475.91\n",
      "    update_time_ms: 1.792\n",
      "  timestamp: 1632134452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279000\n",
      "  training_iteration: 279\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         3168.85</td><td style=\"text-align: right;\">279000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-41-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 281\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.000459575653076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012352052470018925\n",
      "          policy_loss: -0.05697365612205532\n",
      "          total_loss: -0.06937831091798013\n",
      "          vf_explained_var: 0.10399818420410156\n",
      "          vf_loss: 0.0009223019270898982\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.06875\n",
      "    ram_util_percent: 67.63125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049894983998719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.582624847247676\n",
      "    mean_inference_ms: 1.4072273027078213\n",
      "    mean_raw_obs_processing_ms: 0.678775190244786\n",
      "  time_since_restore: 3179.7523379325867\n",
      "  time_this_iter_s: 10.905808210372925\n",
      "  time_total_s: 3179.7523379325867\n",
      "  timers:\n",
      "    learn_throughput: 1593.235\n",
      "    learn_time_ms: 627.654\n",
      "    load_throughput: 292514.297\n",
      "    load_time_ms: 3.419\n",
      "    sample_throughput: 96.602\n",
      "    sample_time_ms: 10351.716\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1632134463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         3179.75</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 281000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-41-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 282\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.95896495713128\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014503246714543196\n",
      "          policy_loss: 0.07665938784678777\n",
      "          total_loss: 0.06604227465060022\n",
      "          vf_explained_var: -0.07551947981119156\n",
      "          vf_loss: 0.001131942910918345\n",
      "    num_agent_steps_sampled: 281000\n",
      "    num_agent_steps_trained: 281000\n",
      "    num_steps_sampled: 281000\n",
      "    num_steps_trained: 281000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.45625\n",
      "    ram_util_percent: 67.67500000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040499307926683444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.582302114214228\n",
      "    mean_inference_ms: 1.4072395414656527\n",
      "    mean_raw_obs_processing_ms: 0.6786237866820913\n",
      "  time_since_restore: 3190.929569005966\n",
      "  time_this_iter_s: 11.177231073379517\n",
      "  time_total_s: 3190.929569005966\n",
      "  timers:\n",
      "    learn_throughput: 1586.653\n",
      "    learn_time_ms: 630.258\n",
      "    load_throughput: 291081.1\n",
      "    load_time_ms: 3.435\n",
      "    sample_throughput: 95.941\n",
      "    sample_time_ms: 10423.098\n",
      "    update_time_ms: 1.792\n",
      "  timestamp: 1632134474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281000\n",
      "  training_iteration: 281\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         3190.93</td><td style=\"text-align: right;\">281000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 282000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-41-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 283\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0114745047357347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008322783834476431\n",
      "          policy_loss: 0.06859834906127718\n",
      "          total_loss: 0.053460049877564114\n",
      "          vf_explained_var: -0.9302147626876831\n",
      "          vf_loss: 0.0004770673577796616\n",
      "    num_agent_steps_sampled: 282000\n",
      "    num_agent_steps_trained: 282000\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.668749999999996\n",
      "    ram_util_percent: 67.7625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040499681622716464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.582021613448722\n",
      "    mean_inference_ms: 1.407252488833865\n",
      "    mean_raw_obs_processing_ms: 0.6784848122230085\n",
      "  time_since_restore: 3201.987128973007\n",
      "  time_this_iter_s: 11.057559967041016\n",
      "  time_total_s: 3201.987128973007\n",
      "  timers:\n",
      "    learn_throughput: 1588.856\n",
      "    learn_time_ms: 629.384\n",
      "    load_throughput: 291097.261\n",
      "    load_time_ms: 3.435\n",
      "    sample_throughput: 95.457\n",
      "    sample_time_ms: 10475.936\n",
      "    update_time_ms: 1.797\n",
      "  timestamp: 1632134485\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         3201.99</td><td style=\"text-align: right;\">282000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 283000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-41-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 284\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.030436250898573\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01379034395309095\n",
      "          policy_loss: -0.04839292052719328\n",
      "          total_loss: -0.059482491761446\n",
      "          vf_explained_var: -0.004065999761223793\n",
      "          vf_loss: 0.001759597561451503\n",
      "    num_agent_steps_sampled: 283000\n",
      "    num_agent_steps_trained: 283000\n",
      "    num_steps_sampled: 283000\n",
      "    num_steps_trained: 283000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.76\n",
      "    ram_util_percent: 67.86666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050006781704784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581777520712755\n",
      "    mean_inference_ms: 1.4072657659983687\n",
      "    mean_raw_obs_processing_ms: 0.6783571355510677\n",
      "  time_since_restore: 3213.004861354828\n",
      "  time_this_iter_s: 11.017732381820679\n",
      "  time_total_s: 3213.004861354828\n",
      "  timers:\n",
      "    learn_throughput: 1590.112\n",
      "    learn_time_ms: 628.887\n",
      "    load_throughput: 294351.582\n",
      "    load_time_ms: 3.397\n",
      "    sample_throughput: 94.949\n",
      "    sample_time_ms: 10531.944\n",
      "    update_time_ms: 1.802\n",
      "  timestamp: 1632134496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 283000\n",
      "  training_iteration: 283\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">            3213</td><td style=\"text-align: right;\">283000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-41-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 285\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8681281632847255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014004274622271565\n",
      "          policy_loss: -0.05884310234751966\n",
      "          total_loss: -0.0687549441949361\n",
      "          vf_explained_var: -0.6885591149330139\n",
      "          vf_loss: 0.001198595870583732\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.576470588235296\n",
      "    ram_util_percent: 66.12352941176471\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040500437366109227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581572874198768\n",
      "    mean_inference_ms: 1.4072801200724505\n",
      "    mean_raw_obs_processing_ms: 0.6782413368876418\n",
      "  time_since_restore: 3224.2350986003876\n",
      "  time_this_iter_s: 11.230237245559692\n",
      "  time_total_s: 3224.2350986003876\n",
      "  timers:\n",
      "    learn_throughput: 1597.197\n",
      "    learn_time_ms: 626.097\n",
      "    load_throughput: 298431.392\n",
      "    load_time_ms: 3.351\n",
      "    sample_throughput: 94.573\n",
      "    sample_time_ms: 10573.833\n",
      "    update_time_ms: 1.799\n",
      "  timestamp: 1632134507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         3224.24</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-41-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 286\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0707671258184646\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009221240770888626\n",
      "          policy_loss: -0.01973360785179668\n",
      "          total_loss: -0.03487013073431121\n",
      "          vf_explained_var: -0.6526657342910767\n",
      "          vf_loss: 0.0005860555879836385\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.80666666666666\n",
      "    ram_util_percent: 66.11333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050082741353153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581378629692233\n",
      "    mean_inference_ms: 1.4072950664029142\n",
      "    mean_raw_obs_processing_ms: 0.6781367136186074\n",
      "  time_since_restore: 3235.0468401908875\n",
      "  time_this_iter_s: 10.811741590499878\n",
      "  time_total_s: 3235.0468401908875\n",
      "  timers:\n",
      "    learn_throughput: 1620.545\n",
      "    learn_time_ms: 617.076\n",
      "    load_throughput: 304568.487\n",
      "    load_time_ms: 3.283\n",
      "    sample_throughput: 95.491\n",
      "    sample_time_ms: 10472.235\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632134518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 285\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         3235.05</td><td style=\"text-align: right;\">285000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 286000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-42-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 287\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6327748417854309\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005424359522020773\n",
      "          policy_loss: -0.023253271645969815\n",
      "          total_loss: -0.036274083620972106\n",
      "          vf_explained_var: -0.10821043699979782\n",
      "          vf_loss: 0.0003744732758301931\n",
      "    num_agent_steps_sampled: 286000\n",
      "    num_agent_steps_trained: 286000\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.237500000000004\n",
      "    ram_util_percent: 66.19375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040501246054986347\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581210609099404\n",
      "    mean_inference_ms: 1.407311171587256\n",
      "    mean_raw_obs_processing_ms: 0.678042423992219\n",
      "  time_since_restore: 3246.200521469116\n",
      "  time_this_iter_s: 11.15368127822876\n",
      "  time_total_s: 3246.200521469116\n",
      "  timers:\n",
      "    learn_throughput: 1640.513\n",
      "    learn_time_ms: 609.565\n",
      "    load_throughput: 306690.845\n",
      "    load_time_ms: 3.261\n",
      "    sample_throughput: 95.566\n",
      "    sample_time_ms: 10463.979\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632134529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">          3246.2</td><td style=\"text-align: right;\">286000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 287000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-42-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 288\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8399222572644551\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010670195564680659\n",
      "          policy_loss: -0.014838238350219198\n",
      "          total_loss: -0.02707422429488765\n",
      "          vf_explained_var: 0.4296565055847168\n",
      "          vf_loss: 0.00039482516650524407\n",
      "    num_agent_steps_sampled: 287000\n",
      "    num_agent_steps_trained: 287000\n",
      "    num_steps_sampled: 287000\n",
      "    num_steps_trained: 287000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.725\n",
      "    ram_util_percent: 66.19375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050169563388201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58107074465904\n",
      "    mean_inference_ms: 1.4073276779229011\n",
      "    mean_raw_obs_processing_ms: 0.6779600944755713\n",
      "  time_since_restore: 3257.349680662155\n",
      "  time_this_iter_s: 11.14915919303894\n",
      "  time_total_s: 3257.349680662155\n",
      "  timers:\n",
      "    learn_throughput: 1661.996\n",
      "    learn_time_ms: 601.686\n",
      "    load_throughput: 308929.432\n",
      "    load_time_ms: 3.237\n",
      "    sample_throughput: 95.637\n",
      "    sample_time_ms: 10456.208\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632134541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287000\n",
      "  training_iteration: 287\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         3257.35</td><td style=\"text-align: right;\">287000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-42-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 289\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3350334458880955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005715337919018282\n",
      "          policy_loss: 0.09307926446199417\n",
      "          total_loss: 0.08310847340358628\n",
      "          vf_explained_var: 0.6060327291488647\n",
      "          vf_loss: 0.0002897754773989113\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.3625\n",
      "    ram_util_percent: 66.34375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050215364586838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.580977016883931\n",
      "    mean_inference_ms: 1.4073440987283983\n",
      "    mean_raw_obs_processing_ms: 0.6778884220072395\n",
      "  time_since_restore: 3268.681046962738\n",
      "  time_this_iter_s: 11.331366300582886\n",
      "  time_total_s: 3268.681046962738\n",
      "  timers:\n",
      "    learn_throughput: 1670.164\n",
      "    learn_time_ms: 598.744\n",
      "    load_throughput: 310979.433\n",
      "    load_time_ms: 3.216\n",
      "    sample_throughput: 95.36\n",
      "    sample_time_ms: 10486.538\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632134552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 288\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         3268.68</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 289000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 290\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0185413201649984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00860862594814904\n",
      "          policy_loss: 0.014915831055906085\n",
      "          total_loss: 0.00013215492169062296\n",
      "          vf_explained_var: -0.36745303869247437\n",
      "          vf_loss: 0.0007478292193910521\n",
      "    num_agent_steps_sampled: 289000\n",
      "    num_agent_steps_trained: 289000\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.75625\n",
      "    ram_util_percent: 66.44375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050263412434092\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58091605626321\n",
      "    mean_inference_ms: 1.4073615352541828\n",
      "    mean_raw_obs_processing_ms: 0.6778274066835364\n",
      "  time_since_restore: 3279.7761268615723\n",
      "  time_this_iter_s: 11.095079898834229\n",
      "  time_total_s: 3279.7761268615723\n",
      "  timers:\n",
      "    learn_throughput: 1670.841\n",
      "    learn_time_ms: 598.501\n",
      "    load_throughput: 311249.434\n",
      "    load_time_ms: 3.213\n",
      "    sample_throughput: 95.366\n",
      "    sample_time_ms: 10485.945\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632134563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         3279.78</td><td style=\"text-align: right;\">289000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 290000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-42-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 291\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8916841559939914\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008246463667407462\n",
      "          policy_loss: 0.055681240641408496\n",
      "          total_loss: 0.04182299772898356\n",
      "          vf_explained_var: -0.9724465012550354\n",
      "          vf_loss: 0.0006004810553147561\n",
      "    num_agent_steps_sampled: 290000\n",
      "    num_agent_steps_trained: 290000\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.431250000000006\n",
      "    ram_util_percent: 66.45625000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050313326866772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.580886418147248\n",
      "    mean_inference_ms: 1.4073793727306607\n",
      "    mean_raw_obs_processing_ms: 0.6777768733000857\n",
      "  time_since_restore: 3290.6732251644135\n",
      "  time_this_iter_s: 10.897098302841187\n",
      "  time_total_s: 3290.6732251644135\n",
      "  timers:\n",
      "    learn_throughput: 1667.404\n",
      "    learn_time_ms: 599.735\n",
      "    load_throughput: 310583.357\n",
      "    load_time_ms: 3.22\n",
      "    sample_throughput: 95.385\n",
      "    sample_time_ms: 10483.814\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632134574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 290\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         3290.67</td><td style=\"text-align: right;\">290000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 291000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-43-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 292\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.918113558822208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0067884976525803215\n",
      "          policy_loss: 0.05839735390618443\n",
      "          total_loss: 0.04312457558181551\n",
      "          vf_explained_var: -0.6783560514450073\n",
      "          vf_loss: 0.00023843018263707764\n",
      "    num_agent_steps_sampled: 291000\n",
      "    num_agent_steps_trained: 291000\n",
      "    num_steps_sampled: 291000\n",
      "    num_steps_trained: 291000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.03125\n",
      "    ram_util_percent: 66.50625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050365848612911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.580899040792678\n",
      "    mean_inference_ms: 1.407398543883784\n",
      "    mean_raw_obs_processing_ms: 0.6777374147752496\n",
      "  time_since_restore: 3301.998881340027\n",
      "  time_this_iter_s: 11.325656175613403\n",
      "  time_total_s: 3301.998881340027\n",
      "  timers:\n",
      "    learn_throughput: 1673.604\n",
      "    learn_time_ms: 597.513\n",
      "    load_throughput: 311976.377\n",
      "    load_time_ms: 3.205\n",
      "    sample_throughput: 95.23\n",
      "    sample_time_ms: 10500.9\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632134585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291000\n",
      "  training_iteration: 291\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">            3302</td><td style=\"text-align: right;\">291000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-43-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 293\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6924762010574341\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008989297762146364\n",
      "          policy_loss: 0.12049550174011124\n",
      "          total_loss: 0.10923779908981589\n",
      "          vf_explained_var: -0.20721906423568726\n",
      "          vf_loss: 0.0008073595360555272\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.462500000000006\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040504222816860794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.580944983985276\n",
      "    mean_inference_ms: 1.4074189015670173\n",
      "    mean_raw_obs_processing_ms: 0.6777087454613677\n",
      "  time_since_restore: 3313.541668653488\n",
      "  time_this_iter_s: 11.542787313461304\n",
      "  time_total_s: 3313.541668653488\n",
      "  timers:\n",
      "    learn_throughput: 1670.479\n",
      "    learn_time_ms: 598.631\n",
      "    load_throughput: 311906.777\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 94.802\n",
      "    sample_time_ms: 10548.289\n",
      "    update_time_ms: 1.741\n",
      "  timestamp: 1632134597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         3313.54</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 293000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 294\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0746490054660374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008367594915510108\n",
      "          policy_loss: 0.10696957976453834\n",
      "          total_loss: 0.0912112014575137\n",
      "          vf_explained_var: -0.6515222191810608\n",
      "          vf_loss: 0.0004645088642266475\n",
      "    num_agent_steps_sampled: 293000\n",
      "    num_agent_steps_trained: 293000\n",
      "    num_steps_sampled: 293000\n",
      "    num_steps_trained: 293000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.45\n",
      "    ram_util_percent: 66.49375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040504802891318595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581017520562868\n",
      "    mean_inference_ms: 1.4074405274258777\n",
      "    mean_raw_obs_processing_ms: 0.6776899284074948\n",
      "  time_since_restore: 3324.8499801158905\n",
      "  time_this_iter_s: 11.308311462402344\n",
      "  time_total_s: 3324.8499801158905\n",
      "  timers:\n",
      "    learn_throughput: 1669.051\n",
      "    learn_time_ms: 599.143\n",
      "    load_throughput: 311605.536\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 94.544\n",
      "    sample_time_ms: 10577.044\n",
      "    update_time_ms: 1.738\n",
      "  timestamp: 1632134608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 293000\n",
      "  training_iteration: 293\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         3324.85</td><td style=\"text-align: right;\">293000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 294000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-43-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 295\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8460317730903626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011366775896237573\n",
      "          policy_loss: -0.019708476670914226\n",
      "          total_loss: -0.030858026320735612\n",
      "          vf_explained_var: -0.16267959773540497\n",
      "          vf_loss: 0.0011657780466420161\n",
      "    num_agent_steps_sampled: 294000\n",
      "    num_agent_steps_trained: 294000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.10588235294117\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050540601841528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581114018267176\n",
      "    mean_inference_ms: 1.4074632005903087\n",
      "    mean_raw_obs_processing_ms: 0.6776803779129444\n",
      "  time_since_restore: 3336.1870708465576\n",
      "  time_this_iter_s: 11.337090730667114\n",
      "  time_total_s: 3336.1870708465576\n",
      "  timers:\n",
      "    learn_throughput: 1672.138\n",
      "    learn_time_ms: 598.037\n",
      "    load_throughput: 310374.213\n",
      "    load_time_ms: 3.222\n",
      "    sample_throughput: 94.439\n",
      "    sample_time_ms: 10588.897\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632134620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 294\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         3336.19</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 295000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 296\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9975844846831428\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0108961609972479\n",
      "          policy_loss: -0.04829165766843491\n",
      "          total_loss: -0.061585019218424956\n",
      "          vf_explained_var: -0.6204128861427307\n",
      "          vf_loss: 0.0007919146483699377\n",
      "    num_agent_steps_sampled: 295000\n",
      "    num_agent_steps_trained: 295000\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.48125\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050601684066685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581254840636653\n",
      "    mean_inference_ms: 1.407486559264414\n",
      "    mean_raw_obs_processing_ms: 0.6776796782986544\n",
      "  time_since_restore: 3347.542089700699\n",
      "  time_this_iter_s: 11.355018854141235\n",
      "  time_total_s: 3347.542089700699\n",
      "  timers:\n",
      "    learn_throughput: 1672.724\n",
      "    learn_time_ms: 597.827\n",
      "    load_throughput: 310069.047\n",
      "    load_time_ms: 3.225\n",
      "    sample_throughput: 93.955\n",
      "    sample_time_ms: 10643.387\n",
      "    update_time_ms: 1.742\n",
      "  timestamp: 1632134631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         3347.54</td><td style=\"text-align: right;\">295000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-44-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 297\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0425413383377924\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00923652380751538\n",
      "          policy_loss: 0.08185073932011923\n",
      "          total_loss: 0.06718219075765874\n",
      "          vf_explained_var: -0.2039126306772232\n",
      "          vf_loss: 0.0007635102246745697\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.5125\n",
      "    ram_util_percent: 66.45625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050664438259503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58142956801193\n",
      "    mean_inference_ms: 1.4075108114465391\n",
      "    mean_raw_obs_processing_ms: 0.6776879295802807\n",
      "  time_since_restore: 3358.805606842041\n",
      "  time_this_iter_s: 11.263517141342163\n",
      "  time_total_s: 3358.805606842041\n",
      "  timers:\n",
      "    learn_throughput: 1671.886\n",
      "    learn_time_ms: 598.127\n",
      "    load_throughput: 310627.06\n",
      "    load_time_ms: 3.219\n",
      "    sample_throughput: 93.861\n",
      "    sample_time_ms: 10654.076\n",
      "    update_time_ms: 1.748\n",
      "  timestamp: 1632134642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 296\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         3358.81</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 297000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-44-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 298\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.061137431197696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008380897770320573\n",
      "          policy_loss: 0.15413998688260713\n",
      "          total_loss: 0.13912271966950762\n",
      "          vf_explained_var: -0.09341638535261154\n",
      "          vf_loss: 0.0010633126535038982\n",
      "    num_agent_steps_sampled: 297000\n",
      "    num_agent_steps_trained: 297000\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.62777777777777\n",
      "    ram_util_percent: 66.48888888888888\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405074116177229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581664613179747\n",
      "    mean_inference_ms: 1.4075386522730235\n",
      "    mean_raw_obs_processing_ms: 0.6777060208261811\n",
      "  time_since_restore: 3371.105087041855\n",
      "  time_this_iter_s: 12.299480199813843\n",
      "  time_total_s: 3371.105087041855\n",
      "  timers:\n",
      "    learn_throughput: 1659.966\n",
      "    learn_time_ms: 602.422\n",
      "    load_throughput: 237218.288\n",
      "    load_time_ms: 4.216\n",
      "    sample_throughput: 92.904\n",
      "    sample_time_ms: 10763.849\n",
      "    update_time_ms: 1.742\n",
      "  timestamp: 1632134655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 297\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         3371.11</td><td style=\"text-align: right;\">297000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 298000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-44-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 299\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.809884656800164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008603183795868259\n",
      "          policy_loss: 0.05093645641269783\n",
      "          total_loss: 0.038096140920081074\n",
      "          vf_explained_var: -0.12435705214738846\n",
      "          vf_loss: 0.0006075661774957552\n",
      "    num_agent_steps_sampled: 298000\n",
      "    num_agent_steps_trained: 298000\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.40555555555554\n",
      "    ram_util_percent: 66.52777777777777\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040508290912034964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581967259186685\n",
      "    mean_inference_ms: 1.4075700427605162\n",
      "    mean_raw_obs_processing_ms: 0.6777336623574347\n",
      "  time_since_restore: 3383.663016796112\n",
      "  time_this_iter_s: 12.557929754257202\n",
      "  time_total_s: 3383.663016796112\n",
      "  timers:\n",
      "    learn_throughput: 1637.924\n",
      "    learn_time_ms: 610.529\n",
      "    load_throughput: 234932.897\n",
      "    load_time_ms: 4.257\n",
      "    sample_throughput: 91.926\n",
      "    sample_time_ms: 10878.35\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632134667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         3383.66</td><td style=\"text-align: right;\">298000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 299000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-44-56\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 300\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7905503378974066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016419507151315346\n",
      "          policy_loss: -0.02847458449088865\n",
      "          total_loss: 0.006327413291566901\n",
      "          vf_explained_var: 0.041837725788354874\n",
      "          vf_loss: 0.04383095637507116\n",
      "    num_agent_steps_sampled: 299000\n",
      "    num_agent_steps_trained: 299000\n",
      "    num_steps_sampled: 299000\n",
      "    num_steps_trained: 299000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.7125\n",
      "    ram_util_percent: 66.32000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050925552093891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.582311003758086\n",
      "    mean_inference_ms: 1.4076037523886111\n",
      "    mean_raw_obs_processing_ms: 0.6783401549348639\n",
      "  time_since_restore: 3412.2346153259277\n",
      "  time_this_iter_s: 28.571598529815674\n",
      "  time_total_s: 3412.2346153259277\n",
      "  timers:\n",
      "    learn_throughput: 1635.176\n",
      "    learn_time_ms: 611.555\n",
      "    load_throughput: 176652.108\n",
      "    load_time_ms: 5.661\n",
      "    sample_throughput: 79.217\n",
      "    sample_time_ms: 12623.548\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632134696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299000\n",
      "  training_iteration: 299\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         3412.23</td><td style=\"text-align: right;\">299000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-45-08\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 301\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9949704647064208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01721405544934665\n",
      "          policy_loss: -0.010198116054137547\n",
      "          total_loss: -0.017961664580636555\n",
      "          vf_explained_var: 0.4735671877861023\n",
      "          vf_loss: 0.0028800738548549515\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.58235294117647\n",
      "    ram_util_percent: 65.79411764705883\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051022239098631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58271568454352\n",
      "    mean_inference_ms: 1.407637058582844\n",
      "    mean_raw_obs_processing_ms: 0.678953297892198\n",
      "  time_since_restore: 3424.058691263199\n",
      "  time_this_iter_s: 11.824075937271118\n",
      "  time_total_s: 3424.058691263199\n",
      "  timers:\n",
      "    learn_throughput: 1635.006\n",
      "    learn_time_ms: 611.618\n",
      "    load_throughput: 176415.088\n",
      "    load_time_ms: 5.668\n",
      "    sample_throughput: 78.64\n",
      "    sample_time_ms: 12716.187\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632134708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 300\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         3424.06</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 301000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-45-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 302\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9438603851530287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011981223401381887\n",
      "          policy_loss: 0.003503947994775242\n",
      "          total_loss: -0.007131182278196017\n",
      "          vf_explained_var: 0.05139264464378357\n",
      "          vf_loss: 0.002326307279549332\n",
      "    num_agent_steps_sampled: 301000\n",
      "    num_agent_steps_trained: 301000\n",
      "    num_steps_sampled: 301000\n",
      "    num_steps_trained: 301000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.728571428571435\n",
      "    ram_util_percent: 66.22857142857144\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040511185677623754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583104174998978\n",
      "    mean_inference_ms: 1.4076698069944777\n",
      "    mean_raw_obs_processing_ms: 0.6795729802271544\n",
      "  time_since_restore: 3433.894317150116\n",
      "  time_this_iter_s: 9.835625886917114\n",
      "  time_total_s: 3433.894317150116\n",
      "  timers:\n",
      "    learn_throughput: 1635.946\n",
      "    learn_time_ms: 611.267\n",
      "    load_throughput: 176260.143\n",
      "    load_time_ms: 5.673\n",
      "    sample_throughput: 79.57\n",
      "    sample_time_ms: 12567.534\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632134718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 301000\n",
      "  training_iteration: 301\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         3433.89</td><td style=\"text-align: right;\">301000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 302000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 303\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0039694984753926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016155580369932258\n",
      "          policy_loss: -0.005358210412992372\n",
      "          total_loss: -0.014967958629131316\n",
      "          vf_explained_var: 0.5456902980804443\n",
      "          vf_loss: 0.0016960825383042295\n",
      "    num_agent_steps_sampled: 302000\n",
      "    num_agent_steps_trained: 302000\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.4\n",
      "    ram_util_percent: 66.30714285714285\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040512154316083245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583471215258948\n",
      "    mean_inference_ms: 1.4077020172618335\n",
      "    mean_raw_obs_processing_ms: 0.6801983813597998\n",
      "  time_since_restore: 3443.620297193527\n",
      "  time_this_iter_s: 9.725980043411255\n",
      "  time_total_s: 3443.620297193527\n",
      "  timers:\n",
      "    learn_throughput: 1637.911\n",
      "    learn_time_ms: 610.534\n",
      "    load_throughput: 176449.97\n",
      "    load_time_ms: 5.667\n",
      "    sample_throughput: 80.733\n",
      "    sample_time_ms: 12386.581\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1632134727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         3443.62</td><td style=\"text-align: right;\">302000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 303000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-45-37\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 304\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8755589803059896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073523532711009115\n",
      "          policy_loss: -0.022939568902883265\n",
      "          total_loss: -0.03670684998441073\n",
      "          vf_explained_var: -0.7802573442459106\n",
      "          vf_loss: 0.001013555348941332\n",
      "    num_agent_steps_sampled: 303000\n",
      "    num_agent_steps_trained: 303000\n",
      "    num_steps_sampled: 303000\n",
      "    num_steps_trained: 303000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.478571428571435\n",
      "    ram_util_percent: 66.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040513126754491945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583813923141372\n",
      "    mean_inference_ms: 1.4077334612373664\n",
      "    mean_raw_obs_processing_ms: 0.6808300950008721\n",
      "  time_since_restore: 3453.189260005951\n",
      "  time_this_iter_s: 9.568962812423706\n",
      "  time_total_s: 3453.189260005951\n",
      "  timers:\n",
      "    learn_throughput: 1639.323\n",
      "    learn_time_ms: 610.008\n",
      "    load_throughput: 176356.488\n",
      "    load_time_ms: 5.67\n",
      "    sample_throughput: 81.879\n",
      "    sample_time_ms: 12213.173\n",
      "    update_time_ms: 1.784\n",
      "  timestamp: 1632134737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303000\n",
      "  training_iteration: 303\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         3453.19</td><td style=\"text-align: right;\">303000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 305\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6953318211767407\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01205053069773765\n",
      "          policy_loss: -0.09445230346173047\n",
      "          total_loss: -0.10303681651130318\n",
      "          vf_explained_var: 0.41491517424583435\n",
      "          vf_loss: 0.0018541720741066254\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.292857142857144\n",
      "    ram_util_percent: 66.28571428571428\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040514095235520686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.584134504014768\n",
      "    mean_inference_ms: 1.4077640421548339\n",
      "    mean_raw_obs_processing_ms: 0.6814673354034504\n",
      "  time_since_restore: 3463.0605096817017\n",
      "  time_this_iter_s: 9.871249675750732\n",
      "  time_total_s: 3463.0605096817017\n",
      "  timers:\n",
      "    learn_throughput: 1639.649\n",
      "    learn_time_ms: 609.887\n",
      "    load_throughput: 176522.746\n",
      "    load_time_ms: 5.665\n",
      "    sample_throughput: 82.873\n",
      "    sample_time_ms: 12066.724\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632134747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         3463.06</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 305000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 306\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.947467189364963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009712935914261086\n",
      "          policy_loss: -0.1221901125792\n",
      "          total_loss: -0.13482701459692584\n",
      "          vf_explained_var: 0.6289328932762146\n",
      "          vf_loss: 0.0015868588273103038\n",
      "    num_agent_steps_sampled: 305000\n",
      "    num_agent_steps_trained: 305000\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.59333333333333\n",
      "    ram_util_percent: 66.26666666666668\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040515064865028975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.584462002630099\n",
      "    mean_inference_ms: 1.4077943792541154\n",
      "    mean_raw_obs_processing_ms: 0.6821108271921511\n",
      "  time_since_restore: 3473.6579699516296\n",
      "  time_this_iter_s: 10.597460269927979\n",
      "  time_total_s: 3473.6579699516296\n",
      "  timers:\n",
      "    learn_throughput: 1641.985\n",
      "    learn_time_ms: 609.019\n",
      "    load_throughput: 176347.591\n",
      "    load_time_ms: 5.671\n",
      "    sample_throughput: 83.39\n",
      "    sample_time_ms: 11991.867\n",
      "    update_time_ms: 1.785\n",
      "  timestamp: 1632134757\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 305\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         3473.66</td><td style=\"text-align: right;\">305000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 306000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-46-08\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 307\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8805075526237487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009811327252743791\n",
      "          policy_loss: -0.11035167516933547\n",
      "          total_loss: -0.12172435273726781\n",
      "          vf_explained_var: 0.6724485158920288\n",
      "          vf_loss: 0.0021282950894803638\n",
      "    num_agent_steps_sampled: 306000\n",
      "    num_agent_steps_trained: 306000\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.24666666666666\n",
      "    ram_util_percent: 66.21333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040516032704838036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.584780585377565\n",
      "    mean_inference_ms: 1.407823957874185\n",
      "    mean_raw_obs_processing_ms: 0.6827602696153343\n",
      "  time_since_restore: 3484.0660498142242\n",
      "  time_this_iter_s: 10.408079862594604\n",
      "  time_total_s: 3484.0660498142242\n",
      "  timers:\n",
      "    learn_throughput: 1642.809\n",
      "    learn_time_ms: 608.714\n",
      "    load_throughput: 176374.287\n",
      "    load_time_ms: 5.67\n",
      "    sample_throughput: 83.987\n",
      "    sample_time_ms: 11906.596\n",
      "    update_time_ms: 1.791\n",
      "  timestamp: 1632134768\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 306\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         3484.07</td><td style=\"text-align: right;\">306000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 307000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 308\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8887527015474108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012732322619914981\n",
      "          policy_loss: -0.07551529332995414\n",
      "          total_loss: -0.08592779090007147\n",
      "          vf_explained_var: -0.35899651050567627\n",
      "          vf_loss: 0.0015918112230590648\n",
      "    num_agent_steps_sampled: 307000\n",
      "    num_agent_steps_trained: 307000\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.28000000000001\n",
      "    ram_util_percent: 66.12000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040517001689327786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58510255890434\n",
      "    mean_inference_ms: 1.407852979353443\n",
      "    mean_raw_obs_processing_ms: 0.6834152260238673\n",
      "  time_since_restore: 3494.482253551483\n",
      "  time_this_iter_s: 10.416203737258911\n",
      "  time_total_s: 3494.482253551483\n",
      "  timers:\n",
      "    learn_throughput: 1656.519\n",
      "    learn_time_ms: 603.676\n",
      "    load_throughput: 213908.884\n",
      "    load_time_ms: 4.675\n",
      "    sample_throughput: 85.293\n",
      "    sample_time_ms: 11724.274\n",
      "    update_time_ms: 1.789\n",
      "  timestamp: 1632134778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         3494.48</td><td style=\"text-align: right;\">307000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 309\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8873982654677497\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015962125144868743\n",
      "          policy_loss: -0.08213117898752292\n",
      "          total_loss: -0.09085779384606414\n",
      "          vf_explained_var: -0.6224194169044495\n",
      "          vf_loss: 0.0015180851122648972\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.51428571428572\n",
      "    ram_util_percent: 66.10000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051797365516432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.585422049270656\n",
      "    mean_inference_ms: 1.4078816692356135\n",
      "    mean_raw_obs_processing_ms: 0.6840759801615114\n",
      "  time_since_restore: 3504.631334066391\n",
      "  time_this_iter_s: 10.149080514907837\n",
      "  time_total_s: 3504.631334066391\n",
      "  timers:\n",
      "    learn_throughput: 1681.581\n",
      "    learn_time_ms: 594.678\n",
      "    load_throughput: 216542.794\n",
      "    load_time_ms: 4.618\n",
      "    sample_throughput: 87.014\n",
      "    sample_time_ms: 11492.431\n",
      "    update_time_ms: 1.782\n",
      "  timestamp: 1632134788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 308\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         3504.63</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 309000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-46-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 310\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7718158562978108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012125223901225328\n",
      "          policy_loss: -0.055092759989202024\n",
      "          total_loss: -0.06446616227428119\n",
      "          vf_explained_var: -0.8369994163513184\n",
      "          vf_loss: 0.0017897434978901099\n",
      "    num_agent_steps_sampled: 309000\n",
      "    num_agent_steps_trained: 309000\n",
      "    num_steps_sampled: 309000\n",
      "    num_steps_trained: 309000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.266666666666666\n",
      "    ram_util_percent: 66.09333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051894154813674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.585667574675513\n",
      "    mean_inference_ms: 1.4079099329553943\n",
      "    mean_raw_obs_processing_ms: 0.6839383592700851\n",
      "  time_since_restore: 3514.912202358246\n",
      "  time_this_iter_s: 10.280868291854858\n",
      "  time_total_s: 3514.912202358246\n",
      "  timers:\n",
      "    learn_throughput: 1686.659\n",
      "    learn_time_ms: 592.888\n",
      "    load_throughput: 311152.457\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 103.449\n",
      "    sample_time_ms: 9666.573\n",
      "    update_time_ms: 1.777\n",
      "  timestamp: 1632134799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309000\n",
      "  training_iteration: 309\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         3514.91</td><td style=\"text-align: right;\">309000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 310000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-46-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 311\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7340350773599413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011231726956947666\n",
      "          policy_loss: -0.07738058293859164\n",
      "          total_loss: -0.08707094275289112\n",
      "          vf_explained_var: -0.2602353096008301\n",
      "          vf_loss: 0.0015780124011346036\n",
      "    num_agent_steps_sampled: 310000\n",
      "    num_agent_steps_trained: 310000\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.339999999999996\n",
      "    ram_util_percent: 66.10000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040519888860731976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.585899332556272\n",
      "    mean_inference_ms: 1.40793709505935\n",
      "    mean_raw_obs_processing_ms: 0.6838100352783815\n",
      "  time_since_restore: 3525.3382959365845\n",
      "  time_this_iter_s: 10.426093578338623\n",
      "  time_total_s: 3525.3382959365845\n",
      "  timers:\n",
      "    learn_throughput: 1688.963\n",
      "    learn_time_ms: 592.079\n",
      "    load_throughput: 309230.077\n",
      "    load_time_ms: 3.234\n",
      "    sample_throughput: 104.958\n",
      "    sample_time_ms: 9527.576\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632134809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         3525.34</td><td style=\"text-align: right;\">310000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 311000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-47-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 312\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.747229023774465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009992715043404803\n",
      "          policy_loss: -0.07814139065643151\n",
      "          total_loss: -0.08950086790654395\n",
      "          vf_explained_var: 0.1460922211408615\n",
      "          vf_loss: 0.0007106545122547282\n",
      "    num_agent_steps_sampled: 311000\n",
      "    num_agent_steps_trained: 311000\n",
      "    num_steps_sampled: 311000\n",
      "    num_steps_trained: 311000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.11333333333335\n",
      "    ram_util_percent: 66.12000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040520827493656014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.586130465071278\n",
      "    mean_inference_ms: 1.4079634967134256\n",
      "    mean_raw_obs_processing_ms: 0.6836902913828825\n",
      "  time_since_restore: 3536.0011439323425\n",
      "  time_this_iter_s: 10.662847995758057\n",
      "  time_total_s: 3536.0011439323425\n",
      "  timers:\n",
      "    learn_throughput: 1684.12\n",
      "    learn_time_ms: 593.782\n",
      "    load_throughput: 309737.031\n",
      "    load_time_ms: 3.229\n",
      "    sample_throughput: 104.074\n",
      "    sample_time_ms: 9608.566\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632134820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311000\n",
      "  training_iteration: 311\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">            3536</td><td style=\"text-align: right;\">311000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-47-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 313\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7449092043770684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011831804054317432\n",
      "          policy_loss: -0.01544135750995742\n",
      "          total_loss: -0.025353673431608412\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001140388004326572\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.4125\n",
      "    ram_util_percent: 66.1875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040521753965809836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.586355047882998\n",
      "    mean_inference_ms: 1.4079887753076248\n",
      "    mean_raw_obs_processing_ms: 0.6835787105492769\n",
      "  time_since_restore: 3546.826362848282\n",
      "  time_this_iter_s: 10.825218915939331\n",
      "  time_total_s: 3546.826362848282\n",
      "  timers:\n",
      "    learn_throughput: 1684.18\n",
      "    learn_time_ms: 593.761\n",
      "    load_throughput: 309508.468\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 102.896\n",
      "    sample_time_ms: 9718.526\n",
      "    update_time_ms: 1.774\n",
      "  timestamp: 1632134831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 312\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         3546.83</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 313000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-47-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 314\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7130690958764818\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012619835603657098\n",
      "          policy_loss: -0.05032253202257885\n",
      "          total_loss: -0.05899775771734615\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0016330600237577325\n",
      "    num_agent_steps_sampled: 313000\n",
      "    num_agent_steps_trained: 313000\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.32666666666667\n",
      "    ram_util_percent: 66.23333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040522661980968974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58654870984836\n",
      "    mean_inference_ms: 1.4080124519975576\n",
      "    mean_raw_obs_processing_ms: 0.6834759985742267\n",
      "  time_since_restore: 3557.2011160850525\n",
      "  time_this_iter_s: 10.37475323677063\n",
      "  time_total_s: 3557.2011160850525\n",
      "  timers:\n",
      "    learn_throughput: 1683.277\n",
      "    learn_time_ms: 594.079\n",
      "    load_throughput: 309734.743\n",
      "    load_time_ms: 3.229\n",
      "    sample_throughput: 102.053\n",
      "    sample_time_ms: 9798.793\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632134841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">          3557.2</td><td style=\"text-align: right;\">313000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 314000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 315\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8665450440512763\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020885423647906107\n",
      "          policy_loss: -0.022844653824965158\n",
      "          total_loss: -0.02801751486129231\n",
      "          vf_explained_var: -0.1751057654619217\n",
      "          vf_loss: 0.002201728167064074\n",
      "    num_agent_steps_sampled: 314000\n",
      "    num_agent_steps_trained: 314000\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.84\n",
      "    ram_util_percent: 66.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052355461443166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58671642188993\n",
      "    mean_inference_ms: 1.4080349814748385\n",
      "    mean_raw_obs_processing_ms: 0.6833811981850727\n",
      "  time_since_restore: 3567.673676967621\n",
      "  time_this_iter_s: 10.47256088256836\n",
      "  time_total_s: 3567.673676967621\n",
      "  timers:\n",
      "    learn_throughput: 1682.545\n",
      "    learn_time_ms: 594.338\n",
      "    load_throughput: 309622.707\n",
      "    load_time_ms: 3.23\n",
      "    sample_throughput: 101.434\n",
      "    sample_time_ms: 9858.665\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632134852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 314\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         3567.67</td><td style=\"text-align: right;\">314000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-47-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 316\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8186957505014207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007806524369608553\n",
      "          policy_loss: -0.0066756411662532225\n",
      "          total_loss: -0.01730455056660705\n",
      "          vf_explained_var: -0.750335156917572\n",
      "          vf_loss: 0.0012276231181911296\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.32000000000001\n",
      "    ram_util_percent: 66.32666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052443535955248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.586874148055967\n",
      "    mean_inference_ms: 1.4080566372849574\n",
      "    mean_raw_obs_processing_ms: 0.6832943534722925\n",
      "  time_since_restore: 3578.21320104599\n",
      "  time_this_iter_s: 10.53952407836914\n",
      "  time_total_s: 3578.21320104599\n",
      "  timers:\n",
      "    learn_throughput: 1681.465\n",
      "    learn_time_ms: 594.719\n",
      "    load_throughput: 309984.258\n",
      "    load_time_ms: 3.226\n",
      "    sample_throughput: 101.497\n",
      "    sample_time_ms: 9852.479\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632134862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 315\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         3578.21</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-47-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 317\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5697224564022487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00828612456800065\n",
      "          policy_loss: -0.012243078007466263\n",
      "          total_loss: -0.020328491657144492\n",
      "          vf_explained_var: -0.7366056442260742\n",
      "          vf_loss: 0.0008924730613620745\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.93333333333332\n",
      "    ram_util_percent: 66.36666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052531014996309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587010895233965\n",
      "    mean_inference_ms: 1.4080767270645305\n",
      "    mean_raw_obs_processing_ms: 0.6832159023253666\n",
      "  time_since_restore: 3588.805200576782\n",
      "  time_this_iter_s: 10.591999530792236\n",
      "  time_total_s: 3588.805200576782\n",
      "  timers:\n",
      "    learn_throughput: 1679.359\n",
      "    learn_time_ms: 595.465\n",
      "    load_throughput: 309392.03\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 101.316\n",
      "    sample_time_ms: 9870.126\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632134873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         3588.81</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 317000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 318\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.563564842277103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004321402757333913\n",
      "          policy_loss: -0.00048337398717800774\n",
      "          total_loss: -0.011978410619000594\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006363219106181835\n",
      "    num_agent_steps_sampled: 317000\n",
      "    num_agent_steps_trained: 317000\n",
      "    num_steps_sampled: 317000\n",
      "    num_steps_trained: 317000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.35333333333333\n",
      "    ram_util_percent: 66.39999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052618838352747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587135696918274\n",
      "    mean_inference_ms: 1.4080959674628069\n",
      "    mean_raw_obs_processing_ms: 0.6831450159229857\n",
      "  time_since_restore: 3599.5287976264954\n",
      "  time_this_iter_s: 10.723597049713135\n",
      "  time_total_s: 3599.5287976264954\n",
      "  timers:\n",
      "    learn_throughput: 1678.197\n",
      "    learn_time_ms: 595.878\n",
      "    load_throughput: 310046.127\n",
      "    load_time_ms: 3.225\n",
      "    sample_throughput: 101.005\n",
      "    sample_time_ms: 9900.483\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632134884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 317000\n",
      "  training_iteration: 317\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         3599.53</td><td style=\"text-align: right;\">317000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 318000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-48-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 319\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7728585958480836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009794852512048516\n",
      "          policy_loss: -0.08391383373075062\n",
      "          total_loss: -0.09615049593978459\n",
      "          vf_explained_var: -0.8206034302711487\n",
      "          vf_loss: 0.001520527598525708\n",
      "    num_agent_steps_sampled: 318000\n",
      "    num_agent_steps_trained: 318000\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.32666666666668\n",
      "    ram_util_percent: 66.45333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040527056603566196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58725148605131\n",
      "    mean_inference_ms: 1.408114440697457\n",
      "    mean_raw_obs_processing_ms: 0.6830819953101294\n",
      "  time_since_restore: 3610.1292283535004\n",
      "  time_this_iter_s: 10.600430727005005\n",
      "  time_total_s: 3610.1292283535004\n",
      "  timers:\n",
      "    learn_throughput: 1680.193\n",
      "    learn_time_ms: 595.17\n",
      "    load_throughput: 311198.629\n",
      "    load_time_ms: 3.213\n",
      "    sample_throughput: 100.539\n",
      "    sample_time_ms: 9946.344\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1632134894\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 318\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         3610.13</td><td style=\"text-align: right;\">318000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 319000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-48-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 320\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8037275910377502\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011329662377837361\n",
      "          policy_loss: -0.046173419720596735\n",
      "          total_loss: -0.058962669213198954\n",
      "          vf_explained_var: -0.493179053068161\n",
      "          vf_loss: 0.0006543302620735227\n",
      "    num_agent_steps_sampled: 319000\n",
      "    num_agent_steps_trained: 319000\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.306250000000006\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052792304704158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58734940474778\n",
      "    mean_inference_ms: 1.4081313066401107\n",
      "    mean_raw_obs_processing_ms: 0.6830261907992171\n",
      "  time_since_restore: 3620.8342835903168\n",
      "  time_this_iter_s: 10.705055236816406\n",
      "  time_total_s: 3620.8342835903168\n",
      "  timers:\n",
      "    learn_throughput: 1677.216\n",
      "    learn_time_ms: 596.226\n",
      "    load_throughput: 309563.292\n",
      "    load_time_ms: 3.23\n",
      "    sample_throughput: 100.124\n",
      "    sample_time_ms: 9987.653\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1632134905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         3620.83</td><td style=\"text-align: right;\">319000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-48-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 321\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9194135705629984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018963495354736626\n",
      "          policy_loss: -0.014875595023234685\n",
      "          total_loss: -0.0252363334927294\n",
      "          vf_explained_var: -0.22332662343978882\n",
      "          vf_loss: 0.0011445122091875723\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.20666666666666\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052879129775597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587461152868102\n",
      "    mean_inference_ms: 1.4081477275829655\n",
      "    mean_raw_obs_processing_ms: 0.6829775479417376\n",
      "  time_since_restore: 3631.7075226306915\n",
      "  time_this_iter_s: 10.873239040374756\n",
      "  time_total_s: 3631.7075226306915\n",
      "  timers:\n",
      "    learn_throughput: 1677.548\n",
      "    learn_time_ms: 596.108\n",
      "    load_throughput: 313262.579\n",
      "    load_time_ms: 3.192\n",
      "    sample_throughput: 99.676\n",
      "    sample_time_ms: 10032.52\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632134916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 320\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         3631.71</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 321000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-48-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 322\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6199654499689737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013707445279856106\n",
      "          policy_loss: -0.0027510821405384274\n",
      "          total_loss: -0.007260130387213495\n",
      "          vf_explained_var: 0.5955359935760498\n",
      "          vf_loss: 0.0061328223007472435\n",
      "    num_agent_steps_sampled: 321000\n",
      "    num_agent_steps_trained: 321000\n",
      "    num_steps_sampled: 321000\n",
      "    num_steps_trained: 321000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.18125\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04052966039646054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587571950591233\n",
      "    mean_inference_ms: 1.408163492265356\n",
      "    mean_raw_obs_processing_ms: 0.6829369810853858\n",
      "  time_since_restore: 3642.7025175094604\n",
      "  time_this_iter_s: 10.994994878768921\n",
      "  time_total_s: 3642.7025175094604\n",
      "  timers:\n",
      "    learn_throughput: 1681.43\n",
      "    learn_time_ms: 594.732\n",
      "    load_throughput: 313463.921\n",
      "    load_time_ms: 3.19\n",
      "    sample_throughput: 99.333\n",
      "    sample_time_ms: 10067.133\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1632134927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 321000\n",
      "  training_iteration: 321\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">          3642.7</td><td style=\"text-align: right;\">321000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 322000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 323\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4816246377097235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011028205241972146\n",
      "          policy_loss: -0.12422055666231446\n",
      "          total_loss: -0.13320479943520494\n",
      "          vf_explained_var: 0.24199725687503815\n",
      "          vf_loss: 0.0013605371744941093\n",
      "    num_agent_steps_sampled: 322000\n",
      "    num_agent_steps_trained: 322000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.90625\n",
      "    ram_util_percent: 66.5125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053050826652882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587670938959707\n",
      "    mean_inference_ms: 1.4081778349450895\n",
      "    mean_raw_obs_processing_ms: 0.6829032689751158\n",
      "  time_since_restore: 3653.654325246811\n",
      "  time_this_iter_s: 10.951807737350464\n",
      "  time_total_s: 3653.654325246811\n",
      "  timers:\n",
      "    learn_throughput: 1680.36\n",
      "    learn_time_ms: 595.111\n",
      "    load_throughput: 311837.208\n",
      "    load_time_ms: 3.207\n",
      "    sample_throughput: 99.212\n",
      "    sample_time_ms: 10079.403\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1632134938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         3653.65</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 323000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-49-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 324\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0810250394874148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012230150959284458\n",
      "          policy_loss: -0.25288767367601395\n",
      "          total_loss: -0.2578519797987408\n",
      "          vf_explained_var: -0.02381834201514721\n",
      "          vf_loss: 0.0008871434429440544\n",
      "    num_agent_steps_sampled: 323000\n",
      "    num_agent_steps_trained: 323000\n",
      "    num_steps_sampled: 323000\n",
      "    num_steps_trained: 323000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.28666666666667\n",
      "    ram_util_percent: 66.54\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040531359147973955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587785197018572\n",
      "    mean_inference_ms: 1.4081917709828402\n",
      "    mean_raw_obs_processing_ms: 0.6828761812785717\n",
      "  time_since_restore: 3664.3625440597534\n",
      "  time_this_iter_s: 10.708218812942505\n",
      "  time_total_s: 3664.3625440597534\n",
      "  timers:\n",
      "    learn_throughput: 1682.873\n",
      "    learn_time_ms: 594.222\n",
      "    load_throughput: 306404.068\n",
      "    load_time_ms: 3.264\n",
      "    sample_throughput: 98.877\n",
      "    sample_time_ms: 10113.591\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632134949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323000\n",
      "  training_iteration: 323\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         3664.36</td><td style=\"text-align: right;\">323000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-49-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 325\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7131269428465101\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013095579343094244\n",
      "          policy_loss: -0.06989991416533788\n",
      "          total_loss: -0.07824290895627604\n",
      "          vf_explained_var: 0.3987870216369629\n",
      "          vf_loss: 0.0034785797712781155\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.22\n",
      "    ram_util_percent: 66.53333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040532212012221215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58790335931691\n",
      "    mean_inference_ms: 1.4082051634136032\n",
      "    mean_raw_obs_processing_ms: 0.6828565875576509\n",
      "  time_since_restore: 3675.128385782242\n",
      "  time_this_iter_s: 10.765841722488403\n",
      "  time_total_s: 3675.128385782242\n",
      "  timers:\n",
      "    learn_throughput: 1685.014\n",
      "    learn_time_ms: 593.467\n",
      "    load_throughput: 306998.382\n",
      "    load_time_ms: 3.257\n",
      "    sample_throughput: 98.584\n",
      "    sample_time_ms: 10143.675\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632134959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 324\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         3675.13</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 325000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-49-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 326\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.870251903269026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0148079621586018\n",
      "          policy_loss: -0.07404629927542475\n",
      "          total_loss: -0.08430833361215062\n",
      "          vf_explained_var: 0.016492793336510658\n",
      "          vf_loss: 0.0024364891073976954\n",
      "    num_agent_steps_sampled: 325000\n",
      "    num_agent_steps_trained: 325000\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.23125\n",
      "    ram_util_percent: 66.525\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053306634641836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588022964684592\n",
      "    mean_inference_ms: 1.4082182779968446\n",
      "    mean_raw_obs_processing_ms: 0.6828435476682985\n",
      "  time_since_restore: 3686.074187040329\n",
      "  time_this_iter_s: 10.945801258087158\n",
      "  time_total_s: 3686.074187040329\n",
      "  timers:\n",
      "    learn_throughput: 1684.411\n",
      "    learn_time_ms: 593.679\n",
      "    load_throughput: 307455.212\n",
      "    load_time_ms: 3.253\n",
      "    sample_throughput: 98.192\n",
      "    sample_time_ms: 10184.092\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632134970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         3686.07</td><td style=\"text-align: right;\">325000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 326000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-49-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 327\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.886760343445672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01667941847036529\n",
      "          policy_loss: -0.036847026439176665\n",
      "          total_loss: -0.04725706121987767\n",
      "          vf_explained_var: -0.028647324070334435\n",
      "          vf_loss: 0.001694775273790583\n",
      "    num_agent_steps_sampled: 326000\n",
      "    num_agent_steps_trained: 326000\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.19375\n",
      "    ram_util_percent: 66.53125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040533918804396815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58812225072216\n",
      "    mean_inference_ms: 1.4082299896114039\n",
      "    mean_raw_obs_processing_ms: 0.6828367337743125\n",
      "  time_since_restore: 3697.013430118561\n",
      "  time_this_iter_s: 10.939243078231812\n",
      "  time_total_s: 3697.013430118561\n",
      "  timers:\n",
      "    learn_throughput: 1683.616\n",
      "    learn_time_ms: 593.96\n",
      "    load_throughput: 302538.572\n",
      "    load_time_ms: 3.305\n",
      "    sample_throughput: 97.862\n",
      "    sample_time_ms: 10218.506\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1632134981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 326\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         3697.01</td><td style=\"text-align: right;\">326000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 327000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-49-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 328\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8648561795552572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013438327318378759\n",
      "          policy_loss: -0.06949953602419959\n",
      "          total_loss: -0.08120179822047552\n",
      "          vf_explained_var: -0.3186487555503845\n",
      "          vf_loss: 0.0014976335783204477\n",
      "    num_agent_steps_sampled: 327000\n",
      "    num_agent_steps_trained: 327000\n",
      "    num_steps_sampled: 327000\n",
      "    num_steps_trained: 327000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.35999999999999\n",
      "    ram_util_percent: 66.51333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053476467121892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588218463779292\n",
      "    mean_inference_ms: 1.4082411889032012\n",
      "    mean_raw_obs_processing_ms: 0.6828363204928475\n",
      "  time_since_restore: 3707.9130668640137\n",
      "  time_this_iter_s: 10.89963674545288\n",
      "  time_total_s: 3707.9130668640137\n",
      "  timers:\n",
      "    learn_throughput: 1680.848\n",
      "    learn_time_ms: 594.938\n",
      "    load_throughput: 298565.225\n",
      "    load_time_ms: 3.349\n",
      "    sample_throughput: 97.703\n",
      "    sample_time_ms: 10235.084\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1632134992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327000\n",
      "  training_iteration: 327\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         3707.91</td><td style=\"text-align: right;\">327000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-50-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 329\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9612156179216174\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011606822977985824\n",
      "          policy_loss: -0.027778062990142238\n",
      "          total_loss: -0.04112515008697907\n",
      "          vf_explained_var: -0.14528556168079376\n",
      "          vf_loss: 0.0015589984877604163\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.03333333333333\n",
      "    ram_util_percent: 66.46666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040535612071476976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588298236878174\n",
      "    mean_inference_ms: 1.4082513930704206\n",
      "    mean_raw_obs_processing_ms: 0.6828423567437936\n",
      "  time_since_restore: 3718.497889518738\n",
      "  time_this_iter_s: 10.584822654724121\n",
      "  time_total_s: 3718.497889518738\n",
      "  timers:\n",
      "    learn_throughput: 1678.054\n",
      "    learn_time_ms: 595.928\n",
      "    load_throughput: 297428.29\n",
      "    load_time_ms: 3.362\n",
      "    sample_throughput: 97.728\n",
      "    sample_time_ms: 10232.522\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632135003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">          3718.5</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 329000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-50-31\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 330\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6474666661686368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015174591815599816\n",
      "          policy_loss: -0.0740691903564665\n",
      "          total_loss: -0.08183638652165731\n",
      "          vf_explained_var: 0.05016997829079628\n",
      "          vf_loss: 0.0025548203266225755\n",
      "    num_agent_steps_sampled: 329000\n",
      "    num_agent_steps_trained: 329000\n",
      "    num_steps_sampled: 329000\n",
      "    num_steps_trained: 329000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.02439024390243\n",
      "    ram_util_percent: 66.4\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053645341838333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588350823637851\n",
      "    mean_inference_ms: 1.4082600297433445\n",
      "    mean_raw_obs_processing_ms: 0.6833962023383393\n",
      "  time_since_restore: 3746.901195049286\n",
      "  time_this_iter_s: 28.403305530548096\n",
      "  time_total_s: 3746.901195049286\n",
      "  timers:\n",
      "    learn_throughput: 1678.535\n",
      "    learn_time_ms: 595.758\n",
      "    load_throughput: 213223.798\n",
      "    load_time_ms: 4.69\n",
      "    sample_throughput: 83.325\n",
      "    sample_time_ms: 12001.18\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632135031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329000\n",
      "  training_iteration: 329\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">          3746.9</td><td style=\"text-align: right;\">329000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-50-41\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 331\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0169853700531855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00859781444803383\n",
      "          policy_loss: -0.0848570140104534\n",
      "          total_loss: -0.10067851868872013\n",
      "          vf_explained_var: -0.6564794182777405\n",
      "          vf_loss: 0.0008623032371461805\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.07857142857143\n",
      "    ram_util_percent: 66.34999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053728912511831\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588373893136161\n",
      "    mean_inference_ms: 1.408267362176162\n",
      "    mean_raw_obs_processing_ms: 0.6839546694037201\n",
      "  time_since_restore: 3757.0759303569794\n",
      "  time_this_iter_s: 10.174735307693481\n",
      "  time_total_s: 3757.0759303569794\n",
      "  timers:\n",
      "    learn_throughput: 1677.584\n",
      "    learn_time_ms: 596.095\n",
      "    load_throughput: 212751.159\n",
      "    load_time_ms: 4.7\n",
      "    sample_throughput: 83.815\n",
      "    sample_time_ms: 11930.997\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632135041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 330\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         3757.08</td><td style=\"text-align: right;\">330000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 331000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-50-52\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 332\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5329846249686347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03949135070864563\n",
      "          policy_loss: -0.039820499821669526\n",
      "          total_loss: -0.02841847269899315\n",
      "          vf_explained_var: 0.15261149406433105\n",
      "          vf_loss: 0.01071981567527271\n",
      "    num_agent_steps_sampled: 331000\n",
      "    num_agent_steps_trained: 331000\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.620000000000005\n",
      "    ram_util_percent: 66.35999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053812432634229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588354936470548\n",
      "    mean_inference_ms: 1.4082730621125557\n",
      "    mean_raw_obs_processing_ms: 0.6845176720179438\n",
      "  time_since_restore: 3767.363397359848\n",
      "  time_this_iter_s: 10.287467002868652\n",
      "  time_total_s: 3767.363397359848\n",
      "  timers:\n",
      "    learn_throughput: 1675.82\n",
      "    learn_time_ms: 596.723\n",
      "    load_throughput: 212746.843\n",
      "    load_time_ms: 4.7\n",
      "    sample_throughput: 84.32\n",
      "    sample_time_ms: 11859.635\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1632135052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         3767.36</td><td style=\"text-align: right;\">331000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-51-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 333\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.979060267077552\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009016473082263943\n",
      "          policy_loss: -0.10538320764899253\n",
      "          total_loss: -0.11816666159364912\n",
      "          vf_explained_var: -0.2855156660079956\n",
      "          vf_loss: 0.0015234566325994414\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.833333333333336\n",
      "    ram_util_percent: 66.36666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053895147054949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588309371044973\n",
      "    mean_inference_ms: 1.4082776472176695\n",
      "    mean_raw_obs_processing_ms: 0.6850847724250984\n",
      "  time_since_restore: 3777.598438024521\n",
      "  time_this_iter_s: 10.235040664672852\n",
      "  time_total_s: 3777.598438024521\n",
      "  timers:\n",
      "    learn_throughput: 1679.096\n",
      "    learn_time_ms: 595.559\n",
      "    load_throughput: 213023.454\n",
      "    load_time_ms: 4.694\n",
      "    sample_throughput: 84.824\n",
      "    sample_time_ms: 11789.141\n",
      "    update_time_ms: 1.748\n",
      "  timestamp: 1632135062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 332\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">          3777.6</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 333000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-51-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 334\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.688345472017924\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01019368195598916\n",
      "          policy_loss: -0.07773552669419183\n",
      "          total_loss: -0.08618392737375366\n",
      "          vf_explained_var: -0.199337899684906\n",
      "          vf_loss: 0.0022354034357704223\n",
      "    num_agent_steps_sampled: 333000\n",
      "    num_agent_steps_trained: 333000\n",
      "    num_steps_sampled: 333000\n",
      "    num_steps_trained: 333000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.628571428571426\n",
      "    ram_util_percent: 66.34999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053977362287716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588240707184157\n",
      "    mean_inference_ms: 1.4082812309833974\n",
      "    mean_raw_obs_processing_ms: 0.6856562372057797\n",
      "  time_since_restore: 3787.7165598869324\n",
      "  time_this_iter_s: 10.118121862411499\n",
      "  time_total_s: 3787.7165598869324\n",
      "  timers:\n",
      "    learn_throughput: 1677.899\n",
      "    learn_time_ms: 595.983\n",
      "    load_throughput: 215665.409\n",
      "    load_time_ms: 4.637\n",
      "    sample_throughput: 85.253\n",
      "    sample_time_ms: 11729.766\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1632135072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333000\n",
      "  training_iteration: 333\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         3787.72</td><td style=\"text-align: right;\">333000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 334000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-51-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 335\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6830611824989319\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009091837603557036\n",
      "          policy_loss: -0.0014435506322317652\n",
      "          total_loss: -0.01081117902778917\n",
      "          vf_explained_var: 0.3212560713291168\n",
      "          vf_loss: 0.0019334549653447337\n",
      "    num_agent_steps_sampled: 334000\n",
      "    num_agent_steps_trained: 334000\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.226666666666674\n",
      "    ram_util_percent: 66.24666666666668\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054058542078499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588142803943205\n",
      "    mean_inference_ms: 1.408283128067696\n",
      "    mean_raw_obs_processing_ms: 0.6862318024476821\n",
      "  time_since_restore: 3798.0324823856354\n",
      "  time_this_iter_s: 10.315922498703003\n",
      "  time_total_s: 3798.0324823856354\n",
      "  timers:\n",
      "    learn_throughput: 1677.87\n",
      "    learn_time_ms: 595.994\n",
      "    load_throughput: 215273.563\n",
      "    load_time_ms: 4.645\n",
      "    sample_throughput: 85.582\n",
      "    sample_time_ms: 11684.762\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1632135082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         3798.03</td><td style=\"text-align: right;\">334000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 335000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-51-33\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 336\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9292768438657124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009369979300950978\n",
      "          policy_loss: -0.08377193667822413\n",
      "          total_loss: -0.0966934902800454\n",
      "          vf_explained_var: -0.5132607221603394\n",
      "          vf_loss: 0.0006725247419025335\n",
      "    num_agent_steps_sampled: 335000\n",
      "    num_agent_steps_trained: 335000\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.27333333333333\n",
      "    ram_util_percent: 66.10666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054139153202887\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588011665630933\n",
      "    mean_inference_ms: 1.408283507179476\n",
      "    mean_raw_obs_processing_ms: 0.6868108776303687\n",
      "  time_since_restore: 3808.355881690979\n",
      "  time_this_iter_s: 10.323399305343628\n",
      "  time_total_s: 3808.355881690979\n",
      "  timers:\n",
      "    learn_throughput: 1677.969\n",
      "    learn_time_ms: 595.958\n",
      "    load_throughput: 213741.012\n",
      "    load_time_ms: 4.679\n",
      "    sample_throughput: 86.04\n",
      "    sample_time_ms: 11622.536\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1632135093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 335\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         3808.36</td><td style=\"text-align: right;\">335000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-51-43\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 337\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6856680909792583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011008265454796036\n",
      "          policy_loss: -0.08503080142868889\n",
      "          total_loss: -0.08824968057063719\n",
      "          vf_explained_var: -0.3232773542404175\n",
      "          vf_loss: 0.006942731771980308\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.48666666666667\n",
      "    ram_util_percent: 66.09333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054219716417693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587845834867005\n",
      "    mean_inference_ms: 1.4082823535296234\n",
      "    mean_raw_obs_processing_ms: 0.6873939648979502\n",
      "  time_since_restore: 3818.8298163414\n",
      "  time_this_iter_s: 10.473934650421143\n",
      "  time_total_s: 3818.8298163414\n",
      "  timers:\n",
      "    learn_throughput: 1681.489\n",
      "    learn_time_ms: 594.711\n",
      "    load_throughput: 213168.53\n",
      "    load_time_ms: 4.691\n",
      "    sample_throughput: 86.376\n",
      "    sample_time_ms: 11577.226\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1632135103\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 336\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         3818.83</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 337000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-51-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 338\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2228057709005145\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009186776838152848\n",
      "          policy_loss: -0.09368549858530363\n",
      "          total_loss: -0.09760434627532959\n",
      "          vf_explained_var: 0.1517707258462906\n",
      "          vf_loss: 0.0027219413404559922\n",
      "    num_agent_steps_sampled: 337000\n",
      "    num_agent_steps_trained: 337000\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.26428571428572\n",
      "    ram_util_percent: 66.03571428571429\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405429905590763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587662917821433\n",
      "    mean_inference_ms: 1.4082800026179643\n",
      "    mean_raw_obs_processing_ms: 0.6879808119728724\n",
      "  time_since_restore: 3829.1513090133667\n",
      "  time_this_iter_s: 10.321492671966553\n",
      "  time_total_s: 3829.1513090133667\n",
      "  timers:\n",
      "    learn_throughput: 1682.428\n",
      "    learn_time_ms: 594.379\n",
      "    load_throughput: 214901.856\n",
      "    load_time_ms: 4.653\n",
      "    sample_throughput: 86.807\n",
      "    sample_time_ms: 11519.778\n",
      "    update_time_ms: 1.736\n",
      "  timestamp: 1632135114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         3829.15</td><td style=\"text-align: right;\">337000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 338000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-52-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 339\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8984643353356256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008881942140023035\n",
      "          policy_loss: -0.07452462166547776\n",
      "          total_loss: -0.0873049985203478\n",
      "          vf_explained_var: -0.47122472524642944\n",
      "          vf_loss: 0.0008023935438056166\n",
      "    num_agent_steps_sampled: 338000\n",
      "    num_agent_steps_trained: 338000\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.23333333333334\n",
      "    ram_util_percent: 66.03333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054377045215882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587458938913741\n",
      "    mean_inference_ms: 1.4082767218998544\n",
      "    mean_raw_obs_processing_ms: 0.6878379110373348\n",
      "  time_since_restore: 3839.3870000839233\n",
      "  time_this_iter_s: 10.23569107055664\n",
      "  time_total_s: 3839.3870000839233\n",
      "  timers:\n",
      "    learn_throughput: 1683.062\n",
      "    learn_time_ms: 594.155\n",
      "    load_throughput: 215655.429\n",
      "    load_time_ms: 4.637\n",
      "    sample_throughput: 87.069\n",
      "    sample_time_ms: 11485.117\n",
      "    update_time_ms: 1.736\n",
      "  timestamp: 1632135124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 338\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         3839.39</td><td style=\"text-align: right;\">338000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 339000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-52-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 340\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2264016065332624\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005519062960332929\n",
      "          policy_loss: -0.10677755607499016\n",
      "          total_loss: -0.11483494308259752\n",
      "          vf_explained_var: -0.3203792870044708\n",
      "          vf_loss: 0.0008500132628897619\n",
      "    num_agent_steps_sampled: 339000\n",
      "    num_agent_steps_trained: 339000\n",
      "    num_steps_sampled: 339000\n",
      "    num_steps_trained: 339000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.13333333333334\n",
      "    ram_util_percent: 66.05333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040544545271327614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587244953371682\n",
      "    mean_inference_ms: 1.4082728197240548\n",
      "    mean_raw_obs_processing_ms: 0.6876780224269738\n",
      "  time_since_restore: 3849.8065118789673\n",
      "  time_this_iter_s: 10.419511795043945\n",
      "  time_total_s: 3849.8065118789673\n",
      "  timers:\n",
      "    learn_throughput: 1683.989\n",
      "    learn_time_ms: 593.828\n",
      "    load_throughput: 303052.268\n",
      "    load_time_ms: 3.3\n",
      "    sample_throughput: 103.216\n",
      "    sample_time_ms: 9688.438\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632135134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339000\n",
      "  training_iteration: 339\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         3849.81</td><td style=\"text-align: right;\">339000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-52-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 341\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.025268618265788\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010576220102798621\n",
      "          policy_loss: 0.025785790632168452\n",
      "          total_loss: 0.012534950425227483\n",
      "          vf_explained_var: -0.5918915867805481\n",
      "          vf_loss: 0.000569538078788254\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.32\n",
      "    ram_util_percent: 66.06\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054531495860151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58696863384658\n",
      "    mean_inference_ms: 1.4082685714151524\n",
      "    mean_raw_obs_processing_ms: 0.6875242979837176\n",
      "  time_since_restore: 3860.1489536762238\n",
      "  time_this_iter_s: 10.34244179725647\n",
      "  time_total_s: 3860.1489536762238\n",
      "  timers:\n",
      "    learn_throughput: 1683.437\n",
      "    learn_time_ms: 594.023\n",
      "    load_throughput: 303477.657\n",
      "    load_time_ms: 3.295\n",
      "    sample_throughput: 103.04\n",
      "    sample_time_ms: 9704.986\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1632135145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         3860.15</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 341000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-52-35\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 342\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.661635650528802\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011228748196677607\n",
      "          policy_loss: -0.04274010078774558\n",
      "          total_loss: -0.051156511571672225\n",
      "          vf_explained_var: -0.3397802412509918\n",
      "          vf_loss: 0.0013707781695605566\n",
      "    num_agent_steps_sampled: 341000\n",
      "    num_agent_steps_trained: 341000\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.37333333333333\n",
      "    ram_util_percent: 66.10000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040546074076026235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.586684837327235\n",
      "    mean_inference_ms: 1.4082636168114147\n",
      "    mean_raw_obs_processing_ms: 0.6873776641688617\n",
      "  time_since_restore: 3870.536210298538\n",
      "  time_this_iter_s: 10.387256622314453\n",
      "  time_total_s: 3870.536210298538\n",
      "  timers:\n",
      "    learn_throughput: 1683.526\n",
      "    learn_time_ms: 593.991\n",
      "    load_throughput: 302394.613\n",
      "    load_time_ms: 3.307\n",
      "    sample_throughput: 102.935\n",
      "    sample_time_ms: 9714.883\n",
      "    update_time_ms: 1.747\n",
      "  timestamp: 1632135155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 341\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         3870.54</td><td style=\"text-align: right;\">341000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 342000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-52-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 343\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8306031505266824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00899878143239209\n",
      "          policy_loss: 0.09700491498741838\n",
      "          total_loss: 0.08511161055001948\n",
      "          vf_explained_var: -0.008797891438007355\n",
      "          vf_loss: 0.0009397945150137982\n",
      "    num_agent_steps_sampled: 342000\n",
      "    num_agent_steps_trained: 342000\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.266666666666666\n",
      "    ram_util_percent: 66.14000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040546820086332994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.586398462868354\n",
      "    mean_inference_ms: 1.4082573836965238\n",
      "    mean_raw_obs_processing_ms: 0.6872362298286405\n",
      "  time_since_restore: 3881.0075132846832\n",
      "  time_this_iter_s: 10.47130298614502\n",
      "  time_total_s: 3881.0075132846832\n",
      "  timers:\n",
      "    learn_throughput: 1681.901\n",
      "    learn_time_ms: 594.565\n",
      "    load_throughput: 303328.416\n",
      "    load_time_ms: 3.297\n",
      "    sample_throughput: 102.691\n",
      "    sample_time_ms: 9737.937\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1632135166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 342\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         3881.01</td><td style=\"text-align: right;\">342000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 343000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-52-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 344\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3523992829852634\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010225516171985147\n",
      "          policy_loss: -0.10172886732551786\n",
      "          total_loss: -0.10710753616359499\n",
      "          vf_explained_var: -0.7604258060455322\n",
      "          vf_loss: 0.0019263081795846424\n",
      "    num_agent_steps_sampled: 343000\n",
      "    num_agent_steps_trained: 343000\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.95333333333334\n",
      "    ram_util_percent: 66.23333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054756838674401\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.586104820200084\n",
      "    mean_inference_ms: 1.4082507798884407\n",
      "    mean_raw_obs_processing_ms: 0.687100285774371\n",
      "  time_since_restore: 3891.4128453731537\n",
      "  time_this_iter_s: 10.405332088470459\n",
      "  time_total_s: 3891.4128453731537\n",
      "  timers:\n",
      "    learn_throughput: 1679.124\n",
      "    learn_time_ms: 595.549\n",
      "    load_throughput: 303363.518\n",
      "    load_time_ms: 3.296\n",
      "    sample_throughput: 102.4\n",
      "    sample_time_ms: 9765.668\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632135176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         3891.41</td><td style=\"text-align: right;\">343000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-53-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 345\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7761719796392652\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008415811103602582\n",
      "          policy_loss: 0.012974415346980095\n",
      "          total_loss: 0.002208592463284731\n",
      "          vf_explained_var: -0.9656311869621277\n",
      "          vf_loss: 0.001877517158087964\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.39999999999999\n",
      "    ram_util_percent: 66.32142857142856\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040548309046202435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.585791591855502\n",
      "    mean_inference_ms: 1.4082435916589378\n",
      "    mean_raw_obs_processing_ms: 0.6869688243862561\n",
      "  time_since_restore: 3901.742782354355\n",
      "  time_this_iter_s: 10.329936981201172\n",
      "  time_total_s: 3901.742782354355\n",
      "  timers:\n",
      "    learn_throughput: 1677.069\n",
      "    learn_time_ms: 596.278\n",
      "    load_throughput: 303405.213\n",
      "    load_time_ms: 3.296\n",
      "    sample_throughput: 102.393\n",
      "    sample_time_ms: 9766.323\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632135186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 344\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         3901.74</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-53-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 346\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3364394161436293\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005736783990244179\n",
      "          policy_loss: 0.03589568022224638\n",
      "          total_loss: 0.026756393329964743\n",
      "          vf_explained_var: -0.4243237376213074\n",
      "          vf_loss: 0.0007360744138067174\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.24666666666666\n",
      "    ram_util_percent: 66.38666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040549047811371794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.585456641115636\n",
      "    mean_inference_ms: 1.408235721415857\n",
      "    mean_raw_obs_processing_ms: 0.6868435103618714\n",
      "  time_since_restore: 3912.0964863300323\n",
      "  time_this_iter_s: 10.35370397567749\n",
      "  time_total_s: 3912.0964863300323\n",
      "  timers:\n",
      "    learn_throughput: 1676.261\n",
      "    learn_time_ms: 596.566\n",
      "    load_throughput: 306319.034\n",
      "    load_time_ms: 3.265\n",
      "    sample_throughput: 102.364\n",
      "    sample_time_ms: 9769.106\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632135197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 345\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">          3912.1</td><td style=\"text-align: right;\">345000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 346000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-53-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 347\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5573357396655612\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0072809228044818835\n",
      "          policy_loss: -0.058141652189402114\n",
      "          total_loss: -0.06822809044064747\n",
      "          vf_explained_var: 0.010736346244812012\n",
      "          vf_loss: 0.0010587605570132534\n",
      "    num_agent_steps_sampled: 346000\n",
      "    num_agent_steps_trained: 346000\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.34\n",
      "    ram_util_percent: 66.39999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04054977847749904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.585105937375925\n",
      "    mean_inference_ms: 1.4082269331964994\n",
      "    mean_raw_obs_processing_ms: 0.6867247452061277\n",
      "  time_since_restore: 3922.458319425583\n",
      "  time_this_iter_s: 10.361833095550537\n",
      "  time_total_s: 3922.458319425583\n",
      "  timers:\n",
      "    learn_throughput: 1676.471\n",
      "    learn_time_ms: 596.491\n",
      "    load_throughput: 310126.363\n",
      "    load_time_ms: 3.224\n",
      "    sample_throughput: 102.48\n",
      "    sample_time_ms: 9758.037\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632135207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         3922.46</td><td style=\"text-align: right;\">346000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 347000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 348\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7233409431245592\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008031253167177689\n",
      "          policy_loss: -0.09389852492345704\n",
      "          total_loss: -0.10500289855731858\n",
      "          vf_explained_var: -0.02353358082473278\n",
      "          vf_loss: 0.0012445413378170795\n",
      "    num_agent_steps_sampled: 347000\n",
      "    num_agent_steps_trained: 347000\n",
      "    num_steps_sampled: 347000\n",
      "    num_steps_trained: 347000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.506666666666675\n",
      "    ram_util_percent: 66.43333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055050445551533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.584732729108653\n",
      "    mean_inference_ms: 1.408217366507801\n",
      "    mean_raw_obs_processing_ms: 0.6866101740862891\n",
      "  time_since_restore: 3932.7422692775726\n",
      "  time_this_iter_s: 10.283949851989746\n",
      "  time_total_s: 3932.7422692775726\n",
      "  timers:\n",
      "    learn_throughput: 1676.416\n",
      "    learn_time_ms: 596.51\n",
      "    load_throughput: 310735.22\n",
      "    load_time_ms: 3.218\n",
      "    sample_throughput: 102.519\n",
      "    sample_time_ms: 9754.26\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632135217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347000\n",
      "  training_iteration: 347\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         3932.74</td><td style=\"text-align: right;\">347000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-53-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 349\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3245220455858442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013344929213880037\n",
      "          policy_loss: -0.017315981537103654\n",
      "          total_loss: -0.021244467629326715\n",
      "          vf_explained_var: -0.8706501722335815\n",
      "          vf_loss: 0.001200534233228407\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.519999999999996\n",
      "    ram_util_percent: 66.48\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055122561191172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58436099631841\n",
      "    mean_inference_ms: 1.4082071619296606\n",
      "    mean_raw_obs_processing_ms: 0.686502951383837\n",
      "  time_since_restore: 3943.224653482437\n",
      "  time_this_iter_s: 10.482384204864502\n",
      "  time_total_s: 3943.224653482437\n",
      "  timers:\n",
      "    learn_throughput: 1675.673\n",
      "    learn_time_ms: 596.775\n",
      "    load_throughput: 310280.075\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 102.263\n",
      "    sample_time_ms: 9778.669\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632135228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 348\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         3943.22</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 349000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-53-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 350\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5082452456156412\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013357533656562066\n",
      "          policy_loss: -0.018051489111449985\n",
      "          total_loss: -0.023323331276575723\n",
      "          vf_explained_var: -0.44739672541618347\n",
      "          vf_loss: 0.001686745316773239\n",
      "    num_agent_steps_sampled: 349000\n",
      "    num_agent_steps_trained: 349000\n",
      "    num_steps_sampled: 349000\n",
      "    num_steps_trained: 349000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.2\n",
      "    ram_util_percent: 66.52142857142857\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040551957613124176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58404595559539\n",
      "    mean_inference_ms: 1.4081972965508343\n",
      "    mean_raw_obs_processing_ms: 0.6864027846485564\n",
      "  time_since_restore: 3953.600754737854\n",
      "  time_this_iter_s: 10.37610125541687\n",
      "  time_total_s: 3953.600754737854\n",
      "  timers:\n",
      "    learn_throughput: 1672.6\n",
      "    learn_time_ms: 597.872\n",
      "    load_throughput: 310544.265\n",
      "    load_time_ms: 3.22\n",
      "    sample_throughput: 102.32\n",
      "    sample_time_ms: 9773.223\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1632135238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349000\n",
      "  training_iteration: 349\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">          3953.6</td><td style=\"text-align: right;\">349000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 350000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-54-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 351\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3890292432573106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00835044686705161\n",
      "          policy_loss: -0.086578376011716\n",
      "          total_loss: -0.0935351861640811\n",
      "          vf_explained_var: 0.2791643440723419\n",
      "          vf_loss: 0.0018548576361758428\n",
      "    num_agent_steps_sampled: 350000\n",
      "    num_agent_steps_trained: 350000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.8235294117647\n",
      "    ram_util_percent: 67.00588235294117\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055274843810309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58379172524862\n",
      "    mean_inference_ms: 1.4081893801928238\n",
      "    mean_raw_obs_processing_ms: 0.6863058046430878\n",
      "  time_since_restore: 3964.916693210602\n",
      "  time_this_iter_s: 11.315938472747803\n",
      "  time_total_s: 3964.916693210602\n",
      "  timers:\n",
      "    learn_throughput: 1660.15\n",
      "    learn_time_ms: 602.355\n",
      "    load_throughput: 310917.191\n",
      "    load_time_ms: 3.216\n",
      "    sample_throughput: 101.359\n",
      "    sample_time_ms: 9865.922\n",
      "    update_time_ms: 1.843\n",
      "  timestamp: 1632135250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         3964.92</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 351000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-54-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 352\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.797961163520813\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0127029634348449\n",
      "          policy_loss: -0.05308809619810846\n",
      "          total_loss: -0.06178599341462056\n",
      "          vf_explained_var: -0.6081445813179016\n",
      "          vf_loss: 0.001555949338944629\n",
      "    num_agent_steps_sampled: 351000\n",
      "    num_agent_steps_trained: 351000\n",
      "    num_steps_sampled: 351000\n",
      "    num_steps_trained: 351000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.006249999999994\n",
      "    ram_util_percent: 66.85625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055356554792434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583538654996424\n",
      "    mean_inference_ms: 1.4081817186494865\n",
      "    mean_raw_obs_processing_ms: 0.6862128581207806\n",
      "  time_since_restore: 3976.056656599045\n",
      "  time_this_iter_s: 11.139963388442993\n",
      "  time_total_s: 3976.056656599045\n",
      "  timers:\n",
      "    learn_throughput: 1649.267\n",
      "    learn_time_ms: 606.33\n",
      "    load_throughput: 305446.813\n",
      "    load_time_ms: 3.274\n",
      "    sample_throughput: 100.632\n",
      "    sample_time_ms: 9937.225\n",
      "    update_time_ms: 1.848\n",
      "  timestamp: 1632135261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351000\n",
      "  training_iteration: 351\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         3976.06</td><td style=\"text-align: right;\">351000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-54-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 353\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7781921717855664\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01056886362011518\n",
      "          policy_loss: -0.07901222080820136\n",
      "          total_loss: -0.08897791153026952\n",
      "          vf_explained_var: -0.9851500391960144\n",
      "          vf_loss: 0.0013883986639686757\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.446666666666665\n",
      "    ram_util_percent: 66.81999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040554413038053835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583269259253571\n",
      "    mean_inference_ms: 1.408174401396206\n",
      "    mean_raw_obs_processing_ms: 0.6861236739536138\n",
      "  time_since_restore: 3986.71630358696\n",
      "  time_this_iter_s: 10.659646987915039\n",
      "  time_total_s: 3986.71630358696\n",
      "  timers:\n",
      "    learn_throughput: 1640.682\n",
      "    learn_time_ms: 609.503\n",
      "    load_throughput: 304195.181\n",
      "    load_time_ms: 3.287\n",
      "    sample_throughput: 100.474\n",
      "    sample_time_ms: 9952.858\n",
      "    update_time_ms: 1.844\n",
      "  timestamp: 1632135272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 352\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         3986.72</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 353000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-54-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 354\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2823723395665487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010755297319974844\n",
      "          policy_loss: -0.014616735610697004\n",
      "          total_loss: -0.018549224237600963\n",
      "          vf_explained_var: 0.27713140845298767\n",
      "          vf_loss: 0.00235001179907057\n",
      "    num_agent_steps_sampled: 353000\n",
      "    num_agent_steps_trained: 353000\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.61333333333334\n",
      "    ram_util_percent: 66.79333333333331\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055526515703814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58298082678891\n",
      "    mean_inference_ms: 1.4081666917166757\n",
      "    mean_raw_obs_processing_ms: 0.686041147883428\n",
      "  time_since_restore: 3997.0496003627777\n",
      "  time_this_iter_s: 10.333296775817871\n",
      "  time_total_s: 3997.0496003627777\n",
      "  timers:\n",
      "    learn_throughput: 1644.185\n",
      "    learn_time_ms: 608.204\n",
      "    load_throughput: 304747.733\n",
      "    load_time_ms: 3.281\n",
      "    sample_throughput: 100.533\n",
      "    sample_time_ms: 9946.977\n",
      "    update_time_ms: 1.849\n",
      "  timestamp: 1632135282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         3997.05</td><td style=\"text-align: right;\">353000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 354000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-54-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 355\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2231032894717322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00983837545099217\n",
      "          policy_loss: -0.09316289871931076\n",
      "          total_loss: -0.09786164263884227\n",
      "          vf_explained_var: 0.22345517575740814\n",
      "          vf_loss: 0.001548726023368848\n",
      "    num_agent_steps_sampled: 354000\n",
      "    num_agent_steps_trained: 354000\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.22857142857141\n",
      "    ram_util_percent: 66.79999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055611404453799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58273555487342\n",
      "    mean_inference_ms: 1.408158853991444\n",
      "    mean_raw_obs_processing_ms: 0.6859653351924418\n",
      "  time_since_restore: 4007.464471101761\n",
      "  time_this_iter_s: 10.414870738983154\n",
      "  time_total_s: 4007.464471101761\n",
      "  timers:\n",
      "    learn_throughput: 1646.228\n",
      "    learn_time_ms: 607.449\n",
      "    load_throughput: 304391.66\n",
      "    load_time_ms: 3.285\n",
      "    sample_throughput: 100.461\n",
      "    sample_time_ms: 9954.08\n",
      "    update_time_ms: 1.85\n",
      "  timestamp: 1632135292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 354\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         4007.46</td><td style=\"text-align: right;\">354000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 355000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-55-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 356\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6621894001960755\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013192824748083767\n",
      "          policy_loss: -0.026060366278721228\n",
      "          total_loss: -0.033734940530525316\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009236289576316873\n",
      "    num_agent_steps_sampled: 355000\n",
      "    num_agent_steps_trained: 355000\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.033333333333324\n",
      "    ram_util_percent: 66.80666666666664\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055694883515862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.582502103785643\n",
      "    mean_inference_ms: 1.4081497037320907\n",
      "    mean_raw_obs_processing_ms: 0.6858946886326901\n",
      "  time_since_restore: 4017.785484790802\n",
      "  time_this_iter_s: 10.321013689041138\n",
      "  time_total_s: 4017.785484790802\n",
      "  timers:\n",
      "    learn_throughput: 1646.59\n",
      "    learn_time_ms: 607.316\n",
      "    load_throughput: 304477.837\n",
      "    load_time_ms: 3.284\n",
      "    sample_throughput: 100.493\n",
      "    sample_time_ms: 9950.933\n",
      "    update_time_ms: 1.854\n",
      "  timestamp: 1632135303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         4017.79</td><td style=\"text-align: right;\">355000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-55-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 357\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7551595316992865\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008348230566311917\n",
      "          policy_loss: -0.04212605512390534\n",
      "          total_loss: -0.05324752912339237\n",
      "          vf_explained_var: -0.5034589171409607\n",
      "          vf_loss: 0.0013528449677525916\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.50666666666666\n",
      "    ram_util_percent: 66.79999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055777644947356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.582249033337249\n",
      "    mean_inference_ms: 1.4081397574075745\n",
      "    mean_raw_obs_processing_ms: 0.6858272982587007\n",
      "  time_since_restore: 4028.062188386917\n",
      "  time_this_iter_s: 10.276703596115112\n",
      "  time_total_s: 4028.062188386917\n",
      "  timers:\n",
      "    learn_throughput: 1646.574\n",
      "    learn_time_ms: 607.322\n",
      "    load_throughput: 307480.005\n",
      "    load_time_ms: 3.252\n",
      "    sample_throughput: 100.579\n",
      "    sample_time_ms: 9942.437\n",
      "    update_time_ms: 1.857\n",
      "  timestamp: 1632135313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 356\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         4028.06</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 357000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-55-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 358\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7473497986793518\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01185837671313838\n",
      "          policy_loss: -0.05701265782117844\n",
      "          total_loss: -0.06591226856948601\n",
      "          vf_explained_var: -0.838439404964447\n",
      "          vf_loss: 0.001361789150784413\n",
      "    num_agent_steps_sampled: 357000\n",
      "    num_agent_steps_trained: 357000\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.36000000000001\n",
      "    ram_util_percent: 66.79999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055860413784417\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581992092123235\n",
      "    mean_inference_ms: 1.4081296787776054\n",
      "    mean_raw_obs_processing_ms: 0.6857665999071153\n",
      "  time_since_restore: 4038.715152025223\n",
      "  time_this_iter_s: 10.652963638305664\n",
      "  time_total_s: 4038.715152025223\n",
      "  timers:\n",
      "    learn_throughput: 1645.354\n",
      "    learn_time_ms: 607.772\n",
      "    load_throughput: 306171.455\n",
      "    load_time_ms: 3.266\n",
      "    sample_throughput: 100.212\n",
      "    sample_time_ms: 9978.886\n",
      "    update_time_ms: 1.863\n",
      "  timestamp: 1632135324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 357\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         4038.72</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 358000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-55-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 359\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.735439772076077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010391633076366327\n",
      "          policy_loss: -0.09072039205994871\n",
      "          total_loss: -0.09909183792769909\n",
      "          vf_explained_var: -0.5399098992347717\n",
      "          vf_loss: 0.002662905705316613\n",
      "    num_agent_steps_sampled: 358000\n",
      "    num_agent_steps_trained: 358000\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.33333333333334\n",
      "    ram_util_percent: 66.78666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055942627109988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581719551449918\n",
      "    mean_inference_ms: 1.408118769716043\n",
      "    mean_raw_obs_processing_ms: 0.6857120612111038\n",
      "  time_since_restore: 4049.080975294113\n",
      "  time_this_iter_s: 10.36582326889038\n",
      "  time_total_s: 4049.080975294113\n",
      "  timers:\n",
      "    learn_throughput: 1646.443\n",
      "    learn_time_ms: 607.37\n",
      "    load_throughput: 306661.695\n",
      "    load_time_ms: 3.261\n",
      "    sample_throughput: 100.325\n",
      "    sample_time_ms: 9967.618\n",
      "    update_time_ms: 1.873\n",
      "  timestamp: 1632135334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         4049.08</td><td style=\"text-align: right;\">358000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 359000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-56-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 360\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8907342301474677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010381341907389293\n",
      "          policy_loss: 0.006982209616237216\n",
      "          total_loss: -0.004506669814387957\n",
      "          vf_explained_var: -0.6952505111694336\n",
      "          vf_loss: 0.0011046747896923787\n",
      "    num_agent_steps_sampled: 359000\n",
      "    num_agent_steps_trained: 359000\n",
      "    num_steps_sampled: 359000\n",
      "    num_steps_trained: 359000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.06829268292683\n",
      "    ram_util_percent: 66.74390243902438\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056025017218258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581447834465928\n",
      "    mean_inference_ms: 1.4081082875651167\n",
      "    mean_raw_obs_processing_ms: 0.6861658129808745\n",
      "  time_since_restore: 4077.4799637794495\n",
      "  time_this_iter_s: 28.398988485336304\n",
      "  time_total_s: 4077.4799637794495\n",
      "  timers:\n",
      "    learn_throughput: 1649.139\n",
      "    learn_time_ms: 606.377\n",
      "    load_throughput: 212996.41\n",
      "    load_time_ms: 4.695\n",
      "    sample_throughput: 84.966\n",
      "    sample_time_ms: 11769.476\n",
      "    update_time_ms: 1.87\n",
      "  timestamp: 1632135362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359000\n",
      "  training_iteration: 359\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         4077.48</td><td style=\"text-align: right;\">359000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-56-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 361\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.77456876039505\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011820411446950433\n",
      "          policy_loss: 0.019297995335525935\n",
      "          total_loss: 0.0149624432126681\n",
      "          vf_explained_var: -0.45867758989334106\n",
      "          vf_loss: 0.006221126183582884\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.46428571428571\n",
      "    ram_util_percent: 66.66428571428574\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056107371317338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581166352506234\n",
      "    mean_inference_ms: 1.4080974738931593\n",
      "    mean_raw_obs_processing_ms: 0.6866247718357039\n",
      "  time_since_restore: 4087.8140206336975\n",
      "  time_this_iter_s: 10.334056854248047\n",
      "  time_total_s: 4087.8140206336975\n",
      "  timers:\n",
      "    learn_throughput: 1661.66\n",
      "    learn_time_ms: 601.808\n",
      "    load_throughput: 212848.327\n",
      "    load_time_ms: 4.698\n",
      "    sample_throughput: 85.646\n",
      "    sample_time_ms: 11676.025\n",
      "    update_time_ms: 1.792\n",
      "  timestamp: 1632135373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 360\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         4087.81</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 361000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 362\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5793175458908082\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006647344995759339\n",
      "          policy_loss: -0.00971400292797221\n",
      "          total_loss: -0.020585074979397985\n",
      "          vf_explained_var: -0.8837099671363831\n",
      "          vf_loss: 0.000879282683147014\n",
      "    num_agent_steps_sampled: 361000\n",
      "    num_agent_steps_trained: 361000\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.173333333333325\n",
      "    ram_util_percent: 66.52\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056190191499372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58093321451743\n",
      "    mean_inference_ms: 1.408087776114569\n",
      "    mean_raw_obs_processing_ms: 0.6870884845844576\n",
      "  time_since_restore: 4097.79577755928\n",
      "  time_this_iter_s: 9.981756925582886\n",
      "  time_total_s: 4097.79577755928\n",
      "  timers:\n",
      "    learn_throughput: 1671.6\n",
      "    learn_time_ms: 598.229\n",
      "    load_throughput: 215975.242\n",
      "    load_time_ms: 4.63\n",
      "    sample_throughput: 86.476\n",
      "    sample_time_ms: 11563.866\n",
      "    update_time_ms: 1.779\n",
      "  timestamp: 1632135383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">          4097.8</td><td style=\"text-align: right;\">361000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 362000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-56-33\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 363\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5434486091136932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011834501570686011\n",
      "          policy_loss: -0.041211957360307375\n",
      "          total_loss: -0.04825195976429515\n",
      "          vf_explained_var: -0.16804921627044678\n",
      "          vf_loss: 0.0011969049506458558\n",
      "    num_agent_steps_sampled: 362000\n",
      "    num_agent_steps_trained: 362000\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.01428571428571\n",
      "    ram_util_percent: 66.39999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056273979600482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.580749718544753\n",
      "    mean_inference_ms: 1.4080789463430574\n",
      "    mean_raw_obs_processing_ms: 0.6875568696869422\n",
      "  time_since_restore: 4107.76496553421\n",
      "  time_this_iter_s: 9.96918797492981\n",
      "  time_total_s: 4107.76496553421\n",
      "  timers:\n",
      "    learn_throughput: 1677.945\n",
      "    learn_time_ms: 595.967\n",
      "    load_throughput: 216739.735\n",
      "    load_time_ms: 4.614\n",
      "    sample_throughput: 86.978\n",
      "    sample_time_ms: 11497.097\n",
      "    update_time_ms: 1.781\n",
      "  timestamp: 1632135393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 362\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         4107.76</td><td style=\"text-align: right;\">362000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 363000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-56-43\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 364\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4342959576182894\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008332905257242413\n",
      "          policy_loss: -0.12313013217515416\n",
      "          total_loss: -0.13071352276537154\n",
      "          vf_explained_var: -0.08252442628145218\n",
      "          vf_loss: 0.0016916164229365273\n",
      "    num_agent_steps_sampled: 363000\n",
      "    num_agent_steps_trained: 363000\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.61428571428571\n",
      "    ram_util_percent: 66.39999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056359036332914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.580615237123462\n",
      "    mean_inference_ms: 1.4080709599447758\n",
      "    mean_raw_obs_processing_ms: 0.6880266737345698\n",
      "  time_since_restore: 4117.938157558441\n",
      "  time_this_iter_s: 10.173192024230957\n",
      "  time_total_s: 4117.938157558441\n",
      "  timers:\n",
      "    learn_throughput: 1677.825\n",
      "    learn_time_ms: 596.01\n",
      "    load_throughput: 216628.912\n",
      "    load_time_ms: 4.616\n",
      "    sample_throughput: 87.101\n",
      "    sample_time_ms: 11480.988\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1632135403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 363\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         4117.94</td><td style=\"text-align: right;\">363000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-56-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 365\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8756020029385885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007884557616971656\n",
      "          policy_loss: 0.0624717615544796\n",
      "          total_loss: 0.05024895038869646\n",
      "          vf_explained_var: 0.18485477566719055\n",
      "          vf_loss: 0.001737929865743758\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.540000000000006\n",
      "    ram_util_percent: 66.39333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040564429224168724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58044769995229\n",
      "    mean_inference_ms: 1.4080624341048593\n",
      "    mean_raw_obs_processing_ms: 0.6884992297412451\n",
      "  time_since_restore: 4128.091169595718\n",
      "  time_this_iter_s: 10.153012037277222\n",
      "  time_total_s: 4128.091169595718\n",
      "  timers:\n",
      "    learn_throughput: 1676.993\n",
      "    learn_time_ms: 596.305\n",
      "    load_throughput: 216878.704\n",
      "    load_time_ms: 4.611\n",
      "    sample_throughput: 87.286\n",
      "    sample_time_ms: 11456.649\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632135413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         4128.09</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 365000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-57-03\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 366\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9980422695477804\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010236179668495755\n",
      "          policy_loss: 0.06879485332303577\n",
      "          total_loss: 0.05643889158964157\n",
      "          vf_explained_var: -0.46049371361732483\n",
      "          vf_loss: 0.0013989594105320673\n",
      "    num_agent_steps_sampled: 365000\n",
      "    num_agent_steps_trained: 365000\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.260000000000005\n",
      "    ram_util_percent: 66.39333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040565275142289484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.580240099939035\n",
      "    mean_inference_ms: 1.4080530523428165\n",
      "    mean_raw_obs_processing_ms: 0.6889753788308082\n",
      "  time_since_restore: 4138.355778455734\n",
      "  time_this_iter_s: 10.26460886001587\n",
      "  time_total_s: 4138.355778455734\n",
      "  timers:\n",
      "    learn_throughput: 1676.961\n",
      "    learn_time_ms: 596.317\n",
      "    load_throughput: 216531.615\n",
      "    load_time_ms: 4.618\n",
      "    sample_throughput: 87.329\n",
      "    sample_time_ms: 11451.007\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632135423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 365\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         4138.36</td><td style=\"text-align: right;\">365000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 366000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-57-14\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 367\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5923027568393284\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006588190919110737\n",
      "          policy_loss: -0.03732351664867666\n",
      "          total_loss: -0.04833868708875444\n",
      "          vf_explained_var: -0.7750734090805054\n",
      "          vf_loss: 0.0009010100743681607\n",
      "    num_agent_steps_sampled: 366000\n",
      "    num_agent_steps_trained: 366000\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.5\n",
      "    ram_util_percent: 66.39285714285714\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056612323793006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.580025318985285\n",
      "    mean_inference_ms: 1.408043675983401\n",
      "    mean_raw_obs_processing_ms: 0.6894533424023601\n",
      "  time_since_restore: 4148.637734651566\n",
      "  time_this_iter_s: 10.281956195831299\n",
      "  time_total_s: 4148.637734651566\n",
      "  timers:\n",
      "    learn_throughput: 1674.512\n",
      "    learn_time_ms: 597.189\n",
      "    load_throughput: 216322.779\n",
      "    load_time_ms: 4.623\n",
      "    sample_throughput: 87.331\n",
      "    sample_time_ms: 11450.643\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1632135434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 366\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         4148.64</td><td style=\"text-align: right;\">366000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 367000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-57-24\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 368\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8650979545381334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009255500515942493\n",
      "          policy_loss: -0.10930547018845876\n",
      "          total_loss: -0.12163748509354061\n",
      "          vf_explained_var: -0.10215996950864792\n",
      "          vf_loss: 0.0006899034028821108\n",
      "    num_agent_steps_sampled: 367000\n",
      "    num_agent_steps_trained: 367000\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.220000000000006\n",
      "    ram_util_percent: 66.34666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040566973159950025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.579797810749593\n",
      "    mean_inference_ms: 1.4080342622193565\n",
      "    mean_raw_obs_processing_ms: 0.6899329330442759\n",
      "  time_since_restore: 4158.95413851738\n",
      "  time_this_iter_s: 10.316403865814209\n",
      "  time_total_s: 4158.95413851738\n",
      "  timers:\n",
      "    learn_throughput: 1675.301\n",
      "    learn_time_ms: 596.908\n",
      "    load_throughput: 216846.187\n",
      "    load_time_ms: 4.612\n",
      "    sample_throughput: 87.587\n",
      "    sample_time_ms: 11417.259\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632135444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         4158.95</td><td style=\"text-align: right;\">367000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-57-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 369\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.81657821337382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01594108612359205\n",
      "          policy_loss: 0.1209571444325977\n",
      "          total_loss: 0.11299212310049268\n",
      "          vf_explained_var: -0.17095589637756348\n",
      "          vf_loss: 0.0005056116448637719\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.37333333333334\n",
      "    ram_util_percent: 66.39333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405678219019594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.579552692580517\n",
      "    mean_inference_ms: 1.408024606167365\n",
      "    mean_raw_obs_processing_ms: 0.6904164394333158\n",
      "  time_since_restore: 4169.302490711212\n",
      "  time_this_iter_s: 10.348352193832397\n",
      "  time_total_s: 4169.302490711212\n",
      "  timers:\n",
      "    learn_throughput: 1674.997\n",
      "    learn_time_ms: 597.016\n",
      "    load_throughput: 216866.369\n",
      "    load_time_ms: 4.611\n",
      "    sample_throughput: 87.601\n",
      "    sample_time_ms: 11415.402\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632135455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 368\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">          4169.3</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 369000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-57-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 370\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6534322222073874\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015708222759406847\n",
      "          policy_loss: -0.1476359297004011\n",
      "          total_loss: -0.15364471334550117\n",
      "          vf_explained_var: -0.412384957075119\n",
      "          vf_loss: 0.0009720180723686805\n",
      "    num_agent_steps_sampled: 369000\n",
      "    num_agent_steps_trained: 369000\n",
      "    num_steps_sampled: 369000\n",
      "    num_steps_trained: 369000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.08\n",
      "    ram_util_percent: 66.50666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040568709355095614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.579308855593643\n",
      "    mean_inference_ms: 1.408016344978284\n",
      "    mean_raw_obs_processing_ms: 0.6902546499497746\n",
      "  time_since_restore: 4180.096911907196\n",
      "  time_this_iter_s: 10.794421195983887\n",
      "  time_total_s: 4180.096911907196\n",
      "  timers:\n",
      "    learn_throughput: 1648.668\n",
      "    learn_time_ms: 606.55\n",
      "    load_throughput: 313881.476\n",
      "    load_time_ms: 3.186\n",
      "    sample_throughput: 103.661\n",
      "    sample_time_ms: 9646.844\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632135465\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369000\n",
      "  training_iteration: 369\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">          4180.1</td><td style=\"text-align: right;\">369000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 370000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-57-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 371\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9465789251857333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008276514088337903\n",
      "          policy_loss: -0.022400380671024324\n",
      "          total_loss: -0.03509891579548518\n",
      "          vf_explained_var: 0.16879907250404358\n",
      "          vf_loss: 0.001733593058048023\n",
      "    num_agent_steps_sampled: 370000\n",
      "    num_agent_steps_trained: 370000\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.162499999999994\n",
      "    ram_util_percent: 66.7\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04056962341683803\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.579016372449745\n",
      "    mean_inference_ms: 1.4080099013645933\n",
      "    mean_raw_obs_processing_ms: 0.6900979163249452\n",
      "  time_since_restore: 4190.959651708603\n",
      "  time_this_iter_s: 10.86273980140686\n",
      "  time_total_s: 4190.959651708603\n",
      "  timers:\n",
      "    learn_throughput: 1649.053\n",
      "    learn_time_ms: 606.409\n",
      "    load_throughput: 313667.868\n",
      "    load_time_ms: 3.188\n",
      "    sample_throughput: 103.096\n",
      "    sample_time_ms: 9699.732\n",
      "    update_time_ms: 1.742\n",
      "  timestamp: 1632135476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         4190.96</td><td style=\"text-align: right;\">370000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 371000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-58-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 372\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8189167592260573\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010119379068465958\n",
      "          policy_loss: -0.10545620796167188\n",
      "          total_loss: -0.11633456285215087\n",
      "          vf_explained_var: -0.766081690788269\n",
      "          vf_loss: 0.0011563505763964106\n",
      "    num_agent_steps_sampled: 371000\n",
      "    num_agent_steps_trained: 371000\n",
      "    num_steps_sampled: 371000\n",
      "    num_steps_trained: 371000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.51333333333333\n",
      "    ram_util_percent: 66.65333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057053080857913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.578724417119858\n",
      "    mean_inference_ms: 1.408003228666442\n",
      "    mean_raw_obs_processing_ms: 0.6899474764280847\n",
      "  time_since_restore: 4201.261158466339\n",
      "  time_this_iter_s: 10.301506757736206\n",
      "  time_total_s: 4201.261158466339\n",
      "  timers:\n",
      "    learn_throughput: 1651.16\n",
      "    learn_time_ms: 605.635\n",
      "    load_throughput: 314255.402\n",
      "    load_time_ms: 3.182\n",
      "    sample_throughput: 102.749\n",
      "    sample_time_ms: 9732.466\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632135487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 371000\n",
      "  training_iteration: 371\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         4201.26</td><td style=\"text-align: right;\">371000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 373\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8400133861435783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01047317064953993\n",
      "          policy_loss: -0.06404599299033482\n",
      "          total_loss: -0.07439022431564\n",
      "          vf_explained_var: -0.9913510680198669\n",
      "          vf_loss: 0.0016862651087447174\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.40714285714286\n",
      "    ram_util_percent: 66.72142857142858\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040571438470812425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.578428906701655\n",
      "    mean_inference_ms: 1.4079966244492983\n",
      "    mean_raw_obs_processing_ms: 0.6898032303154856\n",
      "  time_since_restore: 4211.541726350784\n",
      "  time_this_iter_s: 10.28056788444519\n",
      "  time_total_s: 4211.541726350784\n",
      "  timers:\n",
      "    learn_throughput: 1654.017\n",
      "    learn_time_ms: 604.589\n",
      "    load_throughput: 313853.291\n",
      "    load_time_ms: 3.186\n",
      "    sample_throughput: 102.41\n",
      "    sample_time_ms: 9764.672\n",
      "    update_time_ms: 1.738\n",
      "  timestamp: 1632135497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 372\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         4211.54</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 373000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-58-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 374\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7026996188693577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01029703765101182\n",
      "          policy_loss: -0.057619182537827225\n",
      "          total_loss: -0.06621214394561119\n",
      "          vf_explained_var: -0.30418068170547485\n",
      "          vf_loss: 0.0021715232709539124\n",
      "    num_agent_steps_sampled: 373000\n",
      "    num_agent_steps_trained: 373000\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.05\n",
      "    ram_util_percent: 66.85\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057235203345664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.578141211513625\n",
      "    mean_inference_ms: 1.4079901441617892\n",
      "    mean_raw_obs_processing_ms: 0.6896656665311254\n",
      "  time_since_restore: 4222.207428455353\n",
      "  time_this_iter_s: 10.665702104568481\n",
      "  time_total_s: 4222.207428455353\n",
      "  timers:\n",
      "    learn_throughput: 1641.348\n",
      "    learn_time_ms: 609.255\n",
      "    load_throughput: 308561.255\n",
      "    load_time_ms: 3.241\n",
      "    sample_throughput: 101.945\n",
      "    sample_time_ms: 9809.21\n",
      "    update_time_ms: 1.74\n",
      "  timestamp: 1632135508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         4222.21</td><td style=\"text-align: right;\">373000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 374000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-58-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 375\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7603898233837552\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012018649793520296\n",
      "          policy_loss: -0.16680159771607983\n",
      "          total_loss: -0.17522145410378773\n",
      "          vf_explained_var: -0.5845116972923279\n",
      "          vf_loss: 0.0018744650128711428\n",
      "    num_agent_steps_sampled: 374000\n",
      "    num_agent_steps_trained: 374000\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.933333333333344\n",
      "    ram_util_percent: 67.29333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040573256660557326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.577860866259691\n",
      "    mean_inference_ms: 1.4079832700086066\n",
      "    mean_raw_obs_processing_ms: 0.6895342446800619\n",
      "  time_since_restore: 4233.235823869705\n",
      "  time_this_iter_s: 11.028395414352417\n",
      "  time_total_s: 4233.235823869705\n",
      "  timers:\n",
      "    learn_throughput: 1639.21\n",
      "    learn_time_ms: 610.05\n",
      "    load_throughput: 309134.354\n",
      "    load_time_ms: 3.235\n",
      "    sample_throughput: 101.051\n",
      "    sample_time_ms: 9895.974\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632135519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 374\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         4233.24</td><td style=\"text-align: right;\">374000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-58-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 376\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7587525950537788\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005680767946604654\n",
      "          policy_loss: -0.05706324838101864\n",
      "          total_loss: -0.07077419641945097\n",
      "          vf_explained_var: -0.8638802170753479\n",
      "          vf_loss: 0.0004216126341311287\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.45\n",
      "    ram_util_percent: 67.38125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405739557694343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.577557135397413\n",
      "    mean_inference_ms: 1.4079718869715143\n",
      "    mean_raw_obs_processing_ms: 0.6894086331516176\n",
      "  time_since_restore: 4244.330107212067\n",
      "  time_this_iter_s: 11.09428334236145\n",
      "  time_total_s: 4244.330107212067\n",
      "  timers:\n",
      "    learn_throughput: 1641.518\n",
      "    learn_time_ms: 609.192\n",
      "    load_throughput: 310169.938\n",
      "    load_time_ms: 3.224\n",
      "    sample_throughput: 100.203\n",
      "    sample_time_ms: 9979.789\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632135530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 375\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         4244.33</td><td style=\"text-align: right;\">375000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-59-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 377\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.890447575516171\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01162412966954507\n",
      "          policy_loss: -0.0628697567515903\n",
      "          total_loss: -0.07408046672741572\n",
      "          vf_explained_var: -0.7109578251838684\n",
      "          vf_loss: 0.0006241358028394946\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.05625\n",
      "    ram_util_percent: 67.41250000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057456285842709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.57725586894837\n",
      "    mean_inference_ms: 1.4079584757174315\n",
      "    mean_raw_obs_processing_ms: 0.6892891288038774\n",
      "  time_since_restore: 4255.713800668716\n",
      "  time_this_iter_s: 11.38369345664978\n",
      "  time_total_s: 4255.713800668716\n",
      "  timers:\n",
      "    learn_throughput: 1642.841\n",
      "    learn_time_ms: 608.701\n",
      "    load_throughput: 310836.545\n",
      "    load_time_ms: 3.217\n",
      "    sample_throughput: 99.104\n",
      "    sample_time_ms: 10090.448\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632135541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         4255.71</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 377000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-59-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 378\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.698745055993398\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0072645375258118825\n",
      "          policy_loss: 0.07234637161923779\n",
      "          total_loss: 0.06052774435116185\n",
      "          vf_explained_var: -0.6515911221504211\n",
      "          vf_loss: 0.0007506343815394858\n",
      "    num_agent_steps_sampled: 377000\n",
      "    num_agent_steps_trained: 377000\n",
      "    num_steps_sampled: 377000\n",
      "    num_steps_trained: 377000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.475\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040575022251289894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.576943036442556\n",
      "    mean_inference_ms: 1.4079429628129183\n",
      "    mean_raw_obs_processing_ms: 0.6891757358249493\n",
      "  time_since_restore: 4266.558828830719\n",
      "  time_this_iter_s: 10.845028162002563\n",
      "  time_total_s: 4266.558828830719\n",
      "  timers:\n",
      "    learn_throughput: 1643.901\n",
      "    learn_time_ms: 608.309\n",
      "    load_throughput: 309599.852\n",
      "    load_time_ms: 3.23\n",
      "    sample_throughput: 98.583\n",
      "    sample_time_ms: 10143.714\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1632135552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 377000\n",
      "  training_iteration: 377\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         4266.56</td><td style=\"text-align: right;\">377000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 378000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 379\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3418995552592807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007076946266950972\n",
      "          policy_loss: -0.010067561165326172\n",
      "          total_loss: -0.01781955978108777\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0013628986864609436\n",
      "    num_agent_steps_sampled: 378000\n",
      "    num_agent_steps_trained: 378000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.07333333333334\n",
      "    ram_util_percent: 67.47333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040575446628537015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.576623760139242\n",
      "    mean_inference_ms: 1.4079267619045999\n",
      "    mean_raw_obs_processing_ms: 0.6890681862000614\n",
      "  time_since_restore: 4277.376797437668\n",
      "  time_this_iter_s: 10.817968606948853\n",
      "  time_total_s: 4277.376797437668\n",
      "  timers:\n",
      "    learn_throughput: 1640.538\n",
      "    learn_time_ms: 609.556\n",
      "    load_throughput: 309556.438\n",
      "    load_time_ms: 3.23\n",
      "    sample_throughput: 98.142\n",
      "    sample_time_ms: 10189.342\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632135563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 378\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         4277.38</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 379000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-59-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 380\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4580859422683716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01683182718490832\n",
      "          policy_loss: -0.10164823486573166\n",
      "          total_loss: -0.10426628018418947\n",
      "          vf_explained_var: -0.1834937483072281\n",
      "          vf_loss: 0.0017259319455155896\n",
      "    num_agent_steps_sampled: 379000\n",
      "    num_agent_steps_trained: 379000\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.858823529411765\n",
      "    ram_util_percent: 66.67058823529412\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057590499575053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.576311211917389\n",
      "    mean_inference_ms: 1.407911093375615\n",
      "    mean_raw_obs_processing_ms: 0.6889658336250755\n",
      "  time_since_restore: 4288.847331523895\n",
      "  time_this_iter_s: 11.470534086227417\n",
      "  time_total_s: 4288.847331523895\n",
      "  timers:\n",
      "    learn_throughput: 1658.726\n",
      "    learn_time_ms: 602.872\n",
      "    load_throughput: 310997.879\n",
      "    load_time_ms: 3.215\n",
      "    sample_throughput: 97.432\n",
      "    sample_time_ms: 10263.618\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632135574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         4288.85</td><td style=\"text-align: right;\">379000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-59-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 381\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.576186606619093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01370494925620815\n",
      "          policy_loss: -0.0523650118874179\n",
      "          total_loss: -0.05856644759575526\n",
      "          vf_explained_var: 0.1879500448703766\n",
      "          vf_loss: 0.0012252719536061502\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.8125\n",
      "    ram_util_percent: 66.35\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057638329366247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.576002048252276\n",
      "    mean_inference_ms: 1.407896363364949\n",
      "    mean_raw_obs_processing_ms: 0.6888691099318607\n",
      "  time_since_restore: 4299.980098962784\n",
      "  time_this_iter_s: 11.13276743888855\n",
      "  time_total_s: 4299.980098962784\n",
      "  timers:\n",
      "    learn_throughput: 1647.183\n",
      "    learn_time_ms: 607.097\n",
      "    load_throughput: 305322.298\n",
      "    load_time_ms: 3.275\n",
      "    sample_throughput: 97.215\n",
      "    sample_time_ms: 10286.455\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1632135585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 380\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         4299.98</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 381000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_10-59-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 382\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4651257064607408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014279855728471761\n",
      "          policy_loss: -0.04047796502709389\n",
      "          total_loss: -0.04557227368156115\n",
      "          vf_explained_var: -0.8913498520851135\n",
      "          vf_loss: 0.0008721407282994025\n",
      "    num_agent_steps_sampled: 381000\n",
      "    num_agent_steps_trained: 381000\n",
      "    num_steps_sampled: 381000\n",
      "    num_steps_trained: 381000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.050000000000004\n",
      "    ram_util_percent: 66.80000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057687936972167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.575693379949575\n",
      "    mean_inference_ms: 1.4078818632125356\n",
      "    mean_raw_obs_processing_ms: 0.6887780775098171\n",
      "  time_since_restore: 4311.306245803833\n",
      "  time_this_iter_s: 11.326146841049194\n",
      "  time_total_s: 4311.306245803833\n",
      "  timers:\n",
      "    learn_throughput: 1638.105\n",
      "    learn_time_ms: 610.461\n",
      "    load_throughput: 304975.969\n",
      "    load_time_ms: 3.279\n",
      "    sample_throughput: 96.287\n",
      "    sample_time_ms: 10385.577\n",
      "    update_time_ms: 1.779\n",
      "  timestamp: 1632135597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 381000\n",
      "  training_iteration: 381\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         4311.31</td><td style=\"text-align: right;\">381000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 382000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-00-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 383\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5453392095035976\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009587442728908612\n",
      "          policy_loss: -0.07194218726621734\n",
      "          total_loss: -0.08089163576563199\n",
      "          vf_explained_var: -0.857505202293396\n",
      "          vf_loss: 0.0006729960040603247\n",
      "    num_agent_steps_sampled: 382000\n",
      "    num_agent_steps_trained: 382000\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.587500000000006\n",
      "    ram_util_percent: 66.88125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057741810383852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.575389855671398\n",
      "    mean_inference_ms: 1.4078676939005075\n",
      "    mean_raw_obs_processing_ms: 0.6886916184943146\n",
      "  time_since_restore: 4322.6732478141785\n",
      "  time_this_iter_s: 11.367002010345459\n",
      "  time_total_s: 4322.6732478141785\n",
      "  timers:\n",
      "    learn_throughput: 1619.959\n",
      "    learn_time_ms: 617.3\n",
      "    load_throughput: 303880.022\n",
      "    load_time_ms: 3.291\n",
      "    sample_throughput: 95.353\n",
      "    sample_time_ms: 10487.355\n",
      "    update_time_ms: 1.793\n",
      "  timestamp: 1632135608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         4322.67</td><td style=\"text-align: right;\">382000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 383000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-00-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 384\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3702028367254468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013994586439810617\n",
      "          policy_loss: -0.013956234024630653\n",
      "          total_loss: -0.017288256312410037\n",
      "          vf_explained_var: -0.06564337760210037\n",
      "          vf_loss: 0.0018586957293640202\n",
      "    num_agent_steps_sampled: 383000\n",
      "    num_agent_steps_trained: 383000\n",
      "    num_steps_sampled: 383000\n",
      "    num_steps_trained: 383000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.38235294117647\n",
      "    ram_util_percent: 66.82941176470588\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040578008717968454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.575098246766492\n",
      "    mean_inference_ms: 1.4078548204982602\n",
      "    mean_raw_obs_processing_ms: 0.6886108241803638\n",
      "  time_since_restore: 4334.2856822013855\n",
      "  time_this_iter_s: 11.612434387207031\n",
      "  time_total_s: 4334.2856822013855\n",
      "  timers:\n",
      "    learn_throughput: 1623.505\n",
      "    learn_time_ms: 615.951\n",
      "    load_throughput: 306001.693\n",
      "    load_time_ms: 3.268\n",
      "    sample_throughput: 94.487\n",
      "    sample_time_ms: 10583.444\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1632135620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383000\n",
      "  training_iteration: 383\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         4334.29</td><td style=\"text-align: right;\">383000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-00-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 385\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.47712418768141\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014529751142145544\n",
      "          policy_loss: 0.018669214472174643\n",
      "          total_loss: 0.013794914922780462\n",
      "          vf_explained_var: -0.18208403885364532\n",
      "          vf_loss: 0.0010601513870319144\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.849999999999994\n",
      "    ram_util_percent: 66.6125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057859087412404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.574805545416446\n",
      "    mean_inference_ms: 1.407841727405449\n",
      "    mean_raw_obs_processing_ms: 0.6885344169399381\n",
      "  time_since_restore: 4345.542842388153\n",
      "  time_this_iter_s: 11.257160186767578\n",
      "  time_total_s: 4345.542842388153\n",
      "  timers:\n",
      "    learn_throughput: 1623.535\n",
      "    learn_time_ms: 615.94\n",
      "    load_throughput: 305874.494\n",
      "    load_time_ms: 3.269\n",
      "    sample_throughput: 94.283\n",
      "    sample_time_ms: 10606.315\n",
      "    update_time_ms: 1.788\n",
      "  timestamp: 1632135631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 384\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         4345.54</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 385000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 386\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.758509369691213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010043548641614209\n",
      "          policy_loss: -0.010000590317779117\n",
      "          total_loss: -0.02102396645479732\n",
      "          vf_explained_var: -0.3671807646751404\n",
      "          vf_loss: 0.0004533737369153338\n",
      "    num_agent_steps_sampled: 385000\n",
      "    num_agent_steps_trained: 385000\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.99375\n",
      "    ram_util_percent: 66.48750000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057916643318433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.574523722476622\n",
      "    mean_inference_ms: 1.407828613214946\n",
      "    mean_raw_obs_processing_ms: 0.688463785527172\n",
      "  time_since_restore: 4356.753355741501\n",
      "  time_this_iter_s: 11.210513353347778\n",
      "  time_total_s: 4356.753355741501\n",
      "  timers:\n",
      "    learn_throughput: 1623.982\n",
      "    learn_time_ms: 615.77\n",
      "    load_throughput: 305346.748\n",
      "    load_time_ms: 3.275\n",
      "    sample_throughput: 94.179\n",
      "    sample_time_ms: 10618.127\n",
      "    update_time_ms: 1.79\n",
      "  timestamp: 1632135642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         4356.75</td><td style=\"text-align: right;\">385000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 386000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-00-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 387\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5471841428014967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007124802488276948\n",
      "          policy_loss: -0.11043652420242628\n",
      "          total_loss: -0.12087535547713439\n",
      "          vf_explained_var: -0.1342013031244278\n",
      "          vf_loss: 0.0006998061901968968\n",
      "    num_agent_steps_sampled: 386000\n",
      "    num_agent_steps_trained: 386000\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.068749999999994\n",
      "    ram_util_percent: 66.47500000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057974171365022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.574239444062345\n",
      "    mean_inference_ms: 1.4078150056183327\n",
      "    mean_raw_obs_processing_ms: 0.688399173515893\n",
      "  time_since_restore: 4367.965173482895\n",
      "  time_this_iter_s: 11.211817741394043\n",
      "  time_total_s: 4367.965173482895\n",
      "  timers:\n",
      "    learn_throughput: 1615.069\n",
      "    learn_time_ms: 619.168\n",
      "    load_throughput: 305044.728\n",
      "    load_time_ms: 3.278\n",
      "    sample_throughput: 94.362\n",
      "    sample_time_ms: 10597.534\n",
      "    update_time_ms: 1.785\n",
      "  timestamp: 1632135654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 386\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         4367.97</td><td style=\"text-align: right;\">386000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 387000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-01-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 388\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3687037693129644\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017023996465155337\n",
      "          policy_loss: 0.03235841426584456\n",
      "          total_loss: 0.02982647998465432\n",
      "          vf_explained_var: -0.45859605073928833\n",
      "          vf_loss: 0.0008013490315837165\n",
      "    num_agent_steps_sampled: 387000\n",
      "    num_agent_steps_trained: 387000\n",
      "    num_steps_sampled: 387000\n",
      "    num_steps_trained: 387000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.21875\n",
      "    ram_util_percent: 66.5375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058032880663693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.573956468713945\n",
      "    mean_inference_ms: 1.4078013821726603\n",
      "    mean_raw_obs_processing_ms: 0.6883387774007923\n",
      "  time_since_restore: 4379.24783873558\n",
      "  time_this_iter_s: 11.282665252685547\n",
      "  time_total_s: 4379.24783873558\n",
      "  timers:\n",
      "    learn_throughput: 1611.521\n",
      "    learn_time_ms: 620.532\n",
      "    load_throughput: 279305.582\n",
      "    load_time_ms: 3.58\n",
      "    sample_throughput: 93.988\n",
      "    sample_time_ms: 10639.632\n",
      "    update_time_ms: 1.785\n",
      "  timestamp: 1632135665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 387000\n",
      "  training_iteration: 387\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         4379.25</td><td style=\"text-align: right;\">387000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-01-16\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 389\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2429530554347568\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012819592336662211\n",
      "          policy_loss: 0.006031782883736823\n",
      "          total_loss: 0.003971727440754572\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.002572779753892165\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.368750000000006\n",
      "    ram_util_percent: 66.3375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040580937739516564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.573664458885865\n",
      "    mean_inference_ms: 1.4077879683723262\n",
      "    mean_raw_obs_processing_ms: 0.6882835326898485\n",
      "  time_since_restore: 4390.400974988937\n",
      "  time_this_iter_s: 11.153136253356934\n",
      "  time_total_s: 4390.400974988937\n",
      "  timers:\n",
      "    learn_throughput: 1610.709\n",
      "    learn_time_ms: 620.845\n",
      "    load_throughput: 277818.153\n",
      "    load_time_ms: 3.599\n",
      "    sample_throughput: 93.695\n",
      "    sample_time_ms: 10672.911\n",
      "    update_time_ms: 1.785\n",
      "  timestamp: 1632135676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">          4390.4</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 389000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-01-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 390\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2133811526828342\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008775719141279205\n",
      "          policy_loss: -0.02657515984028578\n",
      "          total_loss: -0.03184930593189266\n",
      "          vf_explained_var: -0.7731363773345947\n",
      "          vf_loss: 0.0015223962778691202\n",
      "    num_agent_steps_sampled: 389000\n",
      "    num_agent_steps_trained: 389000\n",
      "    num_steps_sampled: 389000\n",
      "    num_steps_trained: 389000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.61\n",
      "    ram_util_percent: 66.00999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040581535136738414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.573353133133018\n",
      "    mean_inference_ms: 1.4077735890567928\n",
      "    mean_raw_obs_processing_ms: 0.6886838626269024\n",
      "  time_since_restore: 4418.325160980225\n",
      "  time_this_iter_s: 27.92418599128723\n",
      "  time_total_s: 4418.325160980225\n",
      "  timers:\n",
      "    learn_throughput: 1617.371\n",
      "    learn_time_ms: 618.287\n",
      "    load_throughput: 198867.948\n",
      "    load_time_ms: 5.028\n",
      "    sample_throughput: 81.173\n",
      "    sample_time_ms: 12319.443\n",
      "    update_time_ms: 1.778\n",
      "  timestamp: 1632135704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389000\n",
      "  training_iteration: 389\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         4418.33</td><td style=\"text-align: right;\">389000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-01-56\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 391\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2370854437351226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010666815849498738\n",
      "          policy_loss: -0.030886611052685312\n",
      "          total_loss: -0.03579764482047823\n",
      "          vf_explained_var: 0.7969749569892883\n",
      "          vf_loss: 0.0009724141632129128\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.738888888888894\n",
      "    ram_util_percent: 66.19999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040582127287087336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.573080520876058\n",
      "    mean_inference_ms: 1.407758848261593\n",
      "    mean_raw_obs_processing_ms: 0.6890877330953152\n",
      "  time_since_restore: 4430.706671714783\n",
      "  time_this_iter_s: 12.381510734558105\n",
      "  time_total_s: 4430.706671714783\n",
      "  timers:\n",
      "    learn_throughput: 1626.733\n",
      "    learn_time_ms: 614.729\n",
      "    load_throughput: 201406.188\n",
      "    load_time_ms: 4.965\n",
      "    sample_throughput: 80.334\n",
      "    sample_time_ms: 12447.98\n",
      "    update_time_ms: 1.769\n",
      "  timestamp: 1632135716\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 390\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         4430.71</td><td style=\"text-align: right;\">390000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 391000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-02-07\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 392\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9094038936826918\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009040016088644463\n",
      "          policy_loss: -0.2090684473514557\n",
      "          total_loss: -0.21151999996768103\n",
      "          vf_explained_var: 0.7141673564910889\n",
      "          vf_loss: 0.0011444733154753017\n",
      "    num_agent_steps_sampled: 391000\n",
      "    num_agent_steps_trained: 391000\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.36666666666666\n",
      "    ram_util_percent: 66.24000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058270078874703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.572785612449257\n",
      "    mean_inference_ms: 1.4077428494057367\n",
      "    mean_raw_obs_processing_ms: 0.689494764454324\n",
      "  time_since_restore: 4441.25376367569\n",
      "  time_this_iter_s: 10.547091960906982\n",
      "  time_total_s: 4441.25376367569\n",
      "  timers:\n",
      "    learn_throughput: 1633.09\n",
      "    learn_time_ms: 612.336\n",
      "    load_throughput: 201401.352\n",
      "    load_time_ms: 4.965\n",
      "    sample_throughput: 80.825\n",
      "    sample_time_ms: 12372.463\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1632135727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         4441.25</td><td style=\"text-align: right;\">391000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-02-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 393\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5169032520718044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011825158530888362\n",
      "          policy_loss: -0.1346972319814894\n",
      "          total_loss: -0.14109491176075406\n",
      "          vf_explained_var: 0.2549324929714203\n",
      "          vf_loss: 0.0015794584585819393\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.126666666666665\n",
      "    ram_util_percent: 66.39333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040583268292431855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.572459804945323\n",
      "    mean_inference_ms: 1.4077264920155659\n",
      "    mean_raw_obs_processing_ms: 0.6899046740131008\n",
      "  time_since_restore: 4451.757532119751\n",
      "  time_this_iter_s: 10.50376844406128\n",
      "  time_total_s: 4451.757532119751\n",
      "  timers:\n",
      "    learn_throughput: 1650.165\n",
      "    learn_time_ms: 606.0\n",
      "    load_throughput: 201162.765\n",
      "    load_time_ms: 4.971\n",
      "    sample_throughput: 81.353\n",
      "    sample_time_ms: 12292.047\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632135738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 392\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         4451.76</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 393000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 394\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4576893223656548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013297891310159413\n",
      "          policy_loss: -0.030095245213144356\n",
      "          total_loss: -0.035065915756341486\n",
      "          vf_explained_var: -0.27177894115448\n",
      "          vf_loss: 0.0015186309746544188\n",
      "    num_agent_steps_sampled: 393000\n",
      "    num_agent_steps_trained: 393000\n",
      "    num_steps_sampled: 393000\n",
      "    num_steps_trained: 393000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.94666666666667\n",
      "    ram_util_percent: 66.22666666666669\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058381871360184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.572108120318244\n",
      "    mean_inference_ms: 1.4077087041630636\n",
      "    mean_raw_obs_processing_ms: 0.6903181624803366\n",
      "  time_since_restore: 4462.125222444534\n",
      "  time_this_iter_s: 10.367690324783325\n",
      "  time_total_s: 4462.125222444534\n",
      "  timers:\n",
      "    learn_throughput: 1656.434\n",
      "    learn_time_ms: 603.706\n",
      "    load_throughput: 202416.076\n",
      "    load_time_ms: 4.94\n",
      "    sample_throughput: 82.17\n",
      "    sample_time_ms: 12169.892\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632135748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 393000\n",
      "  training_iteration: 393\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         4462.13</td><td style=\"text-align: right;\">393000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 394000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-02-38\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 395\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7967515624231762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008785422380175748\n",
      "          policy_loss: -0.04593408033251763\n",
      "          total_loss: -0.04745233034094175\n",
      "          vf_explained_var: -0.04354729503393173\n",
      "          vf_loss: 0.0011060960744442936\n",
      "    num_agent_steps_sampled: 394000\n",
      "    num_agent_steps_trained: 394000\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.357142857142854\n",
      "    ram_util_percent: 66.08571428571429\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058435386734451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.571726237760096\n",
      "    mean_inference_ms: 1.4076895693438123\n",
      "    mean_raw_obs_processing_ms: 0.6907353337157919\n",
      "  time_since_restore: 4472.361462593079\n",
      "  time_this_iter_s: 10.236240148544312\n",
      "  time_total_s: 4472.361462593079\n",
      "  timers:\n",
      "    learn_throughput: 1658.52\n",
      "    learn_time_ms: 602.947\n",
      "    load_throughput: 201769.516\n",
      "    load_time_ms: 4.956\n",
      "    sample_throughput: 82.86\n",
      "    sample_time_ms: 12068.571\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1632135758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         4472.36</td><td style=\"text-align: right;\">394000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 395000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 396\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6347625851631165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008261552199068333\n",
      "          policy_loss: -0.048330842910541426\n",
      "          total_loss: -0.05867074992921617\n",
      "          vf_explained_var: 0.16588488221168518\n",
      "          vf_loss: 0.0009831582163719254\n",
      "    num_agent_steps_sampled: 395000\n",
      "    num_agent_steps_trained: 395000\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.666666666666664\n",
      "    ram_util_percent: 66.26\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058490621726315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.571319337339066\n",
      "    mean_inference_ms: 1.4076700481332185\n",
      "    mean_raw_obs_processing_ms: 0.6911559713453171\n",
      "  time_since_restore: 4482.858950376511\n",
      "  time_this_iter_s: 10.497487783432007\n",
      "  time_total_s: 4482.858950376511\n",
      "  timers:\n",
      "    learn_throughput: 1654.907\n",
      "    learn_time_ms: 604.263\n",
      "    load_throughput: 201494.235\n",
      "    load_time_ms: 4.963\n",
      "    sample_throughput: 83.363\n",
      "    sample_time_ms: 11995.714\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632135769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 395\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         4482.86</td><td style=\"text-align: right;\">395000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-02-59\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 397\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3215034001403385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011403875673643189\n",
      "          policy_loss: -0.06406602569752269\n",
      "          total_loss: -0.06959373176925712\n",
      "          vf_explained_var: -0.06769371777772903\n",
      "          vf_loss: 0.0007516495349894588\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.862500000000004\n",
      "    ram_util_percent: 66.6625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058549161479797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.570894986675267\n",
      "    mean_inference_ms: 1.407651063837342\n",
      "    mean_raw_obs_processing_ms: 0.6915800658616097\n",
      "  time_since_restore: 4493.564420223236\n",
      "  time_this_iter_s: 10.705469846725464\n",
      "  time_total_s: 4493.564420223236\n",
      "  timers:\n",
      "    learn_throughput: 1663.657\n",
      "    learn_time_ms: 601.085\n",
      "    load_throughput: 201255.428\n",
      "    load_time_ms: 4.969\n",
      "    sample_throughput: 83.694\n",
      "    sample_time_ms: 11948.244\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632135779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 396\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         4493.56</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 397000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-03-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 398\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7079002406862047\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008652889392545287\n",
      "          policy_loss: -0.012519898141423862\n",
      "          total_loss: -0.023499952008326847\n",
      "          vf_explained_var: 0.25180870294570923\n",
      "          vf_loss: 0.0008363814371275819\n",
      "    num_agent_steps_sampled: 397000\n",
      "    num_agent_steps_trained: 397000\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.228571428571435\n",
      "    ram_util_percent: 66.54285714285714\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405859430243305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.570417788728896\n",
      "    mean_inference_ms: 1.4076285042100227\n",
      "    mean_raw_obs_processing_ms: 0.6920069136191964\n",
      "  time_since_restore: 4503.966836214066\n",
      "  time_this_iter_s: 10.402415990829468\n",
      "  time_total_s: 4503.966836214066\n",
      "  timers:\n",
      "    learn_throughput: 1666.0\n",
      "    learn_time_ms: 600.24\n",
      "    load_throughput: 214751.114\n",
      "    load_time_ms: 4.657\n",
      "    sample_throughput: 84.307\n",
      "    sample_time_ms: 11861.375\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632135790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         4503.97</td><td style=\"text-align: right;\">397000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 398000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-03-21\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 399\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8051272922092014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008137153588573495\n",
      "          policy_loss: 0.03411690750055843\n",
      "          total_loss: 0.02182379456029998\n",
      "          vf_explained_var: 0.23820200562477112\n",
      "          vf_loss: 0.0008092565966459612\n",
      "    num_agent_steps_sampled: 398000\n",
      "    num_agent_steps_trained: 398000\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.5125\n",
      "    ram_util_percent: 66.36875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058631144000793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.569884143166032\n",
      "    mean_inference_ms: 1.4076026925951652\n",
      "    mean_raw_obs_processing_ms: 0.6924369634668177\n",
      "  time_since_restore: 4514.569407224655\n",
      "  time_this_iter_s: 10.6025710105896\n",
      "  time_total_s: 4514.569407224655\n",
      "  timers:\n",
      "    learn_throughput: 1664.705\n",
      "    learn_time_ms: 600.707\n",
      "    load_throughput: 214415.181\n",
      "    load_time_ms: 4.664\n",
      "    sample_throughput: 84.704\n",
      "    sample_time_ms: 11805.829\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632135801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 398\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         4514.57</td><td style=\"text-align: right;\">398000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 399000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 400\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3532179491387473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009864428789744138\n",
      "          policy_loss: -0.08169915891355939\n",
      "          total_loss: -0.08630273408359951\n",
      "          vf_explained_var: -0.3608376085758209\n",
      "          vf_loss: 0.0029291947234822953\n",
      "    num_agent_steps_sampled: 399000\n",
      "    num_agent_steps_trained: 399000\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.366666666666674\n",
      "    ram_util_percent: 66.49333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058661950593076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.569324394242694\n",
      "    mean_inference_ms: 1.4075752710145295\n",
      "    mean_raw_obs_processing_ms: 0.6923007305022921\n",
      "  time_since_restore: 4525.15874505043\n",
      "  time_this_iter_s: 10.589337825775146\n",
      "  time_total_s: 4525.15874505043\n",
      "  timers:\n",
      "    learn_throughput: 1664.177\n",
      "    learn_time_ms: 600.898\n",
      "    load_throughput: 307992.539\n",
      "    load_time_ms: 3.247\n",
      "    sample_throughput: 99.27\n",
      "    sample_time_ms: 10073.526\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1632135811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 399\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         4525.16</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 401\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3256505290667215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008971468343945056\n",
      "          policy_loss: -0.0934809274557564\n",
      "          total_loss: -0.09987500326500999\n",
      "          vf_explained_var: -0.3493940234184265\n",
      "          vf_loss: 0.0014061078991896162\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.34666666666667\n",
      "    ram_util_percent: 66.36000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058697181671636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.568732512421809\n",
      "    mean_inference_ms: 1.4075495285970945\n",
      "    mean_raw_obs_processing_ms: 0.6921702240383141\n",
      "  time_since_restore: 4536.096079111099\n",
      "  time_this_iter_s: 10.937334060668945\n",
      "  time_total_s: 4536.096079111099\n",
      "  timers:\n",
      "    learn_throughput: 1640.775\n",
      "    learn_time_ms: 609.468\n",
      "    load_throughput: 307678.494\n",
      "    load_time_ms: 3.25\n",
      "    sample_throughput: 100.802\n",
      "    sample_time_ms: 9920.481\n",
      "    update_time_ms: 1.813\n",
      "  timestamp: 1632135822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">          4536.1</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 401000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-03-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 402\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6745774428049722\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009227312502087375\n",
      "          policy_loss: 0.009801766234967443\n",
      "          total_loss: -0.0006875007930729124\n",
      "          vf_explained_var: 0.3041757643222809\n",
      "          vf_loss: 0.0006445851849599017\n",
      "    num_agent_steps_sampled: 401000\n",
      "    num_agent_steps_trained: 401000\n",
      "    num_steps_sampled: 401000\n",
      "    num_steps_trained: 401000\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.40625\n",
      "    ram_util_percent: 66.975\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058737806085132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.568171830027875\n",
      "    mean_inference_ms: 1.4075255040377193\n",
      "    mean_raw_obs_processing_ms: 0.6920452329172001\n",
      "  time_since_restore: 4546.88939166069\n",
      "  time_this_iter_s: 10.793312549591064\n",
      "  time_total_s: 4546.88939166069\n",
      "  timers:\n",
      "    learn_throughput: 1642.41\n",
      "    learn_time_ms: 608.861\n",
      "    load_throughput: 306928.74\n",
      "    load_time_ms: 3.258\n",
      "    sample_throughput: 100.546\n",
      "    sample_time_ms: 9945.699\n",
      "    update_time_ms: 1.809\n",
      "  timestamp: 1632135833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 401000\n",
      "  training_iteration: 401\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         4546.89</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 402000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 403\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1103875577449798\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01695779271245641\n",
      "          policy_loss: -0.009511200586954753\n",
      "          total_loss: -0.009001850874887573\n",
      "          vf_explained_var: 0.2311231791973114\n",
      "          vf_loss: 0.0012997355945925746\n",
      "    num_agent_steps_sampled: 402000\n",
      "    num_agent_steps_trained: 402000\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.00666666666667\n",
      "    ram_util_percent: 66.77333333333331\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058783330314875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.56764792863803\n",
      "    mean_inference_ms: 1.4075033732557578\n",
      "    mean_raw_obs_processing_ms: 0.6919262362849494\n",
      "  time_since_restore: 4557.803538322449\n",
      "  time_this_iter_s: 10.914146661758423\n",
      "  time_total_s: 4557.803538322449\n",
      "  timers:\n",
      "    learn_throughput: 1634.055\n",
      "    learn_time_ms: 611.974\n",
      "    load_throughput: 304533.105\n",
      "    load_time_ms: 3.284\n",
      "    sample_throughput: 100.161\n",
      "    sample_time_ms: 9983.975\n",
      "    update_time_ms: 1.841\n",
      "  timestamp: 1632135844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 402\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">          4557.8</td><td style=\"text-align: right;\">402000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 403000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-04-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 404\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.652472554312812\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017529705176508672\n",
      "          policy_loss: -0.0013937959240542517\n",
      "          total_loss: -0.006309830314583249\n",
      "          vf_explained_var: -0.1293349415063858\n",
      "          vf_loss: 0.0009473741164482716\n",
      "    num_agent_steps_sampled: 403000\n",
      "    num_agent_steps_trained: 403000\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.39375\n",
      "    ram_util_percent: 66.76249999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058831867904331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.56716092343348\n",
      "    mean_inference_ms: 1.4074825380757667\n",
      "    mean_raw_obs_processing_ms: 0.6918123542343083\n",
      "  time_since_restore: 4568.443351507187\n",
      "  time_this_iter_s: 10.63981318473816\n",
      "  time_total_s: 4568.443351507187\n",
      "  timers:\n",
      "    learn_throughput: 1637.005\n",
      "    learn_time_ms: 610.872\n",
      "    load_throughput: 304261.382\n",
      "    load_time_ms: 3.287\n",
      "    sample_throughput: 99.877\n",
      "    sample_time_ms: 10012.269\n",
      "    update_time_ms: 1.841\n",
      "  timestamp: 1632135855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         4568.44</td><td style=\"text-align: right;\">403000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-04-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 405\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3773672395282321\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01115505441921398\n",
      "          policy_loss: -0.07746232002973556\n",
      "          total_loss: -0.08238969660467571\n",
      "          vf_explained_var: -0.19330334663391113\n",
      "          vf_loss: 0.0020619491315705496\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.72\n",
      "    ram_util_percent: 66.41333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058883342617104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.566698321251899\n",
      "    mean_inference_ms: 1.4074626802261938\n",
      "    mean_raw_obs_processing_ms: 0.6917036495424134\n",
      "  time_since_restore: 4579.020316839218\n",
      "  time_this_iter_s: 10.57696533203125\n",
      "  time_total_s: 4579.020316839218\n",
      "  timers:\n",
      "    learn_throughput: 1628.472\n",
      "    learn_time_ms: 614.072\n",
      "    load_throughput: 301700.738\n",
      "    load_time_ms: 3.315\n",
      "    sample_throughput: 99.572\n",
      "    sample_time_ms: 10043.028\n",
      "    update_time_ms: 1.851\n",
      "  timestamp: 1632135865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 404\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         4579.02</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-04-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 406\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.259493159585529\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013260195966479\n",
      "          policy_loss: -0.11613802661498387\n",
      "          total_loss: -0.11955680168337292\n",
      "          vf_explained_var: 0.029379654675722122\n",
      "          vf_loss: 0.0011114920198451728\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.233333333333334\n",
      "    ram_util_percent: 66.34666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058936089148763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.566234584284947\n",
      "    mean_inference_ms: 1.4074431935380998\n",
      "    mean_raw_obs_processing_ms: 0.69159947642089\n",
      "  time_since_restore: 4589.461524486542\n",
      "  time_this_iter_s: 10.441207647323608\n",
      "  time_total_s: 4589.461524486542\n",
      "  timers:\n",
      "    learn_throughput: 1629.207\n",
      "    learn_time_ms: 613.796\n",
      "    load_throughput: 301826.66\n",
      "    load_time_ms: 3.313\n",
      "    sample_throughput: 99.623\n",
      "    sample_time_ms: 10037.87\n",
      "    update_time_ms: 1.844\n",
      "  timestamp: 1632135876\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 405\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         4589.46</td><td style=\"text-align: right;\">405000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 406000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-04-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 407\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9884219725926717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00784265242475719\n",
      "          policy_loss: -0.051843269252114826\n",
      "          total_loss: -0.056457676904069054\n",
      "          vf_explained_var: 0.3398872911930084\n",
      "          vf_loss: 0.000500021998595912\n",
      "    num_agent_steps_sampled: 406000\n",
      "    num_agent_steps_trained: 406000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.65999999999999\n",
      "    ram_util_percent: 66.18000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058991153065619\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.565774740805796\n",
      "    mean_inference_ms: 1.4074244451734006\n",
      "    mean_raw_obs_processing_ms: 0.6915002749347359\n",
      "  time_since_restore: 4599.925337314606\n",
      "  time_this_iter_s: 10.463812828063965\n",
      "  time_total_s: 4599.925337314606\n",
      "  timers:\n",
      "    learn_throughput: 1619.293\n",
      "    learn_time_ms: 617.553\n",
      "    load_throughput: 302252.969\n",
      "    load_time_ms: 3.308\n",
      "    sample_throughput: 99.901\n",
      "    sample_time_ms: 10009.949\n",
      "    update_time_ms: 1.848\n",
      "  timestamp: 1632135886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         4599.93</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 407000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-04-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 408\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.167760557598538\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005384229719276945\n",
      "          policy_loss: 0.0005713513327969445\n",
      "          total_loss: -0.007326686878999075\n",
      "          vf_explained_var: 0.19990603625774384\n",
      "          vf_loss: 0.000504954400498213\n",
      "    num_agent_steps_sampled: 407000\n",
      "    num_agent_steps_trained: 407000\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.379999999999995\n",
      "    ram_util_percent: 66.21333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059047906322651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.565323930654555\n",
      "    mean_inference_ms: 1.407406517795073\n",
      "    mean_raw_obs_processing_ms: 0.6914059770616013\n",
      "  time_since_restore: 4610.642676115036\n",
      "  time_this_iter_s: 10.717338800430298\n",
      "  time_total_s: 4610.642676115036\n",
      "  timers:\n",
      "    learn_throughput: 1604.7\n",
      "    learn_time_ms: 623.17\n",
      "    load_throughput: 302562.578\n",
      "    load_time_ms: 3.305\n",
      "    sample_throughput: 99.643\n",
      "    sample_time_ms: 10035.799\n",
      "    update_time_ms: 1.849\n",
      "  timestamp: 1632135897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 407\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         4610.64</td><td style=\"text-align: right;\">407000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-05-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 409\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8669059342808194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012034675517477132\n",
      "          policy_loss: -0.10573495075934464\n",
      "          total_loss: -0.11567112861408127\n",
      "          vf_explained_var: 0.20938527584075928\n",
      "          vf_loss: 0.001413561669889734\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.54666666666667\n",
      "    ram_util_percent: 66.39333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059108102462845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.564888689620094\n",
      "    mean_inference_ms: 1.407389707299036\n",
      "    mean_raw_obs_processing_ms: 0.6913168645526229\n",
      "  time_since_restore: 4621.281954526901\n",
      "  time_this_iter_s: 10.639278411865234\n",
      "  time_total_s: 4621.281954526901\n",
      "  timers:\n",
      "    learn_throughput: 1596.904\n",
      "    learn_time_ms: 626.212\n",
      "    load_throughput: 301141.872\n",
      "    load_time_ms: 3.321\n",
      "    sample_throughput: 99.638\n",
      "    sample_time_ms: 10036.361\n",
      "    update_time_ms: 1.84\n",
      "  timestamp: 1632135907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 408\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         4621.28</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 409000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-05-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 410\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2762946976555718\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010901187007898017\n",
      "          policy_loss: -0.08544086451745696\n",
      "          total_loss: -0.09029326368537215\n",
      "          vf_explained_var: 0.09157675504684448\n",
      "          vf_loss: 0.001280596288658368\n",
      "    num_agent_steps_sampled: 409000\n",
      "    num_agent_steps_trained: 409000\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.3235294117647\n",
      "    ram_util_percent: 67.12941176470588\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059176356140197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.564485728535722\n",
      "    mean_inference_ms: 1.4073756985191537\n",
      "    mean_raw_obs_processing_ms: 0.6912329700696672\n",
      "  time_since_restore: 4632.887242078781\n",
      "  time_this_iter_s: 11.605287551879883\n",
      "  time_total_s: 4632.887242078781\n",
      "  timers:\n",
      "    learn_throughput: 1582.243\n",
      "    learn_time_ms: 632.014\n",
      "    load_throughput: 300073.26\n",
      "    load_time_ms: 3.333\n",
      "    sample_throughput: 98.695\n",
      "    sample_time_ms: 10132.186\n",
      "    update_time_ms: 1.831\n",
      "  timestamp: 1632135919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         4632.89</td><td style=\"text-align: right;\">409000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 410000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-05-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 411\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.432637596130371\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009909956865298655\n",
      "          policy_loss: -0.0731591092215644\n",
      "          total_loss: -0.07979412604537275\n",
      "          vf_explained_var: 0.11775247007608414\n",
      "          vf_loss: 0.001664261203000529\n",
      "    num_agent_steps_sampled: 410000\n",
      "    num_agent_steps_trained: 410000\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.725\n",
      "    ram_util_percent: 67.46875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059248987570016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.564098493136028\n",
      "    mean_inference_ms: 1.4073634310126126\n",
      "    mean_raw_obs_processing_ms: 0.6911541999450947\n",
      "  time_since_restore: 4643.895755529404\n",
      "  time_this_iter_s: 11.008513450622559\n",
      "  time_total_s: 4643.895755529404\n",
      "  timers:\n",
      "    learn_throughput: 1606.189\n",
      "    learn_time_ms: 622.592\n",
      "    load_throughput: 299118.826\n",
      "    load_time_ms: 3.343\n",
      "    sample_throughput: 98.539\n",
      "    sample_time_ms: 10148.239\n",
      "    update_time_ms: 1.818\n",
      "  timestamp: 1632135930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 410\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">          4643.9</td><td style=\"text-align: right;\">410000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 411000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-05-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 412\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.417228321896659\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006183664179238511\n",
      "          policy_loss: -0.06802652205030123\n",
      "          total_loss: -0.07788567981786199\n",
      "          vf_explained_var: 0.6598657369613647\n",
      "          vf_loss: 0.0005523054439335182\n",
      "    num_agent_steps_sampled: 411000\n",
      "    num_agent_steps_trained: 411000\n",
      "    num_steps_sampled: 411000\n",
      "    num_steps_trained: 411000\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.506249999999994\n",
      "    ram_util_percent: 67.38125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059327235511435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.563724009786036\n",
      "    mean_inference_ms: 1.4073532429194067\n",
      "    mean_raw_obs_processing_ms: 0.6910804163533889\n",
      "  time_since_restore: 4655.192197799683\n",
      "  time_this_iter_s: 11.29644227027893\n",
      "  time_total_s: 4655.192197799683\n",
      "  timers:\n",
      "    learn_throughput: 1590.618\n",
      "    learn_time_ms: 628.686\n",
      "    load_throughput: 299796.576\n",
      "    load_time_ms: 3.336\n",
      "    sample_throughput: 98.112\n",
      "    sample_time_ms: 10192.456\n",
      "    update_time_ms: 1.822\n",
      "  timestamp: 1632135941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411000\n",
      "  training_iteration: 411\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         4655.19</td><td style=\"text-align: right;\">411000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-05-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 413\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.689107678996192\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008312550374919386\n",
      "          policy_loss: -0.08824391981793775\n",
      "          total_loss: -0.09932836931612757\n",
      "          vf_explained_var: 0.27808326482772827\n",
      "          vf_loss: 0.0007510491234522002\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.77333333333332\n",
      "    ram_util_percent: 67.37999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059408340894625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.563347321126333\n",
      "    mean_inference_ms: 1.4073443211061685\n",
      "    mean_raw_obs_processing_ms: 0.6910112200772136\n",
      "  time_since_restore: 4665.960650205612\n",
      "  time_this_iter_s: 10.768452405929565\n",
      "  time_total_s: 4665.960650205612\n",
      "  timers:\n",
      "    learn_throughput: 1597.107\n",
      "    learn_time_ms: 626.132\n",
      "    load_throughput: 302422.958\n",
      "    load_time_ms: 3.307\n",
      "    sample_throughput: 98.227\n",
      "    sample_time_ms: 10180.529\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1632135952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         4665.96</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 413000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-06-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 414\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6587088346481322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006885702601343545\n",
      "          policy_loss: -0.16060699510077636\n",
      "          total_loss: -0.1726071504669057\n",
      "          vf_explained_var: 0.7502858638763428\n",
      "          vf_loss: 0.000399143395204899\n",
      "    num_agent_steps_sampled: 413000\n",
      "    num_agent_steps_trained: 413000\n",
      "    num_steps_sampled: 413000\n",
      "    num_steps_trained: 413000\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.962500000000006\n",
      "    ram_util_percent: 67.3375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059490474255594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.562986056515335\n",
      "    mean_inference_ms: 1.407335947133646\n",
      "    mean_raw_obs_processing_ms: 0.6909467913054739\n",
      "  time_since_restore: 4676.826662540436\n",
      "  time_this_iter_s: 10.866012334823608\n",
      "  time_total_s: 4676.826662540436\n",
      "  timers:\n",
      "    learn_throughput: 1595.941\n",
      "    learn_time_ms: 626.589\n",
      "    load_throughput: 302486.207\n",
      "    load_time_ms: 3.306\n",
      "    sample_throughput: 98.013\n",
      "    sample_time_ms: 10202.684\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1632135963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 413000\n",
      "  training_iteration: 413\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         4676.83</td><td style=\"text-align: right;\">413000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 414000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-06-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 415\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1192789879110125\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007649259616130561\n",
      "          policy_loss: -0.05247427796324094\n",
      "          total_loss: -0.058100932236346936\n",
      "          vf_explained_var: 0.20344725251197815\n",
      "          vf_loss: 0.0009139638248598203\n",
      "    num_agent_steps_sampled: 414000\n",
      "    num_agent_steps_trained: 414000\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.09333333333334\n",
      "    ram_util_percent: 67.33999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059575886920989\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.56263659069058\n",
      "    mean_inference_ms: 1.40732850873311\n",
      "    mean_raw_obs_processing_ms: 0.6908873319252529\n",
      "  time_since_restore: 4687.713206768036\n",
      "  time_this_iter_s: 10.886544227600098\n",
      "  time_total_s: 4687.713206768036\n",
      "  timers:\n",
      "    learn_throughput: 1602.061\n",
      "    learn_time_ms: 624.196\n",
      "    load_throughput: 306171.455\n",
      "    load_time_ms: 3.266\n",
      "    sample_throughput: 97.693\n",
      "    sample_time_ms: 10236.106\n",
      "    update_time_ms: 1.784\n",
      "  timestamp: 1632135974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 414\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         4687.71</td><td style=\"text-align: right;\">414000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 415000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-06-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 416\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1927681763966878\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0098227387056128\n",
      "          policy_loss: -0.04625847459667259\n",
      "          total_loss: -0.05147730078962114\n",
      "          vf_explained_var: 0.265259325504303\n",
      "          vf_loss: 0.0007347996893157768\n",
      "    num_agent_steps_sampled: 415000\n",
      "    num_agent_steps_trained: 415000\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.862500000000004\n",
      "    ram_util_percent: 67.375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059667598452936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.562304799394482\n",
      "    mean_inference_ms: 1.4073227568960782\n",
      "    mean_raw_obs_processing_ms: 0.6908324203256587\n",
      "  time_since_restore: 4698.946691036224\n",
      "  time_this_iter_s: 11.233484268188477\n",
      "  time_total_s: 4698.946691036224\n",
      "  timers:\n",
      "    learn_throughput: 1599.128\n",
      "    learn_time_ms: 625.341\n",
      "    load_throughput: 305934.733\n",
      "    load_time_ms: 3.269\n",
      "    sample_throughput: 96.954\n",
      "    sample_time_ms: 10314.183\n",
      "    update_time_ms: 1.784\n",
      "  timestamp: 1632135985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         4698.95</td><td style=\"text-align: right;\">415000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-06-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 417\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4034481830067105\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013080820550232157\n",
      "          policy_loss: 0.010633154627349642\n",
      "          total_loss: 0.005208550973070992\n",
      "          vf_explained_var: -0.2423780858516693\n",
      "          vf_loss: 0.0006543083662513203\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.462500000000006\n",
      "    ram_util_percent: 67.38125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405976078518653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.56199001246384\n",
      "    mean_inference_ms: 1.4073180791616366\n",
      "    mean_raw_obs_processing_ms: 0.6907826262351624\n",
      "  time_since_restore: 4710.218729734421\n",
      "  time_this_iter_s: 11.272038698196411\n",
      "  time_total_s: 4710.218729734421\n",
      "  timers:\n",
      "    learn_throughput: 1608.177\n",
      "    learn_time_ms: 621.822\n",
      "    load_throughput: 306292.191\n",
      "    load_time_ms: 3.265\n",
      "    sample_throughput: 96.167\n",
      "    sample_time_ms: 10398.537\n",
      "    update_time_ms: 1.789\n",
      "  timestamp: 1632135997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 416\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         4710.22</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 417000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-06-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 418\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4527569995986092\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00751344403603969\n",
      "          policy_loss: -0.03278137466145886\n",
      "          total_loss: -0.04191116522997618\n",
      "          vf_explained_var: -0.5201424360275269\n",
      "          vf_loss: 0.0008282080341208105\n",
      "    num_agent_steps_sampled: 417000\n",
      "    num_agent_steps_trained: 417000\n",
      "    num_steps_sampled: 417000\n",
      "    num_steps_trained: 417000\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.6125\n",
      "    ram_util_percent: 67.225\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059853459057073\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.561681991166338\n",
      "    mean_inference_ms: 1.407313784232163\n",
      "    mean_raw_obs_processing_ms: 0.6907375957402977\n",
      "  time_since_restore: 4721.19091296196\n",
      "  time_this_iter_s: 10.972183227539062\n",
      "  time_total_s: 4721.19091296196\n",
      "  timers:\n",
      "    learn_throughput: 1622.039\n",
      "    learn_time_ms: 616.508\n",
      "    load_throughput: 305747.401\n",
      "    load_time_ms: 3.271\n",
      "    sample_throughput: 95.883\n",
      "    sample_time_ms: 10429.35\n",
      "    update_time_ms: 1.791\n",
      "  timestamp: 1632136008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417000\n",
      "  training_iteration: 417\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">         4721.19</td><td style=\"text-align: right;\">417000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 418000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 419\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.425469586584303\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013247024501062852\n",
      "          policy_loss: -0.14589108646743829\n",
      "          total_loss: -0.15130216624173853\n",
      "          vf_explained_var: -0.26200976967811584\n",
      "          vf_loss: 0.0007869607379284894\n",
      "    num_agent_steps_sampled: 418000\n",
      "    num_agent_steps_trained: 418000\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.38\n",
      "    ram_util_percent: 67.11333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059945516034017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.561372218444923\n",
      "    mean_inference_ms: 1.407309800099268\n",
      "    mean_raw_obs_processing_ms: 0.6906967554165682\n",
      "  time_since_restore: 4731.648275613785\n",
      "  time_this_iter_s: 10.457362651824951\n",
      "  time_total_s: 4731.648275613785\n",
      "  timers:\n",
      "    learn_throughput: 1633.521\n",
      "    learn_time_ms: 612.175\n",
      "    load_throughput: 309398.877\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 96.01\n",
      "    sample_time_ms: 10415.59\n",
      "    update_time_ms: 1.788\n",
      "  timestamp: 1632136018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         4731.65</td><td style=\"text-align: right;\">418000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 419000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-07-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 420\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8139725353982713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007609994066422211\n",
      "          policy_loss: -0.1495703445540534\n",
      "          total_loss: -0.1527299698856142\n",
      "          vf_explained_var: 0.559560239315033\n",
      "          vf_loss: 0.0003518083737516362\n",
      "    num_agent_steps_sampled: 419000\n",
      "    num_agent_steps_trained: 419000\n",
      "    num_steps_sampled: 419000\n",
      "    num_steps_trained: 419000\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.34285714285714\n",
      "    ram_util_percent: 66.97142857142858\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040600372873711786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.561095381048965\n",
      "    mean_inference_ms: 1.4073061228018648\n",
      "    mean_raw_obs_processing_ms: 0.6910792454871517\n",
      "  time_since_restore: 4761.2647356987\n",
      "  time_this_iter_s: 29.61646008491516\n",
      "  time_total_s: 4761.2647356987\n",
      "  timers:\n",
      "    learn_throughput: 1647.703\n",
      "    learn_time_ms: 606.906\n",
      "    load_throughput: 217580.744\n",
      "    load_time_ms: 4.596\n",
      "    sample_throughput: 81.829\n",
      "    sample_time_ms: 12220.608\n",
      "    update_time_ms: 1.784\n",
      "  timestamp: 1632136048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419000\n",
      "  training_iteration: 419\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         4761.26</td><td style=\"text-align: right;\">419000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-07-38\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 421\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7292224314477709\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013793181667857684\n",
      "          policy_loss: -0.21168670116199387\n",
      "          total_loss: -0.2193331956035561\n",
      "          vf_explained_var: 0.09749246388673782\n",
      "          vf_loss: 0.0012569141710021845\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.179999999999986\n",
      "    ram_util_percent: 67.01333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060129958358579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.560809283379722\n",
      "    mean_inference_ms: 1.4073033751968327\n",
      "    mean_raw_obs_processing_ms: 0.6914649098830811\n",
      "  time_since_restore: 4771.790663957596\n",
      "  time_this_iter_s: 10.525928258895874\n",
      "  time_total_s: 4771.790663957596\n",
      "  timers:\n",
      "    learn_throughput: 1646.473\n",
      "    learn_time_ms: 607.359\n",
      "    load_throughput: 218391.911\n",
      "    load_time_ms: 4.579\n",
      "    sample_throughput: 82.153\n",
      "    sample_time_ms: 12172.418\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632136058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 420\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         4771.79</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 421000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-07-49\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 422\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6930737045076159\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014045076761440543\n",
      "          policy_loss: 0.0022001500758859847\n",
      "          total_loss: -0.0054042495787143706\n",
      "          vf_explained_var: -0.13895238935947418\n",
      "          vf_loss: 0.000784318180457275\n",
      "    num_agent_steps_sampled: 421000\n",
      "    num_agent_steps_trained: 421000\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.14\n",
      "    ram_util_percent: 67.08666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406022339195704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.560508724621677\n",
      "    mean_inference_ms: 1.4073008219969037\n",
      "    mean_raw_obs_processing_ms: 0.6918539274847575\n",
      "  time_since_restore: 4782.286847114563\n",
      "  time_this_iter_s: 10.496183156967163\n",
      "  time_total_s: 4782.286847114563\n",
      "  timers:\n",
      "    learn_throughput: 1650.361\n",
      "    learn_time_ms: 605.928\n",
      "    load_throughput: 215808.555\n",
      "    load_time_ms: 4.634\n",
      "    sample_throughput: 82.688\n",
      "    sample_time_ms: 12093.692\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632136069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         4782.29</td><td style=\"text-align: right;\">421000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 422000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 423\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4664870613151126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009425834633947734\n",
      "          policy_loss: -0.03598168028725518\n",
      "          total_loss: -0.04410423512260119\n",
      "          vf_explained_var: 0.05058125779032707\n",
      "          vf_loss: 0.0008096538066941623\n",
      "    num_agent_steps_sampled: 422000\n",
      "    num_agent_steps_trained: 422000\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.075\n",
      "    ram_util_percent: 67.1\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060321085013272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.560202782413759\n",
      "    mean_inference_ms: 1.407299351514573\n",
      "    mean_raw_obs_processing_ms: 0.6922461051027778\n",
      "  time_since_restore: 4793.076612949371\n",
      "  time_this_iter_s: 10.78976583480835\n",
      "  time_total_s: 4793.076612949371\n",
      "  timers:\n",
      "    learn_throughput: 1650.659\n",
      "    learn_time_ms: 605.819\n",
      "    load_throughput: 216320.548\n",
      "    load_time_ms: 4.623\n",
      "    sample_throughput: 82.672\n",
      "    sample_time_ms: 12095.934\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632136080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 422\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         4793.08</td><td style=\"text-align: right;\">422000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 423000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 424\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2749207099278768\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013327754712858998\n",
      "          policy_loss: -0.0726699552188317\n",
      "          total_loss: -0.07686750143766403\n",
      "          vf_explained_var: 0.016486018896102905\n",
      "          vf_loss: 0.00044590362845661324\n",
      "    num_agent_steps_sampled: 423000\n",
      "    num_agent_steps_trained: 423000\n",
      "    num_steps_sampled: 423000\n",
      "    num_steps_trained: 423000\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.666666666666664\n",
      "    ram_util_percent: 67.02000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040604187617770224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.559892674561457\n",
      "    mean_inference_ms: 1.4072980256252992\n",
      "    mean_raw_obs_processing_ms: 0.692641690301464\n",
      "  time_since_restore: 4803.582986831665\n",
      "  time_this_iter_s: 10.506373882293701\n",
      "  time_total_s: 4803.582986831665\n",
      "  timers:\n",
      "    learn_throughput: 1652.967\n",
      "    learn_time_ms: 604.973\n",
      "    load_throughput: 216488.028\n",
      "    load_time_ms: 4.619\n",
      "    sample_throughput: 82.923\n",
      "    sample_time_ms: 12059.349\n",
      "    update_time_ms: 3.267\n",
      "  timestamp: 1632136090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423000\n",
      "  training_iteration: 423\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         4803.58</td><td style=\"text-align: right;\">423000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 425\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4732457445727454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010428423383472957\n",
      "          policy_loss: 0.05554707853330506\n",
      "          total_loss: 0.04769387145837148\n",
      "          vf_explained_var: -0.957500696182251\n",
      "          vf_loss: 0.0005368288117501329\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.24375\n",
      "    ram_util_percent: 67.14375000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060524075930396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.559585772003436\n",
      "    mean_inference_ms: 1.4072985095419117\n",
      "    mean_raw_obs_processing_ms: 0.693040370153994\n",
      "  time_since_restore: 4814.616379976273\n",
      "  time_this_iter_s: 11.033393144607544\n",
      "  time_total_s: 4814.616379976273\n",
      "  timers:\n",
      "    learn_throughput: 1633.093\n",
      "    learn_time_ms: 612.335\n",
      "    load_throughput: 216319.432\n",
      "    load_time_ms: 4.623\n",
      "    sample_throughput: 82.874\n",
      "    sample_time_ms: 12066.547\n",
      "    update_time_ms: 3.278\n",
      "  timestamp: 1632136101\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         4814.62</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 425000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-08-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 426\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3386192017131382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009828692361820442\n",
      "          policy_loss: -0.06313189657198058\n",
      "          total_loss: -0.06982550927334362\n",
      "          vf_explained_var: -0.3742022216320038\n",
      "          vf_loss: 0.0007149059965740889\n",
      "    num_agent_steps_sampled: 425000\n",
      "    num_agent_steps_trained: 425000\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.693333333333335\n",
      "    ram_util_percent: 66.95333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040606293302477045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.559267630631126\n",
      "    mean_inference_ms: 1.4072988233800168\n",
      "    mean_raw_obs_processing_ms: 0.6934423730651958\n",
      "  time_since_restore: 4825.152968168259\n",
      "  time_this_iter_s: 10.536588191986084\n",
      "  time_total_s: 4825.152968168259\n",
      "  timers:\n",
      "    learn_throughput: 1624.647\n",
      "    learn_time_ms: 615.518\n",
      "    load_throughput: 216859.641\n",
      "    load_time_ms: 4.611\n",
      "    sample_throughput: 83.377\n",
      "    sample_time_ms: 11993.696\n",
      "    update_time_ms: 3.296\n",
      "  timestamp: 1632136112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 425\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         4825.15</td><td style=\"text-align: right;\">425000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 426000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-08-43\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 427\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.581328304608663\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008702014538066965\n",
      "          policy_loss: 0.038520285238822304\n",
      "          total_loss: 0.02957984076605903\n",
      "          vf_explained_var: -0.3829003572463989\n",
      "          vf_loss: 0.0015803961924070286\n",
      "    num_agent_steps_sampled: 426000\n",
      "    num_agent_steps_trained: 426000\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.34\n",
      "    ram_util_percent: 66.92666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060736904763955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.558949371798997\n",
      "    mean_inference_ms: 1.4073002315437049\n",
      "    mean_raw_obs_processing_ms: 0.6938474733755439\n",
      "  time_since_restore: 4836.144005060196\n",
      "  time_this_iter_s: 10.991036891937256\n",
      "  time_total_s: 4836.144005060196\n",
      "  timers:\n",
      "    learn_throughput: 1619.38\n",
      "    learn_time_ms: 617.52\n",
      "    load_throughput: 216624.436\n",
      "    load_time_ms: 4.616\n",
      "    sample_throughput: 83.587\n",
      "    sample_time_ms: 11963.585\n",
      "    update_time_ms: 3.312\n",
      "  timestamp: 1632136123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 426\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         4836.14</td><td style=\"text-align: right;\">426000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 427000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-08-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 428\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3367787506845263\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010031366461423295\n",
      "          policy_loss: -0.07271979277332624\n",
      "          total_loss: -0.07867603302001953\n",
      "          vf_explained_var: 0.17331212759017944\n",
      "          vf_loss: 0.0013106131754789707\n",
      "    num_agent_steps_sampled: 427000\n",
      "    num_agent_steps_trained: 427000\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.24375\n",
      "    ram_util_percent: 66.91875000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060848983686362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.558632502697385\n",
      "    mean_inference_ms: 1.4073030775461624\n",
      "    mean_raw_obs_processing_ms: 0.694255572083576\n",
      "  time_since_restore: 4847.194540977478\n",
      "  time_this_iter_s: 11.050535917282104\n",
      "  time_total_s: 4847.194540977478\n",
      "  timers:\n",
      "    learn_throughput: 1613.254\n",
      "    learn_time_ms: 619.865\n",
      "    load_throughput: 215576.731\n",
      "    load_time_ms: 4.639\n",
      "    sample_throughput: 83.549\n",
      "    sample_time_ms: 11969.062\n",
      "    update_time_ms: 3.311\n",
      "  timestamp: 1632136134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         4847.19</td><td style=\"text-align: right;\">427000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-09-05\n",
      "  done: false\n",
      "  episode_len_mean: 994.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 429\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4753958781560261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007316889296583421\n",
      "          policy_loss: -0.008224940569036537\n",
      "          total_loss: -0.018054343428876664\n",
      "          vf_explained_var: -0.8383804559707642\n",
      "          vf_loss: 0.0004745255914814253\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.39375\n",
      "    ram_util_percent: 66.73125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040609616449169404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.558322089100212\n",
      "    mean_inference_ms: 1.4073062554797904\n",
      "    mean_raw_obs_processing_ms: 0.6946668055501516\n",
      "  time_since_restore: 4857.978430747986\n",
      "  time_this_iter_s: 10.783889770507812\n",
      "  time_total_s: 4857.978430747986\n",
      "  timers:\n",
      "    learn_throughput: 1615.378\n",
      "    learn_time_ms: 619.05\n",
      "    load_throughput: 213888.158\n",
      "    load_time_ms: 4.675\n",
      "    sample_throughput: 83.316\n",
      "    sample_time_ms: 12002.494\n",
      "    update_time_ms: 3.31\n",
      "  timestamp: 1632136145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 428\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">         4857.98</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 429000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-09-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 430\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7550573574172126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01223929939633166\n",
      "          policy_loss: 0.004054866317245696\n",
      "          total_loss: -0.004940741322934627\n",
      "          vf_explained_var: -0.6312506794929504\n",
      "          vf_loss: 0.0011111976003222582\n",
      "    num_agent_steps_sampled: 429000\n",
      "    num_agent_steps_trained: 429000\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.66666666666666\n",
      "    ram_util_percent: 66.37333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061075122364535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.558013468045397\n",
      "    mean_inference_ms: 1.4073099133771922\n",
      "    mean_raw_obs_processing_ms: 0.6945394269671458\n",
      "  time_since_restore: 4868.5752720832825\n",
      "  time_this_iter_s: 10.59684133529663\n",
      "  time_total_s: 4868.5752720832825\n",
      "  timers:\n",
      "    learn_throughput: 1611.556\n",
      "    learn_time_ms: 620.518\n",
      "    load_throughput: 303424.967\n",
      "    load_time_ms: 3.296\n",
      "    sample_throughput: 99.006\n",
      "    sample_time_ms: 10100.447\n",
      "    update_time_ms: 3.321\n",
      "  timestamp: 1632136155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 429\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         4868.58</td><td style=\"text-align: right;\">429000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 430000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-09-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 431\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5328472640779283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00964664938759512\n",
      "          policy_loss: -0.03738418229752117\n",
      "          total_loss: -0.04648125130269262\n",
      "          vf_explained_var: -0.9391605854034424\n",
      "          vf_loss: 0.00036444753972722944\n",
      "    num_agent_steps_sampled: 430000\n",
      "    num_agent_steps_trained: 430000\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.56666666666667\n",
      "    ram_util_percent: 66.39333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061191256035055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.557722500519022\n",
      "    mean_inference_ms: 1.40731415267153\n",
      "    mean_raw_obs_processing_ms: 0.6944167125261133\n",
      "  time_since_restore: 4879.349699258804\n",
      "  time_this_iter_s: 10.77442717552185\n",
      "  time_total_s: 4879.349699258804\n",
      "  timers:\n",
      "    learn_throughput: 1605.977\n",
      "    learn_time_ms: 622.674\n",
      "    load_throughput: 299366.48\n",
      "    load_time_ms: 3.34\n",
      "    sample_throughput: 98.784\n",
      "    sample_time_ms: 10123.089\n",
      "    update_time_ms: 3.326\n",
      "  timestamp: 1632136166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         4879.35</td><td style=\"text-align: right;\">430000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 431000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-09-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 432\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4319037437438964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008175828546118345\n",
      "          policy_loss: -0.009058454881111781\n",
      "          total_loss: -0.01759073966079288\n",
      "          vf_explained_var: -0.5090411901473999\n",
      "          vf_loss: 0.0008143299808984415\n",
      "    num_agent_steps_sampled: 431000\n",
      "    num_agent_steps_trained: 431000\n",
      "    num_steps_sampled: 431000\n",
      "    num_steps_trained: 431000\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.25\n",
      "    ram_util_percent: 66.16875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061310454290778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.557447059476129\n",
      "    mean_inference_ms: 1.4073197136729574\n",
      "    mean_raw_obs_processing_ms: 0.694298575124767\n",
      "  time_since_restore: 4890.215391159058\n",
      "  time_this_iter_s: 10.865691900253296\n",
      "  time_total_s: 4890.215391159058\n",
      "  timers:\n",
      "    learn_throughput: 1611.903\n",
      "    learn_time_ms: 620.385\n",
      "    load_throughput: 301182.959\n",
      "    load_time_ms: 3.32\n",
      "    sample_throughput: 98.402\n",
      "    sample_time_ms: 10162.427\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1632136177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431000\n",
      "  training_iteration: 431\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         4890.22</td><td style=\"text-align: right;\">431000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 433\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6665477196375529\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011537237838031681\n",
      "          policy_loss: -0.03643635271324052\n",
      "          total_loss: -0.04488609561489688\n",
      "          vf_explained_var: 0.20504429936408997\n",
      "          vf_loss: 0.001198945902999387\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.20666666666666\n",
      "    ram_util_percent: 66.23333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061432221695224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.557185207627171\n",
      "    mean_inference_ms: 1.4073265239611445\n",
      "    mean_raw_obs_processing_ms: 0.694185567038277\n",
      "  time_since_restore: 4900.959467887878\n",
      "  time_this_iter_s: 10.7440767288208\n",
      "  time_total_s: 4900.959467887878\n",
      "  timers:\n",
      "    learn_throughput: 1609.03\n",
      "    learn_time_ms: 621.493\n",
      "    load_throughput: 300789.857\n",
      "    load_time_ms: 3.325\n",
      "    sample_throughput: 98.457\n",
      "    sample_time_ms: 10156.726\n",
      "    update_time_ms: 3.322\n",
      "  timestamp: 1632136188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 432\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         4900.96</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 433000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-09-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 434\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6020319037967259\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006021012617177421\n",
      "          policy_loss: -0.01230623938350214\n",
      "          total_loss: -0.023952460930579237\n",
      "          vf_explained_var: -0.7638553977012634\n",
      "          vf_loss: 0.0007122015845703168\n",
      "    num_agent_steps_sampled: 433000\n",
      "    num_agent_steps_trained: 433000\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.1375\n",
      "    ram_util_percent: 66.31875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061557589057027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.556943328281037\n",
      "    mean_inference_ms: 1.4073347612792322\n",
      "    mean_raw_obs_processing_ms: 0.6940769944707135\n",
      "  time_since_restore: 4911.803693294525\n",
      "  time_this_iter_s: 10.844225406646729\n",
      "  time_total_s: 4911.803693294525\n",
      "  timers:\n",
      "    learn_throughput: 1602.154\n",
      "    learn_time_ms: 624.16\n",
      "    load_throughput: 300936.61\n",
      "    load_time_ms: 3.323\n",
      "    sample_throughput: 98.142\n",
      "    sample_time_ms: 10189.305\n",
      "    update_time_ms: 1.819\n",
      "  timestamp: 1632136199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">          4911.8</td><td style=\"text-align: right;\">433000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 434000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 435\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8323963589138454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009990213722108225\n",
      "          policy_loss: -0.06859484650194644\n",
      "          total_loss: -0.07940442003309726\n",
      "          vf_explained_var: 0.022949734702706337\n",
      "          vf_loss: 0.0014384841837454588\n",
      "    num_agent_steps_sampled: 434000\n",
      "    num_agent_steps_trained: 434000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.906666666666666\n",
      "    ram_util_percent: 66.25999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061684370399643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.556714892188989\n",
      "    mean_inference_ms: 1.4073438003943701\n",
      "    mean_raw_obs_processing_ms: 0.6939726246680016\n",
      "  time_since_restore: 4922.589542388916\n",
      "  time_this_iter_s: 10.78584909439087\n",
      "  time_total_s: 4922.589542388916\n",
      "  timers:\n",
      "    learn_throughput: 1620.524\n",
      "    learn_time_ms: 617.085\n",
      "    load_throughput: 299665.919\n",
      "    load_time_ms: 3.337\n",
      "    sample_throughput: 98.314\n",
      "    sample_time_ms: 10171.501\n",
      "    update_time_ms: 1.807\n",
      "  timestamp: 1632136209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 434\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         4922.59</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-10-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 436\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.989873190720876\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010708440773289772\n",
      "          policy_loss: -0.012771044505967034\n",
      "          total_loss: -0.025121561106708316\n",
      "          vf_explained_var: -0.31811249256134033\n",
      "          vf_loss: 0.0010354940339715945\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.88\n",
      "    ram_util_percent: 66.40666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061815615494015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.556498067333033\n",
      "    mean_inference_ms: 1.40735398020041\n",
      "    mean_raw_obs_processing_ms: 0.6938727527959517\n",
      "  time_since_restore: 4933.32331943512\n",
      "  time_this_iter_s: 10.733777046203613\n",
      "  time_total_s: 4933.32331943512\n",
      "  timers:\n",
      "    learn_throughput: 1632.362\n",
      "    learn_time_ms: 612.609\n",
      "    load_throughput: 299161.496\n",
      "    load_time_ms: 3.343\n",
      "    sample_throughput: 98.081\n",
      "    sample_time_ms: 10195.703\n",
      "    update_time_ms: 1.785\n",
      "  timestamp: 1632136220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 435\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         4933.32</td><td style=\"text-align: right;\">435000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-10-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 437\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.960682651731703\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006685728261713445\n",
      "          policy_loss: -0.12119293734431266\n",
      "          total_loss: -0.13452457408938143\n",
      "          vf_explained_var: 0.10326720029115677\n",
      "          vf_loss: 0.0022090222277458653\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.900000000000006\n",
      "    ram_util_percent: 66.44\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04061946251113281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.556281612139292\n",
      "    mean_inference_ms: 1.4073643742759996\n",
      "    mean_raw_obs_processing_ms: 0.6937770670233074\n",
      "  time_since_restore: 4943.696405172348\n",
      "  time_this_iter_s: 10.373085737228394\n",
      "  time_total_s: 4943.696405172348\n",
      "  timers:\n",
      "    learn_throughput: 1637.329\n",
      "    learn_time_ms: 610.751\n",
      "    load_throughput: 299231.927\n",
      "    load_time_ms: 3.342\n",
      "    sample_throughput: 98.66\n",
      "    sample_time_ms: 10135.797\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632136231\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">          4943.7</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 438\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.650204144583808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010432813227267662\n",
      "          policy_loss: -0.0002237795541683833\n",
      "          total_loss: -0.009354051036967171\n",
      "          vf_explained_var: -0.9031399488449097\n",
      "          vf_loss: 0.001026677913129485\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.32666666666667\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040620765375502875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.556069949728682\n",
      "    mean_inference_ms: 1.407374712680226\n",
      "    mean_raw_obs_processing_ms: 0.6936857037587201\n",
      "  time_since_restore: 4954.079403162003\n",
      "  time_this_iter_s: 10.382997989654541\n",
      "  time_total_s: 4954.079403162003\n",
      "  timers:\n",
      "    learn_throughput: 1643.638\n",
      "    learn_time_ms: 608.407\n",
      "    load_throughput: 301453.542\n",
      "    load_time_ms: 3.317\n",
      "    sample_throughput: 99.291\n",
      "    sample_time_ms: 10071.392\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632136241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         4954.08</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 438000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-10-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 439\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6976191732618544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006688354948971047\n",
      "          policy_loss: 0.00043340639935599435\n",
      "          total_loss: -0.01200753812574678\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004674837452411238\n",
      "    num_agent_steps_sampled: 438000\n",
      "    num_agent_steps_trained: 438000\n",
      "    num_steps_sampled: 438000\n",
      "    num_steps_trained: 438000\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.30000000000002\n",
      "    ram_util_percent: 66.50666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04062206080399066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55587131901348\n",
      "    mean_inference_ms: 1.4073849279586796\n",
      "    mean_raw_obs_processing_ms: 0.6935984383480522\n",
      "  time_since_restore: 4964.703370094299\n",
      "  time_this_iter_s: 10.623966932296753\n",
      "  time_total_s: 4964.703370094299\n",
      "  timers:\n",
      "    learn_throughput: 1642.6\n",
      "    learn_time_ms: 608.791\n",
      "    load_throughput: 300915.02\n",
      "    load_time_ms: 3.323\n",
      "    sample_throughput: 99.453\n",
      "    sample_time_ms: 10054.987\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632136252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 438000\n",
      "  training_iteration: 438\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">          4964.7</td><td style=\"text-align: right;\">438000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 439000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-11-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 440\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7984605418311226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00808868472791957\n",
      "          policy_loss: 0.008156573741386335\n",
      "          total_loss: -0.004434058453059859\n",
      "          vf_explained_var: -0.33373865485191345\n",
      "          vf_loss: 0.00047454891949503993\n",
      "    num_agent_steps_sampled: 439000\n",
      "    num_agent_steps_trained: 439000\n",
      "    num_steps_sampled: 439000\n",
      "    num_steps_trained: 439000\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.093333333333334\n",
      "    ram_util_percent: 66.57333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04062335292217858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.555677341520061\n",
      "    mean_inference_ms: 1.4073952031632968\n",
      "    mean_raw_obs_processing_ms: 0.6935151752619905\n",
      "  time_since_restore: 4975.194256782532\n",
      "  time_this_iter_s: 10.490886688232422\n",
      "  time_total_s: 4975.194256782532\n",
      "  timers:\n",
      "    learn_throughput: 1651.212\n",
      "    learn_time_ms: 605.616\n",
      "    load_throughput: 300567.841\n",
      "    load_time_ms: 3.327\n",
      "    sample_throughput: 99.527\n",
      "    sample_time_ms: 10047.55\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632136262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439000\n",
      "  training_iteration: 439\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">         4975.19</td><td style=\"text-align: right;\">439000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-11-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 441\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5800835569699605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012894108406465874\n",
      "          policy_loss: 0.011280825568570031\n",
      "          total_loss: 0.003986479341983795\n",
      "          vf_explained_var: -0.41894498467445374\n",
      "          vf_loss: 0.0006644728811807\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.51875\n",
      "    ram_util_percent: 66.65\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040624652314465354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.555493822898633\n",
      "    mean_inference_ms: 1.407405909907441\n",
      "    mean_raw_obs_processing_ms: 0.6934359626268503\n",
      "  time_since_restore: 4985.89275097847\n",
      "  time_this_iter_s: 10.69849419593811\n",
      "  time_total_s: 4985.89275097847\n",
      "  timers:\n",
      "    learn_throughput: 1651.455\n",
      "    learn_time_ms: 605.527\n",
      "    load_throughput: 302870.636\n",
      "    load_time_ms: 3.302\n",
      "    sample_throughput: 99.601\n",
      "    sample_time_ms: 10040.024\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632136273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 440\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">         4985.89</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 441000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 442\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7116729789310032\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013220297754029097\n",
      "          policy_loss: 0.03296915123032199\n",
      "          total_loss: 0.024209677883320384\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003168557663836206\n",
      "    num_agent_steps_sampled: 441000\n",
      "    num_agent_steps_trained: 441000\n",
      "    num_steps_sampled: 441000\n",
      "    num_steps_trained: 441000\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.279999999999994\n",
      "    ram_util_percent: 66.69333333333336\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406259528874459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.555316832013776\n",
      "    mean_inference_ms: 1.4074168408180225\n",
      "    mean_raw_obs_processing_ms: 0.6933606590338974\n",
      "  time_since_restore: 4996.457635402679\n",
      "  time_this_iter_s: 10.564884424209595\n",
      "  time_total_s: 4996.457635402679\n",
      "  timers:\n",
      "    learn_throughput: 1651.746\n",
      "    learn_time_ms: 605.42\n",
      "    load_throughput: 306303.375\n",
      "    load_time_ms: 3.265\n",
      "    sample_throughput: 99.899\n",
      "    sample_time_ms: 10010.103\n",
      "    update_time_ms: 1.782\n",
      "  timestamp: 1632136283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 441000\n",
      "  training_iteration: 441\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         4996.46</td><td style=\"text-align: right;\">441000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 442000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-11-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 443\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.946662535932329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010974512385262248\n",
      "          policy_loss: -0.06580792433685727\n",
      "          total_loss: -0.07743279097808732\n",
      "          vf_explained_var: -0.16249491274356842\n",
      "          vf_loss: 0.0011672150751110167\n",
      "    num_agent_steps_sampled: 442000\n",
      "    num_agent_steps_trained: 442000\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.25333333333333\n",
      "    ram_util_percent: 67.15333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040627333133536635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.5551476283258\n",
      "    mean_inference_ms: 1.4074296030648004\n",
      "    mean_raw_obs_processing_ms: 0.6932891194145401\n",
      "  time_since_restore: 5007.275163412094\n",
      "  time_this_iter_s: 10.817528009414673\n",
      "  time_total_s: 5007.275163412094\n",
      "  timers:\n",
      "    learn_throughput: 1649.137\n",
      "    learn_time_ms: 606.378\n",
      "    load_throughput: 304073.889\n",
      "    load_time_ms: 3.289\n",
      "    sample_throughput: 99.836\n",
      "    sample_time_ms: 10016.424\n",
      "    update_time_ms: 1.791\n",
      "  timestamp: 1632136294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 442\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">         5007.28</td><td style=\"text-align: right;\">442000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 443000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-11-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 444\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7948654214541118\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013758280261806435\n",
      "          policy_loss: -0.036990583708716766\n",
      "          total_loss: -0.046279282743732136\n",
      "          vf_explained_var: -0.967132031917572\n",
      "          vf_loss: 0.00029236245981236505\n",
      "    num_agent_steps_sampled: 443000\n",
      "    num_agent_steps_trained: 443000\n",
      "    num_steps_sampled: 443000\n",
      "    num_steps_trained: 443000\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.96875\n",
      "    ram_util_percent: 67.50625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04062876201257128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.554990904511943\n",
      "    mean_inference_ms: 1.4074437990326754\n",
      "    mean_raw_obs_processing_ms: 0.6932218006770864\n",
      "  time_since_restore: 5018.201018571854\n",
      "  time_this_iter_s: 10.925855159759521\n",
      "  time_total_s: 5018.201018571854\n",
      "  timers:\n",
      "    learn_throughput: 1649.535\n",
      "    learn_time_ms: 606.231\n",
      "    load_throughput: 299182.835\n",
      "    load_time_ms: 3.342\n",
      "    sample_throughput: 99.753\n",
      "    sample_time_ms: 10024.722\n",
      "    update_time_ms: 1.795\n",
      "  timestamp: 1632136305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 443000\n",
      "  training_iteration: 443\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">          5018.2</td><td style=\"text-align: right;\">443000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-11-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 445\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.600283020072513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008600725005449188\n",
      "          policy_loss: -0.04099046836296717\n",
      "          total_loss: -0.05150047540664673\n",
      "          vf_explained_var: -0.9897683262825012\n",
      "          vf_loss: 0.00026198224050959755\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.23333333333335\n",
      "    ram_util_percent: 68.40666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04063023763313321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.554847513495785\n",
      "    mean_inference_ms: 1.4074591923338842\n",
      "    mean_raw_obs_processing_ms: 0.6931583786621122\n",
      "  time_since_restore: 5029.079985141754\n",
      "  time_this_iter_s: 10.878966569900513\n",
      "  time_total_s: 5029.079985141754\n",
      "  timers:\n",
      "    learn_throughput: 1642.958\n",
      "    learn_time_ms: 608.658\n",
      "    load_throughput: 300227.91\n",
      "    load_time_ms: 3.331\n",
      "    sample_throughput: 99.683\n",
      "    sample_time_ms: 10031.821\n",
      "    update_time_ms: 1.809\n",
      "  timestamp: 1632136316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 444\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         5029.08</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 445000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-12-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 446\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1302122672398887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012654704353608616\n",
      "          policy_loss: -0.10694534260158738\n",
      "          total_loss: -0.11969716443369786\n",
      "          vf_explained_var: -0.04102559760212898\n",
      "          vf_loss: 0.0008538890005891314\n",
      "    num_agent_steps_sampled: 445000\n",
      "    num_agent_steps_trained: 445000\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.61764705882354\n",
      "    ram_util_percent: 68.8529411764706\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040631802390305255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.554729352578187\n",
      "    mean_inference_ms: 1.407477545701251\n",
      "    mean_raw_obs_processing_ms: 0.693098940873283\n",
      "  time_since_restore: 5040.622889995575\n",
      "  time_this_iter_s: 11.5429048538208\n",
      "  time_total_s: 5040.622889995575\n",
      "  timers:\n",
      "    learn_throughput: 1628.255\n",
      "    learn_time_ms: 614.155\n",
      "    load_throughput: 298442.009\n",
      "    load_time_ms: 3.351\n",
      "    sample_throughput: 98.939\n",
      "    sample_time_ms: 10107.206\n",
      "    update_time_ms: 1.814\n",
      "  timestamp: 1632136328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 445\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         5040.62</td><td style=\"text-align: right;\">445000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 446000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-12-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 447\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0370157215330336\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0019474752666860558\n",
      "          policy_loss: -0.13359069791105058\n",
      "          total_loss: -0.1520075872540474\n",
      "          vf_explained_var: -0.5719944834709167\n",
      "          vf_loss: 0.000768840851702003\n",
      "    num_agent_steps_sampled: 446000\n",
      "    num_agent_steps_trained: 446000\n",
      "    num_steps_sampled: 446000\n",
      "    num_steps_trained: 446000\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.95625\n",
      "    ram_util_percent: 69.70625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04063339698123112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.554625748988178\n",
      "    mean_inference_ms: 1.4074972447992504\n",
      "    mean_raw_obs_processing_ms: 0.6930437259529896\n",
      "  time_since_restore: 5051.59840798378\n",
      "  time_this_iter_s: 10.975517988204956\n",
      "  time_total_s: 5051.59840798378\n",
      "  timers:\n",
      "    learn_throughput: 1628.55\n",
      "    learn_time_ms: 614.043\n",
      "    load_throughput: 298092.037\n",
      "    load_time_ms: 3.355\n",
      "    sample_throughput: 98.352\n",
      "    sample_time_ms: 10167.545\n",
      "    update_time_ms: 1.813\n",
      "  timestamp: 1632136339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 446000\n",
      "  training_iteration: 446\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">          5051.6</td><td style=\"text-align: right;\">446000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 447000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-12-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 448\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.634993741247389\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006761988914947716\n",
      "          policy_loss: -0.00623565970454365\n",
      "          total_loss: -0.006309724330074257\n",
      "          vf_explained_var: -0.6526934504508972\n",
      "          vf_loss: 0.014219599713720122\n",
      "    num_agent_steps_sampled: 447000\n",
      "    num_agent_steps_trained: 447000\n",
      "    num_steps_sampled: 447000\n",
      "    num_steps_trained: 447000\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.98\n",
      "    ram_util_percent: 69.22000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04063499487350903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55453621437102\n",
      "    mean_inference_ms: 1.407517112215808\n",
      "    mean_raw_obs_processing_ms: 0.6929924245933341\n",
      "  time_since_restore: 5062.392290115356\n",
      "  time_this_iter_s: 10.793882131576538\n",
      "  time_total_s: 5062.392290115356\n",
      "  timers:\n",
      "    learn_throughput: 1625.9\n",
      "    learn_time_ms: 615.044\n",
      "    load_throughput: 297428.29\n",
      "    load_time_ms: 3.362\n",
      "    sample_throughput: 97.966\n",
      "    sample_time_ms: 10207.655\n",
      "    update_time_ms: 1.812\n",
      "  timestamp: 1632136350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447000\n",
      "  training_iteration: 447\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         5062.39</td><td style=\"text-align: right;\">447000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-12-58\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 450\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6737669547398886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011879341606902629\n",
      "          policy_loss: 0.004516191884047455\n",
      "          total_loss: -0.008212958741933108\n",
      "          vf_explained_var: -0.8141162395477295\n",
      "          vf_loss: 0.0003960942332115438\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.31463414634147\n",
      "    ram_util_percent: 69.01951219512196\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040638245697265214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.554394518132991\n",
      "    mean_inference_ms: 1.4075605018551531\n",
      "    mean_raw_obs_processing_ms: 0.6936903778515577\n",
      "  time_since_restore: 5090.859262228012\n",
      "  time_this_iter_s: 28.46697211265564\n",
      "  time_total_s: 5090.859262228012\n",
      "  timers:\n",
      "    learn_throughput: 1627.985\n",
      "    learn_time_ms: 614.256\n",
      "    load_throughput: 204838.007\n",
      "    load_time_ms: 4.882\n",
      "    sample_throughput: 83.394\n",
      "    sample_time_ms: 11991.226\n",
      "    update_time_ms: 1.808\n",
      "  timestamp: 1632136378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 448\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         5090.86</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 449000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-13-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 451\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.683262778653039\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012714124404220841\n",
      "          policy_loss: -0.045427487588798006\n",
      "          total_loss: -0.05806313932666348\n",
      "          vf_explained_var: -0.9708525538444519\n",
      "          vf_loss: 0.0003307004680714777\n",
      "    num_agent_steps_sampled: 449000\n",
      "    num_agent_steps_trained: 449000\n",
      "    num_steps_sampled: 449000\n",
      "    num_steps_trained: 449000\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.129411764705885\n",
      "    ram_util_percent: 68.74705882352941\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04063980111142898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55434476044865\n",
      "    mean_inference_ms: 1.4075802640093271\n",
      "    mean_raw_obs_processing_ms: 0.6940499046134192\n",
      "  time_since_restore: 5102.852288246155\n",
      "  time_this_iter_s: 11.9930260181427\n",
      "  time_total_s: 5102.852288246155\n",
      "  timers:\n",
      "    learn_throughput: 1624.997\n",
      "    learn_time_ms: 615.386\n",
      "    load_throughput: 204869.023\n",
      "    load_time_ms: 4.881\n",
      "    sample_throughput: 82.37\n",
      "    sample_time_ms: 12140.303\n",
      "    update_time_ms: 1.822\n",
      "  timestamp: 1632136390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 449000\n",
      "  training_iteration: 449\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         5102.85</td><td style=\"text-align: right;\">449000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-13-20\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 452\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8981020463837517\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009507253414527739\n",
      "          policy_loss: -0.03293915684852335\n",
      "          total_loss: -0.04822647555006875\n",
      "          vf_explained_var: -0.3064396381378174\n",
      "          vf_loss: 0.0008026122936927196\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_agent_steps_trained: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.285714285714285\n",
      "    ram_util_percent: 68.99285714285715\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406413230364412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55427237715549\n",
      "    mean_inference_ms: 1.4075993415363808\n",
      "    mean_raw_obs_processing_ms: 0.6944120617569038\n",
      "  time_since_restore: 5112.987644910812\n",
      "  time_this_iter_s: 10.135356664657593\n",
      "  time_total_s: 5112.987644910812\n",
      "  timers:\n",
      "    learn_throughput: 1631.571\n",
      "    learn_time_ms: 612.906\n",
      "    load_throughput: 205425.907\n",
      "    load_time_ms: 4.868\n",
      "    sample_throughput: 82.737\n",
      "    sample_time_ms: 12086.513\n",
      "    update_time_ms: 1.815\n",
      "  timestamp: 1632136400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 450\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">         5112.99</td><td style=\"text-align: right;\">450000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 451000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-13-30\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 453\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7292574485143026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006826119837555211\n",
      "          policy_loss: -0.03868531059059832\n",
      "          total_loss: -0.05359688670270973\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003052236104849726\n",
      "    num_agent_steps_sampled: 451000\n",
      "    num_agent_steps_trained: 451000\n",
      "    num_steps_sampled: 451000\n",
      "    num_steps_trained: 451000\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.92666666666666\n",
      "    ram_util_percent: 68.88666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04064280564681719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.554194013550811\n",
      "    mean_inference_ms: 1.4076175386741585\n",
      "    mean_raw_obs_processing_ms: 0.6947770576894646\n",
      "  time_since_restore: 5123.265298366547\n",
      "  time_this_iter_s: 10.277653455734253\n",
      "  time_total_s: 5123.265298366547\n",
      "  timers:\n",
      "    learn_throughput: 1639.852\n",
      "    learn_time_ms: 609.811\n",
      "    load_throughput: 205156.62\n",
      "    load_time_ms: 4.874\n",
      "    sample_throughput: 82.913\n",
      "    sample_time_ms: 12060.862\n",
      "    update_time_ms: 1.801\n",
      "  timestamp: 1632136410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 451000\n",
      "  training_iteration: 451\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         5123.27</td><td style=\"text-align: right;\">451000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-13-41\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 454\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0598373227649267\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01578591001962923\n",
      "          policy_loss: 0.010771971113151974\n",
      "          total_loss: -0.004434583129154311\n",
      "          vf_explained_var: 0.13150723278522491\n",
      "          vf_loss: 0.0005914354690402333\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.6\n",
      "    ram_util_percent: 68.92\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040644278950774805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.554115020578458\n",
      "    mean_inference_ms: 1.4076356475591312\n",
      "    mean_raw_obs_processing_ms: 0.6951450328201263\n",
      "  time_since_restore: 5133.452189683914\n",
      "  time_this_iter_s: 10.186891317367554\n",
      "  time_total_s: 5133.452189683914\n",
      "  timers:\n",
      "    learn_throughput: 1647.41\n",
      "    learn_time_ms: 607.014\n",
      "    load_throughput: 206639.373\n",
      "    load_time_ms: 4.839\n",
      "    sample_throughput: 83.329\n",
      "    sample_time_ms: 12000.665\n",
      "    update_time_ms: 1.79\n",
      "  timestamp: 1632136421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 452\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">         5133.45</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 453000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-13-51\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 455\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.734063282277849\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008865022324724907\n",
      "          policy_loss: -0.0020436628411213556\n",
      "          total_loss: -0.016210566750831074\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004779382578110219\n",
      "    num_agent_steps_sampled: 453000\n",
      "    num_agent_steps_trained: 453000\n",
      "    num_steps_sampled: 453000\n",
      "    num_steps_trained: 453000\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.55333333333333\n",
      "    ram_util_percent: 68.87333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04064574972269159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.554039564452351\n",
      "    mean_inference_ms: 1.4076539097196843\n",
      "    mean_raw_obs_processing_ms: 0.6955154926710518\n",
      "  time_since_restore: 5143.907359361649\n",
      "  time_this_iter_s: 10.455169677734375\n",
      "  time_total_s: 5143.907359361649\n",
      "  timers:\n",
      "    learn_throughput: 1652.739\n",
      "    learn_time_ms: 605.056\n",
      "    load_throughput: 207642.924\n",
      "    load_time_ms: 4.816\n",
      "    sample_throughput: 83.643\n",
      "    sample_time_ms: 11955.574\n",
      "    update_time_ms: 1.78\n",
      "  timestamp: 1632136431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 453000\n",
      "  training_iteration: 453\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         5143.91</td><td style=\"text-align: right;\">453000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 454000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-14-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 456\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6906702796618143\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008997525630371426\n",
      "          policy_loss: 0.0053117964416742325\n",
      "          total_loss: -0.00856664946509732\n",
      "          vf_explained_var: -0.9993357062339783\n",
      "          vf_loss: 0.0002921747992837077\n",
      "    num_agent_steps_sampled: 454000\n",
      "    num_agent_steps_trained: 454000\n",
      "    num_steps_sampled: 454000\n",
      "    num_steps_trained: 454000\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.52142857142858\n",
      "    ram_util_percent: 68.70714285714287\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04064722034538956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553968532632682\n",
      "    mean_inference_ms: 1.4076722844695362\n",
      "    mean_raw_obs_processing_ms: 0.6958884618241434\n",
      "  time_since_restore: 5154.298641443253\n",
      "  time_this_iter_s: 10.391282081604004\n",
      "  time_total_s: 5154.298641443253\n",
      "  timers:\n",
      "    learn_throughput: 1660.929\n",
      "    learn_time_ms: 602.073\n",
      "    load_throughput: 207089.308\n",
      "    load_time_ms: 4.829\n",
      "    sample_throughput: 83.964\n",
      "    sample_time_ms: 11909.843\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1632136442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 454000\n",
      "  training_iteration: 454\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">          5154.3</td><td style=\"text-align: right;\">454000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 455000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-14-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 457\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6734193232324388\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010423090297489177\n",
      "          policy_loss: 0.00741298809233639\n",
      "          total_loss: -0.005651730971617831\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004998862056203911\n",
      "    num_agent_steps_sampled: 455000\n",
      "    num_agent_steps_trained: 455000\n",
      "    num_steps_sampled: 455000\n",
      "    num_steps_trained: 455000\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.480000000000004\n",
      "    ram_util_percent: 68.48666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04064868591207319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553905697243032\n",
      "    mean_inference_ms: 1.4076907866914368\n",
      "    mean_raw_obs_processing_ms: 0.6962639750287274\n",
      "  time_since_restore: 5164.828599691391\n",
      "  time_this_iter_s: 10.529958248138428\n",
      "  time_total_s: 5164.828599691391\n",
      "  timers:\n",
      "    learn_throughput: 1674.951\n",
      "    learn_time_ms: 597.032\n",
      "    load_throughput: 208188.14\n",
      "    load_time_ms: 4.803\n",
      "    sample_throughput: 84.648\n",
      "    sample_time_ms: 11813.605\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632136452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455000\n",
      "  training_iteration: 455\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">         5164.83</td><td style=\"text-align: right;\">455000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-14-23\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 458\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7095012373394436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00708403366116972\n",
      "          policy_loss: -0.07431302136845058\n",
      "          total_loss: -0.08906440018779702\n",
      "          vf_explained_var: -0.999800443649292\n",
      "          vf_loss: 0.0001894273125799373\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.45333333333334\n",
      "    ram_util_percent: 68.37999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04065014945751051\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553839778140585\n",
      "    mean_inference_ms: 1.4077094404955737\n",
      "    mean_raw_obs_processing_ms: 0.6966415256552988\n",
      "  time_since_restore: 5175.237856864929\n",
      "  time_this_iter_s: 10.409257173538208\n",
      "  time_total_s: 5175.237856864929\n",
      "  timers:\n",
      "    learn_throughput: 1675.178\n",
      "    learn_time_ms: 596.951\n",
      "    load_throughput: 208554.594\n",
      "    load_time_ms: 4.795\n",
      "    sample_throughput: 85.055\n",
      "    sample_time_ms: 11757.044\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632136463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 456\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         5175.24</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 457000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-14-33\n",
      "  done: false\n",
      "  episode_len_mean: 994.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 459\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6598049230045742\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011444420374910318\n",
      "          policy_loss: -0.0554173581302166\n",
      "          total_loss: -0.06818350938459238\n",
      "          vf_explained_var: -0.8252612352371216\n",
      "          vf_loss: 0.0003517284436384216\n",
      "    num_agent_steps_sampled: 457000\n",
      "    num_agent_steps_trained: 457000\n",
      "    num_steps_sampled: 457000\n",
      "    num_steps_trained: 457000\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.419999999999995\n",
      "    ram_util_percent: 68.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04065161110104891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55377773034408\n",
      "    mean_inference_ms: 1.4077281644185502\n",
      "    mean_raw_obs_processing_ms: 0.6970220674597661\n",
      "  time_since_restore: 5185.747431278229\n",
      "  time_this_iter_s: 10.50957441329956\n",
      "  time_total_s: 5185.747431278229\n",
      "  timers:\n",
      "    learn_throughput: 1676.495\n",
      "    learn_time_ms: 596.482\n",
      "    load_throughput: 208696.76\n",
      "    load_time_ms: 4.792\n",
      "    sample_throughput: 85.258\n",
      "    sample_time_ms: 11729.061\n",
      "    update_time_ms: 1.778\n",
      "  timestamp: 1632136473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 457000\n",
      "  training_iteration: 457\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         5185.75</td><td style=\"text-align: right;\">457000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 458000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-14-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 460\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8559256937768724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014816495131248135\n",
      "          policy_loss: 0.012583499236239328\n",
      "          total_loss: -0.0010045219626691607\n",
      "          vf_explained_var: -0.2798888683319092\n",
      "          vf_loss: 0.00046564099514701717\n",
      "    num_agent_steps_sampled: 458000\n",
      "    num_agent_steps_trained: 458000\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.76666666666667\n",
      "    ram_util_percent: 68.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04065306745871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553720143181279\n",
      "    mean_inference_ms: 1.4077469423769151\n",
      "    mean_raw_obs_processing_ms: 0.6969030129808556\n",
      "  time_since_restore: 5196.198181390762\n",
      "  time_this_iter_s: 10.45075011253357\n",
      "  time_total_s: 5196.198181390762\n",
      "  timers:\n",
      "    learn_throughput: 1674.768\n",
      "    learn_time_ms: 597.098\n",
      "    load_throughput: 310254.828\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 100.721\n",
      "    sample_time_ms: 9928.392\n",
      "    update_time_ms: 1.79\n",
      "  timestamp: 1632136484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 458\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">          5196.2</td><td style=\"text-align: right;\">458000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 459000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-14-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 461\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.747800894578298\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015445979096165156\n",
      "          policy_loss: -0.09592320397496223\n",
      "          total_loss: -0.10802764271696409\n",
      "          vf_explained_var: -0.2018609195947647\n",
      "          vf_loss: 0.0006765577968003021\n",
      "    num_agent_steps_sampled: 459000\n",
      "    num_agent_steps_trained: 459000\n",
      "    num_steps_sampled: 459000\n",
      "    num_steps_trained: 459000\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.43999999999999\n",
      "    ram_util_percent: 68.27333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040654519895337415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553666465614862\n",
      "    mean_inference_ms: 1.4077658037834644\n",
      "    mean_raw_obs_processing_ms: 0.696787347485574\n",
      "  time_since_restore: 5206.573171615601\n",
      "  time_this_iter_s: 10.374990224838257\n",
      "  time_total_s: 5206.573171615601\n",
      "  timers:\n",
      "    learn_throughput: 1675.148\n",
      "    learn_time_ms: 596.962\n",
      "    load_throughput: 311027.86\n",
      "    load_time_ms: 3.215\n",
      "    sample_throughput: 102.388\n",
      "    sample_time_ms: 9766.756\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632136494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 459000\n",
      "  training_iteration: 459\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">         5206.57</td><td style=\"text-align: right;\">459000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-15-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 462\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7334299471643235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008133758352264467\n",
      "          policy_loss: -0.05019074127905899\n",
      "          total_loss: -0.06469245750664009\n",
      "          vf_explained_var: -0.442300409078598\n",
      "          vf_loss: 0.0003591654744620124\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.480000000000004\n",
      "    ram_util_percent: 68.28666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04065597238812142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553628686135744\n",
      "    mean_inference_ms: 1.407784811480805\n",
      "    mean_raw_obs_processing_ms: 0.6966755063363496\n",
      "  time_since_restore: 5217.070899248123\n",
      "  time_this_iter_s: 10.497727632522583\n",
      "  time_total_s: 5217.070899248123\n",
      "  timers:\n",
      "    learn_throughput: 1675.614\n",
      "    learn_time_ms: 596.796\n",
      "    load_throughput: 310604.057\n",
      "    load_time_ms: 3.22\n",
      "    sample_throughput: 102.008\n",
      "    sample_time_ms: 9803.179\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632136505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 460\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">         5217.07</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 461000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-15-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 463\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7123467644055685\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015515050658277553\n",
      "          policy_loss: -0.12357410291830699\n",
      "          total_loss: -0.13549091120560963\n",
      "          vf_explained_var: -0.07998088002204895\n",
      "          vf_loss: 0.0004886427546605571\n",
      "    num_agent_steps_sampled: 461000\n",
      "    num_agent_steps_trained: 461000\n",
      "    num_steps_sampled: 461000\n",
      "    num_steps_trained: 461000\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.54\n",
      "    ram_util_percent: 68.29333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04065742454577972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553605680134728\n",
      "    mean_inference_ms: 1.407804135992921\n",
      "    mean_raw_obs_processing_ms: 0.6965675943380889\n",
      "  time_since_restore: 5227.516890048981\n",
      "  time_this_iter_s: 10.445990800857544\n",
      "  time_total_s: 5227.516890048981\n",
      "  timers:\n",
      "    learn_throughput: 1674.664\n",
      "    learn_time_ms: 597.135\n",
      "    load_throughput: 308574.876\n",
      "    load_time_ms: 3.241\n",
      "    sample_throughput: 101.837\n",
      "    sample_time_ms: 9819.642\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632136515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 461000\n",
      "  training_iteration: 461\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         5227.52</td><td style=\"text-align: right;\">461000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 462000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 464\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6959128975868225\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011361611184698549\n",
      "          policy_loss: -0.08504523684581121\n",
      "          total_loss: -0.09721843898296356\n",
      "          vf_explained_var: -0.1505029946565628\n",
      "          vf_loss: 0.0013309400832642697\n",
      "    num_agent_steps_sampled: 462000\n",
      "    num_agent_steps_trained: 462000\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.446666666666665\n",
      "    ram_util_percent: 68.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04065887825258793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553591632326963\n",
      "    mean_inference_ms: 1.4078237411625834\n",
      "    mean_raw_obs_processing_ms: 0.6964634138885764\n",
      "  time_since_restore: 5237.9717535972595\n",
      "  time_this_iter_s: 10.454863548278809\n",
      "  time_total_s: 5237.9717535972595\n",
      "  timers:\n",
      "    learn_throughput: 1671.242\n",
      "    learn_time_ms: 598.357\n",
      "    load_throughput: 307277.269\n",
      "    load_time_ms: 3.254\n",
      "    sample_throughput: 101.572\n",
      "    sample_time_ms: 9845.246\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632136525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 462\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   462</td><td style=\"text-align: right;\">         5237.97</td><td style=\"text-align: right;\">462000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 463000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-15-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 465\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7481434928046333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007800079223927901\n",
      "          policy_loss: -0.10114671231971847\n",
      "          total_loss: -0.11584152405460675\n",
      "          vf_explained_var: -0.5631569623947144\n",
      "          vf_loss: 0.00041467044179120823\n",
      "    num_agent_steps_sampled: 463000\n",
      "    num_agent_steps_trained: 463000\n",
      "    num_steps_sampled: 463000\n",
      "    num_steps_trained: 463000\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.40666666666666\n",
      "    ram_util_percent: 68.34666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04066033624747344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553587173297398\n",
      "    mean_inference_ms: 1.4078436380740924\n",
      "    mean_raw_obs_processing_ms: 0.6963629008985504\n",
      "  time_since_restore: 5248.430443763733\n",
      "  time_this_iter_s: 10.458690166473389\n",
      "  time_total_s: 5248.430443763733\n",
      "  timers:\n",
      "    learn_throughput: 1670.256\n",
      "    learn_time_ms: 598.711\n",
      "    load_throughput: 310153.883\n",
      "    load_time_ms: 3.224\n",
      "    sample_throughput: 101.572\n",
      "    sample_time_ms: 9845.255\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1632136536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463000\n",
      "  training_iteration: 463\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   463</td><td style=\"text-align: right;\">         5248.43</td><td style=\"text-align: right;\">463000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-15-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 466\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7640856928295559\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010108653000673219\n",
      "          policy_loss: -0.05413470034384065\n",
      "          total_loss: -0.06835486601210303\n",
      "          vf_explained_var: -0.9083054661750793\n",
      "          vf_loss: 0.00034671848099808103\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.79333333333333\n",
      "    ram_util_percent: 68.42666666666668\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040661797639697655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553590820386933\n",
      "    mean_inference_ms: 1.4078639685153602\n",
      "    mean_raw_obs_processing_ms: 0.696265862489319\n",
      "  time_since_restore: 5258.971605539322\n",
      "  time_this_iter_s: 10.54116177558899\n",
      "  time_total_s: 5258.971605539322\n",
      "  timers:\n",
      "    learn_throughput: 1664.296\n",
      "    learn_time_ms: 600.855\n",
      "    load_throughput: 312196.981\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 101.44\n",
      "    sample_time_ms: 9858.092\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632136546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 464\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   464</td><td style=\"text-align: right;\">         5258.97</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-15-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 467\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8181776483853658\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008807931640520966\n",
      "          policy_loss: -0.07644647728237841\n",
      "          total_loss: -0.09140252992510796\n",
      "          vf_explained_var: -0.7753193378448486\n",
      "          vf_loss: 0.0005472941432445724\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_agent_steps_trained: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.586666666666666\n",
      "    ram_util_percent: 68.53999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406632790712807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553603832051353\n",
      "    mean_inference_ms: 1.4078853778568092\n",
      "    mean_raw_obs_processing_ms: 0.6961724850712271\n",
      "  time_since_restore: 5269.618040323257\n",
      "  time_this_iter_s: 10.646434783935547\n",
      "  time_total_s: 5269.618040323257\n",
      "  timers:\n",
      "    learn_throughput: 1665.983\n",
      "    learn_time_ms: 600.246\n",
      "    load_throughput: 311587.017\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 101.313\n",
      "    sample_time_ms: 9870.363\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632136557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 465\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">         5269.62</td><td style=\"text-align: right;\">465000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 466000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-16-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 468\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8178137395117018\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009455422772329945\n",
      "          policy_loss: 0.0329044450695316\n",
      "          total_loss: 0.017801982723176478\n",
      "          vf_explained_var: -0.6287462711334229\n",
      "          vf_loss: 0.00020034787521581166\n",
      "    num_agent_steps_sampled: 466000\n",
      "    num_agent_steps_trained: 466000\n",
      "    num_steps_sampled: 466000\n",
      "    num_steps_trained: 466000\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.38666666666666\n",
      "    ram_util_percent: 68.58\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04066477111124159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553623727857376\n",
      "    mean_inference_ms: 1.4079075066009616\n",
      "    mean_raw_obs_processing_ms: 0.6960829740244434\n",
      "  time_since_restore: 5280.176951885223\n",
      "  time_this_iter_s: 10.558911561965942\n",
      "  time_total_s: 5280.176951885223\n",
      "  timers:\n",
      "    learn_throughput: 1665.744\n",
      "    learn_time_ms: 600.332\n",
      "    load_throughput: 311277.153\n",
      "    load_time_ms: 3.213\n",
      "    sample_throughput: 101.161\n",
      "    sample_time_ms: 9885.255\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632136568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 466000\n",
      "  training_iteration: 466\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   466</td><td style=\"text-align: right;\">         5280.18</td><td style=\"text-align: right;\">466000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 467000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-16-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 469\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.978594920370314\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00859585086964548\n",
      "          policy_loss: 0.00346029092454248\n",
      "          total_loss: -0.012948209916551908\n",
      "          vf_explained_var: -0.47386741638183594\n",
      "          vf_loss: 0.0007635072546286715\n",
      "    num_agent_steps_sampled: 467000\n",
      "    num_agent_steps_trained: 467000\n",
      "    num_steps_sampled: 467000\n",
      "    num_steps_trained: 467000\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.54666666666667\n",
      "    ram_util_percent: 68.62666666666668\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040666261495169206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553641916341567\n",
      "    mean_inference_ms: 1.4079297498168821\n",
      "    mean_raw_obs_processing_ms: 0.6959968124475783\n",
      "  time_since_restore: 5290.352988243103\n",
      "  time_this_iter_s: 10.176036357879639\n",
      "  time_total_s: 5290.352988243103\n",
      "  timers:\n",
      "    learn_throughput: 1666.859\n",
      "    learn_time_ms: 599.931\n",
      "    load_throughput: 311663.422\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 101.499\n",
      "    sample_time_ms: 9852.291\n",
      "    update_time_ms: 1.762\n",
      "  timestamp: 1632136578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 467000\n",
      "  training_iteration: 467\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   467</td><td style=\"text-align: right;\">         5290.35</td><td style=\"text-align: right;\">467000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-16-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 470\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8151022950808207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011185516016381274\n",
      "          policy_loss: -0.11814996939566401\n",
      "          total_loss: -0.13234559413459565\n",
      "          vf_explained_var: -0.9530873894691467\n",
      "          vf_loss: 0.0005539603676879779\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.49333333333333\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04066771470328679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553656715657164\n",
      "    mean_inference_ms: 1.4079510069128207\n",
      "    mean_raw_obs_processing_ms: 0.6959139806190382\n",
      "  time_since_restore: 5300.833432674408\n",
      "  time_this_iter_s: 10.480444431304932\n",
      "  time_total_s: 5300.833432674408\n",
      "  timers:\n",
      "    learn_throughput: 1665.284\n",
      "    learn_time_ms: 600.498\n",
      "    load_throughput: 310988.656\n",
      "    load_time_ms: 3.216\n",
      "    sample_throughput: 101.474\n",
      "    sample_time_ms: 9854.697\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632136588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 468\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   468</td><td style=\"text-align: right;\">         5300.83</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 469000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-16-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 471\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7049630999565124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019142494334334344\n",
      "          policy_loss: -0.06589969461783766\n",
      "          total_loss: -0.07483141608536244\n",
      "          vf_explained_var: -0.45830488204956055\n",
      "          vf_loss: 0.0022968110327686495\n",
      "    num_agent_steps_sampled: 469000\n",
      "    num_agent_steps_trained: 469000\n",
      "    num_steps_sampled: 469000\n",
      "    num_steps_trained: 469000\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.56666666666668\n",
      "    ram_util_percent: 68.77333333333331\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04066915254672963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553659009514886\n",
      "    mean_inference_ms: 1.407971235686172\n",
      "    mean_raw_obs_processing_ms: 0.6958344711778922\n",
      "  time_since_restore: 5311.084294319153\n",
      "  time_this_iter_s: 10.250861644744873\n",
      "  time_total_s: 5311.084294319153\n",
      "  timers:\n",
      "    learn_throughput: 1666.332\n",
      "    learn_time_ms: 600.12\n",
      "    load_throughput: 309604.423\n",
      "    load_time_ms: 3.23\n",
      "    sample_throughput: 101.599\n",
      "    sample_time_ms: 9842.64\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632136599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 469000\n",
      "  training_iteration: 469\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         5311.08</td><td style=\"text-align: right;\">469000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 470000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-16-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 472\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7892375694380867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009544922374260563\n",
      "          policy_loss: -0.03450334332883358\n",
      "          total_loss: -0.0490371404784835\n",
      "          vf_explained_var: -0.9954811334609985\n",
      "          vf_loss: 0.00045603531697351073\n",
      "    num_agent_steps_sampled: 470000\n",
      "    num_agent_steps_trained: 470000\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.673333333333325\n",
      "    ram_util_percent: 68.87999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040670591997896285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553670212176495\n",
      "    mean_inference_ms: 1.4079918270089005\n",
      "    mean_raw_obs_processing_ms: 0.69575844727703\n",
      "  time_since_restore: 5321.738842964172\n",
      "  time_this_iter_s: 10.654548645019531\n",
      "  time_total_s: 5321.738842964172\n",
      "  timers:\n",
      "    learn_throughput: 1658.92\n",
      "    learn_time_ms: 602.802\n",
      "    load_throughput: 306948.955\n",
      "    load_time_ms: 3.258\n",
      "    sample_throughput: 101.465\n",
      "    sample_time_ms: 9855.596\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632136609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 470\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   470</td><td style=\"text-align: right;\">         5321.74</td><td style=\"text-align: right;\">470000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 471000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-17-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 473\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8150219149059719\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012021553367390069\n",
      "          policy_loss: -0.01869225891100036\n",
      "          total_loss: -0.0325640960286061\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000622711110918317\n",
      "    num_agent_steps_sampled: 471000\n",
      "    num_agent_steps_trained: 471000\n",
      "    num_steps_sampled: 471000\n",
      "    num_steps_trained: 471000\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.666666666666664\n",
      "    ram_util_percent: 68.96000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040672031315644334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553690958522044\n",
      "    mean_inference_ms: 1.4080128186936867\n",
      "    mean_raw_obs_processing_ms: 0.6956859270578781\n",
      "  time_since_restore: 5332.384187698364\n",
      "  time_this_iter_s: 10.645344734191895\n",
      "  time_total_s: 5332.384187698364\n",
      "  timers:\n",
      "    learn_throughput: 1656.196\n",
      "    learn_time_ms: 603.793\n",
      "    load_throughput: 309487.914\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 101.27\n",
      "    sample_time_ms: 9874.586\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632136620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471000\n",
      "  training_iteration: 471\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   471</td><td style=\"text-align: right;\">         5332.38</td><td style=\"text-align: right;\">471000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-17-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 474\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8699987106853062\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011321325460206496\n",
      "          policy_loss: -0.0344854135480192\n",
      "          total_loss: -0.040011297166347506\n",
      "          vf_explained_var: -0.5715206861495972\n",
      "          vf_loss: 0.009731368813016969\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.6125\n",
      "    ram_util_percent: 69.00625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040673460187449544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55371777862451\n",
      "    mean_inference_ms: 1.4080340806598952\n",
      "    mean_raw_obs_processing_ms: 0.6956165543223478\n",
      "  time_since_restore: 5343.256672859192\n",
      "  time_this_iter_s: 10.872485160827637\n",
      "  time_total_s: 5343.256672859192\n",
      "  timers:\n",
      "    learn_throughput: 1660.873\n",
      "    learn_time_ms: 602.093\n",
      "    load_throughput: 310250.239\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 100.826\n",
      "    sample_time_ms: 9918.032\n",
      "    update_time_ms: 1.768\n",
      "  timestamp: 1632136631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 472\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">         5343.26</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 473000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 475\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8411512639787462\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01684482735623843\n",
      "          policy_loss: 0.05025360302792655\n",
      "          total_loss: 0.07607605341407987\n",
      "          vf_explained_var: -0.14601878821849823\n",
      "          vf_loss: 0.03911157234752965\n",
      "    num_agent_steps_sampled: 473000\n",
      "    num_agent_steps_trained: 473000\n",
      "    num_steps_sampled: 473000\n",
      "    num_steps_trained: 473000\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.85333333333333\n",
      "    ram_util_percent: 69.08\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040674850534570935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553739736217777\n",
      "    mean_inference_ms: 1.4080546541073014\n",
      "    mean_raw_obs_processing_ms: 0.6955504951670661\n",
      "  time_since_restore: 5354.137071609497\n",
      "  time_this_iter_s: 10.880398750305176\n",
      "  time_total_s: 5354.137071609497\n",
      "  timers:\n",
      "    learn_throughput: 1655.396\n",
      "    learn_time_ms: 604.085\n",
      "    load_throughput: 306922.002\n",
      "    load_time_ms: 3.258\n",
      "    sample_throughput: 100.42\n",
      "    sample_time_ms: 9958.192\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632136642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 473000\n",
      "  training_iteration: 473\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         5354.14</td><td style=\"text-align: right;\">473000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 474000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-17-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 476\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9334402402242026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013531625992307995\n",
      "          policy_loss: -0.073523530157076\n",
      "          total_loss: -0.06399068819979827\n",
      "          vf_explained_var: 0.10787691175937653\n",
      "          vf_loss: 0.024752374545722787\n",
      "    num_agent_steps_sampled: 474000\n",
      "    num_agent_steps_trained: 474000\n",
      "    num_steps_sampled: 474000\n",
      "    num_steps_trained: 474000\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.6375\n",
      "    ram_util_percent: 69.33125000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040676289549882644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553764234588687\n",
      "    mean_inference_ms: 1.408077023223938\n",
      "    mean_raw_obs_processing_ms: 0.6954876862795879\n",
      "  time_since_restore: 5365.554469347\n",
      "  time_this_iter_s: 11.417397737503052\n",
      "  time_total_s: 5365.554469347\n",
      "  timers:\n",
      "    learn_throughput: 1658.002\n",
      "    learn_time_ms: 603.136\n",
      "    load_throughput: 303098.258\n",
      "    load_time_ms: 3.299\n",
      "    sample_throughput: 99.535\n",
      "    sample_time_ms: 10046.767\n",
      "    update_time_ms: 1.769\n",
      "  timestamp: 1632136653\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 474000\n",
      "  training_iteration: 474\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   474</td><td style=\"text-align: right;\">         5365.55</td><td style=\"text-align: right;\">474000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 475000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-17-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 477\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6400721311569213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017451846004181013\n",
      "          policy_loss: -0.020514689220322502\n",
      "          total_loss: -0.006132721114489767\n",
      "          vf_explained_var: 0.004722881130874157\n",
      "          vf_loss: 0.025475705004323067\n",
      "    num_agent_steps_sampled: 475000\n",
      "    num_agent_steps_trained: 475000\n",
      "    num_steps_sampled: 475000\n",
      "    num_steps_trained: 475000\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.16875\n",
      "    ram_util_percent: 69.43124999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04067774547865548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553776205233614\n",
      "    mean_inference_ms: 1.4080992285252933\n",
      "    mean_raw_obs_processing_ms: 0.6954284036229218\n",
      "  time_since_restore: 5376.547629594803\n",
      "  time_this_iter_s: 10.993160247802734\n",
      "  time_total_s: 5376.547629594803\n",
      "  timers:\n",
      "    learn_throughput: 1651.789\n",
      "    learn_time_ms: 605.404\n",
      "    load_throughput: 303701.794\n",
      "    load_time_ms: 3.293\n",
      "    sample_throughput: 99.214\n",
      "    sample_time_ms: 10079.177\n",
      "    update_time_ms: 1.78\n",
      "  timestamp: 1632136664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 475000\n",
      "  training_iteration: 475\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   475</td><td style=\"text-align: right;\">         5376.55</td><td style=\"text-align: right;\">475000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-17-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 478\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8175673908657497\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012693872788907731\n",
      "          policy_loss: 0.0294212330546644\n",
      "          total_loss: 0.015562412722243204\n",
      "          vf_explained_var: -0.7038480639457703\n",
      "          vf_loss: 0.00045673436511100993\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.9875\n",
      "    ram_util_percent: 69.41875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04067920835829404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55378750442572\n",
      "    mean_inference_ms: 1.4081220696731982\n",
      "    mean_raw_obs_processing_ms: 0.6953716435003672\n",
      "  time_since_restore: 5387.387704610825\n",
      "  time_this_iter_s: 10.840075016021729\n",
      "  time_total_s: 5387.387704610825\n",
      "  timers:\n",
      "    learn_throughput: 1649.572\n",
      "    learn_time_ms: 606.218\n",
      "    load_throughput: 301941.819\n",
      "    load_time_ms: 3.312\n",
      "    sample_throughput: 98.947\n",
      "    sample_time_ms: 10106.445\n",
      "    update_time_ms: 1.794\n",
      "  timestamp: 1632136675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 476\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   476</td><td style=\"text-align: right;\">         5387.39</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 477000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-18-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 479\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8867312762472364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009778116240845512\n",
      "          policy_loss: -0.13101996150281695\n",
      "          total_loss: -0.1391363185313013\n",
      "          vf_explained_var: -0.5991606116294861\n",
      "          vf_loss: 0.007777500227611098\n",
      "    num_agent_steps_sampled: 477000\n",
      "    num_agent_steps_trained: 477000\n",
      "    num_steps_sampled: 477000\n",
      "    num_steps_trained: 477000\n",
      "  iterations_since_restore: 477\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.89333333333333\n",
      "    ram_util_percent: 69.28666666666665\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040680665151603616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553791952118392\n",
      "    mean_inference_ms: 1.408144881448838\n",
      "    mean_raw_obs_processing_ms: 0.6953179783690286\n",
      "  time_since_restore: 5397.881100893021\n",
      "  time_this_iter_s: 10.493396282196045\n",
      "  time_total_s: 5397.881100893021\n",
      "  timers:\n",
      "    learn_throughput: 1650.563\n",
      "    learn_time_ms: 605.854\n",
      "    load_throughput: 301907.045\n",
      "    load_time_ms: 3.312\n",
      "    sample_throughput: 98.633\n",
      "    sample_time_ms: 10138.544\n",
      "    update_time_ms: 1.786\n",
      "  timestamp: 1632136686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 477000\n",
      "  training_iteration: 477\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   477</td><td style=\"text-align: right;\">         5397.88</td><td style=\"text-align: right;\">477000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 478000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 480\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.926879644393921\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011020008690977849\n",
      "          policy_loss: -0.07897354952163166\n",
      "          total_loss: -0.08980505536827776\n",
      "          vf_explained_var: 0.3285982310771942\n",
      "          vf_loss: 0.0050861859053839\n",
      "    num_agent_steps_sampled: 478000\n",
      "    num_agent_steps_trained: 478000\n",
      "    num_steps_sampled: 478000\n",
      "    num_steps_trained: 478000\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.3775\n",
      "    ram_util_percent: 69.2025\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04068206772901415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553782180685673\n",
      "    mean_inference_ms: 1.408166090733695\n",
      "    mean_raw_obs_processing_ms: 0.6956339472304862\n",
      "  time_since_restore: 5426.279827594757\n",
      "  time_this_iter_s: 28.39872670173645\n",
      "  time_total_s: 5426.279827594757\n",
      "  timers:\n",
      "    learn_throughput: 1649.085\n",
      "    learn_time_ms: 606.397\n",
      "    load_throughput: 214425.046\n",
      "    load_time_ms: 4.664\n",
      "    sample_throughput: 83.833\n",
      "    sample_time_ms: 11928.462\n",
      "    update_time_ms: 1.795\n",
      "  timestamp: 1632136714\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 478000\n",
      "  training_iteration: 478\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   478</td><td style=\"text-align: right;\">         5426.28</td><td style=\"text-align: right;\">478000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 479000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-18-46\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 481\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9272206915749444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010689074802706502\n",
      "          policy_loss: -0.04325679308838314\n",
      "          total_loss: -0.05342018351786666\n",
      "          vf_explained_var: 0.0029630574863404036\n",
      "          vf_loss: 0.005858343371397091\n",
      "    num_agent_steps_sampled: 479000\n",
      "    num_agent_steps_trained: 479000\n",
      "    num_steps_sampled: 479000\n",
      "    num_steps_trained: 479000\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.68888888888889\n",
      "    ram_util_percent: 68.84444444444443\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04068343559882447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553794362837138\n",
      "    mean_inference_ms: 1.4081860868378164\n",
      "    mean_raw_obs_processing_ms: 0.6959521940107011\n",
      "  time_since_restore: 5438.455116033554\n",
      "  time_this_iter_s: 12.175288438796997\n",
      "  time_total_s: 5438.455116033554\n",
      "  timers:\n",
      "    learn_throughput: 1645.633\n",
      "    learn_time_ms: 607.669\n",
      "    load_throughput: 214896.351\n",
      "    load_time_ms: 4.653\n",
      "    sample_throughput: 82.511\n",
      "    sample_time_ms: 12119.621\n",
      "    update_time_ms: 1.801\n",
      "  timestamp: 1632136726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479000\n",
      "  training_iteration: 479\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   479</td><td style=\"text-align: right;\">         5438.46</td><td style=\"text-align: right;\">479000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-18-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 482\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9559771829181247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01421764315326336\n",
      "          policy_loss: 0.01906017205781407\n",
      "          total_loss: 0.005599641717142529\n",
      "          vf_explained_var: -0.25826889276504517\n",
      "          vf_loss: 0.0017757566448482168\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.81333333333334\n",
      "    ram_util_percent: 69.05333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04068478727689718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553785980993581\n",
      "    mean_inference_ms: 1.408205313789355\n",
      "    mean_raw_obs_processing_ms: 0.6962726315617445\n",
      "  time_since_restore: 5448.872614145279\n",
      "  time_this_iter_s: 10.417498111724854\n",
      "  time_total_s: 5448.872614145279\n",
      "  timers:\n",
      "    learn_throughput: 1648.861\n",
      "    learn_time_ms: 606.479\n",
      "    load_throughput: 214796.204\n",
      "    load_time_ms: 4.656\n",
      "    sample_throughput: 82.664\n",
      "    sample_time_ms: 12097.125\n",
      "    update_time_ms: 1.799\n",
      "  timestamp: 1632136737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 480\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   480</td><td style=\"text-align: right;\">         5448.87</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 481000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-19-07\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 483\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.879900218380822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009883977764742606\n",
      "          policy_loss: -0.031028990757962067\n",
      "          total_loss: -0.04637696420152982\n",
      "          vf_explained_var: -0.9917554259300232\n",
      "          vf_loss: 0.00044538067013490947\n",
      "    num_agent_steps_sampled: 481000\n",
      "    num_agent_steps_trained: 481000\n",
      "    num_steps_sampled: 481000\n",
      "    num_steps_trained: 481000\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.99285714285715\n",
      "    ram_util_percent: 69.08571428571429\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040686091063439477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553752076839972\n",
      "    mean_inference_ms: 1.408223480501525\n",
      "    mean_raw_obs_processing_ms: 0.696595238928959\n",
      "  time_since_restore: 5459.068782567978\n",
      "  time_this_iter_s: 10.196168422698975\n",
      "  time_total_s: 5459.068782567978\n",
      "  timers:\n",
      "    learn_throughput: 1641.066\n",
      "    learn_time_ms: 609.36\n",
      "    load_throughput: 214689.557\n",
      "    load_time_ms: 4.658\n",
      "    sample_throughput: 82.992\n",
      "    sample_time_ms: 12049.284\n",
      "    update_time_ms: 1.809\n",
      "  timestamp: 1632136747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 481000\n",
      "  training_iteration: 481\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   481</td><td style=\"text-align: right;\">         5459.07</td><td style=\"text-align: right;\">481000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 482000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-19-17\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 484\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8901672813627455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017494307707347487\n",
      "          policy_loss: -0.05835503372881148\n",
      "          total_loss: -0.0711410397870673\n",
      "          vf_explained_var: -0.4874001443386078\n",
      "          vf_loss: 0.0007957712339702993\n",
      "    num_agent_steps_sampled: 482000\n",
      "    num_agent_steps_trained: 482000\n",
      "    num_steps_sampled: 482000\n",
      "    num_steps_trained: 482000\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.986666666666665\n",
      "    ram_util_percent: 69.09333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406873300527454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553691646926275\n",
      "    mean_inference_ms: 1.4082396145126865\n",
      "    mean_raw_obs_processing_ms: 0.6969198226700213\n",
      "  time_since_restore: 5469.471817731857\n",
      "  time_this_iter_s: 10.403035163879395\n",
      "  time_total_s: 5469.471817731857\n",
      "  timers:\n",
      "    learn_throughput: 1636.225\n",
      "    learn_time_ms: 611.163\n",
      "    load_throughput: 214781.905\n",
      "    load_time_ms: 4.656\n",
      "    sample_throughput: 83.33\n",
      "    sample_time_ms: 12000.547\n",
      "    update_time_ms: 1.808\n",
      "  timestamp: 1632136757\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 482000\n",
      "  training_iteration: 482\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   482</td><td style=\"text-align: right;\">         5469.47</td><td style=\"text-align: right;\">482000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 483000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-19-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 485\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8681828445858426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014797946168073435\n",
      "          policy_loss: -0.12373300087120798\n",
      "          total_loss: -0.13467323068115447\n",
      "          vf_explained_var: -0.15469810366630554\n",
      "          vf_loss: 0.003241648310278025\n",
      "    num_agent_steps_sampled: 483000\n",
      "    num_agent_steps_trained: 483000\n",
      "    num_steps_sampled: 483000\n",
      "    num_steps_trained: 483000\n",
      "  iterations_since_restore: 483\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.57333333333334\n",
      "    ram_util_percent: 69.1\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04068855138986772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553608935424826\n",
      "    mean_inference_ms: 1.4082548559563453\n",
      "    mean_raw_obs_processing_ms: 0.6972466320497972\n",
      "  time_since_restore: 5479.72910118103\n",
      "  time_this_iter_s: 10.257283449172974\n",
      "  time_total_s: 5479.72910118103\n",
      "  timers:\n",
      "    learn_throughput: 1641.711\n",
      "    learn_time_ms: 609.121\n",
      "    load_throughput: 215816.328\n",
      "    load_time_ms: 4.634\n",
      "    sample_throughput: 83.75\n",
      "    sample_time_ms: 11940.269\n",
      "    update_time_ms: 1.806\n",
      "  timestamp: 1632136768\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 483000\n",
      "  training_iteration: 483\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   483</td><td style=\"text-align: right;\">         5479.73</td><td style=\"text-align: right;\">483000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-19-38\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 486\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8292755524317423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022403173102126868\n",
      "          policy_loss: -0.016681531071662904\n",
      "          total_loss: -0.007618860155344009\n",
      "          vf_explained_var: 0.265838623046875\n",
      "          vf_loss: 0.020542778165286615\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.46428571428572\n",
      "    ram_util_percent: 69.10000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04068977328775647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553502046822189\n",
      "    mean_inference_ms: 1.40826965531392\n",
      "    mean_raw_obs_processing_ms: 0.6975751311292501\n",
      "  time_since_restore: 5489.84566783905\n",
      "  time_this_iter_s: 10.11656665802002\n",
      "  time_total_s: 5489.84566783905\n",
      "  timers:\n",
      "    learn_throughput: 1642.353\n",
      "    learn_time_ms: 608.882\n",
      "    load_throughput: 217819.162\n",
      "    load_time_ms: 4.591\n",
      "    sample_throughput: 84.671\n",
      "    sample_time_ms: 11810.442\n",
      "    update_time_ms: 1.801\n",
      "  timestamp: 1632136778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 484\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   484</td><td style=\"text-align: right;\">         5489.85</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 485000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-19-48\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 487\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.776714289188385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019036461440395978\n",
      "          policy_loss: -0.06262420585585965\n",
      "          total_loss: -0.0657983311969373\n",
      "          vf_explained_var: 0.08613990247249603\n",
      "          vf_loss: 0.005909735311676437\n",
      "    num_agent_steps_sampled: 485000\n",
      "    num_agent_steps_trained: 485000\n",
      "    num_steps_sampled: 485000\n",
      "    num_steps_trained: 485000\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17999999999999\n",
      "    ram_util_percent: 69.06666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040691001514325455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553375509587081\n",
      "    mean_inference_ms: 1.408284538329608\n",
      "    mean_raw_obs_processing_ms: 0.6979050633815153\n",
      "  time_since_restore: 5500.139939785004\n",
      "  time_this_iter_s: 10.29427194595337\n",
      "  time_total_s: 5500.139939785004\n",
      "  timers:\n",
      "    learn_throughput: 1645.226\n",
      "    learn_time_ms: 607.819\n",
      "    load_throughput: 217256.161\n",
      "    load_time_ms: 4.603\n",
      "    sample_throughput: 85.167\n",
      "    sample_time_ms: 11741.596\n",
      "    update_time_ms: 1.791\n",
      "  timestamp: 1632136788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485000\n",
      "  training_iteration: 485\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   485</td><td style=\"text-align: right;\">         5500.14</td><td style=\"text-align: right;\">485000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 486000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 488\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7083157036039565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008684083231016004\n",
      "          policy_loss: -0.08286655309299627\n",
      "          total_loss: -0.09479970182809565\n",
      "          vf_explained_var: -0.48722195625305176\n",
      "          vf_loss: 0.00118885353505094\n",
      "    num_agent_steps_sampled: 486000\n",
      "    num_agent_steps_trained: 486000\n",
      "    num_steps_sampled: 486000\n",
      "    num_steps_trained: 486000\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.10714285714285\n",
      "    ram_util_percent: 69.02142857142857\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04069219953370165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553224953599285\n",
      "    mean_inference_ms: 1.4082985342967809\n",
      "    mean_raw_obs_processing_ms: 0.6982370359517934\n",
      "  time_since_restore: 5510.296754360199\n",
      "  time_this_iter_s: 10.156814575195312\n",
      "  time_total_s: 5510.296754360199\n",
      "  timers:\n",
      "    learn_throughput: 1646.789\n",
      "    learn_time_ms: 607.242\n",
      "    load_throughput: 218036.566\n",
      "    load_time_ms: 4.586\n",
      "    sample_throughput: 85.661\n",
      "    sample_time_ms: 11673.907\n",
      "    update_time_ms: 1.778\n",
      "  timestamp: 1632136798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 486000\n",
      "  training_iteration: 486\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   486</td><td style=\"text-align: right;\">          5510.3</td><td style=\"text-align: right;\">486000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 487000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-20-08\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 489\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6123149832089743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013704761404802292\n",
      "          policy_loss: -0.04439457183082898\n",
      "          total_loss: -0.043708311021327974\n",
      "          vf_explained_var: 0.10977283120155334\n",
      "          vf_loss: 0.010558127047907975\n",
      "    num_agent_steps_sampled: 487000\n",
      "    num_agent_steps_trained: 487000\n",
      "    num_steps_sampled: 487000\n",
      "    num_steps_trained: 487000\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.83333333333334\n",
      "    ram_util_percent: 69.02\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04069336524044817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.553048950248076\n",
      "    mean_inference_ms: 1.408311487762412\n",
      "    mean_raw_obs_processing_ms: 0.6985713796898526\n",
      "  time_since_restore: 5520.251857280731\n",
      "  time_this_iter_s: 9.955102920532227\n",
      "  time_total_s: 5520.251857280731\n",
      "  timers:\n",
      "    learn_throughput: 1646.459\n",
      "    learn_time_ms: 607.364\n",
      "    load_throughput: 217962.917\n",
      "    load_time_ms: 4.588\n",
      "    sample_throughput: 86.059\n",
      "    sample_time_ms: 11619.981\n",
      "    update_time_ms: 1.78\n",
      "  timestamp: 1632136808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487000\n",
      "  training_iteration: 487\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   487</td><td style=\"text-align: right;\">         5520.25</td><td style=\"text-align: right;\">487000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-20-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 490\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6516992886861166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010809115329630439\n",
      "          policy_loss: -0.03412971713890632\n",
      "          total_loss: -0.039895733156137994\n",
      "          vf_explained_var: 0.31219005584716797\n",
      "          vf_loss: 0.005820512104805352\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.214285714285715\n",
      "    ram_util_percent: 69.06428571428572\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04069452563446311\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55287401585155\n",
      "    mean_inference_ms: 1.4083243150226172\n",
      "    mean_raw_obs_processing_ms: 0.6984567663578961\n",
      "  time_since_restore: 5530.53649687767\n",
      "  time_this_iter_s: 10.284639596939087\n",
      "  time_total_s: 5530.53649687767\n",
      "  timers:\n",
      "    learn_throughput: 1646.562\n",
      "    learn_time_ms: 607.326\n",
      "    load_throughput: 308683.883\n",
      "    load_time_ms: 3.24\n",
      "    sample_throughput: 101.937\n",
      "    sample_time_ms: 9809.937\n",
      "    update_time_ms: 1.774\n",
      "  timestamp: 1632136819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 488\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   488</td><td style=\"text-align: right;\">         5530.54</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 489000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-20-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 491\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9499860048294066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00905911919056847\n",
      "          policy_loss: -0.12162883256872495\n",
      "          total_loss: -0.13562895469367503\n",
      "          vf_explained_var: -0.1183161735534668\n",
      "          vf_loss: 0.0013675148010305646\n",
      "    num_agent_steps_sampled: 489000\n",
      "    num_agent_steps_trained: 489000\n",
      "    num_steps_sampled: 489000\n",
      "    num_steps_trained: 489000\n",
      "  iterations_since_restore: 489\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.366666666666674\n",
      "    ram_util_percent: 69.07333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04069568513157644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.552653415359655\n",
      "    mean_inference_ms: 1.40833719292116\n",
      "    mean_raw_obs_processing_ms: 0.698345083439122\n",
      "  time_since_restore: 5541.063103675842\n",
      "  time_this_iter_s: 10.526606798171997\n",
      "  time_total_s: 5541.063103675842\n",
      "  timers:\n",
      "    learn_throughput: 1647.297\n",
      "    learn_time_ms: 607.055\n",
      "    load_throughput: 307811.716\n",
      "    load_time_ms: 3.249\n",
      "    sample_throughput: 103.677\n",
      "    sample_time_ms: 9645.344\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632136829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 489000\n",
      "  training_iteration: 489\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   489</td><td style=\"text-align: right;\">         5541.06</td><td style=\"text-align: right;\">489000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 490000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-20-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 492\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9044922206136916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011087835734808532\n",
      "          policy_loss: -0.1629276697834333\n",
      "          total_loss: -0.1761064424696896\n",
      "          vf_explained_var: -0.42513301968574524\n",
      "          vf_loss: 0.0008085491239196725\n",
      "    num_agent_steps_sampled: 490000\n",
      "    num_agent_steps_trained: 490000\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.4875\n",
      "    ram_util_percent: 69.1\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04069684619098965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.552436642211063\n",
      "    mean_inference_ms: 1.40835031923095\n",
      "    mean_raw_obs_processing_ms: 0.6982362994773128\n",
      "  time_since_restore: 5551.7107536792755\n",
      "  time_this_iter_s: 10.647650003433228\n",
      "  time_total_s: 5551.7107536792755\n",
      "  timers:\n",
      "    learn_throughput: 1649.047\n",
      "    learn_time_ms: 606.411\n",
      "    load_throughput: 311053.233\n",
      "    load_time_ms: 3.215\n",
      "    sample_throughput: 103.423\n",
      "    sample_time_ms: 9669.015\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632136840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 490\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   490</td><td style=\"text-align: right;\">         5551.71</td><td style=\"text-align: right;\">490000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 491000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-20-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 493\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.770911615424686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006662153223543419\n",
      "          policy_loss: -0.06148558596356048\n",
      "          total_loss: -0.0759151664459043\n",
      "          vf_explained_var: -0.8731422424316406\n",
      "          vf_loss: 0.0002406638817370145\n",
      "    num_agent_steps_sampled: 491000\n",
      "    num_agent_steps_trained: 491000\n",
      "    num_steps_sampled: 491000\n",
      "    num_steps_trained: 491000\n",
      "  iterations_since_restore: 491\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.50666666666667\n",
      "    ram_util_percent: 69.1\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04069798847123606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.552224832316774\n",
      "    mean_inference_ms: 1.4083629717696085\n",
      "    mean_raw_obs_processing_ms: 0.698130736324609\n",
      "  time_since_restore: 5562.344646930695\n",
      "  time_this_iter_s: 10.633893251419067\n",
      "  time_total_s: 5562.344646930695\n",
      "  timers:\n",
      "    learn_throughput: 1657.209\n",
      "    learn_time_ms: 603.424\n",
      "    load_throughput: 311923.014\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 102.925\n",
      "    sample_time_ms: 9715.828\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632136851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 491000\n",
      "  training_iteration: 491\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   491</td><td style=\"text-align: right;\">         5562.34</td><td style=\"text-align: right;\">491000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-21-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 494\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8601184765497842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008431342570036375\n",
      "          policy_loss: -0.05498699000519183\n",
      "          total_loss: -0.06935776667669416\n",
      "          vf_explained_var: -0.9405680894851685\n",
      "          vf_loss: 0.0003845387190166447\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.693333333333335\n",
      "    ram_util_percent: 69.15333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04069913838853629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.552018303676798\n",
      "    mean_inference_ms: 1.4083759745506348\n",
      "    mean_raw_obs_processing_ms: 0.698028394883611\n",
      "  time_since_restore: 5572.870460987091\n",
      "  time_this_iter_s: 10.525814056396484\n",
      "  time_total_s: 5572.870460987091\n",
      "  timers:\n",
      "    learn_throughput: 1657.194\n",
      "    learn_time_ms: 603.43\n",
      "    load_throughput: 311837.208\n",
      "    load_time_ms: 3.207\n",
      "    sample_throughput: 102.795\n",
      "    sample_time_ms: 9728.104\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1632136861\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 492\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   492</td><td style=\"text-align: right;\">         5572.87</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 493000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-21-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 495\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8140512466430665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01738488867083247\n",
      "          policy_loss: -0.06592611326939529\n",
      "          total_loss: -0.07242584141592184\n",
      "          vf_explained_var: -0.3302379548549652\n",
      "          vf_loss: 0.003710848863960968\n",
      "    num_agent_steps_sampled: 493000\n",
      "    num_agent_steps_trained: 493000\n",
      "    num_steps_sampled: 493000\n",
      "    num_steps_trained: 493000\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.36\n",
      "    ram_util_percent: 69.20000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04070029461404839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.551817196250603\n",
      "    mean_inference_ms: 1.4083894329575477\n",
      "    mean_raw_obs_processing_ms: 0.697929470431787\n",
      "  time_since_restore: 5583.298985242844\n",
      "  time_this_iter_s: 10.428524255752563\n",
      "  time_total_s: 5583.298985242844\n",
      "  timers:\n",
      "    learn_throughput: 1652.3\n",
      "    learn_time_ms: 605.217\n",
      "    load_throughput: 312590.197\n",
      "    load_time_ms: 3.199\n",
      "    sample_throughput: 102.651\n",
      "    sample_time_ms: 9741.763\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632136872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 493000\n",
      "  training_iteration: 493\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   493</td><td style=\"text-align: right;\">          5583.3</td><td style=\"text-align: right;\">493000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 494000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-21-22\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 496\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8047197765774197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009170802913044599\n",
      "          policy_loss: -0.03386100406448046\n",
      "          total_loss: -0.04738998711109162\n",
      "          vf_explained_var: -0.357991486787796\n",
      "          vf_loss: 0.00033504880864509485\n",
      "    num_agent_steps_sampled: 494000\n",
      "    num_agent_steps_trained: 494000\n",
      "    num_steps_sampled: 494000\n",
      "    num_steps_trained: 494000\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.733333333333334\n",
      "    ram_util_percent: 69.26666666666667\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040701436291113016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.551620793050724\n",
      "    mean_inference_ms: 1.4084027873882303\n",
      "    mean_raw_obs_processing_ms: 0.6978337248075238\n",
      "  time_since_restore: 5593.936604499817\n",
      "  time_this_iter_s: 10.637619256973267\n",
      "  time_total_s: 5593.936604499817\n",
      "  timers:\n",
      "    learn_throughput: 1652.501\n",
      "    learn_time_ms: 605.143\n",
      "    load_throughput: 310489.092\n",
      "    load_time_ms: 3.221\n",
      "    sample_throughput: 102.104\n",
      "    sample_time_ms: 9793.937\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632136882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 494000\n",
      "  training_iteration: 494\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   494</td><td style=\"text-align: right;\">         5593.94</td><td style=\"text-align: right;\">494000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-21-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 497\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.868179319964515\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007479127019791873\n",
      "          policy_loss: 0.03138053661419286\n",
      "          total_loss: 0.016309486495123968\n",
      "          vf_explained_var: -0.9494247436523438\n",
      "          vf_loss: 0.00019921819249349128\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_agent_steps_trained: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 495\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.63333333333334\n",
      "    ram_util_percent: 69.29999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040702529434941834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.551422710354787\n",
      "    mean_inference_ms: 1.4084145819211327\n",
      "    mean_raw_obs_processing_ms: 0.6977410382669196\n",
      "  time_since_restore: 5604.45969414711\n",
      "  time_this_iter_s: 10.52308964729309\n",
      "  time_total_s: 5604.45969414711\n",
      "  timers:\n",
      "    learn_throughput: 1654.178\n",
      "    learn_time_ms: 604.53\n",
      "    load_throughput: 311892.861\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 101.859\n",
      "    sample_time_ms: 9817.449\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632136893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 495\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   495</td><td style=\"text-align: right;\">         5604.46</td><td style=\"text-align: right;\">495000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-21-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 498\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8079118119345772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007001318193757693\n",
      "          policy_loss: -0.07898303154442045\n",
      "          total_loss: -0.0934077762067318\n",
      "          vf_explained_var: -0.4961286783218384\n",
      "          vf_loss: 0.00046079594887689585\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.28\n",
      "    ram_util_percent: 69.37333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040703622009491056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.551230759032306\n",
      "    mean_inference_ms: 1.4084265252212742\n",
      "    mean_raw_obs_processing_ms: 0.6976514718341345\n",
      "  time_since_restore: 5615.069182395935\n",
      "  time_this_iter_s: 10.609488248825073\n",
      "  time_total_s: 5615.069182395935\n",
      "  timers:\n",
      "    learn_throughput: 1653.381\n",
      "    learn_time_ms: 604.821\n",
      "    load_throughput: 311869.67\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 101.395\n",
      "    sample_time_ms: 9862.418\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632136903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 496\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   496</td><td style=\"text-align: right;\">         5615.07</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 497000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-21-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 499\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9201025035646226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011658187147000731\n",
      "          policy_loss: -0.0106372299293677\n",
      "          total_loss: -0.01816545298529996\n",
      "          vf_explained_var: -0.09156882017850876\n",
      "          vf_loss: 0.006355047017374697\n",
      "    num_agent_steps_sampled: 497000\n",
      "    num_agent_steps_trained: 497000\n",
      "    num_steps_sampled: 497000\n",
      "    num_steps_trained: 497000\n",
      "  iterations_since_restore: 497\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.10625\n",
      "    ram_util_percent: 69.4\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040704706332945664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55104318748871\n",
      "    mean_inference_ms: 1.4084383627586055\n",
      "    mean_raw_obs_processing_ms: 0.6975646115422101\n",
      "  time_since_restore: 5625.788061618805\n",
      "  time_this_iter_s: 10.718879222869873\n",
      "  time_total_s: 5625.788061618805\n",
      "  timers:\n",
      "    learn_throughput: 1653.835\n",
      "    learn_time_ms: 604.655\n",
      "    load_throughput: 312399.282\n",
      "    load_time_ms: 3.201\n",
      "    sample_throughput: 100.614\n",
      "    sample_time_ms: 9938.964\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632136914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 497000\n",
      "  training_iteration: 497\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   497</td><td style=\"text-align: right;\">         5625.79</td><td style=\"text-align: right;\">497000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 498000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-22-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 500\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8332609335581462\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009575907903095171\n",
      "          policy_loss: -0.11628932348555988\n",
      "          total_loss: -0.1298091856141885\n",
      "          vf_explained_var: -0.7611931562423706\n",
      "          vf_loss: 0.0004447956727947005\n",
      "    num_agent_steps_sampled: 498000\n",
      "    num_agent_steps_trained: 498000\n",
      "    num_steps_sampled: 498000\n",
      "    num_steps_trained: 498000\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.10666666666667\n",
      "    ram_util_percent: 69.41333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04070578125222644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.550858476505578\n",
      "    mean_inference_ms: 1.4084498270628327\n",
      "    mean_raw_obs_processing_ms: 0.6974804828702095\n",
      "  time_since_restore: 5636.415274143219\n",
      "  time_this_iter_s: 10.627212524414062\n",
      "  time_total_s: 5636.415274143219\n",
      "  timers:\n",
      "    learn_throughput: 1655.849\n",
      "    learn_time_ms: 603.92\n",
      "    load_throughput: 310571.858\n",
      "    load_time_ms: 3.22\n",
      "    sample_throughput: 100.261\n",
      "    sample_time_ms: 9973.993\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632136925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 498000\n",
      "  training_iteration: 498\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   498</td><td style=\"text-align: right;\">         5636.42</td><td style=\"text-align: right;\">498000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 499000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-22-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 501\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8512151956558227\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009552477838642047\n",
      "          policy_loss: -0.03677544680734476\n",
      "          total_loss: -0.05034913366867436\n",
      "          vf_explained_var: -0.6352692246437073\n",
      "          vf_loss: 0.0005812019281115176\n",
      "    num_agent_steps_sampled: 499000\n",
      "    num_agent_steps_trained: 499000\n",
      "    num_steps_sampled: 499000\n",
      "    num_steps_trained: 499000\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.66\n",
      "    ram_util_percent: 69.53999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040706808608070084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.550670405300735\n",
      "    mean_inference_ms: 1.4084599571601626\n",
      "    mean_raw_obs_processing_ms: 0.697399008073743\n",
      "  time_since_restore: 5647.00669503212\n",
      "  time_this_iter_s: 10.591420888900757\n",
      "  time_total_s: 5647.00669503212\n",
      "  timers:\n",
      "    learn_throughput: 1658.325\n",
      "    learn_time_ms: 603.018\n",
      "    load_throughput: 311364.962\n",
      "    load_time_ms: 3.212\n",
      "    sample_throughput: 100.187\n",
      "    sample_time_ms: 9981.369\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632136935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 499000\n",
      "  training_iteration: 499\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         5647.01</td><td style=\"text-align: right;\">499000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-22-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 502\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7264758004082574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02012258969850187\n",
      "          policy_loss: -0.09482422429654333\n",
      "          total_loss: -0.10200049384600586\n",
      "          vf_explained_var: -0.5581291913986206\n",
      "          vf_loss: 0.000909779343378937\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.75333333333333\n",
      "    ram_util_percent: 69.52\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04070778862608237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.550478196601691\n",
      "    mean_inference_ms: 1.4084688876985845\n",
      "    mean_raw_obs_processing_ms: 0.6973203179481784\n",
      "  time_since_restore: 5657.515097856522\n",
      "  time_this_iter_s: 10.508402824401855\n",
      "  time_total_s: 5657.515097856522\n",
      "  timers:\n",
      "    learn_throughput: 1658.234\n",
      "    learn_time_ms: 603.051\n",
      "    load_throughput: 311897.499\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 100.327\n",
      "    sample_time_ms: 9967.432\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632136946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 500\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         5657.52</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 501000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 503\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6842092037200928\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8192353659205966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006134625435025853\n",
      "          policy_loss: -0.09255860325776868\n",
      "          total_loss: -0.10617799531254503\n",
      "          vf_explained_var: -0.9726760983467102\n",
      "          vf_loss: 0.00037559265781763115\n",
      "    num_agent_steps_sampled: 501000\n",
      "    num_agent_steps_trained: 501000\n",
      "    num_steps_sampled: 501000\n",
      "    num_steps_trained: 501000\n",
      "  iterations_since_restore: 501\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.69333333333334\n",
      "    ram_util_percent: 69.51333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04070872237423888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.550281008992274\n",
      "    mean_inference_ms: 1.4084764524024345\n",
      "    mean_raw_obs_processing_ms: 0.697243789697358\n",
      "  time_since_restore: 5668.042895555496\n",
      "  time_this_iter_s: 10.52779769897461\n",
      "  time_total_s: 5668.042895555496\n",
      "  timers:\n",
      "    learn_throughput: 1660.266\n",
      "    learn_time_ms: 602.313\n",
      "    load_throughput: 308329.891\n",
      "    load_time_ms: 3.243\n",
      "    sample_throughput: 100.427\n",
      "    sample_time_ms: 9957.485\n",
      "    update_time_ms: 1.778\n",
      "  timestamp: 1632136956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 501000\n",
      "  training_iteration: 501\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   501</td><td style=\"text-align: right;\">         5668.04</td><td style=\"text-align: right;\">501000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 502000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-22-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 504\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6842092037200928\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8117696934276157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010214157532939971\n",
      "          policy_loss: 0.03466015379461977\n",
      "          total_loss: 0.02418600685066647\n",
      "          vf_explained_var: -0.47172534465789795\n",
      "          vf_loss: 0.0006549270629572372\n",
      "    num_agent_steps_sampled: 502000\n",
      "    num_agent_steps_trained: 502000\n",
      "    num_steps_sampled: 502000\n",
      "    num_steps_trained: 502000\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.12666666666665\n",
      "    ram_util_percent: 69.56666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04070964637915948\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.550084340476994\n",
      "    mean_inference_ms: 1.4084836026713377\n",
      "    mean_raw_obs_processing_ms: 0.6971702292793647\n",
      "  time_since_restore: 5678.645133495331\n",
      "  time_this_iter_s: 10.602237939834595\n",
      "  time_total_s: 5678.645133495331\n",
      "  timers:\n",
      "    learn_throughput: 1662.516\n",
      "    learn_time_ms: 601.498\n",
      "    load_throughput: 308590.768\n",
      "    load_time_ms: 3.241\n",
      "    sample_throughput: 100.342\n",
      "    sample_time_ms: 9965.937\n",
      "    update_time_ms: 1.784\n",
      "  timestamp: 1632136967\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 502000\n",
      "  training_iteration: 502\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   502</td><td style=\"text-align: right;\">         5678.65</td><td style=\"text-align: right;\">502000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 503000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-22-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 505\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6842092037200928\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5801924188931784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010379114272763208\n",
      "          policy_loss: -0.05180263924929831\n",
      "          total_loss: -0.05897722426387999\n",
      "          vf_explained_var: 0.2985255718231201\n",
      "          vf_loss: 0.0015258535671617008\n",
      "    num_agent_steps_sampled: 503000\n",
      "    num_agent_steps_trained: 503000\n",
      "    num_steps_sampled: 503000\n",
      "    num_steps_trained: 503000\n",
      "  iterations_since_restore: 503\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.075\n",
      "    ram_util_percent: 69.55625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071055678261099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.549892631277416\n",
      "    mean_inference_ms: 1.408490750600371\n",
      "    mean_raw_obs_processing_ms: 0.6970996101487107\n",
      "  time_since_restore: 5689.3803198337555\n",
      "  time_this_iter_s: 10.735186338424683\n",
      "  time_total_s: 5689.3803198337555\n",
      "  timers:\n",
      "    learn_throughput: 1666.745\n",
      "    learn_time_ms: 599.972\n",
      "    load_throughput: 306086.55\n",
      "    load_time_ms: 3.267\n",
      "    sample_throughput: 100.002\n",
      "    sample_time_ms: 9999.8\n",
      "    update_time_ms: 1.783\n",
      "  timestamp: 1632136978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503000\n",
      "  training_iteration: 503\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   503</td><td style=\"text-align: right;\">         5689.38</td><td style=\"text-align: right;\">503000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-23-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 506\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6842092037200928\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7065417806307475\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0070601862442848705\n",
      "          policy_loss: -0.02654289450082514\n",
      "          total_loss: -0.03789825319415993\n",
      "          vf_explained_var: -0.30530571937561035\n",
      "          vf_loss: 0.0008794137554812349\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.86428571428571\n",
      "    ram_util_percent: 69.51428571428572\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040711460234502776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.549702350280997\n",
      "    mean_inference_ms: 1.4084977563626782\n",
      "    mean_raw_obs_processing_ms: 0.6970318944759984\n",
      "  time_since_restore: 5699.788700580597\n",
      "  time_this_iter_s: 10.40838074684143\n",
      "  time_total_s: 5699.788700580597\n",
      "  timers:\n",
      "    learn_throughput: 1667.978\n",
      "    learn_time_ms: 599.528\n",
      "    load_throughput: 306592.205\n",
      "    load_time_ms: 3.262\n",
      "    sample_throughput: 100.227\n",
      "    sample_time_ms: 9977.311\n",
      "    update_time_ms: 1.777\n",
      "  timestamp: 1632136988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 504\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   504</td><td style=\"text-align: right;\">         5699.79</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 505000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-23-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 507\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6842092037200928\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6394095871183607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010539901527460198\n",
      "          policy_loss: -0.0862783036298222\n",
      "          total_loss: -0.09482292578452163\n",
      "          vf_explained_var: -0.8650169968605042\n",
      "          vf_loss: 0.0006379747346121196\n",
      "    num_agent_steps_sampled: 505000\n",
      "    num_agent_steps_trained: 505000\n",
      "    num_steps_sampled: 505000\n",
      "    num_steps_trained: 505000\n",
      "  iterations_since_restore: 505\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.67333333333334\n",
      "    ram_util_percent: 69.56666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071234640277549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.54951645247068\n",
      "    mean_inference_ms: 1.408504350619813\n",
      "    mean_raw_obs_processing_ms: 0.6969670062570558\n",
      "  time_since_restore: 5710.320321559906\n",
      "  time_this_iter_s: 10.531620979309082\n",
      "  time_total_s: 5710.320321559906\n",
      "  timers:\n",
      "    learn_throughput: 1667.256\n",
      "    learn_time_ms: 599.788\n",
      "    load_throughput: 305069.134\n",
      "    load_time_ms: 3.278\n",
      "    sample_throughput: 100.222\n",
      "    sample_time_ms: 9977.894\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632136999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 505000\n",
      "  training_iteration: 505\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   505</td><td style=\"text-align: right;\">         5710.32</td><td style=\"text-align: right;\">505000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 506000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-23-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 508\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6842092037200928\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.720747062895033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011527376869284664\n",
      "          policy_loss: -0.040904731510414016\n",
      "          total_loss: -0.049079276704125936\n",
      "          vf_explained_var: -0.95653235912323\n",
      "          vf_loss: 0.001145786692440096\n",
      "    num_agent_steps_sampled: 506000\n",
      "    num_agent_steps_trained: 506000\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.9125\n",
      "    ram_util_percent: 69.24375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071321946580165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.549329569443588\n",
      "    mean_inference_ms: 1.4085106051593024\n",
      "    mean_raw_obs_processing_ms: 0.6969049562363822\n",
      "  time_since_restore: 5720.867287635803\n",
      "  time_this_iter_s: 10.546966075897217\n",
      "  time_total_s: 5720.867287635803\n",
      "  timers:\n",
      "    learn_throughput: 1669.073\n",
      "    learn_time_ms: 599.135\n",
      "    load_throughput: 305109.079\n",
      "    load_time_ms: 3.278\n",
      "    sample_throughput: 100.278\n",
      "    sample_time_ms: 9972.274\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632137009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 506\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   506</td><td style=\"text-align: right;\">         5720.87</td><td style=\"text-align: right;\">506000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 507000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 509\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6842092037200928\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6742236720191108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006176395936074641\n",
      "          policy_loss: -0.03536740648042824\n",
      "          total_loss: -0.04733310401853588\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005505923301421313\n",
      "    num_agent_steps_sampled: 507000\n",
      "    num_agent_steps_trained: 507000\n",
      "    num_steps_sampled: 507000\n",
      "    num_steps_trained: 507000\n",
      "  iterations_since_restore: 507\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.653333333333336\n",
      "    ram_util_percent: 68.91333333333334\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071406335421972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.54914204402653\n",
      "    mean_inference_ms: 1.4085162097103952\n",
      "    mean_raw_obs_processing_ms: 0.6968453746997045\n",
      "  time_since_restore: 5731.3182871341705\n",
      "  time_this_iter_s: 10.45099949836731\n",
      "  time_total_s: 5731.3182871341705\n",
      "  timers:\n",
      "    learn_throughput: 1668.819\n",
      "    learn_time_ms: 599.226\n",
      "    load_throughput: 303827.192\n",
      "    load_time_ms: 3.291\n",
      "    sample_throughput: 100.549\n",
      "    sample_time_ms: 9945.382\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1632137020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 507000\n",
      "  training_iteration: 507\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   507</td><td style=\"text-align: right;\">         5731.32</td><td style=\"text-align: right;\">507000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-24-08\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 510\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6842092037200928\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.328189445866479\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02070804718045915\n",
      "          policy_loss: 0.04742543465561337\n",
      "          total_loss: 0.05247969561152988\n",
      "          vf_explained_var: 0.39161545038223267\n",
      "          vf_loss: 0.004167517054722541\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.91794871794871\n",
      "    ram_util_percent: 68.91282051282053\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040714836483137784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.548928443827778\n",
      "    mean_inference_ms: 1.4085195602979608\n",
      "    mean_raw_obs_processing_ms: 0.6971367696939649\n",
      "  time_since_restore: 5759.287751674652\n",
      "  time_this_iter_s: 27.969464540481567\n",
      "  time_total_s: 5759.287751674652\n",
      "  timers:\n",
      "    learn_throughput: 1667.299\n",
      "    learn_time_ms: 599.773\n",
      "    load_throughput: 214255.269\n",
      "    load_time_ms: 4.667\n",
      "    sample_throughput: 85.633\n",
      "    sample_time_ms: 11677.675\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632137048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 508\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   508</td><td style=\"text-align: right;\">         5759.29</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 509000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-24-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 511\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6964731666776869\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00606508455267058\n",
      "          policy_loss: -0.04874051807241307\n",
      "          total_loss: -0.05891980874455637\n",
      "          vf_explained_var: -0.8980711102485657\n",
      "          vf_loss: 0.0005607606610283256\n",
      "    num_agent_steps_sampled: 509000\n",
      "    num_agent_steps_trained: 509000\n",
      "    num_steps_sampled: 509000\n",
      "    num_steps_trained: 509000\n",
      "  iterations_since_restore: 509\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.93999999999999\n",
      "    ram_util_percent: 68.85333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071557947263849\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.548703415822462\n",
      "    mean_inference_ms: 1.408521828716085\n",
      "    mean_raw_obs_processing_ms: 0.6974298295147604\n",
      "  time_since_restore: 5769.664029121399\n",
      "  time_this_iter_s: 10.376277446746826\n",
      "  time_total_s: 5769.664029121399\n",
      "  timers:\n",
      "    learn_throughput: 1667.984\n",
      "    learn_time_ms: 599.526\n",
      "    load_throughput: 214187.434\n",
      "    load_time_ms: 4.669\n",
      "    sample_throughput: 85.79\n",
      "    sample_time_ms: 11656.405\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1632137058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 509000\n",
      "  training_iteration: 509\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   509</td><td style=\"text-align: right;\">         5769.66</td><td style=\"text-align: right;\">509000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 510000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 512\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6665614234076607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006379127448137961\n",
      "          policy_loss: -0.06300055562622017\n",
      "          total_loss: -0.07238388792094257\n",
      "          vf_explained_var: -0.39234063029289246\n",
      "          vf_loss: 0.0007352955083155798\n",
      "    num_agent_steps_sampled: 510000\n",
      "    num_agent_steps_trained: 510000\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.89333333333333\n",
      "    ram_util_percent: 68.70666666666669\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071626817633379\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.548455765049198\n",
      "    mean_inference_ms: 1.4085223008289205\n",
      "    mean_raw_obs_processing_ms: 0.697724550825789\n",
      "  time_since_restore: 5779.679374217987\n",
      "  time_this_iter_s: 10.015345096588135\n",
      "  time_total_s: 5779.679374217987\n",
      "  timers:\n",
      "    learn_throughput: 1667.123\n",
      "    learn_time_ms: 599.836\n",
      "    load_throughput: 214109.804\n",
      "    load_time_ms: 4.671\n",
      "    sample_throughput: 86.157\n",
      "    sample_time_ms: 11606.783\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1632137068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 510\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   510</td><td style=\"text-align: right;\">         5779.68</td><td style=\"text-align: right;\">510000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 511000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-24-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 513\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5797932982444762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005988790634144673\n",
      "          policy_loss: -0.09325747638940811\n",
      "          total_loss: -0.10263515710830688\n",
      "          vf_explained_var: -0.5312452912330627\n",
      "          vf_loss: 0.00027387390808952556\n",
      "    num_agent_steps_sampled: 511000\n",
      "    num_agent_steps_trained: 511000\n",
      "    num_steps_sampled: 511000\n",
      "    num_steps_trained: 511000\n",
      "  iterations_since_restore: 511\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.06666666666667\n",
      "    ram_util_percent: 68.67333333333336\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071693469400012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.548204766587608\n",
      "    mean_inference_ms: 1.408521995078029\n",
      "    mean_raw_obs_processing_ms: 0.6980209670252963\n",
      "  time_since_restore: 5790.190027713776\n",
      "  time_this_iter_s: 10.510653495788574\n",
      "  time_total_s: 5790.190027713776\n",
      "  timers:\n",
      "    learn_throughput: 1665.039\n",
      "    learn_time_ms: 600.587\n",
      "    load_throughput: 215366.415\n",
      "    load_time_ms: 4.643\n",
      "    sample_throughput: 86.174\n",
      "    sample_time_ms: 11604.384\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1632137079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511000\n",
      "  training_iteration: 511\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   511</td><td style=\"text-align: right;\">         5790.19</td><td style=\"text-align: right;\">511000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-24-49\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 514\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.398382908768124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004601291065526346\n",
      "          policy_loss: -0.009514384385612275\n",
      "          total_loss: -0.0183039209081067\n",
      "          vf_explained_var: 0.09033461660146713\n",
      "          vf_loss: 0.00047192599910583034\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.82857142857143\n",
      "    ram_util_percent: 68.62142857142858\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071759741423734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.547943484382138\n",
      "    mean_inference_ms: 1.4085215734892946\n",
      "    mean_raw_obs_processing_ms: 0.6983191145981018\n",
      "  time_since_restore: 5800.508756399155\n",
      "  time_this_iter_s: 10.318728685379028\n",
      "  time_total_s: 5800.508756399155\n",
      "  timers:\n",
      "    learn_throughput: 1665.079\n",
      "    learn_time_ms: 600.572\n",
      "    load_throughput: 215365.309\n",
      "    load_time_ms: 4.643\n",
      "    sample_throughput: 86.385\n",
      "    sample_time_ms: 11576.042\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1632137089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 512\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   512</td><td style=\"text-align: right;\">         5800.51</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 513000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-25-00\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 515\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5777772770987617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005348338955796173\n",
      "          policy_loss: -0.053007147047254774\n",
      "          total_loss: -0.06574022927218014\n",
      "          vf_explained_var: -0.7251086831092834\n",
      "          vf_loss: 0.00030015246698490553\n",
      "    num_agent_steps_sampled: 513000\n",
      "    num_agent_steps_trained: 513000\n",
      "    num_steps_sampled: 513000\n",
      "    num_steps_trained: 513000\n",
      "  iterations_since_restore: 513\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.533333333333324\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040718246155219504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.547672692083125\n",
      "    mean_inference_ms: 1.408520910005786\n",
      "    mean_raw_obs_processing_ms: 0.6986189305859525\n",
      "  time_since_restore: 5810.878118753433\n",
      "  time_this_iter_s: 10.369362354278564\n",
      "  time_total_s: 5810.878118753433\n",
      "  timers:\n",
      "    learn_throughput: 1665.434\n",
      "    learn_time_ms: 600.444\n",
      "    load_throughput: 216513.731\n",
      "    load_time_ms: 4.619\n",
      "    sample_throughput: 86.658\n",
      "    sample_time_ms: 11539.638\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1632137100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 513000\n",
      "  training_iteration: 513\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   513</td><td style=\"text-align: right;\">         5810.88</td><td style=\"text-align: right;\">513000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 514000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-25-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 516\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1918661402331459\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010963322369265071\n",
      "          policy_loss: -0.0115105958448516\n",
      "          total_loss: -0.0171366466416253\n",
      "          vf_explained_var: -0.43163490295410156\n",
      "          vf_loss: 0.0006667075538037655\n",
      "    num_agent_steps_sampled: 514000\n",
      "    num_agent_steps_trained: 514000\n",
      "    num_steps_sampled: 514000\n",
      "    num_steps_trained: 514000\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.36\n",
      "    ram_util_percent: 68.62000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071884268878722\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.547383986840192\n",
      "    mean_inference_ms: 1.4085190077948326\n",
      "    mean_raw_obs_processing_ms: 0.6989219228973336\n",
      "  time_since_restore: 5821.283736467361\n",
      "  time_this_iter_s: 10.405617713928223\n",
      "  time_total_s: 5821.283736467361\n",
      "  timers:\n",
      "    learn_throughput: 1656.911\n",
      "    learn_time_ms: 603.533\n",
      "    load_throughput: 216410.955\n",
      "    load_time_ms: 4.621\n",
      "    sample_throughput: 86.683\n",
      "    sample_time_ms: 11536.242\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632137110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 514000\n",
      "  training_iteration: 514\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   514</td><td style=\"text-align: right;\">         5821.28</td><td style=\"text-align: right;\">514000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 515000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-25-21\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 517\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5328144484096102\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01142768529024857\n",
      "          policy_loss: 0.022441914884580506\n",
      "          total_loss: 0.013387111864156193\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004091451963176951\n",
      "    num_agent_steps_sampled: 515000\n",
      "    num_agent_steps_trained: 515000\n",
      "    num_steps_sampled: 515000\n",
      "    num_steps_trained: 515000\n",
      "  iterations_since_restore: 515\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.2\n",
      "    ram_util_percent: 68.65333333333335\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040719433947857465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.547081446048482\n",
      "    mean_inference_ms: 1.4085169496548164\n",
      "    mean_raw_obs_processing_ms: 0.6992261812491509\n",
      "  time_since_restore: 5831.9115245342255\n",
      "  time_this_iter_s: 10.627788066864014\n",
      "  time_total_s: 5831.9115245342255\n",
      "  timers:\n",
      "    learn_throughput: 1653.273\n",
      "    learn_time_ms: 604.861\n",
      "    load_throughput: 216896.648\n",
      "    load_time_ms: 4.61\n",
      "    sample_throughput: 86.621\n",
      "    sample_time_ms: 11544.478\n",
      "    update_time_ms: 1.757\n",
      "  timestamp: 1632137121\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 515000\n",
      "  training_iteration: 515\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   515</td><td style=\"text-align: right;\">         5831.91</td><td style=\"text-align: right;\">515000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-25-31\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 518\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.628897319899665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006806533979984092\n",
      "          policy_loss: -0.022978492329518\n",
      "          total_loss: -0.03551363905684816\n",
      "          vf_explained_var: -0.5173905491828918\n",
      "          vf_loss: 0.00026100617922363906\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.875\n",
      "    ram_util_percent: 68.70625000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072003325912985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.546772427723528\n",
      "    mean_inference_ms: 1.4085151350886116\n",
      "    mean_raw_obs_processing_ms: 0.6995319805405275\n",
      "  time_since_restore: 5842.58723783493\n",
      "  time_this_iter_s: 10.675713300704956\n",
      "  time_total_s: 5842.58723783493\n",
      "  timers:\n",
      "    learn_throughput: 1644.283\n",
      "    learn_time_ms: 608.168\n",
      "    load_throughput: 214712.637\n",
      "    load_time_ms: 4.657\n",
      "    sample_throughput: 86.55\n",
      "    sample_time_ms: 11554.0\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632137131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 516\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   516</td><td style=\"text-align: right;\">         5842.59</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 517000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-25-42\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 519\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7330945465299818\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009975629475637978\n",
      "          policy_loss: 0.028659396701388888\n",
      "          total_loss: 0.016641395828790134\n",
      "          vf_explained_var: -0.8969638347625732\n",
      "          vf_loss: 0.00019388094515306875\n",
      "    num_agent_steps_sampled: 517000\n",
      "    num_agent_steps_trained: 517000\n",
      "    num_steps_sampled: 517000\n",
      "    num_steps_trained: 517000\n",
      "  iterations_since_restore: 517\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.14\n",
      "    ram_util_percent: 68.80666666666664\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072063696682217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.546468398003674\n",
      "    mean_inference_ms: 1.4085134788392413\n",
      "    mean_raw_obs_processing_ms: 0.6998397014218021\n",
      "  time_since_restore: 5853.204110860825\n",
      "  time_this_iter_s: 10.616873025894165\n",
      "  time_total_s: 5853.204110860825\n",
      "  timers:\n",
      "    learn_throughput: 1644.733\n",
      "    learn_time_ms: 608.002\n",
      "    load_throughput: 215194.04\n",
      "    load_time_ms: 4.647\n",
      "    sample_throughput: 86.425\n",
      "    sample_time_ms: 11570.755\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632137142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 517000\n",
      "  training_iteration: 517\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   517</td><td style=\"text-align: right;\">          5853.2</td><td style=\"text-align: right;\">517000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 518000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-25-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 520\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4553927567270066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006881877195698858\n",
      "          policy_loss: -0.005444410070776939\n",
      "          total_loss: -0.015463084272212452\n",
      "          vf_explained_var: -0.45658746361732483\n",
      "          vf_loss: 0.0010037718892918848\n",
      "    num_agent_steps_sampled: 518000\n",
      "    num_agent_steps_trained: 518000\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.26\n",
      "    ram_util_percent: 68.85333333333331\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040721243581650156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.546134978738861\n",
      "    mean_inference_ms: 1.4085120172226342\n",
      "    mean_raw_obs_processing_ms: 0.6997307173946952\n",
      "  time_since_restore: 5864.021123409271\n",
      "  time_this_iter_s: 10.817012548446655\n",
      "  time_total_s: 5864.021123409271\n",
      "  timers:\n",
      "    learn_throughput: 1645.823\n",
      "    learn_time_ms: 607.599\n",
      "    load_throughput: 306901.79\n",
      "    load_time_ms: 3.258\n",
      "    sample_throughput: 101.448\n",
      "    sample_time_ms: 9857.277\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632137153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 518\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   518</td><td style=\"text-align: right;\">         5864.02</td><td style=\"text-align: right;\">518000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 519000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-26-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 521\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.441567956076728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02392311381209882\n",
      "          policy_loss: 0.10582440942525864\n",
      "          total_loss: 0.10749102847443687\n",
      "          vf_explained_var: 0.14206422865390778\n",
      "          vf_loss: 0.0038059894569919886\n",
      "    num_agent_steps_sampled: 519000\n",
      "    num_agent_steps_trained: 519000\n",
      "    num_steps_sampled: 519000\n",
      "    num_steps_trained: 519000\n",
      "  iterations_since_restore: 519\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.83125\n",
      "    ram_util_percent: 68.8875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072183889083775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.545812541239018\n",
      "    mean_inference_ms: 1.408510421723925\n",
      "    mean_raw_obs_processing_ms: 0.6996249738456736\n",
      "  time_since_restore: 5875.035059213638\n",
      "  time_this_iter_s: 11.013935804367065\n",
      "  time_total_s: 5875.035059213638\n",
      "  timers:\n",
      "    learn_throughput: 1644.175\n",
      "    learn_time_ms: 608.208\n",
      "    load_throughput: 307707.838\n",
      "    load_time_ms: 3.25\n",
      "    sample_throughput: 100.802\n",
      "    sample_time_ms: 9920.47\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632137164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519000\n",
      "  training_iteration: 519\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   519</td><td style=\"text-align: right;\">         5875.04</td><td style=\"text-align: right;\">519000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-26-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 522\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7697353541851042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2415709230634902\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006488534795576949\n",
      "          policy_loss: -0.008060435702403387\n",
      "          total_loss: -0.015019167773425579\n",
      "          vf_explained_var: -0.5238737463951111\n",
      "          vf_loss: 0.00046252283063950015\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.125\n",
      "    ram_util_percent: 68.94375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040722432571336446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.545503466203694\n",
      "    mean_inference_ms: 1.4085092019961336\n",
      "    mean_raw_obs_processing_ms: 0.6995218056574263\n",
      "  time_since_restore: 5886.065294742584\n",
      "  time_this_iter_s: 11.030235528945923\n",
      "  time_total_s: 5886.065294742584\n",
      "  timers:\n",
      "    learn_throughput: 1647.36\n",
      "    learn_time_ms: 607.032\n",
      "    load_throughput: 307002.877\n",
      "    load_time_ms: 3.257\n",
      "    sample_throughput: 99.77\n",
      "    sample_time_ms: 10023.094\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632137175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 520\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   520</td><td style=\"text-align: right;\">         5886.07</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 521000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-26-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 523\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7697353541851042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1444126685460407\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009171064179721987\n",
      "          policy_loss: -0.09583520938952764\n",
      "          total_loss: -0.10921166075600518\n",
      "          vf_explained_var: 0.15495982766151428\n",
      "          vf_loss: 0.0010083818714418965\n",
      "    num_agent_steps_sampled: 521000\n",
      "    num_agent_steps_trained: 521000\n",
      "    num_steps_sampled: 521000\n",
      "    num_steps_trained: 521000\n",
      "  iterations_since_restore: 521\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.12666666666666\n",
      "    ram_util_percent: 69.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040722985364965304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.545186092467786\n",
      "    mean_inference_ms: 1.4085069722893382\n",
      "    mean_raw_obs_processing_ms: 0.6994221384096356\n",
      "  time_since_restore: 5896.376225709915\n",
      "  time_this_iter_s: 10.310930967330933\n",
      "  time_total_s: 5896.376225709915\n",
      "  timers:\n",
      "    learn_throughput: 1646.973\n",
      "    learn_time_ms: 607.174\n",
      "    load_throughput: 306731.216\n",
      "    load_time_ms: 3.26\n",
      "    sample_throughput: 99.97\n",
      "    sample_time_ms: 10002.972\n",
      "    update_time_ms: 1.777\n",
      "  timestamp: 1632137185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 521000\n",
      "  training_iteration: 521\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   521</td><td style=\"text-align: right;\">         5896.38</td><td style=\"text-align: right;\">521000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 522000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-26-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 524\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7697353541851042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0080921954578823\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00560452430092408\n",
      "          policy_loss: 0.014195974792043368\n",
      "          total_loss: -0.0012599003397756152\n",
      "          vf_explained_var: -0.8451082110404968\n",
      "          vf_loss: 0.00031104587639371555\n",
      "    num_agent_steps_sampled: 522000\n",
      "    num_agent_steps_trained: 522000\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.07333333333333\n",
      "    ram_util_percent: 68.98666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040723544573305154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.544876374539328\n",
      "    mean_inference_ms: 1.408505128800592\n",
      "    mean_raw_obs_processing_ms: 0.699325452858851\n",
      "  time_since_restore: 5907.213302612305\n",
      "  time_this_iter_s: 10.837076902389526\n",
      "  time_total_s: 5907.213302612305\n",
      "  timers:\n",
      "    learn_throughput: 1646.284\n",
      "    learn_time_ms: 607.429\n",
      "    load_throughput: 305832.118\n",
      "    load_time_ms: 3.27\n",
      "    sample_throughput: 99.457\n",
      "    sample_time_ms: 10054.554\n",
      "    update_time_ms: 1.779\n",
      "  timestamp: 1632137196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 522\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   522</td><td style=\"text-align: right;\">         5907.21</td><td style=\"text-align: right;\">522000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 523000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-26-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 525\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7697353541851042\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0467291169696384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003772389808384317\n",
      "          policy_loss: 0.02110138883193334\n",
      "          total_loss: 0.013700423017144202\n",
      "          vf_explained_var: -0.8996247053146362\n",
      "          vf_loss: 0.00016258378600468858\n",
      "    num_agent_steps_sampled: 523000\n",
      "    num_agent_steps_trained: 523000\n",
      "    num_steps_sampled: 523000\n",
      "    num_steps_trained: 523000\n",
      "  iterations_since_restore: 523\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.8125\n",
      "    ram_util_percent: 69.0\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040724033264302906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.544569854308152\n",
      "    mean_inference_ms: 1.4085023587785395\n",
      "    mean_raw_obs_processing_ms: 0.6992316729852689\n",
      "  time_since_restore: 5918.35244178772\n",
      "  time_this_iter_s: 11.139139175415039\n",
      "  time_total_s: 5918.35244178772\n",
      "  timers:\n",
      "    learn_throughput: 1643.96\n",
      "    learn_time_ms: 608.287\n",
      "    load_throughput: 301789.741\n",
      "    load_time_ms: 3.314\n",
      "    sample_throughput: 98.711\n",
      "    sample_time_ms: 10130.587\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632137207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 523000\n",
      "  training_iteration: 523\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   523</td><td style=\"text-align: right;\">         5918.35</td><td style=\"text-align: right;\">523000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 526\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.258230945799086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017465697522582285\n",
      "          policy_loss: -0.100302388270696\n",
      "          total_loss: -0.11553467479017046\n",
      "          vf_explained_var: 0.1635987013578415\n",
      "          vf_loss: 0.0006280397742456342\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.15333333333334\n",
      "    ram_util_percent: 69.00666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072453433998682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.544266315062835\n",
      "    mean_inference_ms: 1.4085003147472344\n",
      "    mean_raw_obs_processing_ms: 0.6991412713977984\n",
      "  time_since_restore: 5928.982875108719\n",
      "  time_this_iter_s: 10.630433320999146\n",
      "  time_total_s: 5928.982875108719\n",
      "  timers:\n",
      "    learn_throughput: 1650.965\n",
      "    learn_time_ms: 605.706\n",
      "    load_throughput: 302842.207\n",
      "    load_time_ms: 3.302\n",
      "    sample_throughput: 98.467\n",
      "    sample_time_ms: 10155.671\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632137218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 524\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   524</td><td style=\"text-align: right;\">         5928.98</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 525000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 527\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.636757939391666\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01579143164753061\n",
      "          policy_loss: -0.07651057152284516\n",
      "          total_loss: -0.08459120003713502\n",
      "          vf_explained_var: -0.09005361050367355\n",
      "          vf_loss: 0.002209340505457173\n",
      "    num_agent_steps_sampled: 525000\n",
      "    num_agent_steps_trained: 525000\n",
      "    num_steps_sampled: 525000\n",
      "    num_steps_trained: 525000\n",
      "  iterations_since_restore: 525\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.29999999999999\n",
      "    ram_util_percent: 69.06666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040725019035664924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.543956724197331\n",
      "    mean_inference_ms: 1.408497660921455\n",
      "    mean_raw_obs_processing_ms: 0.6990535547374159\n",
      "  time_since_restore: 5939.647265195847\n",
      "  time_this_iter_s: 10.664390087127686\n",
      "  time_total_s: 5939.647265195847\n",
      "  timers:\n",
      "    learn_throughput: 1647.823\n",
      "    learn_time_ms: 606.861\n",
      "    load_throughput: 296979.721\n",
      "    load_time_ms: 3.367\n",
      "    sample_throughput: 98.443\n",
      "    sample_time_ms: 10158.114\n",
      "    update_time_ms: 1.777\n",
      "  timestamp: 1632137229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525000\n",
      "  training_iteration: 525\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   525</td><td style=\"text-align: right;\">         5939.65</td><td style=\"text-align: right;\">525000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 526000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-27-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 528\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5339202576213413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018000248004423014\n",
      "          policy_loss: -0.08972483020689752\n",
      "          total_loss: -0.09665978004535039\n",
      "          vf_explained_var: 0.04083935543894768\n",
      "          vf_loss: 0.0014765404394387993\n",
      "    num_agent_steps_sampled: 526000\n",
      "    num_agent_steps_trained: 526000\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.7\n",
      "    ram_util_percent: 68.93125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040725488019057086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.543645155444809\n",
      "    mean_inference_ms: 1.4084947151165956\n",
      "    mean_raw_obs_processing_ms: 0.6989687960159742\n",
      "  time_since_restore: 5950.6068069934845\n",
      "  time_this_iter_s: 10.95954179763794\n",
      "  time_total_s: 5950.6068069934845\n",
      "  timers:\n",
      "    learn_throughput: 1652.962\n",
      "    learn_time_ms: 604.974\n",
      "    load_throughput: 296186.992\n",
      "    load_time_ms: 3.376\n",
      "    sample_throughput: 98.151\n",
      "    sample_time_ms: 10188.332\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632137240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 526\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   526</td><td style=\"text-align: right;\">         5950.61</td><td style=\"text-align: right;\">526000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 527000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-27-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 529\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4745606554879083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015339761293147644\n",
      "          policy_loss: -0.05443092265890704\n",
      "          total_loss: -0.06206417063044177\n",
      "          vf_explained_var: -0.2123265117406845\n",
      "          vf_loss: 0.0012085825521757619\n",
      "    num_agent_steps_sampled: 527000\n",
      "    num_agent_steps_trained: 527000\n",
      "    num_steps_sampled: 527000\n",
      "    num_steps_trained: 527000\n",
      "  iterations_since_restore: 527\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.319999999999986\n",
      "    ram_util_percent: 68.89333333333333\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040725947187799594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.543330387675514\n",
      "    mean_inference_ms: 1.4084915179738187\n",
      "    mean_raw_obs_processing_ms: 0.6988864927117763\n",
      "  time_since_restore: 5961.2152364254\n",
      "  time_this_iter_s: 10.608429431915283\n",
      "  time_total_s: 5961.2152364254\n",
      "  timers:\n",
      "    learn_throughput: 1642.509\n",
      "    learn_time_ms: 608.825\n",
      "    load_throughput: 295773.44\n",
      "    load_time_ms: 3.381\n",
      "    sample_throughput: 98.197\n",
      "    sample_time_ms: 10183.639\n",
      "    update_time_ms: 1.777\n",
      "  timestamp: 1632137250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527000\n",
      "  training_iteration: 527\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   527</td><td style=\"text-align: right;\">         5961.22</td><td style=\"text-align: right;\">527000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-27-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 530\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5981004556020102\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009952463145038198\n",
      "          policy_loss: 0.00018081259396341112\n",
      "          total_loss: -0.0108655855887466\n",
      "          vf_explained_var: -0.32038185000419617\n",
      "          vf_loss: 0.001104223489528522\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.3125\n",
      "    ram_util_percent: 69.075\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072643898076683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.543020579642045\n",
      "    mean_inference_ms: 1.4084887245245725\n",
      "    mean_raw_obs_processing_ms: 0.6988069612924954\n",
      "  time_since_restore: 5972.000737905502\n",
      "  time_this_iter_s: 10.785501480102539\n",
      "  time_total_s: 5972.000737905502\n",
      "  timers:\n",
      "    learn_throughput: 1642.802\n",
      "    learn_time_ms: 608.716\n",
      "    load_throughput: 295875.676\n",
      "    load_time_ms: 3.38\n",
      "    sample_throughput: 98.226\n",
      "    sample_time_ms: 10180.617\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632137261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 528\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   528</td><td style=\"text-align: right;\">            5972</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 529000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 531\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3423578103383382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01770146994912939\n",
      "          policy_loss: -0.03024194824198882\n",
      "          total_loss: -0.03605713757375876\n",
      "          vf_explained_var: -0.11908955127000809\n",
      "          vf_loss: 0.0007956639342915474\n",
      "    num_agent_steps_sampled: 529000\n",
      "    num_agent_steps_trained: 529000\n",
      "    num_steps_sampled: 529000\n",
      "    num_steps_trained: 529000\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.99333333333333\n",
      "    ram_util_percent: 68.79999999999998\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040726902669168424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.542711241494661\n",
      "    mean_inference_ms: 1.4084856045737686\n",
      "    mean_raw_obs_processing_ms: 0.6987312937185096\n",
      "  time_since_restore: 5982.782255411148\n",
      "  time_this_iter_s: 10.781517505645752\n",
      "  time_total_s: 5982.782255411148\n",
      "  timers:\n",
      "    learn_throughput: 1644.115\n",
      "    learn_time_ms: 608.23\n",
      "    load_throughput: 295617.093\n",
      "    load_time_ms: 3.383\n",
      "    sample_throughput: 98.446\n",
      "    sample_time_ms: 10157.848\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1632137272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 529000\n",
      "  training_iteration: 529\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   529</td><td style=\"text-align: right;\">         5982.78</td><td style=\"text-align: right;\">529000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 530000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-28-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 532\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3353896366225348\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007249203879901166\n",
      "          policy_loss: -0.076660807368656\n",
      "          total_loss: -0.08684381078928709\n",
      "          vf_explained_var: -0.8178730607032776\n",
      "          vf_loss: 0.00038090934774825453\n",
      "    num_agent_steps_sampled: 530000\n",
      "    num_agent_steps_trained: 530000\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.79375\n",
      "    ram_util_percent: 68.7625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072733083920221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.542404203357227\n",
      "    mean_inference_ms: 1.4084814158061512\n",
      "    mean_raw_obs_processing_ms: 0.6986597431557882\n",
      "  time_since_restore: 5993.749006986618\n",
      "  time_this_iter_s: 10.96675157546997\n",
      "  time_total_s: 5993.749006986618\n",
      "  timers:\n",
      "    learn_throughput: 1643.798\n",
      "    learn_time_ms: 608.347\n",
      "    load_throughput: 295750.499\n",
      "    load_time_ms: 3.381\n",
      "    sample_throughput: 98.509\n",
      "    sample_time_ms: 10151.394\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632137283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 530\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   530</td><td style=\"text-align: right;\">         5993.75</td><td style=\"text-align: right;\">530000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 531000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-28-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 533\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0717774000432756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013379570829434565\n",
      "          policy_loss: -0.1065781257632706\n",
      "          total_loss: -0.11131133023235533\n",
      "          vf_explained_var: -0.6532154679298401\n",
      "          vf_loss: 0.0008352039625202047\n",
      "    num_agent_steps_sampled: 531000\n",
      "    num_agent_steps_trained: 531000\n",
      "    num_steps_sampled: 531000\n",
      "    num_steps_trained: 531000\n",
      "  iterations_since_restore: 531\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.266666666666666\n",
      "    ram_util_percent: 68.72666666666669\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0407277344244395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.542102243331822\n",
      "    mean_inference_ms: 1.4084763821799908\n",
      "    mean_raw_obs_processing_ms: 0.6985902045552382\n",
      "  time_since_restore: 6004.629893064499\n",
      "  time_this_iter_s: 10.88088607788086\n",
      "  time_total_s: 6004.629893064499\n",
      "  timers:\n",
      "    learn_throughput: 1645.05\n",
      "    learn_time_ms: 607.884\n",
      "    load_throughput: 295167.735\n",
      "    load_time_ms: 3.388\n",
      "    sample_throughput: 97.954\n",
      "    sample_time_ms: 10208.841\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1632137294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 531000\n",
      "  training_iteration: 531\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   531</td><td style=\"text-align: right;\">         6004.63</td><td style=\"text-align: right;\">531000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 534\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.55059638288286\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01254039647907411\n",
      "          policy_loss: -0.08512485290153159\n",
      "          total_loss: -0.09424716946151522\n",
      "          vf_explained_var: -0.4438135027885437\n",
      "          vf_loss: 0.0015572522139538907\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.275\n",
      "    ram_util_percent: 68.74375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072810135012224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.541801173892628\n",
      "    mean_inference_ms: 1.4084703724304135\n",
      "    mean_raw_obs_processing_ms: 0.6985230352247658\n",
      "  time_since_restore: 6015.421199560165\n",
      "  time_this_iter_s: 10.791306495666504\n",
      "  time_total_s: 6015.421199560165\n",
      "  timers:\n",
      "    learn_throughput: 1645.533\n",
      "    learn_time_ms: 607.706\n",
      "    load_throughput: 295675.443\n",
      "    load_time_ms: 3.382\n",
      "    sample_throughput: 97.996\n",
      "    sample_time_ms: 10204.449\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632137305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 532\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   532</td><td style=\"text-align: right;\">         6015.42</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 533000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-28-35\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 535\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3561682899792988\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00790134076819804\n",
      "          policy_loss: -0.03384814324478309\n",
      "          total_loss: -0.04415994178917673\n",
      "          vf_explained_var: -0.6531915664672852\n",
      "          vf_loss: 0.00020891412625335053\n",
      "    num_agent_steps_sampled: 533000\n",
      "    num_agent_steps_trained: 533000\n",
      "    num_steps_sampled: 533000\n",
      "    num_steps_trained: 533000\n",
      "  iterations_since_restore: 533\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.98\n",
      "    ram_util_percent: 68.72000000000001\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040728457903499965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.541502916214242\n",
      "    mean_inference_ms: 1.4084642340285873\n",
      "    mean_raw_obs_processing_ms: 0.6984586379142097\n",
      "  time_since_restore: 6026.318160772324\n",
      "  time_this_iter_s: 10.896961212158203\n",
      "  time_total_s: 6026.318160772324\n",
      "  timers:\n",
      "    learn_throughput: 1650.336\n",
      "    learn_time_ms: 605.937\n",
      "    load_throughput: 300008.869\n",
      "    load_time_ms: 3.333\n",
      "    sample_throughput: 98.212\n",
      "    sample_time_ms: 10182.075\n",
      "    update_time_ms: 1.765\n",
      "  timestamp: 1632137315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 533000\n",
      "  training_iteration: 533\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   533</td><td style=\"text-align: right;\">         6026.32</td><td style=\"text-align: right;\">533000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 534000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-28-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 536\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1525895807478164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013862967629126387\n",
      "          policy_loss: 0.04672346959511439\n",
      "          total_loss: 0.031234707683324814\n",
      "          vf_explained_var: -0.2583144009113312\n",
      "          vf_loss: 0.0007017251558459571\n",
      "    num_agent_steps_sampled: 534000\n",
      "    num_agent_steps_trained: 534000\n",
      "    num_steps_sampled: 534000\n",
      "    num_steps_trained: 534000\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.24117647058823\n",
      "    ram_util_percent: 68.7\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040728767574820336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.54121563008177\n",
      "    mean_inference_ms: 1.4084576241915383\n",
      "    mean_raw_obs_processing_ms: 0.6983983261591399\n",
      "  time_since_restore: 6037.649675607681\n",
      "  time_this_iter_s: 11.331514835357666\n",
      "  time_total_s: 6037.649675607681\n",
      "  timers:\n",
      "    learn_throughput: 1652.915\n",
      "    learn_time_ms: 604.992\n",
      "    load_throughput: 300182.788\n",
      "    load_time_ms: 3.331\n",
      "    sample_throughput: 97.531\n",
      "    sample_time_ms: 10253.126\n",
      "    update_time_ms: 1.768\n",
      "  timestamp: 1632137327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 534000\n",
      "  training_iteration: 534\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   534</td><td style=\"text-align: right;\">         6037.65</td><td style=\"text-align: right;\">534000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 535000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-28-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 537\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3883166948954264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009581521111413282\n",
      "          policy_loss: -0.20479539293381904\n",
      "          total_loss: -0.21468784858783085\n",
      "          vf_explained_var: -0.4002222716808319\n",
      "          vf_loss: 0.0003030939081024068\n",
      "    num_agent_steps_sampled: 535000\n",
      "    num_agent_steps_trained: 535000\n",
      "    num_steps_sampled: 535000\n",
      "    num_steps_trained: 535000\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.73571428571429\n",
      "    ram_util_percent: 68.71428571428572\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072907784630081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.540924934683886\n",
      "    mean_inference_ms: 1.4084514339985332\n",
      "    mean_raw_obs_processing_ms: 0.6983403579400713\n",
      "  time_since_restore: 6047.723936080933\n",
      "  time_this_iter_s: 10.074260473251343\n",
      "  time_total_s: 6047.723936080933\n",
      "  timers:\n",
      "    learn_throughput: 1664.18\n",
      "    learn_time_ms: 600.897\n",
      "    load_throughput: 305892.34\n",
      "    load_time_ms: 3.269\n",
      "    sample_throughput: 98.056\n",
      "    sample_time_ms: 10198.3\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1632137337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535000\n",
      "  training_iteration: 535\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   535</td><td style=\"text-align: right;\">         6047.72</td><td style=\"text-align: right;\">535000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-29-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 538\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8852246562639872\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01186075918036305\n",
      "          policy_loss: -0.003050965360469288\n",
      "          total_loss: -0.01629844597644276\n",
      "          vf_explained_var: 0.5934047698974609\n",
      "          vf_loss: 0.0010399439588784137\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.41428571428571\n",
      "    ram_util_percent: 68.71428571428574\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072939243324217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.540620273573648\n",
      "    mean_inference_ms: 1.4084454840726017\n",
      "    mean_raw_obs_processing_ms: 0.698284719098394\n",
      "  time_since_restore: 6057.240473747253\n",
      "  time_this_iter_s: 9.5165376663208\n",
      "  time_total_s: 6057.240473747253\n",
      "  timers:\n",
      "    learn_throughput: 1667.889\n",
      "    learn_time_ms: 599.56\n",
      "    load_throughput: 311175.541\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 99.449\n",
      "    sample_time_ms: 10055.421\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1632137346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 536\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   536</td><td style=\"text-align: right;\">         6057.24</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 537000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-29-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 539\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9432046267721388\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015334062805443851\n",
      "          policy_loss: -0.014650430240564876\n",
      "          total_loss: -0.027267475343412822\n",
      "          vf_explained_var: -0.002612428506836295\n",
      "          vf_loss: 0.0009134139083067163\n",
      "    num_agent_steps_sampled: 537000\n",
      "    num_agent_steps_trained: 537000\n",
      "    num_steps_sampled: 537000\n",
      "    num_steps_trained: 537000\n",
      "  iterations_since_restore: 537\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.64285714285713\n",
      "    ram_util_percent: 68.70000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072970915178952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.540310969600188\n",
      "    mean_inference_ms: 1.4084398199748487\n",
      "    mean_raw_obs_processing_ms: 0.6982332615686059\n",
      "  time_since_restore: 6067.661032915115\n",
      "  time_this_iter_s: 10.420559167861938\n",
      "  time_total_s: 6067.661032915115\n",
      "  timers:\n",
      "    learn_throughput: 1678.715\n",
      "    learn_time_ms: 595.694\n",
      "    load_throughput: 311619.427\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 99.597\n",
      "    sample_time_ms: 10040.507\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1632137357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 537000\n",
      "  training_iteration: 537\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   537</td><td style=\"text-align: right;\">         6067.66</td><td style=\"text-align: right;\">537000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=526116)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 538000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-29-46\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 540\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8117682602670457\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016696501953821594\n",
      "          policy_loss: -0.12942402909199396\n",
      "          total_loss: -0.13890847778982587\n",
      "          vf_explained_var: 0.1763351410627365\n",
      "          vf_loss: 0.002207291848672968\n",
      "    num_agent_steps_sampled: 538000\n",
      "    num_agent_steps_trained: 538000\n",
      "    num_steps_sampled: 538000\n",
      "    num_steps_trained: 538000\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.80714285714286\n",
      "    ram_util_percent: 68.69999999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040730028145064934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.540002014743132\n",
      "    mean_inference_ms: 1.4084345331871377\n",
      "    mean_raw_obs_processing_ms: 0.6985251659343578\n",
      "  time_since_restore: 6096.4696497917175\n",
      "  time_this_iter_s: 28.808616876602173\n",
      "  time_total_s: 6096.4696497917175\n",
      "  timers:\n",
      "    learn_throughput: 1678.118\n",
      "    learn_time_ms: 595.906\n",
      "    load_throughput: 208087.952\n",
      "    load_time_ms: 4.806\n",
      "    sample_throughput: 84.453\n",
      "    sample_time_ms: 11840.971\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632137386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 538000\n",
      "  training_iteration: 538\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   538</td><td style=\"text-align: right;\">         6096.47</td><td style=\"text-align: right;\">538000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 539000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-29-56\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 541\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7121215674612258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01594240503812981\n",
      "          policy_loss: 0.09455647990107537\n",
      "          total_loss: 0.08419500736312734\n",
      "          vf_explained_var: -0.090845987200737\n",
      "          vf_loss: 0.0006240287202268115\n",
      "    num_agent_steps_sampled: 539000\n",
      "    num_agent_steps_trained: 539000\n",
      "    num_steps_sampled: 539000\n",
      "    num_steps_trained: 539000\n",
      "  iterations_since_restore: 539\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.89333333333334\n",
      "    ram_util_percent: 68.64666666666668\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073033795389388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.539691895882878\n",
      "    mean_inference_ms: 1.4084291390331172\n",
      "    mean_raw_obs_processing_ms: 0.6988197594437245\n",
      "  time_since_restore: 6107.069202423096\n",
      "  time_this_iter_s: 10.599552631378174\n",
      "  time_total_s: 6107.069202423096\n",
      "  timers:\n",
      "    learn_throughput: 1677.428\n",
      "    learn_time_ms: 596.151\n",
      "    load_throughput: 208230.516\n",
      "    load_time_ms: 4.802\n",
      "    sample_throughput: 84.584\n",
      "    sample_time_ms: 11822.556\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1632137396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 539000\n",
      "  training_iteration: 539\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   539</td><td style=\"text-align: right;\">         6107.07</td><td style=\"text-align: right;\">539000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 542\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.70541553762224\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01719036450364027\n",
      "          policy_loss: -0.05289343570669492\n",
      "          total_loss: -0.06265258313053185\n",
      "          vf_explained_var: -0.13467468321323395\n",
      "          vf_loss: 0.0006789936207092575\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.59333333333335\n",
      "    ram_util_percent: 68.6\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040730644044118504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.53938540367228\n",
      "    mean_inference_ms: 1.4084239266545702\n",
      "    mean_raw_obs_processing_ms: 0.6991168493145788\n",
      "  time_since_restore: 6117.7672662734985\n",
      "  time_this_iter_s: 10.698063850402832\n",
      "  time_total_s: 6117.7672662734985\n",
      "  timers:\n",
      "    learn_throughput: 1679.555\n",
      "    learn_time_ms: 595.396\n",
      "    load_throughput: 208111.699\n",
      "    load_time_ms: 4.805\n",
      "    sample_throughput: 84.771\n",
      "    sample_time_ms: 11796.452\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1632137407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 540\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   540</td><td style=\"text-align: right;\">         6117.77</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 541000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-30-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 543\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.816832889450921\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012853045966572947\n",
      "          policy_loss: -0.03327410477730963\n",
      "          total_loss: -0.04567488647169537\n",
      "          vf_explained_var: -0.5312064290046692\n",
      "          vf_loss: 0.0008208250762739529\n",
      "    num_agent_steps_sampled: 541000\n",
      "    num_agent_steps_trained: 541000\n",
      "    num_steps_sampled: 541000\n",
      "    num_steps_trained: 541000\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.50666666666667\n",
      "    ram_util_percent: 68.57333333333332\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040730867235352075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.539075181601563\n",
      "    mean_inference_ms: 1.4084171395378209\n",
      "    mean_raw_obs_processing_ms: 0.6994158067792445\n",
      "  time_since_restore: 6128.246381521225\n",
      "  time_this_iter_s: 10.47911524772644\n",
      "  time_total_s: 6128.246381521225\n",
      "  timers:\n",
      "    learn_throughput: 1680.08\n",
      "    learn_time_ms: 595.21\n",
      "    load_throughput: 208561.853\n",
      "    load_time_ms: 4.795\n",
      "    sample_throughput: 85.059\n",
      "    sample_time_ms: 11756.491\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632137418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 541000\n",
      "  training_iteration: 541\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   541</td><td style=\"text-align: right;\">         6128.25</td><td style=\"text-align: right;\">541000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 542000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-30-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 544\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9807796968354119\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018594786426491854\n",
      "          policy_loss: -0.04231199448307355\n",
      "          total_loss: -0.054382823490434225\n",
      "          vf_explained_var: -0.0824044719338417\n",
      "          vf_loss: 0.0005804346437798813\n",
      "    num_agent_steps_sampled: 542000\n",
      "    num_agent_steps_trained: 542000\n",
      "    num_steps_sampled: 542000\n",
      "    num_steps_trained: 542000\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.720000000000006\n",
      "    ram_util_percent: 68.48666666666666\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073103820839326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.538759343982033\n",
      "    mean_inference_ms: 1.4084090410736123\n",
      "    mean_raw_obs_processing_ms: 0.699716769782577\n",
      "  time_since_restore: 6138.790894508362\n",
      "  time_this_iter_s: 10.54451298713684\n",
      "  time_total_s: 6138.790894508362\n",
      "  timers:\n",
      "    learn_throughput: 1681.99\n",
      "    learn_time_ms: 594.534\n",
      "    load_throughput: 208510.012\n",
      "    load_time_ms: 4.796\n",
      "    sample_throughput: 85.233\n",
      "    sample_time_ms: 11732.498\n",
      "    update_time_ms: 1.779\n",
      "  timestamp: 1632137428\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 542000\n",
      "  training_iteration: 542\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   542</td><td style=\"text-align: right;\">         6138.79</td><td style=\"text-align: right;\">542000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 543000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-30-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 545\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7523143543137445\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012417981819279679\n",
      "          policy_loss: -0.06603674126995934\n",
      "          total_loss: -0.07839805380337768\n",
      "          vf_explained_var: -0.85426265001297\n",
      "          vf_loss: 0.00038254941812030865\n",
      "    num_agent_steps_sampled: 543000\n",
      "    num_agent_steps_trained: 543000\n",
      "    num_steps_sampled: 543000\n",
      "    num_steps_trained: 543000\n",
      "  iterations_since_restore: 543\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.0375\n",
      "    ram_util_percent: 68.51875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040731186609413966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.538449953660889\n",
      "    mean_inference_ms: 1.408400870446063\n",
      "    mean_raw_obs_processing_ms: 0.7000208212140429\n",
      "  time_since_restore: 6150.052051305771\n",
      "  time_this_iter_s: 11.261156797409058\n",
      "  time_total_s: 6150.052051305771\n",
      "  timers:\n",
      "    learn_throughput: 1674.581\n",
      "    learn_time_ms: 597.164\n",
      "    load_throughput: 208138.55\n",
      "    load_time_ms: 4.804\n",
      "    sample_throughput: 84.989\n",
      "    sample_time_ms: 11766.278\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1632137439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543000\n",
      "  training_iteration: 543\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   543</td><td style=\"text-align: right;\">         6150.05</td><td style=\"text-align: right;\">543000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-30-51\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 546\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7095468468136257\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01041904969195771\n",
      "          policy_loss: 0.053435915377404955\n",
      "          total_loss: 0.0407144460413191\n",
      "          vf_explained_var: -0.892215371131897\n",
      "          vf_loss: 0.00036404217567501796\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.5125\n",
      "    ram_util_percent: 68.3125\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040731249238056294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.538132527774877\n",
      "    mean_inference_ms: 1.4083904261393208\n",
      "    mean_raw_obs_processing_ms: 0.7003273873262559\n",
      "  time_since_restore: 6161.133345842361\n",
      "  time_this_iter_s: 11.081294536590576\n",
      "  time_total_s: 6161.133345842361\n",
      "  timers:\n",
      "    learn_throughput: 1675.341\n",
      "    learn_time_ms: 596.894\n",
      "    load_throughput: 208181.94\n",
      "    load_time_ms: 4.803\n",
      "    sample_throughput: 85.168\n",
      "    sample_time_ms: 11741.522\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632137451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 544\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   544</td><td style=\"text-align: right;\">         6161.13</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 545000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-31-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 547\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8644013656510248\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014029563247372343\n",
      "          policy_loss: -0.059861661700738804\n",
      "          total_loss: -0.07238709090484513\n",
      "          vf_explained_var: -0.5994453430175781\n",
      "          vf_loss: 0.0007190589919143046\n",
      "    num_agent_steps_sampled: 545000\n",
      "    num_agent_steps_trained: 545000\n",
      "    num_steps_sampled: 545000\n",
      "    num_steps_trained: 545000\n",
      "  iterations_since_restore: 545\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.71764705882353\n",
      "    ram_util_percent: 68.65882352941176\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073130035321751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.537824941007644\n",
      "    mean_inference_ms: 1.408379933205393\n",
      "    mean_raw_obs_processing_ms: 0.7006352891939862\n",
      "  time_since_restore: 6172.66921710968\n",
      "  time_this_iter_s: 11.535871267318726\n",
      "  time_total_s: 6172.66921710968\n",
      "  timers:\n",
      "    learn_throughput: 1664.863\n",
      "    learn_time_ms: 600.65\n",
      "    load_throughput: 207476.528\n",
      "    load_time_ms: 4.82\n",
      "    sample_throughput: 84.147\n",
      "    sample_time_ms: 11883.948\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632137462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 545000\n",
      "  training_iteration: 545\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   545</td><td style=\"text-align: right;\">         6172.67</td><td style=\"text-align: right;\">545000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 546000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-31-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 548\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.001439356803894\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012867014605734677\n",
      "          policy_loss: -0.06792082277437052\n",
      "          total_loss: -0.08242847683529059\n",
      "          vf_explained_var: -0.8493210077285767\n",
      "          vf_loss: 0.0005546420063991617\n",
      "    num_agent_steps_sampled: 546000\n",
      "    num_agent_steps_trained: 546000\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.51875\n",
      "    ram_util_percent: 68.99375\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073136070129811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.537525368289748\n",
      "    mean_inference_ms: 1.4083698817149506\n",
      "    mean_raw_obs_processing_ms: 0.7009451829292442\n",
      "  time_since_restore: 6183.89541220665\n",
      "  time_this_iter_s: 11.226195096969604\n",
      "  time_total_s: 6183.89541220665\n",
      "  timers:\n",
      "    learn_throughput: 1665.674\n",
      "    learn_time_ms: 600.358\n",
      "    load_throughput: 206707.604\n",
      "    load_time_ms: 4.838\n",
      "    sample_throughput: 82.952\n",
      "    sample_time_ms: 12055.194\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1632137473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 546\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   546</td><td style=\"text-align: right;\">          6183.9</td><td style=\"text-align: right;\">546000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 547000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-31-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 549\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0689183712005614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013996529521300694\n",
      "          policy_loss: -0.09842942961388164\n",
      "          total_loss: -0.11325607423981031\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00047572444390324463\n",
      "    num_agent_steps_sampled: 547000\n",
      "    num_agent_steps_trained: 547000\n",
      "    num_steps_sampled: 547000\n",
      "    num_steps_trained: 547000\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.2625\n",
      "    ram_util_percent: 68.86875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073138508536401\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.53722866275422\n",
      "    mean_inference_ms: 1.4083585630632347\n",
      "    mean_raw_obs_processing_ms: 0.7008677380036709\n",
      "  time_since_restore: 6195.045327425003\n",
      "  time_this_iter_s: 11.149915218353271\n",
      "  time_total_s: 6195.045327425003\n",
      "  timers:\n",
      "    learn_throughput: 1664.963\n",
      "    learn_time_ms: 600.614\n",
      "    load_throughput: 206792.192\n",
      "    load_time_ms: 4.836\n",
      "    sample_throughput: 82.455\n",
      "    sample_time_ms: 12127.853\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632137485\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 547000\n",
      "  training_iteration: 547\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   547</td><td style=\"text-align: right;\">         6195.05</td><td style=\"text-align: right;\">547000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-31-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 550\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7245934353934393\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008449295139585308\n",
      "          policy_loss: -0.04700585572669903\n",
      "          total_loss: -0.06078195008966658\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00021797882412405062\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.6875\n",
      "    ram_util_percent: 68.95\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073140637076282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.53693948872932\n",
      "    mean_inference_ms: 1.4083472134845325\n",
      "    mean_raw_obs_processing_ms: 0.7007803730830746\n",
      "  time_since_restore: 6206.235693216324\n",
      "  time_this_iter_s: 11.1903657913208\n",
      "  time_total_s: 6206.235693216324\n",
      "  timers:\n",
      "    learn_throughput: 1664.745\n",
      "    learn_time_ms: 600.693\n",
      "    load_throughput: 309455.946\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 96.454\n",
      "    sample_time_ms: 10367.62\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632137496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 548\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   548</td><td style=\"text-align: right;\">         6206.24</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 549000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-31-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 551\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6169497556156582\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020367810172768695\n",
      "          policy_loss: 0.06054703820910719\n",
      "          total_loss: 0.05323007247514195\n",
      "          vf_explained_var: -0.5220958590507507\n",
      "          vf_loss: 0.0010136188509932253\n",
      "    num_agent_steps_sampled: 549000\n",
      "    num_agent_steps_trained: 549000\n",
      "    num_steps_sampled: 549000\n",
      "    num_steps_trained: 549000\n",
      "  iterations_since_restore: 549\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.337500000000006\n",
      "    ram_util_percent: 69.10624999999999\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073142301027573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.536631140770346\n",
      "    mean_inference_ms: 1.408336044578324\n",
      "    mean_raw_obs_processing_ms: 0.7006965495989529\n",
      "  time_since_restore: 6217.476287841797\n",
      "  time_this_iter_s: 11.240594625473022\n",
      "  time_total_s: 6217.476287841797\n",
      "  timers:\n",
      "    learn_throughput: 1664.77\n",
      "    learn_time_ms: 600.684\n",
      "    load_throughput: 309077.404\n",
      "    load_time_ms: 3.235\n",
      "    sample_throughput: 95.862\n",
      "    sample_time_ms: 10431.71\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632137507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 549000\n",
      "  training_iteration: 549\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   549</td><td style=\"text-align: right;\">         6217.48</td><td style=\"text-align: right;\">549000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 550000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-31-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 552\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7242072529262966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014292497219541716\n",
      "          policy_loss: -0.020090293553140428\n",
      "          total_loss: -0.01824284733997451\n",
      "          vf_explained_var: -0.377866268157959\n",
      "          vf_loss: 0.010838438687561494\n",
      "    num_agent_steps_sampled: 550000\n",
      "    num_agent_steps_trained: 550000\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.53333333333333\n",
      "    ram_util_percent: 69.12000000000002\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073143642704684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.5363356008387\n",
      "    mean_inference_ms: 1.408324898724112\n",
      "    mean_raw_obs_processing_ms: 0.7006154650294327\n",
      "  time_since_restore: 6228.150493144989\n",
      "  time_this_iter_s: 10.674205303192139\n",
      "  time_total_s: 6228.150493144989\n",
      "  timers:\n",
      "    learn_throughput: 1662.882\n",
      "    learn_time_ms: 601.366\n",
      "    load_throughput: 310025.501\n",
      "    load_time_ms: 3.226\n",
      "    sample_throughput: 95.889\n",
      "    sample_time_ms: 10428.674\n",
      "    update_time_ms: 1.772\n",
      "  timestamp: 1632137518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 550\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   550</td><td style=\"text-align: right;\">         6228.15</td><td style=\"text-align: right;\">550000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 551000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-32-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 553\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7081737160682677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007168301515776084\n",
      "          policy_loss: -0.011031638541155391\n",
      "          total_loss: -0.023730139765474532\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0002449624819241257\n",
      "    num_agent_steps_sampled: 551000\n",
      "    num_agent_steps_trained: 551000\n",
      "    num_steps_sampled: 551000\n",
      "    num_steps_trained: 551000\n",
      "  iterations_since_restore: 551\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.506249999999994\n",
      "    ram_util_percent: 68.9625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073145270038953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.536055853604301\n",
      "    mean_inference_ms: 1.4083141951464375\n",
      "    mean_raw_obs_processing_ms: 0.7005377442521834\n",
      "  time_since_restore: 6239.296886444092\n",
      "  time_this_iter_s: 11.146393299102783\n",
      "  time_total_s: 6239.296886444092\n",
      "  timers:\n",
      "    learn_throughput: 1646.808\n",
      "    learn_time_ms: 607.235\n",
      "    load_throughput: 307746.219\n",
      "    load_time_ms: 3.249\n",
      "    sample_throughput: 95.334\n",
      "    sample_time_ms: 10489.473\n",
      "    update_time_ms: 1.779\n",
      "  timestamp: 1632137529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551000\n",
      "  training_iteration: 551\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   551</td><td style=\"text-align: right;\">          6239.3</td><td style=\"text-align: right;\">551000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-32-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 554\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8400734146436055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005452265554947012\n",
      "          policy_loss: 0.01879281848669052\n",
      "          total_loss: 0.006752095537053214\n",
      "          vf_explained_var: -0.4926300048828125\n",
      "          vf_loss: 0.003212409313305721\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.45\n",
      "    ram_util_percent: 69.00625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040731484372322706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.535798788192393\n",
      "    mean_inference_ms: 1.408304224843178\n",
      "    mean_raw_obs_processing_ms: 0.7004632754033018\n",
      "  time_since_restore: 6250.669238805771\n",
      "  time_this_iter_s: 11.372352361679077\n",
      "  time_total_s: 6250.669238805771\n",
      "  timers:\n",
      "    learn_throughput: 1644.717\n",
      "    learn_time_ms: 608.007\n",
      "    load_throughput: 307807.198\n",
      "    load_time_ms: 3.249\n",
      "    sample_throughput: 94.594\n",
      "    sample_time_ms: 10571.483\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1632137540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 552\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   552</td><td style=\"text-align: right;\">         6250.67</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 553000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-32-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 555\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4225848568810358\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018803135346409483\n",
      "          policy_loss: -0.0970512545771069\n",
      "          total_loss: -0.09857669414745437\n",
      "          vf_explained_var: -0.11555467545986176\n",
      "          vf_loss: 0.0018453323802936615\n",
      "    num_agent_steps_sampled: 553000\n",
      "    num_agent_steps_trained: 553000\n",
      "    num_steps_sampled: 553000\n",
      "    num_steps_trained: 553000\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.06666666666667\n",
      "    ram_util_percent: 68.94\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040731515229638984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.535539948534323\n",
      "    mean_inference_ms: 1.408294304046766\n",
      "    mean_raw_obs_processing_ms: 0.7003913612098894\n",
      "  time_since_restore: 6260.980634689331\n",
      "  time_this_iter_s: 10.31139588356018\n",
      "  time_total_s: 6260.980634689331\n",
      "  timers:\n",
      "    learn_throughput: 1639.745\n",
      "    learn_time_ms: 609.851\n",
      "    load_throughput: 304767.662\n",
      "    load_time_ms: 3.281\n",
      "    sample_throughput: 95.469\n",
      "    sample_time_ms: 10474.613\n",
      "    update_time_ms: 1.777\n",
      "  timestamp: 1632137551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 553000\n",
      "  training_iteration: 553\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   553</td><td style=\"text-align: right;\">         6260.98</td><td style=\"text-align: right;\">553000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 554000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 556\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8564880079693264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0091101337603472\n",
      "          policy_loss: -0.004475519350833363\n",
      "          total_loss: -0.01757411157919301\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0002069935872795112\n",
      "    num_agent_steps_sampled: 554000\n",
      "    num_agent_steps_trained: 554000\n",
      "    num_steps_sampled: 554000\n",
      "    num_steps_trained: 554000\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.825\n",
      "    ram_util_percent: 68.9625\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040731549661640686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.535294784126041\n",
      "    mean_inference_ms: 1.4082848110088946\n",
      "    mean_raw_obs_processing_ms: 0.7003229726852835\n",
      "  time_since_restore: 6272.107392311096\n",
      "  time_this_iter_s: 11.126757621765137\n",
      "  time_total_s: 6272.107392311096\n",
      "  timers:\n",
      "    learn_throughput: 1637.953\n",
      "    learn_time_ms: 610.518\n",
      "    load_throughput: 305200.105\n",
      "    load_time_ms: 3.277\n",
      "    sample_throughput: 95.433\n",
      "    sample_time_ms: 10478.545\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1632137562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 554000\n",
      "  training_iteration: 554\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   554</td><td style=\"text-align: right;\">         6272.11</td><td style=\"text-align: right;\">554000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_cede2_00000:\n",
      "  agent_timesteps_total: 555000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_11-32-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 557\n",
      "  experiment_id: 76801b529e5a458f812cf4d6323518b3\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.871549579832289\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007472600026034115\n",
      "          policy_loss: -0.032107516336772175\n",
      "          total_loss: -0.04619272156722016\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00031634633439049745\n",
      "    num_agent_steps_sampled: 555000\n",
      "    num_agent_steps_trained: 555000\n",
      "    num_steps_sampled: 555000\n",
      "    num_steps_trained: 555000\n",
      "  iterations_since_restore: 555\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.85625\n",
      "    ram_util_percent: 68.9875\n",
      "  pid: 526121\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073158903902068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.535058708246384\n",
      "    mean_inference_ms: 1.4082756616756007\n",
      "    mean_raw_obs_processing_ms: 0.7002584057385951\n",
      "  time_since_restore: 6283.158339262009\n",
      "  time_this_iter_s: 11.050946950912476\n",
      "  time_total_s: 6283.158339262009\n",
      "  timers:\n",
      "    learn_throughput: 1644.967\n",
      "    learn_time_ms: 607.915\n",
      "    load_throughput: 306254.171\n",
      "    load_time_ms: 3.265\n",
      "    sample_throughput: 95.853\n",
      "    sample_time_ms: 10432.66\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1632137573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555000\n",
      "  training_iteration: 555\n",
      "  trial_id: cede2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.37 GiB heap, 0.0/3.18 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_09-47-45<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_cede2_00000</td><td>RUNNING </td><td>192.168.1.100:526121</td><td style=\"text-align: right;\">   555</td><td style=\"text-align: right;\">         6283.16</td><td style=\"text-align: right;\">555000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C17 pretrained (AnnaCNN)\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
